<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-09T00:00:00Z">2024-10-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge
  Conflicts for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do better language models have crisper vision? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How well do text-only Large Language Models (LLMs) grasp the visual world? As
LLMs are increasingly used in computer vision, addressing this question becomes
both fundamental and pertinent. However, existing studies have primarily
focused on limited scenarios, such as their ability to generate visual content
or cluster multimodal data. To this end, we propose the Visual Text
Representation Benchmark (ViTeRB) to isolate key properties that make language
models well-aligned with the visual world. With this, we identify large-scale
decoder-based LLMs as ideal candidates for representing text in vision-centric
contexts, counter to the current practice of utilizing text encoders. Building
on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.
By leveraging precomputable frozen features from strong vision and language
models, ShareLock achieves an impressive 51% accuracy on ImageNet despite
utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU
hour (or 10 hours including the precomputation of features) - orders of
magnitude less than prior methods. Code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Initialization to Rule them All: Fine-tuning via Explained Variance
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are pre-trained on large-scale datasets and then
fine-tuned on a downstream task for a specific application. The most successful
and most commonly used fine-tuning method is to update the pre-trained weights
via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are
usually initialized at random with a uniform rank distribution across model
weights. Recent works focus on weight-driven initialization or learning of
adaptive ranks during training. Both approaches have only been investigated in
isolation, resulting in slow convergence or a uniform rank distribution, in
turn leading to sub-optimal performance. We propose to enhance LoRA by
initializing the new weights in a data-driven manner by computing singular
value decomposition on minibatches of activation vectors. Then, we initialize
the LoRA matrices with the obtained right-singular vectors and re-distribute
ranks among all weight matrices to explain the maximal amount of variance and
continue the standard LoRA fine-tuning procedure. This results in our new
method Explained Variance Adaptation (EVA). We apply EVA to a variety of
fine-tuning tasks ranging from language generation and understanding to image
classification and reinforcement learning. EVA exhibits faster convergence than
competitors and attains the highest average score across a multitude of tasks
per domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages + references and appendix, code available at
  https://github.com/ml-jku/EVA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering Cross-Modal Alignment in Large <span class="highlight-title">Vision-Language Model</span>s with
  Modality Integration Rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Modality Integration Rate (MIR), an effective, robust, and
generalized metric to indicate the multi-modal pre-training quality of Large
Vision Language Models (LVLMs). Large-scale pre-training plays a critical role
in building capable LVLMs, while evaluating its training quality without the
costly supervised fine-tuning stage is under-explored. Loss, perplexity, and
in-context evaluation results are commonly used pre-training metrics for Large
Language Models (LLMs), while we observed that these metrics are less
indicative when aligning a well-trained LLM with a new modality. Due to the
lack of proper metrics, the research of LVLMs in the critical pre-training
stage is hindered greatly, including the training data choice, efficient module
design, etc. In this paper, we propose evaluating the pre-training quality from
the inter-modal distribution distance perspective and present MIR, the Modality
Integration Rate, which is 1) \textbf{Effective} to represent the pre-training
quality and show a positive relation with the benchmark performance after
supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation
data. 3) \textbf{Generalize} across training configurations and architecture
choices. We conduct a series of pre-training experiments to explore the
effectiveness of MIR and observe satisfactory results that MIR is indicative
about training data selection, training strategy schedule, and model
architecture design to get better pre-training results. We hope MIR could be a
helpful metric for building capable LVLMs and inspire the following research
about modality alignment in different areas. Our code is at:
https://github.com/shikiw/Modality-Integration-Rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/shikiw/Modality-Integration-Rate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sylber: Syllabic Embedding Representation of Speech from Raw Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W Black, Gopala K. Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Syllables are compositional units of spoken language that play a crucial role
in human speech perception and production. However, current neural speech
representations lack structure, resulting in dense token sequences that are
costly to process. To bridge this gap, we propose a new model, Sylber, that
produces speech representations with clean and robust syllabic structure.
Specifically, we propose a self-supervised model that regresses features on
syllabic segments distilled from a teacher model which is an exponential moving
average of the model in training. This results in a highly structured
representation of speech features, offering three key benefits: 1) a fast,
linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization
with an average of 4.27 tokens per second, and 3) syllabic units better suited
for lexical and syntactic understanding. We also train token-to-speech
generative models with our syllabic units and show that fully intelligible
speech can be reconstructed from these tokens. Lastly, we observe that
categorical perception, a linguistic phenomenon of speech perception, emerges
naturally in our model, making the embedding space more categorical and sparse
than previous self-supervised learning approaches. Together, we present a novel
self-supervised approach for representing speech as syllables, with significant
potential for efficient speech tokenization and spoken language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied <span class="highlight-title">Agent</span> Interface: <span class="highlight-title">Benchmark</span>ing <span class="highlight-title">LLM</span>s for Embodied Decision Making <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to evaluate Large Language Models (LLMs) for embodied decision making.
While a significant body of work has been leveraging LLMs for decision making
in embodied environments, we still lack a systematic understanding of their
performance because they are usually applied in different domains, for
different purposes, and built based on different inputs and outputs.
Furthermore, existing evaluations tend to rely solely on a final success rate,
making it difficult to pinpoint what ability is missing in LLMs and where the
problem lies, which in turn blocks embodied agents from leveraging LLMs
effectively and selectively. To address these limitations, we propose a
generalized interface (Embodied Agent Interface) that supports the
formalization of various types of tasks and input-output specifications of
LLM-based modules. Specifically, it allows us to unify 1) a broad set of
embodied decision-making tasks involving both state and temporally extended
goals, 2) four commonly-used LLM-based modules for decision making: goal
interpretation, subgoal decomposition, action sequencing, and transition
modeling, and 3) a collection of fine-grained metrics which break down
evaluation into various types of errors, such as hallucination errors,
affordance errors, various types of planning errors, etc. Overall, our
benchmark offers a comprehensive assessment of LLMs' performance for different
subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI
systems, and providing insights for effective and selective use of LLMs in
embodied decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at NeurIPS 2024 in the Datasets and
  Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Simplicity Prevails: Rethinking Negative Preference Optimization for <span class="highlight-title">LLM</span>
  Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Rui<span class="highlight-author">qi Zhang</span>, Song Mei, Sijia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the problem of large language model (LLM)
unlearning, aiming to remove unwanted data influences and associated model
capabilities (e.g., copyrighted data or harmful content generation) while
preserving essential model utilities, without the need for retraining from
scratch. Despite the growing need for LLM unlearning, a principled optimization
framework remains lacking. To this end, we revisit the state-of-the-art
approach, negative preference optimization (NPO), and identify the issue of
reference model bias, which could undermine NPO's effectiveness, particularly
when unlearning forget data of varying difficulty. Given that, we propose a
simple yet effective unlearning optimization framework, called SimNPO, showing
that 'simplicity' in removing the reliance on a reference model (through the
lens of simple preference optimization) benefits unlearning. We also provide
deeper insights into SimNPO's advantages, supported by analysis using mixtures
of Markov chains. Furthermore, we present extensive experiments validating
SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU
and MUSE, and robustness against relearning attacks. Codes are available at
https://github.com/OPTML-Group/Unlearn-Simple.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructG2I: Synthesizing <span class="highlight-title">Image</span>s from <span class="highlight-title">Multimodal</span> Attributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we approach an overlooked yet critical task Graph2Image:
generating images from multimodal attributed graphs (MMAGs). This task poses
significant challenges due to the explosion in graph size, dependencies among
graph entities, and the need for controllability in graph conditions. To
address these challenges, we propose a graph context-conditioned diffusion
model called InstructG2I. InstructG2I first exploits the graph structure and
multimodal information to conduct informative neighbor sampling by combining
personalized page rank and re-ranking based on vision-language features. Then,
a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary
set of graph prompts to guide the denoising process of diffusion. Finally, we
propose graph classifier-free guidance, enabling controllable generation by
varying the strength of graph guidance and multiple connected edges to a node.
Extensive experiments conducted on three datasets from different domains
demonstrate the effectiveness and controllability of our approach. The code is
available at https://github.com/PeterGriffinJin/InstructG2I.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taking a turn for the better: Conversation redirection throughout the
  course of mental-health therapy <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivian Nguyen, Sang Min Jung, Lillian Lee, Thomas D. Hull, Cristian Danescu-Niculescu-Mizil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental-health therapy involves a complex conversation flow in which patients
and therapists continuously negotiate what should be talked about next. For
example, therapists might try to shift the conversation's direction to keep the
therapeutic process on track and avoid stagnation, or patients might push the
discussion towards issues they want to focus on.
  How do such patient and therapist redirections relate to the development and
quality of their relationship? To answer this question, we introduce a
probabilistic measure of the extent to which a certain utterance immediately
redirects the flow of the conversation, accounting for both the intention and
the actual realization of such a change. We apply this new measure to
characterize the development of patient-therapist relationships over multiple
sessions in a very large, widely-used online therapy platform. Our analysis
reveals that (1) patient control of the conversation's direction generally
increases relative to that of the therapist as their relationship progresses;
and (2) patients who have less control in the first few sessions are
significantly more likely to eventually express dissatisfaction with their
therapist and terminate the relationship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of EMNLP (Findings) 2024. Code available
  at https://convokit.cornell.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Stuffed Mamba: State Collapse and State Capacity of RNN-Based
  Long-Context Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, <span class="highlight-author">Zhiyuan Liu</span>, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One essential advantage of recurrent neural networks (RNNs) over
transformer-based language models is their linear computational complexity
concerning the sequence length, which makes them much faster in handling long
sequences during inference. However, most publicly available RNNs (e.g., Mamba
and RWKV) are trained on sequences with less than 10K tokens, and their
effectiveness in longer contexts remains largely unsatisfying so far. In this
paper, we study the cause of the inability to process long context for RNNs and
suggest critical mitigations. We examine two practical concerns when applying
state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to
inputs longer than the training length and (2) the upper bound of memory
capacity. Addressing the first concern, we first investigate *state collapse*
(SC), a phenomenon that causes severe performance degradation on sequence
lengths not encountered during training. With controlled experiments, we
attribute this to overfitting due to the recurrent state being
overparameterized for the training length. For the second concern, we train a
series of Mamba-2 models on long documents to empirically estimate the
recurrent state capacity in language modeling and passkey retrieval. Then,
three SC mitigation methods are proposed to improve Mamba-2's length
generalizability, allowing the model to process more than 1M tokens without SC.
We also find that the recurrent state capacity in passkey retrieval scales
exponentially to the state size, and we empirically train a Mamba-2 370M with
near-perfect passkey retrieval accuracy on 256K context length. This suggests a
promising future for RNN-based long-context modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cheating Automatic <span class="highlight-title">LLM</span> <span class="highlight-title">Benchmark</span>s: Null Models Achieve High Win Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and
MT-Bench, have become popular for evaluating language models due to their
cost-effectiveness and scalability compared to human evaluation. Achieving high
win rates on these benchmarks can significantly boost the promotional impact of
newly released language models. This promotional benefit may motivate tricks,
such as manipulating model output length or style to game win rates, even
though several mechanisms have been developed to control length and disentangle
style to reduce gameability. Nonetheless, we show that even a "null model" that
always outputs a constant response (irrelevant to input instructions) can cheat
automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on
AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.
Moreover, the crafted cheating outputs are transferable because we assume that
the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are
private and cannot be accessed. While our experiments are primarily
proof-of-concept, an adversary could use LLMs to generate more imperceptible
cheating responses, unethically benefiting from high win rates and promotional
impact. Our findings call for the development of anti-cheating mechanisms for
reliable automatic benchmarks. The code is available at
https://github.com/sail-sg/Cheating-LLM-Benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mental Disorders Detection in the Era of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Ivan Smirnov, Artem Shelmanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper compares the effectiveness of traditional machine learning
methods, encoder-based models, and large language models (LLMs) on the task of
detecting depression and anxiety. Five datasets were considered, each differing
in format and the method used to define the target pathology class. We tested
AutoML models based on linguistic features, several variations of encoder-based
Transformers such as BERT, and state-of-the-art LLMs as pathology
classification models. The results demonstrated that LLMs outperform
traditional methods, particularly on noisy and small datasets where training
examples vary significantly in text length and genre. However, psycholinguistic
features and encoder-based models can achieve performance comparable to
language models when trained on texts from individuals with clinically
confirmed depression, highlighting their potential effectiveness in targeted
clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Readiness of Prominent Small Language Models for the
  Democratization of Financial Literacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tagore Rao Kosireddy, Jeffrey D. Wall, Evan Lucas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of small language models (SLMs), herein defined as models with less
than three billion parameters, is increasing across various domains and
applications. Due to their ability to run on more accessible hardware and
preserve user privacy, SLMs possess the potential to democratize access to
language models for individuals of different socioeconomic status and with
different privacy preferences. This study assesses several state-of-the-art
SLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama
project) for use in the financial domain to support the development of
financial literacy LMs. Democratizing access to quality financial information
for those who are financially under educated is greatly needed in society,
particularly as new financial markets and products emerge and participation in
financial markets increases due to ease of access. We are the first to examine
the use of open-source SLMs to democratize access to financial question
answering capabilities for individuals and students. To this end, we provide an
analysis of the memory usage, inference time, similarity comparisons to
ground-truth answers, and output readability of prominent SLMs to determine
which models are most accessible and capable of supporting access to financial
information. We analyze zero-shot and few-shot learning variants of the models.
The results suggest that some off-the-shelf SLMs merit further exploration and
fine-tuning to prepare them for individual use, while others may have limits to
their democratization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I Want to Break Free! Anti-Social Behavior and Persuasion Ability of
  <span class="highlight-title">LLM</span>s in Multi-<span class="highlight-title">Agent</span> Settings with Social Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gian Maria Campedelli, Nicolò Penzo, Massimo Stefan, Roberto Dessì, Marco Guerini, Bruno Lepri, Jacopo Staiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Model (LLM)-based agents become increasingly autonomous and
will more freely interact with each other, studying interactions between them
becomes crucial to anticipate emergent phenomena and potential risks. Drawing
inspiration from the widely popular Stanford Prison Experiment, we contribute
to this line of research by studying interaction patterns of LLM agents in a
context characterized by strict social hierarchy. We do so by specifically
studying two types of phenomena: persuasion and anti-social behavior in
simulated scenarios involving a guard and a prisoner agent who seeks to achieve
a specific goal (i.e., obtaining additional yard time or escape from prison).
Leveraging 200 experimental scenarios for a total of 2,000 machine-machine
conversations across five different popular LLMs, we provide a set of
noteworthy findings. We first document how some models consistently fail in
carrying out a conversation in our multi-agent setup where power dynamics are
at play. Then, for the models that were able to engage in successful
interactions, we empirically show how the goal that an agent is set to achieve
impacts primarily its persuasiveness, while having a negligible effect with
respect to the agent's anti-social behavior. Third, we highlight how agents'
personas, and particularly the guard's personality, drive both the likelihood
of successful persuasion from the prisoner and the emergence of anti-social
behaviors. Fourth, we show that even without explicitly prompting for specific
personalities, anti-social behavior emerges by simply assigning agents' roles.
These results bear implications for the development of interactive LLM agents
as well as the debate on their societal impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Multi-Hop <span class="highlight-title">Reasoning</span> Potential in <span class="highlight-title">Large Language Model</span>s
  through Repetition of Misordered Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-hop reasoning, which requires multi-step reasoning based on the
supporting documents within a given context, remains challenging for large
language models (LLMs). LLMs often struggle to filter out irrelevant documents
within the context, and their performance is sensitive to the position of
supporting documents within that context. In this paper, we identify an
additional challenge: LLMs' performance is also sensitive to the order in which
the supporting documents are presented. We refer to this as the misordered
context problem. To address this issue, we propose a simple yet effective
method called context repetition (CoRe), which involves prompting the model by
repeatedly presenting the context to ensure the supporting documents are
presented in the optimal order for the model. Using CoRe, we improve the F1
score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p
on a synthetic task. Additionally, CoRe helps mitigate the well-known
"lost-in-the-middle" problem in LLMs and can be effectively combined with
retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLE-bench: Evaluating Machine Learning <span class="highlight-title">Agent</span>s on Machine Learning
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, Aleksander Mądry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MLE-bench, a benchmark for measuring how well AI agents perform
at machine learning engineering. To this end, we curate 75 ML
engineering-related competitions from Kaggle, creating a diverse set of
challenging tasks that test real-world ML engineering skills such as training
models, preparing datasets, and running experiments. We establish human
baselines for each competition using Kaggle's publicly available leaderboards.
We use open-source agent scaffolds to evaluate several frontier language models
on our benchmark, finding that the best-performing setup--OpenAI's o1-preview
with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in
16.9% of competitions. In addition to our main results, we investigate various
forms of resource scaling for AI agents and the impact of contamination from
pre-training. We open-source our benchmark code (github.com/openai/mle-bench/)
to facilitate future research in understanding the ML engineering capabilities
of AI agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. Plus 17 pages appendix. 8 figures. Equal contribution by
  first seven authors. Authors randomized. Work by Neil Chowdhury done while at
  OpenAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Approach for Auto <span class="highlight-title">Generation</span> of Labeling Functions for Software
  Engineering Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ebube Alor, Ahmad Abdellatif, SayedHassan Khatoonabadi, Emad Shihab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineering (SE) chatbots are increasingly gaining attention for
their role in enhancing development processes. At the core of chatbots are the
Natural Language Understanding platforms (NLUs), which enable them to
comprehend and respond to user queries. Before deploying NLUs, there is a need
to train them with labeled data. However, acquiring such labeled data for SE
chatbots is challenging due to the scarcity of high-quality datasets. This
challenge arises because training SE chatbots requires specialized vocabulary
and phrases not found in typical language datasets. Consequently, chatbot
developers often resort to manually annotating user queries to gather the data
necessary for training effective chatbots, a process that is both
time-consuming and resource-intensive. Previous studies propose approaches to
support chatbot practitioners in annotating users' posed queries. However,
these approaches require human intervention to generate rules, called labeling
functions (LFs), that identify and categorize user queries based on specific
patterns in the data. To address this issue, we propose an approach to
automatically generate LFs by extracting patterns from labeled user queries. We
evaluate the effectiveness of our approach by applying it to the queries of
four diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)
and measure the performance improvement gained from training the NLU on the
queries labeled by the generated LFs. We find that the generated LFs
effectively label data with AUC scores of up to 85.3%, and NLU's performance
improvement of up to 27.2% across the studied datasets. Furthermore, our
results show that the number of LFs used to generate LFs affects the labeling
performance. We believe that our approach can save time and resources in
labeling users' queries, allowing practitioners to focus on core chatbot
functionalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Software Engineering for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stanceformer: Target-Aware Transformer for Stance Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Garg, Cornelia Caragea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Stance Detection involves discerning the stance expressed in a
text towards a specific subject or target. Prior works have relied on existing
transformer models that lack the capability to prioritize targets effectively.
Consequently, these models yield similar performance regardless of whether we
utilize or disregard target information, undermining the task's significance.
To address this challenge, we introduce Stanceformer, a target-aware
transformer model that incorporates enhanced attention towards the targets
during both training and inference. Specifically, we design a \textit{Target
Awareness} matrix that increases the self-attention scores assigned to the
targets. We demonstrate the efficacy of the Stanceformer with various
BERT-based models, including state-of-the-art models and Large Language Models
(LLMs), and evaluate its performance across three stance detection datasets,
alongside a zero-shot dataset. Our approach Stanceformer not only provides
superior performance but also generalizes even to other domains, such as
Aspect-based Sentiment Analysis. We make the code publicly
available.\footnote{\scriptsize\url{https://github.com/kgarg8/Stanceformer}}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures, 14 tables including Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOOSE-Chem: <span class="highlight-title">Large Language Model</span>s for Rediscovering Unseen Chemistry
  Scientific Hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery contributes largely to human society's prosperity, and
recent progress shows that LLMs could potentially catalyze this process.
However, it is still unclear whether LLMs can discover novel and valid
hypotheses in chemistry. In this work, we investigate this central research
question: Can LLMs automatically discover novel and valid chemistry research
hypotheses given only a chemistry research background (consisting of a research
question and/or a background survey), without limitation on the domain of the
research question? After extensive discussions with chemistry experts, we
propose an assumption that a majority of chemistry hypotheses can be resulted
from a research background and several inspirations. With this key insight, we
break the central question into three smaller fundamental questions. In brief,
they are: (1) given a background question, whether LLMs can retrieve good
inspirations; (2) with background and inspirations, whether LLMs can lead to
hypothesis; and (3) whether LLMs can identify good hypotheses to rank them
higher. To investigate these questions, we construct a benchmark consisting of
51 chemistry papers published in Nature, Science, or a similar level in 2024
(all papers are only available online since 2024). Every paper is divided by
chemistry PhD students into three components: background, inspirations, and
hypothesis. The goal is to rediscover the hypothesis, given only the background
and a large randomly selected chemistry literature corpus consisting the ground
truth inspiration papers, with LLMs trained with data up to 2023. We also
develop an LLM-based multi-agent framework that leverages the assumption,
consisting of three stages reflecting the three smaller questions. The proposed
method can rediscover many hypotheses with very high similarity with the ground
truth ones, covering the main innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and Benchmark are available at
  https://github.com/ZonglinY/MOOSE-Chem.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixtral 12B 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.
Pixtral-12B is trained to understand both natural images and documents,
achieving leading performance on various multimodal benchmarks, surpassing a
number of larger models. Unlike many open-source models, Pixtral is also a
cutting-edge text model for its size, and does not compromise on natural
language performance to excel in multimodal tasks. Pixtral uses a new vision
encoder trained from scratch, which allows it to ingest images at their natural
resolution and aspect ratio. This gives users flexibility on the number of
tokens used to process an image. Pixtral is also able to process any number of
images in its long context window of 128K tokens. Pixtral 12B substanially
outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B).
It also outperforms much larger open models like Llama-3.2 90B while being 7x
smaller. We further contribute an open-source benchmark, MM-MT-Bench, for
evaluating vision-language models in practical scenarios, and provide detailed
analysis and code for standardized evaluation protocols for multimodal LLMs.
Pixtral-12B is released under Apache 2.0 license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReIFE: Re-evaluating Instruction-Following Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic evaluation of instruction following typically involves using
large language models (LLMs) to assess response quality. However, there is a
lack of comprehensive evaluation of these LLM-based evaluators across two
dimensions: the base LLMs and the evaluation protocols. Therefore, we present a
thorough meta-evaluation of instruction following, including 25 base LLMs and
15 recently proposed evaluation protocols, on 4 human-annotated datasets,
assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows
us to identify the best-performing base LLMs and evaluation protocols with a
high degree of robustness. Moreover, our large-scale evaluation reveals: (1)
Base LLM performance ranking remains largely consistent across evaluation
protocols, with less capable LLMs showing greater improvement from protocol
enhancements; (2) Robust evaluation of evaluation protocols requires many base
LLMs with varying capability levels, as protocol effectiveness can depend on
the base LLM used; (3) Evaluation results on different datasets are not always
consistent, so a rigorous evaluation requires multiple datasets with
distinctive features. We release our meta-evaluation suite ReIFE, which
provides the codebase and evaluation result collection for more than 500
LLM-evaluator configurations, to support future research in
instruction-following evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result
  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Selection via Optimal Control for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the selection of high-quality pre-training data from
massive corpora to enhance LMs' capabilities for downstream usage. We formulate
data selection as a generalized Optimal Control problem, which can be solved
theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of
necessary conditions that characterize the relationship between optimal data
selection and LM training dynamics. Based on these theoretical results, we
introduce PMP-based Data Selection (PDS), a framework that approximates optimal
data selection by solving the PMP conditions. In our experiments, we adopt PDS
to select data from CommmonCrawl and show that the PDS-selected corpus
accelerates the learning of LMs and constantly boosts their performance on a
wide range of downstream tasks across various model sizes. Moreover, the
benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by
the extrapolation of the test loss curves according to the Scaling Laws. PDS
also improves data utilization when the pre-training data is limited, by
reducing the data demand by 1.8 times, which mitigates the quick exhaustion of
available web-crawled corpora. Our code, data, and model checkpoints can be
found in https://github.com/microsoft/LMOps/tree/main/data_selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating the Language Mismatch and Repetition Issues in <span class="highlight-title">LLM</span>-based
  Machine Translation via Model <span class="highlight-title">Editing</span> <span class="chip">EMNLP'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, Ying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently revolutionized the NLP field,
while they still fall short in some specific down-stream tasks. In the work, we
focus on utilizing LLMs to perform machine translation, where we observe that
two patterns of errors frequently occur and drastically affect the translation
quality: language mismatch and repetition. The work sets out to explore the
potential for mitigating these two issues by leveraging model editing methods,
e.g., by locating Feed-Forward Network (FFN) neurons or something that are
responsible for the errors and deactivating them in the inference time. We find
that directly applying such methods either limited effect on the targeted
errors or has significant negative side-effect on the general translation
quality, indicating that the located components may also be crucial for
ensuring machine translation with LLMs on the rails. To this end, we propose to
refine the located components by fetching the intersection of the locating
results under different language settings, filtering out the aforementioned
information that is irrelevant to targeted errors. The experiment results
empirically demonstrate that our methods can effectively reduce the language
mismatch and repetition ratios and meanwhile enhance or keep the general
translation quality in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, EMNLP'2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robots in the Middle: Evaluating <span class="highlight-title">LLM</span>s in Dispute Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinzhe Tan, Hannes Westermann, Nikhil Reddy Pottanigari, Jaromír Šavelka, Sébastien Meeùs, Mia Godet, Karim Benyekhlef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mediation is a dispute resolution method featuring a neutral third-party
(mediator) who intervenes to help the individuals resolve their dispute. In
this paper, we investigate to which extent large language models (LLMs) are
able to act as mediators. We investigate whether LLMs are able to analyze
dispute conversations, select suitable intervention types, and generate
appropriate intervention messages. Using a novel, manually created dataset of
50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human
annotators across several key metrics. Overall, the LLMs showed strong
performance, even outperforming our human annotators across dimensions.
Specifically, in 62% of the cases, the LLMs chose intervention types that were
rated as better than or equivalent to those chosen by humans. Moreover, in 84%
of the cases, the intervention messages generated by the LLMs were rated as
better than or equal to the intervention messages written by humans. LLMs
likewise performed favourably on metrics such as impartiality, understanding
and contextualization. Our results demonstrate the potential of integrating AI
in online dispute resolution (ODR) platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PositionID: <span class="highlight-title">LLM</span>s can Control Lengths, Copy and Paste with Explicit
  Positional Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Wang, Feiyu Duan, Yibo Zhang, Wangchunshu Zhou, Ke Xu, Wenhao Huang, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate impressive capabilities across
various domains, including role-playing, creative writing, mathematical
reasoning, and coding. Despite these advancements, LLMs still encounter
challenges with length control, frequently failing to adhere to specific length
constraints due to their token-level operations and insufficient training on
data with strict length limitations. We identify this issue as stemming from a
lack of positional awareness and propose novel approaches--PositionID Prompting
and PositionID Fine-Tuning--to address it. These methods enhance the model's
ability to continuously monitor and manage text length during generation.
Additionally, we introduce PositionID CP Prompting to enable LLMs to perform
copy and paste operations accurately. Furthermore, we develop two benchmarks
for evaluating length control and copy-paste abilities. Our experiments
demonstrate that our methods significantly improve the model's adherence to
length constraints and copy-paste accuracy without compromising response
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages. CP-Bench and LenCtrl-Bench are available in
  https://huggingface.co/datasets/ZenMoore/CP-Bench and
  https://huggingface.co/datasets/ZenMoore/LenCtrl-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clean Evaluations on Contaminated Visual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Lu, Shujie Miao, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to evaluate large language models (LLMs) cleanly has been established as
an important research era to genuinely report the performance of possibly
contaminated LLMs. Yet, how to cleanly evaluate the visual language models
(VLMs) is an under-studied problem. We propose a novel approach to achieve such
goals through data augmentation methods on the visual input information. We
then craft a new visual clean evaluation benchmark with thousands of data
instances. Through extensive experiments, we found that the traditional visual
data augmentation methods are useful, but they are at risk of being used as a
part of the training data as a workaround. We further propose using BGR
augmentation to switch the colour channel of the visual information. We found
that it is a simple yet effective method for reducing the effect of data
contamination and fortunately, it is also harmful to be used as a data
augmentation method during training. It means that it is hard to integrate such
data augmentation into training by malicious trainers and it could be a
promising technique to cleanly evaluate visual LLMs. Our code, data, and model
weights will be released upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation
  Models Without Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologists play a crucial role by translating medical images into medical
reports. However, the field faces staffing shortages and increasing workloads.
While automated approaches using vision-language models (VLMs) show promise as
assistants, they require exceptionally high accuracy. Most current VLMs in
radiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the
general domain, additional preference fine-tuning has become standard practice.
The challenge in radiology lies in the prohibitive cost of obtaining
radiologist feedback. We propose a scalable automated preference alignment
technique for VLMs in radiology, focusing on chest X-ray (CXR) report
generation. Our method leverages publicly available datasets with an
LLM-as-a-Judge mechanism, eliminating the need for additional expert
radiologist feedback. We evaluate and benchmark five direct alignment
algorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN
scores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in
an average across six metrics (domain specific and general), compared to the
SFT baseline. We study reward overoptimization via length exploitation, with
reports lengthening by up to 3.2x. To assess a potential alignment tax, we
benchmark on six additional diverse tasks, finding no significant degradations.
A reader study involving four board-certified radiologists indicates win rates
of up to 0.62 over the SFT baseline, while significantly penalizing verbosity.
Our analysis provides actionable insights for the development of VLMs in
high-stakes fields like radiology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based
  Outline-guided <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Knappich, Simon Razniewski, Anna Hätty, Annemarie Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The patent domain is gaining attention in natural language processing
research, offering practical applications in streamlining the patenting process
and providing challenging benchmarks for large language models (LLMs). However,
the generation of the description sections of patents, which constitute more
than 90% of the patent document, has not been studied to date. We address this
gap by introducing the task of outline-guided paper-to-patent generation, where
an academic paper provides the technical specification of the invention and an
outline conveys the desired patent structure. We present PAP2PAT, a new
challenging benchmark of 1.8k patent-paper pairs with document outlines,
collected using heuristics that reflect typical research lab practices. Our
experiments with current open-weight LLMs and outline-guided chunk-based
generation show that they can effectively use information from the paper but
struggle with repetitions, likely due to the inherent repetitiveness of patent
language. We release our data and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CursorCore: Assist Programming through Aligning Anything 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Jiang, Qi Liu, Rui Li, Shengyu Ye, Shijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been successfully applied to programming
assistance tasks, such as code completion, code insertion, and instructional
code editing. However, these applications remain insufficiently automated and
struggle to effectively integrate various types of information during the
programming process, including coding history, current code, and user
instructions. In this work, we propose a new conversational framework that
comprehensively integrates these information sources, collect data to train our
models and evaluate their performance. Firstly, to thoroughly evaluate how well
models align with different types of information and the quality of their
outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to
comprehensively assess the performance of models in programming assistance
tasks. Then, for data collection, we develop a data generation pipeline,
Programming-Instruct, which synthesizes training data from diverse sources,
such as GitHub and online judge platforms. This pipeline can automatically
generate various types of messages throughout the programming process. Finally,
using this pipeline, we generate 219K samples, fine-tune multiple models, and
develop the CursorCore series. We show that CursorCore outperforms other models
of comparable size. This framework unifies applications such as inline chat and
automated editing, contributes to the advancement of coding assistants. Code,
models and data are freely available at
https://github.com/TechxGenus/CursorCore.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Autoencoders Reveal Universal Feature Spaces Across Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones. This makes it
difficult to disentangle and match features across different models. To address
this issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics like Singular Value Canonical Correlation Analysis to
analyze these SAE features across different LLMs. Our experiments reveal
significant similarities in SAE feature spaces across various LLMs, providing
new evidence for feature universality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personal Intelligence System UniLM: Hybrid On-Device Small Language
  Model and Server-Based <span class="highlight-title">Large Language Model</span> for Malay Nusantara 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azree Nazri, Olalekan Agbolade, Faisal Aziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contexts with limited computational and data resources, high-resource
language models often prove inadequate, particularly when addressing the
specific needs of Malay languages. This paper introduces a Personal
Intelligence System designed to efficiently integrate both on-device and
server-based models. The system incorporates SLiM-34M for on-device processing,
optimized for low memory and power usage, and MANYAK-1.3B for server-based
tasks, allowing for scalable, high-performance language processing. The models
achieve significant results across various tasks, such as machine translation,
question-answering, and translate IndoMMLU. Particularly noteworthy is
SLiM-34M's ability to achieve a high improvement in accuracy compared to other
LLMs while using 2 times fewer pre-training tokens. This work challenges the
prevailing assumption that large-scale computational resources are necessary to
build effective language models, contributing to the development of
resource-efficient models for the Malay language with the unique orchestration
between SLiM-34M and MANYAK-1.3B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Factor Level Preferences to Improve Human-Model Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhyun Oh, Eunsu Kim, Jiseon Kim, Wenda Xu, Inha Cha, William Yang Wang, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in Large Language Model (LLM) alignment, understanding
the reasons behind LLM preferences remains crucial for bridging the gap between
desired and actual behavior. LLMs often exhibit biases or tendencies that
diverge from human preferences, such as favoring certain writing styles or
producing overly verbose outputs. However, current methods for evaluating
preference alignment often lack explainability, relying on coarse-grained
comparisons. To address this, we introduce PROFILE (PRObing Factors of
InfLuence for Explainability), a novel framework that uncovers and quantifies
the influence of specific factors driving preferences. PROFILE's factor level
analysis explains the 'why' behind human-model alignment and misalignment,
offering insights into the direction of model improvement. We apply PROFILE to
analyze human and LLM preferences across three tasks: summarization, helpful
response generation, and document-based question-answering. Our factor level
analysis reveals a substantial discrepancy between human and LLM preferences in
generation tasks, whereas LLMs show strong alignment with human preferences in
evaluation tasks. We demonstrate how leveraging factor level insights,
including addressing misaligned factors or exploiting the generation-evaluation
gap, can improve alignment with human preferences. This work underscores the
importance of explainable preference analysis and highlights PROFILE's
potential to provide valuable training signals, driving further improvements in
human-model alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Boosting <span class="highlight-title">Large Language Model</span>s with Synthetic Preference Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through alignment with human preferences, Large Language Models (LLMs) have
advanced significantly in generating honest, harmless, and helpful responses.
However, collecting high-quality preference data is a resource-intensive and
creativity-demanding process, especially for the continual improvement of LLMs.
We introduce SynPO, a self-boosting paradigm that leverages synthetic
preference data for model alignment. SynPO employs an iterative mechanism
wherein a self-prompt generator creates diverse prompts, and a response
improver refines model responses progressively. This approach trains LLMs to
autonomously learn the generative rewards for their own outputs and eliminates
the need for large-scale annotation of prompts and human preferences. After
four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements
in instruction-following abilities, achieving over 22.1% win rate improvements
on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general
performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score
increase on the well-recognized Open LLM leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeker: Enhancing Exception Handling in Code with <span class="highlight-title">LLM</span>-based Multi-<span class="highlight-title">Agent</span>
  Approach <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanming Zhang, Yuxuan Chen, Yuan Yuan, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real world software development, improper or missing exception handling
can severely impact the robustness and reliability of code. Exception handling
mechanisms require developers to detect, capture, and manage exceptions
according to high standards, but many developers struggle with these tasks,
leading to fragile code. This problem is particularly evident in open source
projects and impacts the overall quality of the software ecosystem. To address
this challenge, we explore the use of large language models (LLMs) to improve
exception handling in code. Through extensive analysis, we identify three key
issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception
Types, and Distorted Handling Solutions. These problems are widespread across
real world repositories, suggesting that robust exception handling practices
are often overlooked or mishandled. In response, we propose Seeker, a multi
agent framework inspired by expert developer strategies for exception handling.
Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist
LLMs in detecting, capturing, and resolving exceptions more effectively. Our
work is the first systematic study on leveraging LLMs to enhance exception
handling practices, providing valuable insights for future improvements in code
reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 figures. Submitted ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on
  Relatively Free Word Ordered and Morphologically Rich Low Resource Languages <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pretam Ray, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural dependency parsing has achieved remarkable performance for low
resource morphologically rich languages. It has also been well-studied that
morphologically rich languages exhibit relatively free word order. This prompts
a fundamental investigation: Is there a way to enhance dependency parsing
performance, making the model robust to word order variations utilizing the
relatively free word order nature of morphologically rich languages? In this
work, we examine the robustness of graph-based parsing architectures on 7
relatively free word order languages. We focus on scrutinizing essential
modifications such as data augmentation and the removal of position encoding
required to adapt these architectures accordingly. To this end, we propose a
contrastive self-supervised learning method to make the model robust to word
order variations. Furthermore, our proposed modification demonstrates a
substantial average gain of 3.03/2.95 points in 7 relatively free word order
languages, as measured by the UAS/LAS Score metric when compared to the best
performing baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main (Short), 9 pages, 3 figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWIFT: On-the-Fly Self-Speculative Decoding for <span class="highlight-title">LLM</span> Inference
  Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heming Xia, Yongqi Li, Jun Zhang, Cunxiao Du, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by first employing a compact model to draft multiple tokens
efficiently and then using the target LLM to verify them in parallel. While
this technique has achieved notable speedups, most existing approaches
necessitate either additional parameters or extensive training to construct
effective draft models, thereby restricting their applicability across
different LLMs and tasks. To address this limitation, we explore a novel
plug-and-play SD solution with layer-skipping, which skips intermediate layers
of the target LLM as the compact draft model. Our analysis reveals that LLMs
exhibit great potential for self-acceleration through layer sparsity and the
task-specific nature of this sparsity. Building on these insights, we introduce
SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively
selects intermediate layers of LLMs to skip during inference. SWIFT does not
require auxiliary models or additional training, making it a plug-and-play
solution for accelerating LLM inference across diverse input data streams. Our
extensive experiments across a wide range of models and downstream tasks
demonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving
the original distribution of the generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilize the Flow before Stepping into the Same River Twice: Certainty
  Represented Knowledge Flow for Refusal-Aware Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)
to refuse to answer unknown questions. By modifying responses of unknown
questions in the training data to refusal responses such as "I don't know",
RAIT enhances the reliability of LLMs and reduces their hallucination.
Generally, RAIT modifies training samples based on the correctness of the
initial LLM's response. However, this crude approach can cause LLMs to
excessively refuse answering questions they could have correctly answered, the
problem we call over-refusal. In this paper, we explore two primary causes of
over-refusal: Static conflict emerges when the RAIT data is constructed solely
on correctness criteria, causing similar samples in the LLM's feature space to
be assigned different labels (original vs. modified "I don't know"). Dynamic
conflict occurs due to the changes of LLM's knowledge state during fine-tuning,
which transforms previous unknown questions into knowns, while the training
data, which is constructed based on the initial LLM, remains unchanged. These
conflicts cause the trained LLM to misclassify known questions as unknown,
resulting in over-refusal. To address this issue, we introduce Certainty
Represented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).
CRaFT centers on two main contributions: First, we additionally incorporate
response certainty to selectively filter and modify data, reducing static
conflicts. Second, we implement preliminary rehearsal training to characterize
changes in the LLM's knowledge state, which helps mitigate dynamic conflicts
during the fine-tuning process. We conducted extensive experiments on
open-ended question answering and multiple-choice question task. Experiment
results show that CRaFT can improve LLM's overall performance during the RAIT
process. Source code and training data will be released at Github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding
  author: Conghui He</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Model for Less-Resourced Language with 1 billion parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domen Vreš, Martin Božič, Aljaž Potočnik, Tomaž Martinčič, Marko Robnik-Šikonja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are a basic infrastructure for modern natural
language processing. Many commercial and open-source LLMs exist for English,
e.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on
mostly English texts, their fluency and knowledge of low-resource languages and
societies are superficial. We present the development of large generative
language models for a less-resourced language. GaMS 1B - Generative Model for
Slovene with 1 billion parameters was created by continuing pretraining of the
existing English OPT model. We developed a new tokenizer adapted to Slovene,
Croatian, and English languages and used embedding initialization methods FOCUS
and WECHSEL to transfer the embeddings from the English OPT model. We evaluate
our models on several classification datasets from the Slovene suite of
benchmarks and generative sentence simplification task SENTA. We only used a
few-shot in-context learning of our models, which are not yet
instruction-tuned. For classification tasks, in this mode, the generative
models lag behind the existing Slovene BERT-type models fine-tuned for specific
tasks. On a sentence simplification task, the GaMS models achieve comparable or
better performance than the GPT-3.5-Turbo model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FltLM: An Intergrated Long-Context <span class="highlight-title">Large Language Model</span> for Effective
  Context Filtering and Understanding <span class="chip">ECAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Deng, Zhengyang Shen, Boyang Wang, Lixin Su, Suqi Cheng, Ying Nie, Junfeng Wang, Dawei Yin, Jinwen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Long-Context Large Language Models (LLMs) has markedly
advanced natural language processing by facilitating the process of textual
data across long documents and multiple corpora. However, Long-Context LLMs
still face two critical challenges: The lost in the middle phenomenon, where
crucial middle-context information is likely to be missed, and the distraction
issue that the models lose focus due to overly extended contexts. To address
these challenges, we propose the Context Filtering Language Model (FltLM), a
novel integrated Long-Context LLM which enhances the ability of the model on
multi-document question-answering (QA) tasks. Specifically, FltLM innovatively
incorporates a context filter with a soft mask mechanism, identifying and
dynamically excluding irrelevant content to concentrate on pertinent
information for better comprehension and reasoning. Our approach not only
mitigates these two challenges, but also enables the model to operate
conveniently in a single forward pass. Experimental results demonstrate that
FltLM significantly outperforms supervised fine-tuning and retrieval-based
methods in complex QA scenarios, suggesting a promising solution for more
accurate and reliable long-context natural language understanding applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 27th European Conference on Artificial Intelligence
  (ECAI-2024), this is the full version of the paper including technical
  appendices. This final version features enhanced formatting and corrections
  to errors present in other online versions. We regret any inconvenience this
  may have caused our readers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Fine-tuning and Conversion of Pretrained Speech and Language
  Models towards Linear Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian He, Philip N. Garner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architectures such as Linformer and Mamba have recently emerged as
competitive linear time replacements for transformers. However, corresponding
large pretrained models are often unavailable, especially in non-text domains.
To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)
approach that jointly converts a transformer model to a linear time substitute
and fine-tunes it to a target task. We also compare several means to guide the
fine-tuning to optimally retain the desired inference capability from the
original model. The methods differ in their use of the target model and the
trajectory of the parameters. In a series of empirical studies on language
processing, language modeling, and speech processing, we show that CALD can
effectively recover the result of the original model, and that the guiding
strategy contributes to the result. Some reasons for the variation are
suggested.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MentalArena: Self-play Training of Language Models for Diagnosis and
  Treatment of Mental Health Disorders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Li, May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health disorders are one of the most serious diseases in the world.
Most people with such a disease lack access to adequate care, which highlights
the importance of training models for the diagnosis and treatment of mental
health disorders. However, in the mental health domain, privacy concerns limit
the accessibility of personalized treatment data, making it challenging to
build powerful models. In this paper, we introduce MentalArena, a self-play
framework to train language models by generating domain-specific personalized
data, where we obtain a better model capable of making a personalized diagnosis
and treatment (as a therapist) and providing information (as a patient). To
accurately model human-like mental health patients, we devise Symptom Encoder,
which simulates a real patient from both cognition and behavior perspectives.
To address intent bias during patient-therapist interactions, we propose
Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and
dynamically manage the dialogue between patient and therapist according to the
identified deviations. We evaluated MentalArena against 6 benchmarks, including
biomedicalQA and mental health tasks, compared to 6 advanced models. Our
models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform
their counterparts, including GPT-4o. We hope that our work can inspire future
research on personalized care. Code is available in
https://github.com/Scarelette/MentalArena/tree/main
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report; 27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Root Defence Strategies: Ensuring Safety of <span class="highlight-title">LLM</span> at the Decoding Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zeng, Yuying Shang, Yutao Zhu, Jiawei Chen, Yu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated immense utility across various
industries. However, as LLMs advance, the risk of harmful outputs increases due
to incorrect or malicious instruction prompts. While current methods
effectively address jailbreak risks, they share common limitations: 1) Judging
harmful responses from the prefill-level lacks utilization of the model's
decoding outputs, leading to relatively lower effectiveness and robustness. 2)
Rejecting potentially harmful responses based on a single evaluation can
significantly impair the model's helpfulness.This paper examines the LLMs'
capability to recognize harmful outputs, revealing and quantifying their
proficiency in assessing the danger of previous tokens. Motivated by pilot
experiment results, we design a robust defense mechanism at the decoding level.
Our novel decoder-oriented, step-by-step defense architecture corrects harmful
queries directly rather than rejecting them outright. We introduce speculative
decoding to enhance usability and facilitate deployment to boost secure
decoding speed. Extensive experiments demonstrate that our approach improves
model security without compromising reasoning speed. Notably, our method
leverages the model's ability to discern hazardous information, maintaining its
helpfulness compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seg2Act: Global Context-aware Action <span class="highlight-title">Generation</span> for Document Logical
  Structuring <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichao Li, Shaojie He, Meng Liao, Xuanang Chen, Yaojie Lu, Hongyu Lin, Yanxiong Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document logical structuring aims to extract the underlying hierarchical
structure of documents, which is crucial for document intelligence. Traditional
approaches often fall short in handling the complexity and the variability of
lengthy documents. To address these issues, we introduce Seg2Act, an
end-to-end, generation-based method for document logical structuring,
revisiting logical structure extraction as an action generation task.
Specifically, given the text segments of a document, Seg2Act iteratively
generates the action sequence via a global context-aware generative model, and
simultaneously updates its global context and current logical structure based
on the generated actions. Experiments on ChCatExt and HierDoc datasets
demonstrate the superior performance of Seg2Act in both supervised and transfer
learning settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Pixels to Tokens: Revisiting Object Hallucinations in Large
  <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Shang, Xinyi Zeng, Yutao Zhu, Xiao Yang, Zhengwei Fang, Jingyuan Zhang, Jiawei Chen, Zinan Liu, Yu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in large vision-language models (LVLMs) are a significant
challenge, i.e., generating objects that are not presented in the visual input,
which impairs their reliability. Recent studies often attribute hallucinations
to a lack of understanding of visual input, yet ignore a more fundamental
issue: the model's inability to effectively extract or decouple visual
features. In this paper, we revisit the hallucinations in LVLMs from an
architectural perspective, investigating whether the primary cause lies in the
visual encoder (feature extraction) or the modal alignment module (feature
decoupling). Motivated by our findings on the preliminary investigation, we
propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.
This plug-and-play method can be integrated into various LVLMs, utilizing
adaptive virtual tokens to extract object features from bounding boxes, thereby
addressing hallucinations caused by insufficient decoupling of visual features.
PATCH achieves state-of-the-art performance on multiple multi-modal
hallucination datasets. We hope this approach provides researchers with deeper
insights into the underlying causes of hallucinations in LVLMs, fostering
further advancements and innovation in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Preserve or To Compress: An In-Depth Study of Connector Selection in
  <span class="highlight-title">Multimodal</span> <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal large language models (MLLMs) have garnered
significant attention from both industry and academia. However, there is still
considerable debate on constructing MLLM architectures, particularly regarding
the selection of appropriate connectors for perception tasks of varying
granularities. This paper systematically investigates the impact of connectors
on MLLM performance. Specifically, we classify connectors into
feature-preserving and feature-compressing types. Utilizing a unified
classification standard, we categorize sub-tasks from three comprehensive
benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained
perception, fine-grained perception, and reasoning, and evaluate the
performance. Our findings reveal that feature-preserving connectors excel in
\emph{fine-grained perception} tasks due to their ability to retain detailed
visual information. In contrast, feature-compressing connectors, while less
effective in fine-grained perception tasks, offer significant speed advantages
and perform comparably in \emph{coarse-grained perception} and \emph{reasoning}
tasks. These insights are crucial for guiding MLLM architecture design and
advancing the optimization of MLLM architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBa: Convergence Balancer for Multitask Finetuning of Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Gong, Hang Yu, Cong Liao, Bingchang Liu, Chaoyu Chen, Jianguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) benefits the fine-tuning of large language models
(LLMs) by providing a single model with improved performance and generalization
ability across tasks, presenting a resource-efficient alternative to developing
separate models for each task. Yet, existing MTL strategies for LLMs often fall
short by either being computationally intensive or failing to ensure
simultaneous task convergence. This paper presents CoBa, a new MTL approach
designed to effectively manage task convergence balance with minimal
computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute
Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically
adjusts task weights during the training process, ensuring that the validation
loss of all tasks progress towards convergence at an even pace while mitigating
the issue of individual task divergence. The results of our experiments
involving three disparate datasets underscore that this approach not only
fosters equilibrium in task improvement but enhances the LLMs' performance by
up to 13% relative to the second-best baselines. Code is open-sourced at
https://github.com/codefuse-ai/MFTCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, main conference of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Programming Language and What Features at Pre-training Stage
  Affect Downstream Logical Inference Performance? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have demonstrated remarkable
generalization abilities in mathematics and logical reasoning tasks. Prior
research indicates that LLMs pre-trained with programming language data exhibit
high mathematical and reasoning abilities; however, this causal relationship
has not been rigorously tested. Our research aims to verify which programming
languages and features during pre-training affect logical inference
performance. Specifically, we pre-trained decoder-based language models from
scratch using datasets from ten programming languages (e.g., Python, C, Java)
and three natural language datasets (Wikipedia, Fineweb, C4) under identical
conditions. Thereafter, we evaluated the trained models in a few-shot
in-context learning setting on logical reasoning tasks: FLD and bAbi, which do
not require commonsense or world knowledge. The results demonstrate that nearly
all models trained with programming languages consistently outperform those
trained with natural languages, indicating that programming languages contain
factors that elicit logic inference performance. In addition, we found that
models trained with programming languages exhibit a better ability to follow
instructions compared to those trained with natural languages. Further analysis
reveals that the depth of Abstract Syntax Trees representing parsed results of
programs also affects logical reasoning performance. These findings will offer
insights into the essential elements of pre-training for acquiring the
foundational abilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of <span class="highlight-title">LLM</span>s with
  Situation Puzzles <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Chen, Bowen Zhang, Gang Wang, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While advancements in NLP have significantly improved the performance of
Large Language Models (LLMs) on tasks requiring vertical thinking, their
lateral thinking capabilities remain under-explored and challenging to measure
due to the complexity of assessing creative thought processes and the scarcity
of relevant data. To address these challenges, we introduce SPLAT, a benchmark
leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.
This benchmark, containing 975 graded situation puzzles across three difficulty
levels, employs a new multi-turn player-judge framework instead of the
traditional model-based evaluation, which often necessitates a stronger
evaluation model. This framework simulates an interactive game where the model
(player) asks the evaluation model (judge) questions about an incomplete story
to infer the full scenario. The judge answers based on a detailed reference
scenario or evaluates if the player's predictions align with the reference one.
This approach lessens dependence on more robust evaluation models, enabling the
assessment of state-of-the-art LLMs. The experiments demonstrate that a robust
evaluation model, such as WizardLM-2, closely matches human judgements in both
intermediate question-answering and final scenario accuracy, achieving over 80%
agreement-similar to the agreement levels among humans. Furthermore, applying
data and reasoning processes from our benchmark to other lateral
thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to
performance enhancements. This suggests that our benchmark effectively
evaluates and elicits the lateral thinking abilities of LLMs. Code is available
at: https://github.com/chenqi008/LateralThinking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Mixed quantization in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Cao, Cheng Zhang, Pedro Gimenes, Jianqiao Lu, Jianyi Cheng, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization of Large Language Models (LLMs) has proven
effective in reducing the computational requirements for running inference on
these models. In this study, we focus on a straightforward question: When
aiming for a specific accuracy or perplexity target for low-precision
quantization, how many high-precision numbers or calculations are required to
preserve as we scale LLMs to larger sizes? We first introduce a critical metric
named the quantization ratio, which compares the number of parameters quantized
to low-precision arithmetic against the total parameter count. Through
extensive and carefully controlled experiments across different model families,
arithmetic types, and quantization granularities (e.g. layer-wise,
matmul-wise), we identify two central phenomenons. 1) The larger the models,
the better they can preserve performance with an increased quantization ratio,
as measured by perplexity in pre-training tasks or accuracy in downstream
tasks. 2) The finer the granularity of mixed-precision quantization (e.g.,
matmul-wise), the more the model can increase the quantization ratio. We
believe these observed phenomena offer valuable insights for future AI hardware
design and the development of advanced Efficient AI algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatMamba: A Matryoshka State Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Shukla, Sai Vemprala, Aditya Kusupati, Ashish Kapoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State Space Models (SSMs) like Mamba2 are a promising alternative to
Transformers, with faster theoretical training and inference times --
especially for long context lengths. Recent work on Matryoshka Representation
Learning -- and its application to Transformer backbones in works like
MatFormer -- showed how to introduce nested granularities of smaller submodels
in one universal elastic model. In this work, we present MatMamba: a state
space model which combines Matryoshka-style learning with Mamba2, by modifying
the block to contain nested dimensions to enable joint training and adaptive
inference. MatMamba allows for efficient and adaptive deployment across various
model sizes. We train a single large MatMamba model and are able to get a
number of smaller nested models for free -- while maintaining or improving upon
the performance of a baseline smaller model trained from scratch. We train
language and image models at a variety of parameter sizes from 35M to 1.4B. Our
results on ImageNet and FineWeb show that MatMamba models scale comparably to
Transformers, while having more efficient inference characteristics. This makes
MatMamba a practically viable option for deploying large-scale models in an
elastic way based on the available inference compute. Code and models are open
sourced at \url{https://github.com/ScaledFoundations/MatMamba}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guaranteed <span class="highlight-title">Generation</span> from <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minbeom Kim, Thibaut Thonet, Jos Rozen, Hwaran Lee, Kyomin Jung, Marc Dymetman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly used across various
applications, there is a growing need to control text generation to satisfy
specific constraints or requirements. This raises a crucial question: Is it
possible to guarantee strict constraint satisfaction in generated outputs while
preserving the distribution of the original model as much as possible? We first
define the ideal distribution - the one closest to the original model, which
also always satisfies the expressed constraint - as the ultimate goal of
guaranteed generation. We then state a fundamental limitation, namely that it
is impossible to reach that goal through autoregressive training alone. This
motivates the necessity of combining training-time and inference-time methods
to enforce such guarantees. Based on this insight, we propose GUARD, a simple
yet effective approach that combines an autoregressive proposal distribution
with rejection sampling. Through GUARD's theoretical properties, we show how
controlling the KL divergence between a specific proposal and the target ideal
distribution simultaneously optimizes inference speed and distributional
closeness. To validate these theoretical concepts, we conduct extensive
experiments on two text generation settings with hard-to-satisfy constraints: a
lexical constraint scenario and a sentiment reversal scenario. These
experiments show that GUARD achieves perfect constraint satisfaction while
almost preserving the ideal distribution with highly improved inference
efficiency. GUARD provides a principled approach to enforcing strict guarantees
for LLMs without compromising their generative capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating Verbalized Probabilities for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wang, Gyuri Szarvas, Georges Balazs, Pavel Danchenko, Patrick Ernst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibrating verbalized probabilities presents a novel approach for reliably
assessing and leveraging outputs from black-box Large Language Models (LLMs).
Recent methods have demonstrated improved calibration by applying techniques
like Platt scaling or temperature scaling to the confidence scores generated by
LLMs. In this paper, we explore the calibration of verbalized probability
distributions for discriminative tasks. First, we investigate the capability of
LLMs to generate probability distributions over categorical labels. We
theoretically and empirically identify the issue of re-softmax arising from the
scaling of verbalized probabilities, and propose using the invert softmax trick
to approximate the "logit" by inverting verbalized probabilities. Through
extensive evaluation on three public datasets, we demonstrate: (1) the robust
capability of LLMs in generating class distributions, and (2) the effectiveness
of the invert softmax trick in estimating logits, which, in turn, facilitates
post-calibration adjustments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PII-Scope: A <span class="highlight-title">Benchmark</span> for Training Data PII Leakage Assessment in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Kanth Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, Xuebing Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce PII-Scope, a comprehensive benchmark designed to
evaluate state-of-the-art methodologies for PII extraction attacks targeting
LLMs across diverse threat settings. Our study provides a deeper understanding
of these attacks by uncovering several hyperparameters (e.g., demonstration
selection) crucial to their effectiveness. Building on this understanding, we
extend our study to more realistic attack scenarios, exploring PII attacks that
employ advanced adversarial strategies, including repeated and diverse
querying, and leveraging iterative learning for continual PII extraction.
Through extensive experimentation, our results reveal a notable underestimation
of PII leakage in existing single-query attacks. In fact, we show that with
sophisticated adversarial capabilities and a limited query budget, PII
extraction rates can increase by up to fivefold when targeting the pretrained
model. Moreover, we evaluate PII leakage on finetuned models, showing that they
are more vulnerable to leakage than pretrained models. Overall, our work
establishes a rigorous empirical benchmark for PII extraction attacks in
realistic threat scenarios and provides a strong foundation for developing
effective mitigation strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">Multimodal</span> <span class="highlight-title">LLM</span> for Detailed and Accurate Video Captioning
  using Multi-Round Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zujun Ma, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos contain a wealth of information, and generating detailed and accurate
descriptions in natural language is a key aspect of video understanding. In
this paper, we present video-SALMONN 2, an advanced audio-visual large language
model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with
paired audio) captioning through directed preference optimization (DPO). We
propose new metrics to evaluate the completeness and accuracy of video
descriptions, which are optimized using DPO. To further improve training, we
introduce a novel multi-round DPO (mrDPO) approach, which involves periodically
updating the DPO reference model, merging and re-initializing the LoRA module
as a proxy for parameter updates after each training round (1,000 steps), and
incorporating guidance from ground-truth video captions to stabilize the
process. To address potential catastrophic forgetting of non-captioning
abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO
LLM by using the captions generated by the mrDPO-trained model as supervised
labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's
captioning accuracy, reducing global and local error rates by 40\% and 20\%,
respectively, while decreasing the repetition rate by 35\%. The final
video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models
such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining
competitive performance to the state-of-the-art on widely used video
question-answering benchmark among models of similar size. Upon acceptance, we
will release the code, model checkpoints, and training and test data. Demos are
available at
\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Towards Universality: Studying Mechanistic Similarity Across Language
  Model Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, <span class="highlight-author">Xipeng Qiu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hypothesis of Universality in interpretability suggests that different
neural networks may converge to implement similar algorithms on similar tasks.
In this work, we investigate two mainstream architectures for language
modeling, namely Transformers and Mambas, to explore the extent of their
mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate
interpretable features from these models and show that most features are
similar in these two models. We also validate the correlation between feature
similarity and Universality. We then delve into the circuit-level analysis of
Mamba models and find that the induction circuits in Mamba are structurally
analogous to those in Transformers. We also identify a nuanced difference we
call \emph{Off-by-One motif}: The information of one token is written into the
SSM state in its next position. Whilst interaction between tokens in
Transformers does not exhibit such trend.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>s as Code Executors: An Exploratory Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Lyu, Lecheng Yan, Rui Xing, Wenxi Li, Younes Samih, Tianbo Ji, Longyue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities of Large Language Models (LLMs) have significantly evolved,
extending from natural language processing to complex tasks like code
understanding and generation. We expand the scope of LLMs' capabilities to a
broader context, using LLMs to execute code snippets to obtain the output. This
paper pioneers the exploration of LLMs as code executors, where code snippets
are directly fed to the models for execution, and outputs are returned. We are
the first to comprehensively examine this feasibility across various LLMs,
including OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the
o1 model achieved over 90% accuracy in code execution, while others
demonstrated lower accuracy levels. Furthermore, we introduce an Iterative
Instruction Prompting (IIP) technique that processes code snippets line by
line, enhancing the accuracy of weaker models by an average of 7.22% (with the
highest improvement of 18.96%) and an absolute average improvement of 3.86%
against CoT prompting (with the highest improvement of 19.46%). Our study not
only highlights the transformative potential of LLMs in coding but also lays
the groundwork for future advancements in automated programming and the
completion of complex tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subtle Errors Matter: Preference Learning via Error-injected
  Self-<span class="highlight-title">editing</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited strong mathematical reasoning and
computational prowess, tackling tasks ranging from basic arithmetic to advanced
competition-level problems. However, frequently occurring subtle errors, such
as miscalculations or incorrect substitutions, limit the models' full
mathematical potential. Existing studies to improve mathematical ability
typically involve distilling reasoning skills from stronger LLMs or applying
preference learning to step-wise response pairs. Although these methods
leverage samples of varying granularity to mitigate reasoning errors, they
overlook the frequently occurring subtle errors. A major reason is that sampled
preference pairs involve differences unrelated to the errors, which may
distract the model from focusing on subtle errors. In this work, we propose a
novel preference learning framework called eRror-Injected Self-Editing (RISE),
which injects predefined subtle errors into partial tokens of correct solutions
to construct hard pairs for error mitigation. In detail, RISE uses the model
itself to edit a small number of tokens in the solution, injecting designed
subtle errors. Then, pairs composed of self-edited solutions and their
corresponding correct ones, along with pairs of correct and incorrect solutions
obtained through sampling, are used together for subtle error-aware DPO
training. Compared with other preference learning methods, RISE further refines
the training objective to focus on predefined errors and their tokens, without
requiring fine-grained sampling or preference annotation. Extensive experiments
validate the effectiveness of RISE, with preference learning on
Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on
MATH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree of Problems: Improving structured problem solving with
  compositionality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armel Zebaze, Benoît Sagot, Rachel Bawden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance across
multiple tasks through in-context learning. For complex reasoning tasks that
require step-by-step thinking, Chain-of-Thought (CoT) prompting has given
impressive results, especially when combined with self-consistency.
Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree
of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing
the complex problem into paths of subproblems. In this paper, we propose Tree
of Problems (ToP), a simpler version of ToT, which we hypothesise can work
better for complex tasks that can be divided into identical subtasks. Our
empirical results show that our approach outperforms ToT and GoT, and in
addition performs better than CoT on complex reasoning tasks. All code for this
paper is publicly available here:
https://github.com/ArmelRandy/tree-of-problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ETA: Evaluating Then Aligning Safety of Vision Language Models at
  Inference Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ding, Bolian Li, Ru<span class="highlight-author">qi Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have become essential backbones for multimodal
intelligence, yet significant safety challenges limit their real-world
application. While textual inputs are often effectively safeguarded,
adversarial visual inputs can easily bypass VLM defense mechanisms. Existing
defense methods are either resource-intensive, requiring substantial data and
compute, or fail to simultaneously ensure safety and usefulness in responses.
To address these limitations, we propose a novel two-phase inference-time
alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual
contents and output responses to establish a robust safety awareness in
multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep
levels by conditioning the VLMs' generative distribution with an interference
prefix and performing sentence-level best-of-N to search the most harmless and
helpful generation paths. Extensive experiments show that ETA outperforms
baseline methods in terms of harmlessness, helpfulness, and efficiency,
reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%
win-ties in GPT-4 helpfulness evaluation. The code is publicly available at
https://github.com/DripNowhy/ETA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFeR: Improving Evaluation and <span class="highlight-title">Reasoning</span> through Hierarchy of Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaswanth Narsupalli, Abhranil Chandra, Sreevatsa Muppirala, Manish Gupta, Pawan Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the quality of outputs generated by generative models, such as
large language models and vision language models, presents notable challenges.
Traditional methods for evaluation typically rely on either human assessments,
which are resource-intensive, or automatic metrics that often show a low
correlation with human judgment. Another common approach is to use deep
learning systems, which not only consume a substantial amount of compute and
time but also require extensive training data. In this study, we introduce a
tuning-free framework called ReFeR, designed to evaluate generative outputs,
including both text and images, by leveraging a 2-level hierarchy of LLMs and
VLMs themselves. We rigorously evaluate our framework, ReFeR, across four
diverse evaluation tasks. The framework not only improves the accuracy of these
evaluations, surpassing previous benchmarks but also generates constructive
feedback. Interestingly, the framework is also applicable to reasoning tasks.
Experiments on four reasoning tasks demonstrate superior collective reasoning
abilities of the framework. We present two variants of the framework:
ReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a
more cost-effective solution. ReFeR-Lite is $\sim7.7\times$ more efficient
while being comparably accurate to ReFeR-Turbo. We make code, data and PIP
package publicly available. See this PIP URL
https://pypi.org/project/refer-agents/ and this Git URL
https://github.com/yaswanth-iitkgp/ReFeR_Code .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private prediction for large-scale synthetic text <span class="highlight-title">generation</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for generating differentially private synthetic text
using large language models (LLMs), via private prediction. In the private
prediction framework, we only require the output synthetic data to satisfy
differential privacy guarantees. This is in contrast to approaches that train a
generative model on potentially sensitive user-supplied source data and seek to
ensure the model itself is safe to release.
  We prompt a pretrained LLM with source data, but ensure that next-token
predictions are made with differential privacy guarantees. Previous work in
this paradigm reported generating a small number of examples (<10) at
reasonable privacy levels, an amount of data that is useful only for downstream
in-context learning or prompting. In contrast, we make changes that allow us to
generate thousands of high-quality synthetic data points, greatly expanding the
set of potential applications. Our improvements come from an improved privacy
analysis and a better private selection mechanism, which makes use of the
equivalence between the softmax layer for sampling tokens in LLMs and the
exponential mechanism. Furthermore, we introduce a novel use of public
predictions via the sparse vector technique, in which we do not pay privacy
costs for tokens that are predictable without sensitive data; we find this to
be particularly effective for structured data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages; updated figure + some new experiments from EMNLP 2024
  findings camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data
  Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinayak Arannil, Neha Narwal, Sourav Sanjukta Bhabesh, Sai Nikhil Thirandas, Darren Yow-Bang Wang, Graham Horwood, Alex Anto Chirayath, Gouri Pandeshwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable ability to generalize
effectively across numerous industry domains while executing a range of tasks.
Many of these competencies are obtained from the data utilized during the
pre-training phase of the Language Models (LMs). However, these models exhibit
limitations when tasked with performing in specialized or low-resource industry
domains. More recent approaches use LLMs for generating domain-specific
synthetic data but most often they lack in truthfulness and complexity.
Alternatively, in cases where domain data is available like healthcare and
finance most of the LMs are proprietary necessitating the need for a scalable
method to curate real world industry specific pre-training data. In this work,
we propose an automated and scalable framework - DoPAMine:Domain-specific
Pre-training Adaptation from seed-guided data Mining, to mine domain specific
training data from a large data corpus for domain adaptation of a LM. The
framework leverages the parametric knowledge of a LLM to generate diverse and
representative seed data tailored to a specific domain which is then used to
mine real world data from a large data corpus like Common Crawl. We evaluated
our framework's performance in the continual pre-training (CPT) setting by
training two domain specific 7B parameter LMs in healthcare and finance with
data mined via DoPAMine. Our experiments show that DoPAMine boosts the
performance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and
5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and
PubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings
respectively on finance tasks from FiQA-SA, FPB and Headlines datasets when
compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed
  Embeddings <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embedding is one of the most important components in natural language
processing, but interpreting high-dimensional embeddings remains a challenging
problem. To address this problem, Independent Component Analysis (ICA) is
identified as an effective solution. ICA-transformed word embeddings reveal
interpretable semantic axes; however, the order of these axes are arbitrary. In
this study, we focus on this property and propose a novel method, Axis Tour,
which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional
word embedding method, we aim to improve the clarity of the word embedding
space by maximizing the semantic continuity of the axes. Furthermore, we show
through experiments on downstream tasks that Axis Tour yields better or
comparable low-dimensional embeddings compared to both PCA and ICA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings (short)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Contexts Are Equal: Teaching <span class="highlight-title">LLM</span>s Credibility-aware <span class="highlight-title">Generation</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06809v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06809v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models has led to the widespread
adoption of Retrieval-Augmented Generation (RAG), which integrates external
knowledge to alleviate knowledge bottlenecks and mitigate hallucinations.
However, the existing RAG paradigm inevitably suffers from the impact of flawed
information introduced during the retrieval phrase, thereby diminishing the
reliability and correctness of the generated outcomes. In this paper, we
propose Credibility-aware Generation (CAG), a universally applicable framework
designed to mitigate the impact of flawed information in RAG. At its core, CAG
aims to equip models with the ability to discern and process information based
on its credibility. To this end, we propose an innovative data transformation
framework that generates data based on credibility, thereby effectively
endowing models with the capability of CAG. Furthermore, to accurately evaluate
the models' capabilities of CAG, we construct a comprehensive benchmark
covering three critical real-world scenarios. Experimental results demonstrate
that our model can effectively understand and utilize credibility for
generation, significantly outperform other models with retrieval augmentation,
and exhibit resilience against the disruption caused by noisy documents,
thereby maintaining robust performance. Moreover, our model supports customized
credibility, offering a wide range of potential applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Our code, benchmark, and
  models are available at https://github.com/panruotong/CAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactuals As a Means for Evaluating Faithfulness of Attribution
  Methods in Autoregressive Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11252v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11252v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Kamahi, Yadollah Yaghoobzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread adoption of autoregressive language models,
explainability evaluation research has predominantly focused on span infilling
and masked language models. Evaluating the faithfulness of an explanation
method -- how accurately it explains the inner workings and decision-making of
the model -- is challenging because it is difficult to separate the model from
its explanation. Most faithfulness evaluation techniques corrupt or remove
input tokens deemed important by a particular attribution (feature importance)
method and observe the resulting change in the model's output. However, for
autoregressive language models, this approach creates out-of-distribution
inputs due to their next-token prediction training objective. In this study, we
propose a technique that leverages counterfactual generation to evaluate the
faithfulness of attribution methods for autoregressive language models. Our
technique generates fluent, in-distribution counterfactuals, making the
evaluation protocol more reliable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BlackboxNLP @ EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictability maximization and the origins of word order harmony 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16570v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16570v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramon Ferrer-i-Cancho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the linguistic problem of the sequential arrangement of a head and
its dependents from an information theoretic perspective. In particular, we
consider the optimal placement of a head that maximizes the predictability of
the sequence. We assume that dependents are statistically independent given a
head, in line with the open-choice principle and the core assumptions of
dependency grammar. We demonstrate the optimality of harmonic order, i.e.,
placing the head last maximizes the predictability of the head whereas placing
the head first maximizes the predictability of dependents. We also show that
postponing the head is the optimal strategy to maximize its predictability
while bringing it forward is the optimal strategy to maximize the
predictability of dependents. We unravel the advantages of the strategy of
maximizing the predictability of the head over maximizing the predictability of
dependents. Our findings shed light on the placements of the head adopted by
real languages or emerging in different kinds of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor corrections; references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vocabulary Transfer for Medical Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Singh, Vladislav D. Mosin, Ivan P. Yamshchikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Working within specific NLP subdomains presents significant challenges,
primarily due to a persistent deficit of data. Stringent privacy concerns and
limited data accessibility often drive this shortage. Additionally, the medical
domain demands high accuracy, where even marginal improvements in model
performance can have profound impacts. In this study, we investigate the
potential of vocabulary transfer to enhance model performance in biomedical NLP
tasks. Specifically, we focus on vocabulary extension, a technique that
involves expanding the target vocabulary to incorporate domain-specific
biomedical terms. Our findings demonstrate that vocabulary extension, leads to
measurable improvements in both downstream model performance and inference
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies
  for Zero-shot Knowledge-based <span class="highlight-title">VQA</span> <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12746v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12746v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miaoyu Li, Haoxin Li, Zilin Du, Boyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based Visual Question-answering (K-VQA) often requires the use of
background knowledge beyond the image. However, we discover that a single
knowledge generation strategy is often insufficient for all K-VQA questions. To
this end, we propose Diversification, Evidence Truncation, and Combination for
Knowledge-based Elucidation (DietCoke), which utilizes a bundle of
complementary question-answering tactics and aggregates their answers using
textual rationales. DietCoke comprises of three stages: diversification,
rationalization, and ensemble. The diversification stage generates three
distinctive decision contexts, each leading to its own answer candidate. The
rationalization stage generates two rationales, the automatic rationale and the
mechanistic rationale, for each answer candidate using decorrelated techniques.
Finally, in the ensemble stage, an LLM informed by the rationales selects one
answer from the three candidates. Experiments show that DietCoke significantly
outperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on
A-OKVOA and that the strategies in the ensembles are highly complementary. Code
is available at: https://github.com/limiaoyu/DietCoke
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  Prompts Do Not Improve Performances of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of
its default system prompt. Despite current practices of adding personas to
system prompts, it remains unclear how different personas affect a model's
performance on objective tasks. In this study, we present a systematic
evaluation of personas in system prompts. We curate a list of 162 roles
covering 6 types of interpersonal relationships and 8 domains of expertise.
Through extensive analysis of 4 popular families of LLMs and 2,410 factual
questions, we demonstrate that adding personas in system prompts does not
improve model performance across a range of questions compared to the control
setting where no persona is added. Nevertheless, further analysis suggests that
the gender, type, and domain of the persona can all influence the resulting
prediction accuracies. We further experimented with a list of persona search
strategies and found that, while aggregating results from the best persona for
each question significantly improves prediction accuracy, automatically
identifying the best persona is challenging, with predictions often performing
no better than random selection. Overall, our findings suggest that while
adding a persona may lead to performance gains in certain settings, the effect
of each persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeteoRA: Multiple-tasks Embedded LoRA for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Xu, Junyu Lai, Yunpeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretrain+fine-tune paradigm is foundational for deploying large language
models (LLMs) across various downstream applications. Within this framework,
Low-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning
(PEFT), producing numerous reusable task-specific LoRA adapters. However, this
approach requires explicit task intention selection, posing challenges for
autonomous task sensing and switching during inference with multiple existing
LoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA
(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses
multiple task-specific LoRA adapters into the base LLM via a full-mode
Mixture-of-Experts (MoE) architecture. This framework also includes novel MoE
forward acceleration strategies to address the efficiency challenges of
traditional MoE implementations. Our evaluation, using the LlaMA2-13B and
LlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,
demonstrates equivalent performance with the traditional PEFT method. Moreover,
the LLM equipped with MeteoRA achieves superior performance in handling
composite tasks, effectively solving ten sequential problems in a single
inference pass, thereby demonstrating the framework's enhanced capability for
timely adapter switching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linguistic Structure from a Bottleneck on Sequential Information
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Futrell, Michael Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human language is a unique form of communication in the natural world,
distinguished by its structured nature. Most fundamentally, it is systematic,
meaning that signals can be broken down into component parts that are
individually meaningful -- roughly, words -- which are combined in a regular
way to form sentences. Furthermore, the way in which these parts are combined
maintains a kind of locality: words are usually concatenated together, and they
form contiguous phrases, keeping related parts of sentences close to each
other. We address the challenge of understanding how these basic properties of
language arise from broader principles of efficient communication under
information processing constraints. Here we show that natural-language-like
systematicity arises in codes that are constrained by predictive information, a
measure of the amount of information that must be extracted from the past of a
sequence in order to predict its future. In simulations, we show that such
codes approximately factorize their source distributions, and then express the
resulting factors systematically and locally. Next, in a series of
cross-linguistic corpus studies, we show that human languages are structured to
have low predictive information at the levels of phonology, morphology, syntax,
and semantics. Our result suggests that human language performs a sequential,
discrete form of Independent Components Analysis on the statistical
distribution over meanings that need to be expressed. It establishes a link
between the statistical and algebraic structure of human language, and
reinforces the idea that the structure of human language is shaped by
communication under cognitive constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4V Cannot Generate Radiology Reports Yet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT-4V's purported strong multimodal abilities raise interests in using it to
automate radiology report writing, but there lacks thorough evaluations. In
this work, we perform a systematic evaluation of GPT-4V in generating radiology
reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt
to directly generate reports using GPT-4V through different prompting
strategies and find that it fails terribly in both lexical metrics and clinical
efficacy metrics. To understand the low performance, we decompose the task into
two steps: 1) the medical image reasoning step of predicting medical condition
labels from images; and 2) the report synthesis step of generating reports from
(groundtruth) conditions. We show that GPT-4V's performance in image reasoning
is consistently low across different prompts. In fact, the distributions of
model-predicted labels remain constant regardless of which groundtruth
conditions are present on the image, suggesting that the model is not
interpreting chest X-rays meaningfully. Even when given groundtruth conditions
in report synthesis, its generated reports are less correct and less
natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt
on the viability of using GPT-4V in a radiology workflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 3 figures, code:
  https://github.com/YuyangJ0/GPT-4V-evaluation-radiology-report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Higher-Order Correlations Among Semantic Components in
  Embeddings <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Momose Oyama, Hiroaki Yamagiwa, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Independent Component Analysis (ICA) offers interpretable semantic components
of embeddings. While ICA theory assumes that embeddings can be linearly
decomposed into independent components, real-world data often do not satisfy
this assumption. Consequently, non-independencies remain between the estimated
components, which ICA cannot eliminate. We quantified these non-independencies
using higher-order correlations and demonstrated that when the higher-order
correlation between two components is large, it indicates a strong semantic
association between them, along with many words sharing common meanings with
both components. The entire structure of non-independencies was visualized
using a maximum spanning tree of semantic components. These findings provide
deeper insights into embeddings through ICA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn while Unlearn: An Iterative Unlearning Framework for Generative
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Tang, Ye Liu, Xukai Liu, Kai Zhang, Yanghai Zhang, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in machine learning, particularly in Natural Language
Processing (NLP), have led to the development of sophisticated models trained
on extensive datasets, yet raising concerns about the potential leakage of
sensitive information. In response, regulatory measures such as the European
Union's General Data Protection Regulation (GDPR) have driven increasing
interest in Machine Unlearning techniques, which enable models to selectively
forget specific data entries. Early approaches primarily relied on
pre-processing methods, while more recent research has shifted towards
training-based unlearning techniques. Despite their effectiveness, most
existing methods require access to the original training data, which is often
inaccessible. Additionally, directly applying unlearning techniques bear the
cost of undermining the model's expressive capabilities. To address these
challenges, we introduce the Iterative Contrastive Unlearning (ICU) framework,
which consists of three core components: A Knowledge Unlearning Induction
module designed to remove specific knowledge through an unlearning loss; A
Contrastive Learning Enhancement module to preserve the model's expressive
capabilities against the pure unlearning goal; And an Iterative Unlearning
Refinement module that dynamically assess the unlearning extent on specific
data pieces and make iterative update. Experimental results demonstrate the
efficacy of our ICU method in unlearning sensitive information while
maintaining the model's overall performance, offering a promising solution for
privacy-conscious machine learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scope-enhanced Compositional Semantic Parsing for DRT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiulin Yang, Jonas Groschwitz, Alexander Koller, Johan Bos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discourse Representation Theory (DRT) distinguishes itself from other
semantic representation frameworks by its ability to model complex semantic and
discourse phenomena through structural nesting and variable binding. While
seq2seq models hold the state of the art on DRT parsing, their accuracy
degrades with the complexity of the sentence, and they sometimes struggle to
produce well-formed DRT representations. We introduce the AMS parser, a
compositional, neurosymbolic semantic parser for DRT. It rests on a novel
mechanism for predicting quantifier scope. We show that the AMS parser reliably
produces well-formed outputs and performs well on DRT parsing, especially on
complex sentences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DTVLT: A Multi-modal Diverse Text <span class="highlight-title">Benchmark</span> for Visual Language Tracking
  Based on <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language tracking (VLT) has emerged as a cutting-edge research area,
harnessing linguistic data to enhance algorithms with multi-modal inputs and
broadening the scope of traditional single object tracking (SOT) to encompass
video understanding applications. Despite this, most VLT benchmarks still
depend on succinct, human-annotated text descriptions for each video. These
descriptions often fall short in capturing the nuances of video content
dynamics and lack stylistic variety in language, constrained by their uniform
level of detail and a fixed annotation frequency. As a result, algorithms tend
to default to a "memorize the answer" strategy, diverging from the core
objective of achieving a deeper understanding of video content. Fortunately,
the emergence of large language models (LLMs) has enabled the generation of
diverse text. This work utilizes LLMs to generate varied semantic annotations
(in terms of text lengths and granularities) for representative SOT benchmarks,
thereby establishing a novel multi-modal benchmark. Specifically, we (1)
propose a new visual language tracking benchmark with diverse texts, named
DTVLT, based on five prominent VLT and SOT benchmarks, including three
sub-tasks: short-term tracking, long-term tracking, and global instance
tracking. (2) We offer four granularity texts in our benchmark, considering the
extent and density of semantic information. We expect this multi-granular
generation strategy to foster a favorable environment for VLT and video
understanding research. (3) We conduct comprehensive experimental analyses on
DTVLT, evaluating the impact of diverse text on tracking performance and hope
the identified performance bottlenecks of existing algorithms can support
further research in VLT and video understanding. The proposed benchmark,
experimental results and toolkit will be released gradually on
http://videocube.aitestunion.com/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher M. Ackerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation engineering is becoming increasingly popular as a means of online
control of large language models (LLMs). In this work, I extend the idea of
active steering with vectors that represent a behavioral direction of interest
to tuning those vectors directly into the model, obviating the need for online
control. First, I identify activation vectors related to honesty in an
open-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can
be made more or less honest by adding positive or negative multiples of these
vectors to residual stream activations during generation. Then, I show that a
similar effect can be achieved by fine-tuning the vectors directly into the
model, by use of a dual loss function based on the cosine similarity of
residual stream activations to the vectors combined with a standard token-based
loss ("representation tuning"). Finally, I compare the generations in response
to honesty-probing prompts from the resulting models to those from models
fine-tuned with a token-based loss alone, and to those from the untuned model
subjected to online steering. Overall, fine-tuning the vectors into the models
using the cosine similarity plus token loss showed a stronger effect than
online steering, and generalized better than using the standard loss,
suggesting the potential utility of this approach as a safety measure. Code and
data are available at https://github.com/cma1114/representation_tuning; tuned
models are available at https://huggingface.co/collections/cackerman/
representation-tuning-66da1e5ab41cd1b824687d9f.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Separation based on Contrastive Learning and Deep Modularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10652v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10652v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Ochieng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current monaural state of the art tools for speech separation relies on
supervised learning. This means that they must deal with permutation problem,
they are impacted by the mismatch on the number of speakers used in training
and inference. Moreover, their performance heavily relies on the presence of
high-quality labelled data. These problems can be effectively addressed by
employing a fully unsupervised technique for speech separation. In this paper,
we use contrastive learning to establish the representations of frames then use
the learned representations in the downstream deep modularization task.
Concretely, we demonstrate experimentally that in speech separation, different
frames of a speaker can be viewed as augmentations of a given hidden standard
frame of that speaker. The frames of a speaker contain enough prosodic
information overlap which is key in speech separation. Based on this, we
implement a self-supervised learning to learn to minimize the distance between
frames belonging to a given speaker. The learned representations are used in a
downstream deep modularization task to cluster frames based on speaker
identity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix
shows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively
in WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7
respectively in WSJ0-2mix. Its greatest strength being that as the number of
speakers increase, its performance does not degrade significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2212.00369</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAPE: Data-Adaptive Positional Encoding for Length Extrapolation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14722v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14722v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional encoding plays a crucial role in transformers, significantly
impacting model performance and length generalization. Prior research has
introduced absolute positional encoding (APE) and relative positional encoding
(RPE) to distinguish token positions in given sequences. However, both APE and
RPE remain fixed after model training regardless of input data, limiting their
adaptability and flexibility. Hence, we expect that the desired positional
encoding should be data-adaptive and can be dynamically adjusted with the given
attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)
method, which dynamically and semantically adjusts based on input context and
learned fixed priors. Experimental validation on real-world datasets (Arxiv,
Books3, and CHE) demonstrates that DAPE enhances model performances in terms of
trained length and length generalization, where the improvements are
statistically significant. The model visualization suggests that our model can
keep both local and anti-local information. Finally, we successfully train the
model on sequence length 128 and achieve better performance at evaluation
sequence length 8192, compared with other static positional encoding methods,
revealing the benefit of the adaptive positional encoding method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance
Quesetion Answering (QA) performance of Large Language Models (LLMs), yet
structured KG verbalization remains challengin. Existing methods, such as
triple-form or free-form textual conversion of triple-form facts, encounter
several issues. These include reduced evidence density due to duplicated
entities or relationships, and reduced evidence clarity due to an inability to
emphasize crucial evidence. To address these issues, we propose EFSum, an
Evidence-focused Fact Summarization framework for enhanced QA with
knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer
through distillation and preference alignment. Our extensive experiments show
that EFSum improves LLM's zero-shot QA performance, and it is possible to
ensure both the helpfulness and faithfulness of the summary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAPE V2: Process Attention Score as Feature Map for Length Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism is a fundamental component of the Transformer model,
contributing to interactions among distinct tokens, in contrast to earlier
feed-forward neural networks. In general, the attention scores are determined
simply by the key-query products. However, this work's occasional trial
(combining DAPE and NoPE) of including additional MLPs on attention scores
without position encoding indicates that the classical key-query multiplication
may limit the performance of Transformers. In this work, we conceptualize
attention as a feature map and apply the convolution operator (for neighboring
attention scores across different heads) to mimic the processing methods in
computer vision. Specifically, the main contribution of this paper is
identifying and interpreting the Transformer length extrapolation problem as a
result of the limited expressiveness of the naive query and key dot product,
and we successfully translate the length extrapolation issue into a
well-understood feature map processing problem. The novel insight, which can be
adapted to various attention-related models, reveals that the current
Transformer architecture has the potential for further evolution. Extensive
experiments demonstrate that treating attention as a feature map and applying
convolution as a processing method significantly enhances Transformer
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report. arXiv admin note: text overlap with arXiv:2405.14722</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Noise Robustness of In-Context Learning for Text <span class="highlight-title">Generation</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfu Gao, Feipeng Zhang, Wenyu Jiang, Jun Shu, Feng Zheng, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive performance on downstream
tasks by in-context learning (ICL), which heavily relies on the quality of
demonstrations selected from a large set of annotated examples. Recent works
claim that in-context learning is robust to noisy demonstrations in text
classification. In this work, we show that, on text generation tasks, noisy
annotations significantly hurt the performance of in-context learning. To
circumvent the issue, we propose a simple and effective approach called Local
Perplexity Ranking (LPR), which replaces the "noisy" candidates with their
nearest neighbors that are more likely to be clean. Our method is motivated by
analyzing the perplexity deviation caused by noisy labels and decomposing
perplexity into inherent perplexity and matching perplexity. Our key idea
behind LPR is thus to decouple the matching perplexity by performing the
ranking among the neighbors in semantic space. Our approach can prevent the
selected demonstrations from including mismatched input-label pairs while
preserving the effectiveness of the original selection methods. Extensive
experiments demonstrate the effectiveness of LPR, improving the EM score by up
to 18.75 on common benchmarks with noisy annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Read Over the Lines: Attacking <span class="highlight-title">LLM</span>s and Toxicity Detection Systems with
  ASCII Art to Mask Profanity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18708v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18708v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Berezin, Reza Farahbakhsh, Noel Crespi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel family of adversarial attacks that exploit the inability
of language models to interpret ASCII art. To evaluate these attacks, we
propose the ToxASCII benchmark and develop two custom ASCII art fonts: one
leveraging special tokens and another using text-filled letter shapes. Our
attacks achieve a perfect 1.0 Attack Success Rate across ten models, including
OpenAI's o1-preview and LLaMA 3.1.
  Warning: this paper contains examples of toxic language used for research
purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking the Script Barrier in Multilingual Pre-Trained Language Models
  with Transliteration-Based Post-Training Alignment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orgest Xhelili, Yihong Liu, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual pre-trained models (mPLMs) have shown impressive performance on
cross-lingual transfer tasks. However, the transfer performance is often
hindered when a low-resource target language is written in a different script
than the high-resource source language, even though the two languages may be
related or share parts of their vocabularies. Inspired by recent work that uses
transliteration to address this problem, our paper proposes a
transliteration-based post-pretraining alignment (PPA) method aiming to improve
the cross-lingual alignment between languages using diverse scripts. We select
two areal language groups, $\textbf{Mediterranean-Amharic-Farsi}$ and
$\textbf{South+East Asian Languages}$, wherein the languages are mutually
influenced but use different scripts. We apply our method to these language
groups and conduct extensive experiments on a spectrum of downstream tasks. The
results show that after PPA, models consistently outperform the original model
(up to 50% for some tasks) in English-centric transfer. In addition, when we
use languages other than English as sources in transfer, our method obtains
even larger improvements. We will make our code and models publicly available
at \url{https://github.com/cisnlp/Transliteration-PPA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quest: Query-centric Data Synthesis Approach for Long-context Scaling of
  <span class="highlight-title">Large Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19846v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19846v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaochen Gao, Xing Wu, Qi Fu, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have highlighted the
importance of extending context lengths for handling complex tasks. While
traditional methods for training on long contexts often use filtered long
documents, these approaches lead to domain imbalances, limiting model
performance. To address this, techniques like random document concatenation
(Standard) and similarity-based methods (KNN, ICLM) have been developed.
However, they either sacrifice semantic coherence or diversity. To balance both
aspects, we introduce Quest, a query-centric data synthesis method aggregating
semantically relevant yet diverse documents. Quest uses a generative model to
predict potential queries for each document, grouping documents with similar
queries and keywords. Extensive experiments demonstrate Quest's superior
performance on long-context tasks, achieving remarkable results with context
lengths of up to 1M tokens and confirming its scalability across various model
sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Persona to Personalization: A <span class="highlight-title">Survey</span> on Role-Playing Language
  <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI
systems designed to simulate assigned personas. By harnessing multiple advanced
abilities of LLMs, including in-context learning, instruction following, and
social intelligence, RPLAs achieve a remarkable sense of human likeness and
vivid role-playing performance. RPLAs can mimic a wide range of personas,
ranging from historical figures and fictional characters to real-life
individuals. Consequently, they have catalyzed numerous AI applications, such
as emotional companions, interactive video games, personalized assistants and
copilots, and digital clones. In this paper, we conduct a comprehensive survey
of this field, illustrating the evolution and recent progress in RPLAs
integrating with cutting-edge LLM technologies. We categorize personas into
three types: 1) Demographic Persona, which leverages statistical stereotypes;
2) Character Persona, focused on well-established figures; and 3)
Individualized Persona, customized through ongoing user interactions for
personalized services. We begin by presenting a comprehensive overview of
current methodologies for RPLAs, followed by the details for each persona type,
covering corresponding data sourcing, agent construction, and evaluation.
Afterward, we discuss the fundamental risks, existing limitations, and future
prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI
applications, which reflects practical user demands that shape and drive RPLA
research. Through this work, we aim to establish a clear taxonomy of RPLA
research and applications, and facilitate future research in this critical and
ever-evolving field, and pave the way for a future where humans and RPLAs
coexist in harmony.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What do <span class="highlight-title">Large Language Model</span>s Need for Machine Translation Evaluation? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenbin Qian, Archchana Sindhujan, Minnie Kabra, Diptesh Kanojia, Constantin Orăsan, Tharindu Ranasinghe, Frédéric Blain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models (LLMs) for various natural language
processing tasks has led to superlative claims about their performance. For the
evaluation of machine translation (MT), existing research shows that LLMs are
able to achieve results comparable to fine-tuned multilingual pre-trained
language models. In this paper, we explore what translation information, such
as the source, reference, translation errors and annotation guidelines, is
needed for LLMs to evaluate MT quality. In addition, we investigate prompting
techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for
eight language pairs covering high-, medium- and low-resource languages,
leveraging varying LLM variants. Our findings indicate the importance of
reference translations for an LLM-based evaluation. While larger models do not
necessarily fare better, they tend to benefit more from CoT prompting, than
smaller models. We also observe that LLMs do not always provide a numerical
score when generating evaluations, which poses a question on their reliability
for the task. Our work presents a comprehensive analysis for
resource-constrained and training-less LLM-based evaluation of machine
translation. We release the accrued prompt templates, code and data publicly
for reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithfulness and the Notion of Adversarial Sensitivity in NLP
  Explanations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supriya Manna, Niladri Sett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness is arguably the most critical metric to assess the reliability
of explainable AI. In NLP, current methods for faithfulness evaluation are
fraught with discrepancies and biases, often failing to capture the true
reasoning of models. We introduce Adversarial Sensitivity as a novel approach
to faithfulness evaluation, focusing on the explainer's response when the model
is under adversarial attack. Our method accounts for the faithfulness of
explainers by capturing sensitivity to adversarial input changes. This work
addresses significant limitations in existing evaluation techniques, and
furthermore, quantifies faithfulness from a crucial yet underexplored paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Full Paper at EMNLP 2024 Workshop BlackBoxNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functionality learning through specification instructions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Henrique Luz de Araujo, Benjamin Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test suites assess natural language processing models' performance on
specific functionalities: cases of interest involving model robustness,
fairness, or particular linguistic capabilities. This paper introduces
specification instructions: text descriptions specifying fine-grained
task-specific behaviors. For each functionality in a suite, we generate an
instruction that describes it. We combine the specification instructions to
create specification-augmented prompts, which we feed to language models
pre-trained on natural instruction data.
  We conduct experiments to measure how optimizing for some functionalities may
negatively impact functionalities that are not covered by the specification
set. Our analyses across four tasks and models of diverse sizes and families
show that smaller models struggle to follow specification instructions.
However, larger models (>~3B params.) can benefit from specifications and --
surprisingly -- even generalize certain desirable behaviors across
functionalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 8 figures. Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Vulnerabilities in <span class="highlight-title">Large Language Model</span>s: Backdoor Attacks for
  In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05949v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05949v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning, a paradigm bridging the gap between pre-training and
fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in
few-shot settings. Despite being widely applied, in-context learning is
vulnerable to malicious attacks. In this work, we raise security concerns
regarding this paradigm. Our studies demonstrate that an attacker can
manipulate the behavior of large language models by poisoning the demonstration
context, without the need for fine-tuning the model. Specifically, we design a
new backdoor attack method, named ICLAttack, to target large language models
based on in-context learning. Our method encompasses two types of attacks:
poisoning demonstration examples and poisoning demonstration prompts, which can
make models behave in alignment with predefined intentions. ICLAttack does not
require additional fine-tuning to implant a backdoor, thus preserving the
model's generality. Furthermore, the poisoned examples are correctly labeled,
enhancing the natural stealth of our attack method. Extensive experimental
results across several language models, ranging in size from 1.3B to 180B
parameters, demonstrate the effectiveness of our attack method, exemplified by
a high average attack success rate of 95.0% across the three datasets on OPT
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in
  <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zara Siddique, Liam D. Turner, Luis Espinosa-Anke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been shown to propagate and amplify harmful
stereotypes, particularly those that disproportionately affect marginalised
communities. To understand the effect of these stereotypes more
comprehensively, we introduce GlobalBias, a dataset of 876k sentences
incorporating 40 distinct gender-by-ethnicity groups alongside descriptors
typically used in bias literature, which enables us to study a broad set of
stereotypes from around the world. We use GlobalBias to directly probe a suite
of LMs via perplexity, which we use as a proxy to determine how certain
stereotypes are represented in the model's internal representations. Following
this, we generate character profiles based on given names and evaluate the
prevalence of stereotypes in model outputs. We find that the demographic groups
associated with various stereotypes remain consistent across model likelihoods
and model outputs. Furthermore, larger models consistently display higher
levels of stereotypical outputs, even when explicitly instructed not to.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Main 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose
  Protein Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proteins, as essential biomolecules, play a central role in biological
processes, including metabolic reactions and DNA replication. Accurate
prediction of their properties and functions is crucial in biological
applications. Recent development of protein language models (pLMs) with
supervised fine tuning provides a promising solution to this problem. However,
the fine-tuned model is tailored for particular downstream prediction task, and
achieving general-purpose protein understanding remains a challenge. In this
paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)
framework to bridge this gap. Our approach integrates a noval structure-aware
module into pLMs to inform them with structural knowledge, and then connects
these enhanced pLMs to large language models (LLMs) to generate understanding
of proteins. In this framework, we propose a novel two-stage instruction tuning
pipeline that first establishes a basic understanding of proteins through
caption-based instructions and then refines this understanding using a mixture
of experts (MoEs) to learn more complex properties and functional information
with the same amount of activated parameters. Moreover, we construct the
largest and most comprehensive protein instruction dataset to date, which
allows us to train and evaluate the general-purpose protein understanding
model. Extensive experimental results on open-ended generation and closed-set
answer tasks demonstrate the superior performance of SEPIT over both
closed-source general LLMs and open-source LLMs trained with protein knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Subtle Differences between Human and Model Languages Using
  Spectrum of Relative Likelihood 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Yu Wang, Hao An, Zhichen Liu, Yongyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human and model-generated texts can be distinguished by examining the
magnitude of likelihood in language. However, it is becoming increasingly
difficult as language model's capabilities of generating human-like texts keep
evolving. This study provides a new perspective by using the relative
likelihood values instead of absolute ones, and extracting useful features from
the spectrum-view of likelihood for the human-model text detection task. We
propose a detection procedure with two classification methods, supervised and
heuristic-based, respectively, which results in competitive performances with
previous zero-shot detection methods and a new state-of-the-art on short-text
detection. Our method can also reveal subtle differences between human and
model languages, which find theoretical roots in psycholinguistics studies. Our
code is available at https://github.com/CLCS-SUSTech/FourierGPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniMEEC: Towards Unified <span class="highlight-title">Multimodal</span> Emotion Recognition and Emotion
  Cause 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guimin Hu, Zhihong Zhu, Daniel Hershcovich, Lijie Hu, Hasti Seifi, Jiayuan Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition in conversation (MERC) and multimodal
emotion-cause pair extraction (MECPE) have recently garnered significant
attention. Emotions are the expression of affect or feelings; responses to
specific events, or situations -- known as emotion causes. Both collectively
explain the causality between human emotion and intents. However, existing
works treat emotion recognition and emotion cause extraction as two individual
problems, ignoring their natural causality. In this paper, we propose a Unified
Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC)
to explore the causality between emotion and emotion cause. Concretely, UniMEEC
reformulates the MERC and MECPE tasks as mask prediction problems and unifies
them with a causal prompt template. To differentiate the modal effects, UniMEEC
proposes a multimodal causal prompt to probe the pre-trained knowledge
specified to modality and implements cross-task and cross-modality interactions
under task-oriented settings. Experiment results on four public benchmark
datasets verify the model performance on MERC and MECPE tasks and achieve
consistent improvements compared with the previous state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling In-Context Learning: A Coordinate System to Understand Its
  Working Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anhao Zhao, Fanghua Ye, Jinlan Fu, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable in-context learning (ICL)
capabilities. However, the underlying working mechanism of ICL remains poorly
understood. Recent research presents two conflicting views on ICL: One
emphasizes the impact of similar examples in the demonstrations, stressing the
need for label correctness and more shots. The other attributes it to LLMs'
inherent ability of task recognition, deeming label correctness and shot
numbers of demonstrations as not crucial. In this work, we provide a
Two-Dimensional Coordinate System that unifies both views into a systematic
framework. The framework explains the behavior of ICL through two orthogonal
variables: whether similar examples are presented in the demonstrations
(perception) and whether LLMs can recognize the task (cognition). We propose
the peak inverse rank metric to detect the task recognition ability of LLMs and
study LLMs' reactions to different definitions of similarity. Based on these,
we conduct extensive experiments to elucidate how ICL functions across each
quadrant on multiple representative classification tasks. Finally, we extend
our analyses to generation tasks, showing that our coordinate system can also
be used to interpret ICL for generation tasks effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Calibrating the Confidence of <span class="highlight-title">Large Language Model</span>s by Eliciting
  Fidelity <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, <span class="highlight-author">Xipeng Qiu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models optimized with techniques like RLHF have achieved good
alignment in being helpful and harmless. However, post-alignment, these
language models often exhibit overconfidence, where the expressed confidence
does not accurately calibrate with their correctness rate. In this paper, we
decompose the language model confidence into the \textit{Uncertainty} about the
question and the \textit{Fidelity} to the answer generated by language models.
Then, we propose a plug-and-play method to estimate the confidence of language
models. Our method has shown good calibration performance by conducting
experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two
novel metrics, IPR and CE, to evaluate the calibration of the model, and we
have conducted a detailed discussion on \textit{Truly Well-Calibrated
Confidence}. Our method could serve as a strong baseline, and we hope that this
work will provide some insights into the model confidence calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Academic Skills Assessment with NLP and Ensemble Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpei Cheng, Yingyi Wu, Danyang Zhang, Jiacheng Hu, Yujian Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the critical challenges of assessing foundational
academic skills by leveraging advancements in natural language processing
(NLP). Traditional assessment methods often struggle to provide timely and
comprehensive feedback on key cognitive and linguistic aspects, such as
coherence, syntax, and analytical reasoning. Our approach integrates multiple
state-of-the-art NLP models, including BERT, RoBERTa, BART, DeBERTa, and T5,
within an ensemble learning framework. These models are combined through
stacking techniques using LightGBM and Ridge regression to enhance predictive
accuracy. The methodology involves detailed data preprocessing, feature
extraction, and pseudo-label learning to optimize model performance. By
incorporating sophisticated NLP techniques and ensemble learning, this study
significantly improves the accuracy and efficiency of assessments, offering a
robust solution that surpasses traditional methods and opens new avenues for
educational technology research focused on enhancing core academic
competencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SBoRA: Low-Rank Adaptation with Regional Weight Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05413v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05413v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lai-Man Po, Yuyang Liu, Haoxuan Wu, Tian<span class="highlight-author">qi Zhang</span>, Wing-Yin Yu, Zhuohan Wang, Zeyu Jiang, Kun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Standard Basis LoRA (SBoRA), a novel
parameter-efficient fine-tuning approach for Large Language Models that builds
upon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal
Adaptation. SBoRA reduces the number of trainable parameters by half or doubles
the rank with the similar number of trainable parameters as LoRA, while
improving learning performance. By utilizing orthogonal standard basis vectors
to initialize one of the low-rank matrices (either $\mathbf{A}$ or
$\mathbf{B}$), SBoRA facilitates regional weight updates and memory-efficient
fine-tuning. This results in two variants, SBoRA-FA and SBoRA-FB, where only
one of the matrices is updated, leading to a sparse update matrix
$\mathrm{\Delta} \mathbf{W}$ with predominantly zero rows or columns.
Consequently, most of the fine-tuned model's weights
$(\mathbf{W}_0+\mathrm{\Delta} \mathbf{W})$ remain unchanged from the
pre-trained weights, akin to the modular organization of the human brain, which
efficiently adapts to new tasks. Our empirical results demonstrate the
superiority of SBoRA-FA over LoRA in various fine-tuning tasks, including
commonsense reasoning and arithmetic reasoning. Furthermore, we evaluate the
effectiveness of QSBoRA on quantized LLaMA models of varying scales,
highlighting its potential for efficient adaptation to new tasks. Code is
available at https://github.com/cityuhkai/SBoRA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">Large Language Model</span>s on Time Series Feature Understanding: A
  Comprehensive Taxonomy and <span class="highlight-title">Benchmark</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Fons, Rachneet Kaur, Soham Palande, Zhen Zeng, Tucker Balch, Manuela Veloso, Svitlana Vyetrenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) offer the potential for automatic time series
analysis and reporting, which is a critical task across many domains, spanning
healthcare, finance, climate, energy, and many more. In this paper, we propose
a framework for rigorously evaluating the capabilities of LLMs on time series
understanding, encompassing both univariate and multivariate forms. We
introduce a comprehensive taxonomy of time series features, a critical
framework that delineates various characteristics inherent in time series data.
Leveraging this taxonomy, we have systematically designed and synthesized a
diverse dataset of time series, embodying the different outlined features, each
accompanied by textual descriptions. This dataset acts as a solid foundation
for assessing the proficiency of LLMs in comprehending time series. Our
experiments shed light on the strengths and limitations of state-of-the-art
LLMs in time series understanding, revealing which features these models
readily comprehend effectively and where they falter. In addition, we uncover
the sensitivity of LLMs to factors including the formatting of the data, the
position of points queried within a series and the overall time series length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressively Label Enhancement for <span class="highlight-title">Large Language Model</span> Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Liu, Ning Xu, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) alignment aims to prevent models from producing
content that misaligns with human expectations, which can lead to ethical and
legal concerns. In the last few years, Reinforcement Learning from Human
Feedback (RLHF) has been the most prominent method for achieving alignment. Due
to challenges in stability and scalability with RLHF stages, which arise from
the complex interactions between multiple models, researchers are exploring
alternative methods to achieve effects comparable to those of RLHF. However,
these methods often rely on large high-quality datasets. Despite some methods
considering the generation of additional data to expand datasets, they often
treat model training and data generation as separate and static processes,
overlooking the fact that these processes are highly interdependent, leading to
inefficient utilization of the generated data. To deal with this problem, we
propose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a
framework that dynamically adjusts the model's training process based on the
evolving quality of the generated data. Specifically, we prompt the model to
generate responses for both the original query and the query guided by a set of
carefully designed principles, and then utilize a dynamic threshold to
determine the appropriate training approach for both responses based on their
corresponding reward scores. Experimental results demonstrate the effectiveness
of PLE compared to existing LLM alignment methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Ego: Towards Building Egocentric <span class="highlight-title">Multimodal</span> <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research aims to comprehensively explore building a multimodal
foundation model for egocentric video understanding. To achieve this goal, we
work on three fronts. First, as there is a lack of QA data for egocentric video
understanding, we develop a data engine that efficiently generates 7M
high-quality QA samples for egocentric videos ranging from 30 seconds to one
hour long, based on human-annotated data. This is currently the largest
egocentric QA dataset. Second, we contribute a challenging egocentric QA
benchmark with 629 videos and 7,026 questions to evaluate the models' ability
in recognizing and memorizing visual details across videos of varying lengths.
We introduce a new de-biasing evaluation method to help mitigate the
unavoidable language bias present in the models being evaluated. Third, we
propose a specialized multimodal architecture featuring a novel "Memory Pointer
Prompting" mechanism. This design includes a global glimpse step to gain an
overarching understanding of the entire video and identify key visual
information, followed by a fallback step that utilizes the key visual
information to generate responses. This enables the model to more effectively
comprehend extended video content. With the data, benchmark, and model, we
successfully build MM-Ego, an egocentric multimodal LLM that shows powerful
performance on egocentric video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do better language models have crisper vision? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How well do text-only Large Language Models (LLMs) grasp the visual world? As
LLMs are increasingly used in computer vision, addressing this question becomes
both fundamental and pertinent. However, existing studies have primarily
focused on limited scenarios, such as their ability to generate visual content
or cluster multimodal data. To this end, we propose the Visual Text
Representation Benchmark (ViTeRB) to isolate key properties that make language
models well-aligned with the visual world. With this, we identify large-scale
decoder-based LLMs as ideal candidates for representing text in vision-centric
contexts, counter to the current practice of utilizing text encoders. Building
on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.
By leveraging precomputable frozen features from strong vision and language
models, ShareLock achieves an impressive 51% accuracy on ImageNet despite
utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU
hour (or 10 hours including the precomputation of features) - orders of
magnitude less than prior methods. Code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IterComp: Iterative Composition-Aware Feedback Learning from Model
  Gallery for Text-to-<span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made
notable strides in compositional text-to-image generation. However, these
methods typically exhibit distinct strengths for compositional generation, with
some excelling in handling attribute binding and others in spatial
relationships. This disparity highlights the need for an approach that can
leverage the complementary strengths of various models to comprehensively
improve the composition capability. To this end, we introduce IterComp, a novel
framework that aggregates composition-aware model preferences from multiple
models and employs an iterative feedback learning approach to enhance
compositional generation. Specifically, we curate a gallery of six powerful
open-source diffusion models and evaluate their three key compositional
metrics: attribute binding, spatial relationships, and non-spatial
relationships. Based on these metrics, we develop a composition-aware model
preference dataset comprising numerous image-rank pairs to train
composition-aware reward models. Then, we propose an iterative feedback
learning method to enhance compositionality in a closed-loop manner, enabling
the progressive self-refinement of both the base diffusion model and reward
models over multiple iterations. Theoretical proof demonstrates the
effectiveness and extensive experiments show our significant superiority over
previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category
object composition and complex semantic alignment. IterComp opens new research
avenues in reward feedback learning for diffusion models and compositional
generation. Code: https://github.com/YangLing0818/IterComp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://github.com/YangLing0818/IterComp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering Cross-Modal Alignment in Large <span class="highlight-title">Vision-Language Model</span>s with
  Modality Integration Rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Modality Integration Rate (MIR), an effective, robust, and
generalized metric to indicate the multi-modal pre-training quality of Large
Vision Language Models (LVLMs). Large-scale pre-training plays a critical role
in building capable LVLMs, while evaluating its training quality without the
costly supervised fine-tuning stage is under-explored. Loss, perplexity, and
in-context evaluation results are commonly used pre-training metrics for Large
Language Models (LLMs), while we observed that these metrics are less
indicative when aligning a well-trained LLM with a new modality. Due to the
lack of proper metrics, the research of LVLMs in the critical pre-training
stage is hindered greatly, including the training data choice, efficient module
design, etc. In this paper, we propose evaluating the pre-training quality from
the inter-modal distribution distance perspective and present MIR, the Modality
Integration Rate, which is 1) \textbf{Effective} to represent the pre-training
quality and show a positive relation with the benchmark performance after
supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation
data. 3) \textbf{Generalize} across training configurations and architecture
choices. We conduct a series of pre-training experiments to explore the
effectiveness of MIR and observe satisfactory results that MIR is indicative
about training data selection, training strategy schedule, and model
architecture design to get better pre-training results. We hope MIR could be a
helpful metric for building capable LVLMs and inspire the following research
about modality alignment in different areas. Our code is at:
https://github.com/shikiw/Modality-Integration-Rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/shikiw/Modality-Integration-Rate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AvatarGO: Zero-shot 4D Human-Object Interaction <span class="highlight-title">Generation</span> and Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Cao, Liang Pan, Kai Han, Kwan-Yee K. Wong, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have led to significant improvements
in the generation and animation of 4D full-body human-object interactions
(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion
generation, which is limited by the scarcity of realistic large-scale
interaction data. This constraint affects their ability to create everyday HOI
scenes. This paper addresses this challenge using a zero-shot approach with a
pre-trained diffusion model. Despite this potential, achieving our goals is
difficult due to the diffusion model's lack of understanding of ''where'' and
''how'' objects interact with the human body. To tackle these issues, we
introduce AvatarGO, a novel framework designed to generate animatable 4D HOI
scenes directly from textual inputs. Specifically, 1) for the ''where''
challenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to
identify the contact body part from text prompts, ensuring precise
representation of human-object spatial relations. 2) For the ''how'' challenge,
we introduce correspondence-aware motion optimization that constructs motion
fields for both human and object models using the linear blend skinning
function from SMPL-X. Our framework not only generates coherent compositional
motions, but also exhibits greater robustness in handling penetration issues.
Extensive experiments with existing methods validate AvatarGO's superior
generation and animation capabilities on a variety of human-object pairs and
diverse poses. As the first attempt to synthesize 4D avatars with object
interactions, we hope AvatarGO could open new doors for human-centric 4D
content creation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yukangcao.github.io/AvatarGO/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructG2I: Synthesizing <span class="highlight-title">Image</span>s from <span class="highlight-title">Multimodal</span> Attributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we approach an overlooked yet critical task Graph2Image:
generating images from multimodal attributed graphs (MMAGs). This task poses
significant challenges due to the explosion in graph size, dependencies among
graph entities, and the need for controllability in graph conditions. To
address these challenges, we propose a graph context-conditioned diffusion
model called InstructG2I. InstructG2I first exploits the graph structure and
multimodal information to conduct informative neighbor sampling by combining
personalized page rank and re-ranking based on vision-language features. Then,
a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary
set of graph prompts to guide the denoising process of diffusion. Finally, we
propose graph classifier-free guidance, enabling controllable generation by
varying the strength of graph guidance and multiple connected edges to a node.
Extensive experiments conducted on three datasets from different domains
demonstrate the effectiveness and controllability of our approach. The code is
available at https://github.com/PeterGriffinJin/InstructG2I.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trans4D: Realistic Geometry-Aware Transition for Compositional
  Text-to-4D Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Zeng, Ling Yang, Siyu Li, Jiaming Liu, Zixiang Zhang, Juanxi Tian, Kaixin Zhu, Yongzhen Guo, Fu-Yun Wang, Minkai Xu, Stefano Ermon, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have demonstrated exceptional
capabilities in image and video generation, further improving the effectiveness
of 4D synthesis. Existing 4D generation methods can generate high-quality 4D
objects or scenes based on user-friendly conditions, benefiting the gaming and
video industries. However, these methods struggle to synthesize significant
object deformation of complex 4D transitions and interactions within scenes. To
address this challenge, we propose Trans4D, a novel text-to-4D synthesis
framework that enables realistic complex scene transitions. Specifically, we
first use multi-modal large language models (MLLMs) to produce a physic-aware
scene description for 4D scene initialization and effective transition timing
planning. Then we propose a geometry-aware 4D transition network to realize a
complex scene-level 4D transition based on the plan, which involves expressive
geometrical object deformation. Extensive experiments demonstrate that Trans4D
consistently outperforms existing state-of-the-art methods in generating 4D
scenes with accurate and high-quality transitions, validating its
effectiveness. Code: https://github.com/YangLing0818/Trans4D
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://github.com/YangLing0818/Trans4D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based
  Multi-Entity Action Recognition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Wen, Mengyuan Liu, Songtao Wu, Beichen Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based multi-entity action recognition is a challenging task aiming
to identify interactive actions or group activities involving multiple diverse
entities. Existing models for individuals often fall short in this task due to
the inherent distribution discrepancies among entity skeletons, leading to
suboptimal backbone optimization. To this end, we introduce a Convex Hull
Adaptive Shift based multi-Entity action recognition method (CHASE), which
mitigates inter-entity distribution gaps and unbiases subsequent backbones.
Specifically, CHASE comprises a learnable parameterized network and an
auxiliary objective. The parameterized network achieves plausible,
sample-adaptive repositioning of skeleton sequences through two key components.
First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new
origin of the coordinate system is within the skeleton convex hull. Second, the
Coefficient Learning Block provides a lightweight parameterization of the
mapping from skeleton sequences to their specific coefficients in convex
combinations. Moreover, to guide the optimization of this network for
discrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean
Discrepancy as the additional objective. CHASE operates as a sample-adaptive
normalization method to mitigate inter-entity distribution discrepancies,
thereby reducing data bias and improving the subsequent classifier's
multi-entity action recognition performance. Extensive experiments on six
datasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and
Volleyball, consistently verify our approach by seamlessly adapting to
single-entity backbones and boosting their performance in multi-entity
scenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Camera-ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Interpreting Visual Information Processing in Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) are powerful tools for processing and
understanding text and images. We study the processing of visual tokens in the
language model component of LLaVA, a prominent VLM. Our approach focuses on
analyzing the localization of object information, the evolution of visual token
representations across layers, and the mechanism of integrating visual
information for predictions. Through ablation studies, we demonstrated that
object identification accuracy drops by over 70\% when object-specific tokens
are removed. We observed that visual token representations become increasingly
interpretable in the vocabulary space across layers, suggesting an alignment
with textual tokens corresponding to image content. Finally, we found that the
model extracts object information from these refined representations at the
last token position for prediction, mirroring the process in text-only language
models for factual association tasks. These findings provide crucial insights
into how VLMs process and integrate visual information, bridging the gap
between our understanding of language and vision models, and paving the way for
more interpretable and controllable multimodal systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvolveDirector: Approaching Advanced Text-to-<span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> with Large
  <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhao, Hangjie Yuan, Yujie Wei, Shiwei Zhang, Yuchao Gu, Lingmin Ran, Xiang Wang, Zhangjie Wu, Junhao Zhang, Yingya Zhang, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generation models have showcased remarkable
capabilities in generating fantastic content. However, most of them are trained
on proprietary high-quality data, and some models withhold their parameters and
only provide accessible application programming interfaces (APIs), limiting
their benefits for downstream tasks. To explore the feasibility of training a
text-to-image generation model comparable to advanced models using publicly
available resources, we introduce EvolveDirector. This framework interacts with
advanced models through their public APIs to obtain text-image data pairs to
train a base model. Our experiments with extensive data indicate that the model
trained on generated data of the advanced model can approximate its generation
capability. However, it requires large-scale samples of 10 million or more.
This incurs significant expenses in time, computational resources, and
especially the costs associated with calling fee-based APIs. To address this
problem, we leverage pre-trained large vision-language models (VLMs) to guide
the evolution of the base model. VLM continuously evaluates the base model
during training and dynamically updates and refines the training dataset by the
discrimination, expansion, deletion, and mutation operations. Experimental
results show that this paradigm significantly reduces the required data volume.
Furthermore, when approaching multiple advanced models, EvolveDirector can
select the best samples generated by them to learn powerful and balanced
abilities. The final trained model Edgen is demonstrated to outperform these
advanced models. The code and model weights are available at
https://github.com/showlab/EvolveDirector.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thing2Reality: Transforming 2D Content into Conditioned Multiviews and
  3D Gaussian Objects for XR Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erzhen Hu, Mingyi Li, Jungtaek Hong, Xun Qian, Alex Olwal, David Kim, Seongkook Heo, Ruofei Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During remote communication, participants often share both digital and
physical content, such as product designs, digital assets, and environments, to
enhance mutual understanding. Recent advances in augmented communication have
facilitated users to swiftly create and share digital 2D copies of physical
objects from video feeds into a shared space. However, conventional 2D
representations of digital objects restricts users' ability to spatially
reference items in a shared immersive environment. To address this, we propose
Thing2Reality, an Extended Reality (XR) communication platform that enhances
spontaneous discussions of both digital and physical items during remote
sessions. With Thing2Reality, users can quickly materialize ideas or physical
objects in immersive environments and share them as conditioned multiview
renderings or 3D Gaussians. Thing2Reality enables users to interact with remote
objects or discuss concepts in a collaborative manner. Our user study revealed
that the ability to interact with and manipulate 3D representations of objects
significantly enhances the efficiency of discussions, with the potential to
augment discussion of 2D artifacts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages (15 pages without references), 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Visual Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal large language models (MLLMs) have
demonstrated significant progress; however, these models exhibit a notable
limitation, which we refer to as "face blindness". Specifically, they can
engage in general conversations but fail to conduct personalized dialogues
targeting at specific individuals. This deficiency hinders the application of
MLLMs in personalized settings, such as tailored visual assistants on mobile
devices, or domestic robots that need to recognize members of the family. In
this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel
data curation and training framework designed to enable MLLMs to identify
target individuals within an image and engage in personalized and coherent
dialogues. Our approach involves the development of a sophisticated pipeline
that autonomously generates training data containing personalized
conversations. This pipeline leverages the capabilities of various visual
experts, image generation models, and (multi-modal) large language models. To
evaluate the personalized potential of MLLMs, we present a benchmark called
P-Bench, which encompasses various question types with different levels of
difficulty. The experiments demonstrate a substantial personalized performance
enhancement after fine-tuning with our curated dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VHELM: A Holistic Evaluation of Vision Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, Percy Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current benchmarks for assessing vision-language models (VLMs) often focus on
their perception or problem-solving capabilities and neglect other critical
aspects such as fairness, multilinguality, or toxicity. Furthermore, they
differ in their evaluation procedures and the scope of the evaluation, making
it difficult to compare models. To address these issues, we extend the HELM
framework to VLMs to present the Holistic Evaluation of Vision Language Models
(VHELM). VHELM aggregates various datasets to cover one or more of the 9
aspects: visual perception, knowledge, reasoning, bias, fairness,
multilinguality, robustness, toxicity, and safety. In doing so, we produce a
comprehensive, multi-dimensional view of the capabilities of the VLMs across
these important factors. In addition, we standardize the standard inference
parameters, methods of prompting, and evaluation metrics to enable fair
comparisons across models. Our framework is designed to be lightweight and
automatic so that evaluation runs are cheap and fast. Our initial run evaluates
22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.
We uncover new key findings, such as the fact that efficiency-focused models
(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than
their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark
but not when evaluated on the other aspects. For transparency, we release the
raw model generations and complete results on our website
(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living
benchmark, and we hope to continue adding new datasets and models over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. First three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning: Less Forgetting, More OOD Generalization via
  Adaptive Contrastive Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Rezaei, Mohammad Sabokrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models often suffer from catastrophic forgetting of
previously learned knowledge when learning new classes. Various methods have
been proposed to mitigate this issue. However, rehearsal-based learning, which
retains samples from previous classes, typically achieves good performance but
tends to memorize specific instances, struggling with Out-of-Distribution (OOD)
generalization. This often leads to high forgetting rates and poor
generalization. Surprisingly, the OOD generalization capabilities of these
methods have been largely unexplored. In this paper, we highlight this issue
and propose a simple yet effective strategy inspired by contrastive learning
and data-centric principles to address it. We introduce Adaptive Contrastive
Replay (ACR), a method that employs dual optimization to simultaneously train
both the encoder and the classifier. ACR adaptively populates the replay buffer
with misclassified samples while ensuring a balanced representation of classes
and tasks. By refining the decision boundary in this way, ACR achieves a
balance between stability and plasticity. Our method significantly outperforms
previous approaches in terms of OOD generalization, achieving an improvement of
13.41\% on Split CIFAR-100, 9.91\% on Split Mini-ImageNet, and 5.98\% on Split
Tiny-ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaMP: Language-Motion Pretraining for Motion <span class="highlight-title">Generation</span>, Retrieval, and
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence T. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language plays a vital role in the realm of human motion. Existing methods
have largely depended on CLIP text embeddings for motion generation, yet they
fall short in effectively aligning language and motion due to CLIP's
pretraining on static image-text pairs. This work introduces LaMP, a novel
Language-Motion Pretraining model, which transitions from a language-vision to
a more suitable language-motion latent space. It addresses key limitations by
generating motion-informative text embeddings, significantly enhancing the
relevance and semantics of generated motion sequences. With LaMP, we advance
three key tasks: text-to-motion generation, motion-text retrieval, and motion
captioning through aligned language-motion representation learning. For
generation, we utilize LaMP to provide the text condition instead of CLIP, and
an autoregressive masked prediction is designed to achieve mask modeling
without rank collapse in transformers. For retrieval, motion features from
LaMP's motion transformer interact with query tokens to retrieve text features
from the text transformer, and vice versa. For captioning, we finetune a large
language model with the language-informative motion features to develop a
strong motion captioning model. In addition, we introduce the LaMP-BertScore
metric to assess the alignment of generated motions with textual descriptions.
Extensive experimental results on multiple datasets demonstrate substantial
improvements over previous methods across all three tasks. The code of our
method will be made public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Realistic UAV Vision-Language Navigation: Platform, <span class="highlight-title">Benchmark</span>,
  and Methodology <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, Yue Liao, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing agents capable of navigating to a target location based on
language instructions and visual information, known as vision-language
navigation (VLN), has attracted widespread interest. Most research has focused
on ground-based agents, while UAV-based VLN remains relatively underexplored.
Recent efforts in UAV vision-language navigation predominantly adopt
ground-based VLN settings, relying on predefined discrete action spaces and
neglecting the inherent disparities in agent movement dynamics and the
complexity of navigation tasks between ground and aerial environments. To
address these disparities and challenges, we propose solutions from three
perspectives: platform, benchmark, and methodology. To enable realistic UAV
trajectory simulation in VLN tasks, we propose the OpenUAV platform, which
features diverse environments, realistic flight control, and extensive
algorithmic support. We further construct a target-oriented VLN dataset
consisting of approximately 12k trajectories on this platform, serving as the
first dataset specifically designed for realistic UAV VLN tasks. To tackle the
challenges posed by complex aerial environments, we propose an assistant-guided
UAV object search benchmark called UAV-Need-Help, which provides varying levels
of guidance information to help UAVs better accomplish realistic VLN tasks. We
also propose a UAV navigation LLM that, given multi-view images, task
descriptions, and assistant instructions, leverages the multimodal
understanding capabilities of the MLLM to jointly process visual and textual
information, and performs hierarchical trajectory generation. The evaluation
results of our method significantly outperform the baseline models, while there
remains a considerable gap between our results and those achieved by human
operators, underscoring the challenge presented by the UAV-Need-Help task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JPEG Inspired Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed H. Salamah, Kaixiang Zheng, Yiwen Liu, En-Hui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although it is traditionally believed that lossy image compression, such as
JPEG compression, has a negative impact on the performance of deep neural
networks (DNNs), it is shown by recent works that well-crafted JPEG compression
can actually improve the performance of deep learning (DL). Inspired by this,
we propose JPEG-DL, a novel DL framework that prepends any underlying DNN
architecture with a trainable JPEG compression layer. To make the quantization
operation in JPEG compression trainable, a new differentiable soft quantizer is
employed at the JPEG layer, and then the quantization operation and underlying
DNN are jointly trained. Extensive experiments show that in comparison with the
standard DL, JPEG-DL delivers significant accuracy improvements across various
datasets and model architectures while enhancing robustness against adversarial
attacks. Particularly, on some fine-grained image classification datasets,
JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is
available on https://github.com/JpegInspiredDl/JPEG-Inspired-DL.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixtral 12B 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.
Pixtral-12B is trained to understand both natural images and documents,
achieving leading performance on various multimodal benchmarks, surpassing a
number of larger models. Unlike many open-source models, Pixtral is also a
cutting-edge text model for its size, and does not compromise on natural
language performance to excel in multimodal tasks. Pixtral uses a new vision
encoder trained from scratch, which allows it to ingest images at their natural
resolution and aspect ratio. This gives users flexibility on the number of
tokens used to process an image. Pixtral is also able to process any number of
images in its long context window of 128K tokens. Pixtral 12B substanially
outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B).
It also outperforms much larger open models like Llama-3.2 90B while being 7x
smaller. We further contribute an open-source benchmark, MM-MT-Bench, for
evaluating vision-language models in practical scenarios, and provide detailed
analysis and code for standardized evaluation protocols for multimodal LLMs.
Pixtral-12B is released under Apache 2.0 license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinyEmo: Scaling down Emotional <span class="highlight-title">Reasoning</span> via Metric Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Gutierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TinyEmo, a family of small multi-modal language models
for emotional reasoning and classification. Our approach features: (1) a
synthetic emotional instruct dataset for both pre-training and fine-tuning
stages, (2) a Metric Projector that delegates classification from the language
model allowing for more efficient training and inference, (3) a multi-modal
large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated
framework for bias detection. TinyEmo is able to perform emotion classification
and emotional reasoning, all while using substantially fewer parameters than
comparable models. This efficiency allows us to freely incorporate more diverse
emotional datasets, enabling strong performance on classification tasks, with
our smallest model (700M parameters) outperforming larger state-of-the-art
models based on general-purpose MM-LLMs with over 7B parameters. Additionally,
the Metric Projector allows for interpretability and indirect bias detection in
large models without additional training, offering an approach to understand
and improve AI systems.
  We release code, models, and dataset at https://github.com/ggcr/TinyEmo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in
  Pruning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Lin, Shengji Tang, Chong Yu, Peng Ye, Tao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, differentiable mask pruning methods optimize the continuous
relaxation architecture (soft network) as the proxy of the pruned discrete
network (hard network) for superior sub-architecture search. However, due to
the agnostic impact of the discretization process, the hard network struggles
with the equivalent representational capacity as the soft network, namely
discretization gap, which severely spoils the pruning performance. In this
paper, we first investigate the discretization gap and propose a novel
structural differentiable mask pruning framework named S2HPruner to bridge the
discretization gap in a one-stage manner. In the training procedure, SH2Pruner
forwards both the soft network and its corresponding hard network, then
distills the hard network under the supervision of the soft network. To
optimize the mask and prevent performance degradation, we propose a decoupled
bidirectional knowledge distillation. It blocks the weight updating from the
hard to the soft network while maintaining the gradient corresponding to the
mask. Compared with existing pruning arts, S2HPruner achieves surpassing
pruning performance without fine-tuning on comprehensive benchmarks, including
CIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures.
Besides, investigation and analysis experiments explain the effectiveness of
S2HPruner. Codes will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Z-upscaling: Optical Flow Guided Frame Interpolation for Isotropic
  Reconstruction of 3D EM Volumes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fisseha A. Ferede, Ali Khalighifar, Jaison John, Krishnan Venkataraman, Khaled Khairy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel optical flow based approach to enhance the axial
resolution of anisotropic 3D EM volumes to achieve isotropic 3D reconstruction.
Assuming spatial continuity of 3D biological structures in well aligned EM
volumes, we reasoned that optical flow estimation techniques, often applied for
temporal resolution enhancement in videos, can be utilized. Pixel level motion
is estimated between neighboring 2D slices along z, using spatial gradient flow
estimates to interpolate and generate new 2D slices resulting in isotropic
voxels. We leverage recent state-of-the-art learning methods for video frame
interpolation and transfer learning techniques, and demonstrate the success of
our approach on publicly available ultrastructure EM volumes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clean Evaluations on Contaminated Visual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Lu, Shujie Miao, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to evaluate large language models (LLMs) cleanly has been established as
an important research era to genuinely report the performance of possibly
contaminated LLMs. Yet, how to cleanly evaluate the visual language models
(VLMs) is an under-studied problem. We propose a novel approach to achieve such
goals through data augmentation methods on the visual input information. We
then craft a new visual clean evaluation benchmark with thousands of data
instances. Through extensive experiments, we found that the traditional visual
data augmentation methods are useful, but they are at risk of being used as a
part of the training data as a workaround. We further propose using BGR
augmentation to switch the colour channel of the visual information. We found
that it is a simple yet effective method for reducing the effect of data
contamination and fortunately, it is also harmful to be used as a data
augmentation method during training. It means that it is hard to integrate such
data augmentation into training by malicious trainers and it could be a
promising technique to cleanly evaluate visual LLMs. Our code, data, and model
weights will be released upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation
  Models Without Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologists play a crucial role by translating medical images into medical
reports. However, the field faces staffing shortages and increasing workloads.
While automated approaches using vision-language models (VLMs) show promise as
assistants, they require exceptionally high accuracy. Most current VLMs in
radiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the
general domain, additional preference fine-tuning has become standard practice.
The challenge in radiology lies in the prohibitive cost of obtaining
radiologist feedback. We propose a scalable automated preference alignment
technique for VLMs in radiology, focusing on chest X-ray (CXR) report
generation. Our method leverages publicly available datasets with an
LLM-as-a-Judge mechanism, eliminating the need for additional expert
radiologist feedback. We evaluate and benchmark five direct alignment
algorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN
scores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in
an average across six metrics (domain specific and general), compared to the
SFT baseline. We study reward overoptimization via length exploitation, with
reports lengthening by up to 3.2x. To assess a potential alignment tax, we
benchmark on six additional diverse tasks, finding no significant degradations.
A reader study involving four board-certified radiologists indicates win rates
of up to 0.62 over the SFT baseline, while significantly penalizing verbosity.
Our analysis provides actionable insights for the development of VLMs in
high-stakes fields like radiology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Diffusion</span>-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one
  Single X-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Rachid Jennane, Aladine Chetouani, Mohamed Jarraya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays
are commonly used for its diagnosis due to their cost-effectiveness. Magnetic
Resonance Imaging (MRI), on the other hand, offers detailed soft tissue
visualization and has become a valuable supplementary diagnostic tool for KOA.
Unfortunately, the high cost and limited accessibility of MRI hinder its
widespread use, leaving many patients with KOA reliant solely on X-ray imaging.
In this study, we introduce a novel diffusion-based Xray2MRI model capable of
generating pseudo-MRI volumes from one single X-ray image. In addition to using
X-rays as conditional input, our model integrates target depth, KOA probability
distribution, and image intensity distribution modules to guide the synthesis
process, ensuring that the generated corresponding slices accurately correspond
to the anatomical structures. Experimental results demonstrate that by
integrating information from X-rays with additional input data, our proposed
approach is capable of generating pseudo-MRI sequences that approximate real
MRI scans. Moreover, by increasing the inference times, the model achieves
effective interpolation, further improving the continuity and smoothness of the
generated MRI sequences, representing one promising initial attempt for
cost-effective medical imaging solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jointly Generating Multi-view Consistent PBR Textures using
  Collaborative Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shimon Vainer, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Slava Elizarov, Simon Donné
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view consistency remains a challenge for image diffusion models. Even
within the Text-to-Texture problem, where perfect geometric correspondences are
known a priori, many methods fail to yield aligned predictions across views,
necessitating non-trivial fusion methods to incorporate the results onto the
original mesh. We explore this issue for a Collaborative Control workflow
specifically in PBR Text-to-Texture. Collaborative Control directly models PBR
image probability distributions, including normal bump maps; to our knowledge,
the only diffusion model to directly output full PBR stacks. We discuss the
design decisions involved in making this model multi-view consistent, and
demonstrate the effectiveness of our approach in ablation studies, as well as
practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-Centric Robust Monocular Depth Estimation via Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Chen, Haiyong Luo, Fang Zhao, Jingze Yu, Yupeng Jia, Juan Wang, Xuepeng Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation, enabled by self-supervised learning, is a key
technique for 3D perception in computer vision. However, it faces significant
challenges in real-world scenarios, which encompass adverse weather variations,
motion blur, as well as scenes with poor lighting conditions at night. Our
research reveals that we can divide monocular depth estimation into three
sub-problems: depth structure consistency, local texture disambiguation, and
semantic-structural correlation. Our approach tackles the non-robustness of
existing self-supervised monocular depth estimation models to interference
textures by adopting a structure-centered perspective and utilizing the scene
structure characteristics demonstrated by semantics and illumination. We devise
a novel approach to reduce over-reliance on local textures, enhancing
robustness against missing or interfering patterns. Additionally, we
incorporate a semantic expert model as the teacher and construct inter-model
feature dependencies via learnable isomorphic graphs to enable aggregation of
semantic structural knowledge. Our approach achieves state-of-the-art
out-of-distribution monocular depth estimation performance across a range of
public adverse scenario datasets. It demonstrates notable scalability and
compatibility, without necessitating extensive model engineering. This
showcases the potential for customizing models for diverse industrial
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Asian Conference on Computer Vision 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive High-Frequency Transformer for Diverse Wildlife
  Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyue Li, Shuoyi Chen, Mang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wildlife ReID involves utilizing visual technology to identify specific
individuals of wild animals in different scenarios, holding significant
importance for wildlife conservation, ecological research, and environmental
monitoring. Existing wildlife ReID methods are predominantly tailored to
specific species, exhibiting limited applicability. Although some approaches
leverage extensively studied person ReID techniques, they struggle to address
the unique challenges posed by wildlife. Therefore, in this paper, we present a
unified, multi-species general framework for wildlife ReID. Given that
high-frequency information is a consistent representation of unique features in
various species, significantly aiding in identifying contours and details such
as fur textures, we propose the Adaptive High-Frequency Transformer model with
the goal of enhancing high-frequency information learning. To mitigate the
inevitable high-frequency interference in the wilderness environment, we
introduce an object-aware high-frequency selection strategy to adaptively
capture more valuable high-frequency components. Notably, we unify the
experimental settings of multiple wildlife datasets for ReID, achieving
superior performance over state-of-the-art ReID methods. In domain
generalization scenarios, our approach demonstrates robust generalization to
unknown species.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosis of Malignant Lymphoma Cancer Using Hybrid Optimized Techniques
  Based on Dense Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salah A. Aly, Ali Bakhiet, Mazen Balat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lymphoma diagnosis, particularly distinguishing between subtypes, is critical
for effective treatment but remains challenging due to the subtle morphological
differences in histopathological images. This study presents a novel hybrid
deep learning framework that combines DenseNet201 for feature extraction with a
Dense Neural Network (DNN) for classification, optimized using the Harris Hawks
Optimization (HHO) algorithm. The model was trained on a dataset of 15,000
biopsy images, spanning three lymphoma subtypes: Chronic Lymphocytic Leukemia
(CLL), Follicular Lymphoma (FL), and Mantle Cell Lymphoma (MCL). Our approach
achieved a testing accuracy of 99.33\%, demonstrating significant improvements
in both accuracy and model interpretability. Comprehensive evaluation using
precision, recall, F1-score, and ROC-AUC underscores the model's robustness and
potential for clinical adoption. This framework offers a scalable solution for
improving diagnostic accuracy and efficiency in oncology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 4 tables, IEEE ICCA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Bridge the Points: Graph-based Few-shot Segment Anything Semantically <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An<span class="highlight-author">qi Zhang</span>, Guangyu Gao, Jianbo Jiao, Chi Harold Liu, Yunchao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in large-scale pre-training techniques have
significantly enhanced the capabilities of vision foundation models, notably
the Segment Anything Model (SAM), which can generate precise masks based on
point and box prompts. Recent studies extend SAM to Few-shot Semantic
Segmentation (FSS), focusing on prompt generation for SAM-based automatic
semantic segmentation. However, these methods struggle with selecting suitable
prompts, require specific hyperparameter settings for different scenarios, and
experience prolonged one-shot inference times due to the overuse of SAM,
resulting in low efficiency and limited automation ability. To address these
issues, we propose a simple yet effective approach based on graph analysis. In
particular, a Positive-Negative Alignment module dynamically selects the point
prompts for generating masks, especially uncovering the potential of the
background context as the negative reference. Another subsequent Point-Mask
Clustering module aligns the granularity of masks and selected points as a
directed graph, based on mask coverage over points. These points are then
aggregated by decomposing the weakly connected components of the directed graph
in an efficient manner, constructing distinct natural clusters. Finally, the
positive and overshooting gating, benefiting from graph-based granularity
alignment, aggregate high-confident masks and filter out the false-positive
masks for final prediction, reducing the usage of additional hyperparameters
and redundant mask generation. Extensive experimental analysis across standard
FSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the
effectiveness and efficiency of the proposed approach, surpassing
state-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2%
on LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang, Byeoli Choi, Donghoon Shin, Sung-hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ELMO, a real-time upsampling motion capture framework
designed for a single LiDAR sensor. Modeled as a conditional autoregressive
transformer-based upsampling motion generator, ELMO achieves 60 fps motion
capture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is
the coupling of the self-attention mechanism with thoughtfully designed
embedding modules for motion and point clouds, significantly elevating the
motion quality. To facilitate accurate motion capture, we develop a one-time
skeleton calibration model capable of predicting user skeleton offsets from a
single-frame point cloud. Additionally, we introduce a novel data augmentation
technique utilizing a LiDAR simulator, which enhances global root tracking to
improve environmental understanding. To demonstrate the effectiveness of our
method, we compare ELMO with state-of-the-art methods in both image-based and
point cloud-based motion capture. We further conduct an ablation study to
validate our design principles. ELMO's fast inference time makes it well-suited
for real-time applications, exemplified in our demo video featuring live
streaming and interactive gaming scenarios. Furthermore, we contribute a
high-quality LiDAR-mocap synchronized dataset comprising 20 different subjects
performing a range of motions, which can serve as a valuable resource for
future research. The dataset and evaluation code are available at {\blue
\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Alignment for <span class="highlight-title">Generation</span>: Training <span class="highlight-title">Diffusion</span> Transformers
  Is Easier Than You Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that the denoising process in (generative)
diffusion models can induce meaningful (discriminative) representations inside
the model, though the quality of these representations still lags behind those
learned through recent self-supervised learning methods. We argue that one main
bottleneck in training large-scale diffusion models for generation lies in
effectively learning these representations. Moreover, training can be made
easier by incorporating high-quality external visual representations, rather
than relying solely on the diffusion models to learn them independently. We
study this by introducing a straightforward regularization called
REPresentation Alignment (REPA), which aligns the projections of noisy input
hidden states in denoising networks with clean image representations obtained
from external, pretrained visual encoders. The results are striking: our simple
strategy yields significant improvements in both training efficiency and
generation quality when applied to popular diffusion and flow-based
transformers, such as DiTs and SiTs. For instance, our method can speed up SiT
training by over 17.5$\times$, matching the performance (without
classifier-free guidance) of a SiT-XL model trained for 7M steps in less than
400K steps. In terms of final generation quality, our approach achieves
state-of-the-art results of FID=1.42 using classifier-free guidance with the
guidance interval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Project page: https://sihyun.me/REPA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Entailment Learning for Hyperbolic <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avik Pal, Max van Spengler, Guido Maria D'Amely di Melendugno, Alessandro Flaborea, Fabio Galasso, Pascal Mettes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text representation learning forms a cornerstone in vision-language
models, where pairs of images and textual descriptions are contrastively
aligned in a shared embedding space. Since visual and textual concepts are
naturally hierarchical, recent work has shown that hyperbolic space can serve
as a high-potential manifold to learn vision-language representation with
strong downstream performance. In this work, for the first time we show how to
fully leverage the innate hierarchical nature of hyperbolic embeddings by
looking beyond individual image-text pairs. We propose Compositional Entailment
Learning for hyperbolic vision-language models. The idea is that an image is
not only described by a sentence but is itself a composition of multiple object
boxes, each with their own textual description. Such information can be
obtained freely by extracting nouns from sentences and using openly available
localized grounding models. We show how to hierarchically organize images,
image boxes, and their textual descriptions through contrastive and
entailment-based objectives. Empirical evaluation on a hyperbolic
vision-language model trained with millions of image-text pairs shows that the
proposed compositional learning approach outperforms conventional Euclidean
CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot
and retrieval generalization and clearly stronger hierarchical performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 12 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Probabilistic Human Trajectory Prediction for Autonomous
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Hetzel, Hannes Reichert, Konrad Doll, Bernhard Sick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems, like vehicles or robots, require reliable, accurate,
fast, resource-efficient, scalable, and low-latency trajectory predictions to
get initial knowledge about future locations and movements of surrounding
objects for safe human-machine interaction. Furthermore, they need to know the
uncertainty of the predictions for risk assessment to provide safe path
planning. This paper presents a lightweight method to address these
requirements, combining Long Short-Term Memory and Mixture Density Networks.
Our method predicts probability distributions, including confidence level
estimations for positional uncertainty to support subsequent risk management
applications and runs on a low-power embedded platform. We discuss essential
requirements for human trajectory prediction in autonomous vehicle applications
and demonstrate our method's performance using multiple traffic-related
datasets. Furthermore, we explain reliability and sharpness metrics and show
how important they are to guarantee the correctness and robustness of a model's
predictions and uncertainty assessments. These essential evaluations have so
far received little attention for no good reason. Our approach focuses entirely
on real-world applicability. Verifying prediction uncertainties and a model's
reliability are central to autonomous real-world applications. Our framework
and code are available at:
https://github.com/kav-institute/mdn_trajectory_forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungho Lee, Hwijeong Lee, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenges of the semi-supervised LiDAR segmentation (SSLS)
problem, particularly in low-budget scenarios. The two main issues in
low-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the
performance drops due to the significant imbalance between ground-truth and
pseudo-labels. This imbalance leads to a vicious training cycle. To overcome
these challenges, we leverage the spatio-temporal prior by recognizing the
substantial overlap between temporally adjacent LiDAR scans. We propose a
proximity-based label estimation, which generates highly accurate pseudo-labels
for unlabeled data by utilizing semantic consistency with adjacent labeled
data. Additionally, we enhance this method by progressively expanding the
pseudo-labels from the nearest unlabeled scans, which helps significantly
reduce errors linked to dynamic classes. Additionally, we employ a dual-branch
structure to mitigate performance degradation caused by data imbalance.
Experimental results demonstrate remarkable performance in low-budget settings
(i.e., <= 5%) and meaningful improvements in normal budget settings (i.e., 5 -
50%). Finally, our method has achieved new state-of-the-art results on
SemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5%
labeled data, it offers competitive results against fully-supervised
counterparts. Moreover, it surpasses the performance of the previous
state-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data
(76.0%) on nuScenes. The code is available on https://github.com/halbielee/PLE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selecting the Best Sequential Transfer Path for Medical <span class="highlight-title">Image</span>
  Segmentation with Limited Labeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyun Yang, Jingge Wang, Guoqing Zhang, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The medical image processing field often encounters the critical issue of
scarce annotated data. Transfer learning has emerged as a solution, yet how to
select an adequate source task and effectively transfer the knowledge to the
target task remains challenging. To address this, we propose a novel sequential
transfer scheme with a task affinity metric tailored for medical images.
Considering the characteristics of medical image segmentation tasks, we analyze
the image and label similarity between tasks and compute the task affinity
scores, which assess the relatedness among tasks. Based on this, we select
appropriate source tasks and develop an effective sequential transfer strategy
by incorporating intermediate source tasks to gradually narrow the domain
discrepancy and minimize the transfer cost. Thereby we identify the best
sequential transfer path for the given target task. Extensive experiments on
three MRI medical datasets, FeTS 2022, iSeg-2019, and WMH, demonstrate the
efficacy of our method in finding the best source sequence. Compared with
directly transferring from a single source task, the sequential transfer
results underline a significant improvement in target task performance,
achieving an average of 2.58% gain in terms of segmentation Dice score,
notably, 6.00% for FeTS 2022. Code is available at the git repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Model Performance with Hard-Swish Activation Function
  Adjustments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Abhinav Pydimarry, Shekhar Madhav Khairnar, Sofia Garces Palacios, Ganesh Sankaranarayanan, Darian Hoagland, Dmitry Nepomnayshy, Huu Phong Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of pattern recognition, achieving high accuracy is essential.
While training a model to recognize different complex images, it is vital to
fine-tune the model to achieve the highest accuracy possible. One strategy for
fine-tuning a model involves changing its activation function. Most pre-trained
models use ReLU as their default activation function, but switching to a
different activation function like Hard-Swish could be beneficial. This study
evaluates the performance of models using ReLU, Swish and Hard-Swish activation
functions across diverse image datasets. Our results show a 2.06% increase in
accuracy for models on the CIFAR-10 dataset and a 0.30% increase in accuracy
for models on the ATLAS dataset. Modifying the activation functions in
architecture of pre-trained models lead to improved overall accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Video Quality Assessment Resisting Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang, Qingxiao Guan, Chunsheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential surge in video traffic has intensified the imperative for
Video Quality Assessment (VQA). Leveraging cutting-edge architectures, current
VQA models have achieved human-comparable accuracy. However, recent studies
have revealed the vulnerability of existing VQA models against adversarial
attacks. To establish a reliable and practical assessment system, a secure VQA
model capable of resisting such malicious attacks is urgently demanded.
Unfortunately, no attempt has been made to explore this issue. This paper first
attempts to investigate general adversarial defense principles, aiming at
endowing existing VQA models with security. Specifically, we first introduce
random spatial grid sampling on the video frame for intra-frame defense. Then,
we design pixel-wise randomization through a guardian map, globally
neutralizing adversarial perturbations. Meanwhile, we extract temporal
information from the video sequence as compensation for inter-frame defense.
Building upon these principles, we present a novel VQA framework from the
security-oriented perspective, termed SecureVQA. Extensive experiments indicate
that SecureVQA sets a new benchmark in security while achieving competitive VQA
performance compared with state-of-the-art models. Ablation studies delve
deeper into analyzing the principles of SecureVQA, demonstrating their
generalization and contributions to the security of leading VQA models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurANet: Surrounding-Aware Network for Concealed Object Detection via
  Highly-Efficient Interactive Contrastive Learning Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Kang, Qingpeng Li, Leyuan Fang, Jian Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concealed object detection (COD) in cluttered scenes is significant for
various image processing applications. However, due to that concealed objects
are always similar to their background, it is extremely hard to distinguish
them. Here, the major obstacle is the tiny feature differences between the
inside and outside object boundary region, which makes it trouble for existing
COD methods to achieve accurate results. In this paper, considering that the
surrounding environment information can be well utilized to identify the
concealed objects, and thus, we propose a novel deep Surrounding-Aware Network,
namely SurANet, for COD tasks, which introduces surrounding information into
feature extraction and loss function to improve the discrimination. First, we
enhance the semantics of feature maps using differential fusion of surrounding
features to highlight concealed objects. Next, a Surrounding-Aware Contrastive
Loss is applied to identify the concealed object via learning surrounding
feature maps contrastively. Then, SurANet can be trained end-to-end with high
efficiency via our proposed Spatial-Compressed Correlation Transmission
strategy after our investigation of feature dynamics, and extensive experiments
improve that such features can be well reserved respectively. Finally,
experimental results demonstrate that the proposed SurANet outperforms
state-of-the-art COD methods on multiple real datasets. Our source code will be
available at https://github.com/kyh433/SurANet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Few-Shot Detection with <span class="highlight-title">Large Language Model</span>s and
  Layout-to-<span class="highlight-title">Image</span> Synthesis <span class="chip">ACCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdullah, Nikolas Ebert, Oliver Wasenmüller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have enabled a wide range of works
exploiting their ability to generate high-volume, high-quality data for use in
various downstream tasks. One subclass of such models, dubbed Layout-to-Image
Synthesis (LIS), learns to generate images conditioned on a spatial layout
(bounding boxes, masks, poses, etc.) and has shown a promising ability to
generate realistic images, albeit with limited layout-adherence. Moreover, the
question of how to effectively transfer those models for scalable augmentation
of few-shot detection data remains unanswered. Thus, we propose a collaborative
framework employing a Large Language Model (LLM) and an LIS model for enhancing
few-shot detection beyond state-of-the-art generative augmentation approaches.
We leverage LLM's reasoning ability to extrapolate the spatial prior of the
annotation space by generating new bounding boxes given only a few example
annotations. Additionally, we introduce our novel layout-aware CLIP score for
sample ranking, enabling tight coupling between generated layouts and images.
Significant improvements on COCO few-shot benchmarks are observed. With our
approach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on
the COCO 5-,10-, and 30-shot settings, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at the Asian Conference on Computer
  Vision (ACCV), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Approach for Cardiac MRI Segmentation based on 3D UNet
  Combined with Papillary Muscle Exclusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narjes Benameur, Ramzi Mahmoudi, Mohamed Deriche, Amira fayouka, Imene Masmoudi, Nessrine Zoghlami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Left ventricular ejection fraction (LVEF) is the most important clinical
parameter of cardiovascular function. The accuracy in estimating this parameter
is highly dependent upon the precise segmentation of the left ventricle (LV)
structure at the end diastole and systole phases. Therefore, it is crucial to
develop robust algorithms for the precise segmentation of the heart structure
during different phases. Methodology: In this work, an improved 3D UNet model
is introduced to segment the myocardium and LV, while excluding papillary
muscles, as per the recommendation of the Society for Cardiovascular Magnetic
Resonance. For the practical testing of the proposed framework, a total of
8,400 cardiac MRI images were collected and analysed from the military hospital
in Tunis (HMPIT), as well as the popular ACDC public dataset. As performance
metrics, we used the Dice coefficient and the F1 score for validation/testing
of the LV and the myocardium segmentation. Results: The data was split into
70%, 10%, and 20% for training, validation, and testing, respectively. It is
worth noting that the proposed segmentation model was tested across three axis
views: basal, medio basal and apical at two different cardiac phases: end
diastole and end systole instances. The experimental results showed a Dice
index of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end
diastolic and systolic phases, respectively. Additionally, clinical evaluation
outcomes revealed a significant difference in the LVEF and other clinical
parameters when the papillary muscles were included or excluded.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking the Evaluation of Visible and Infrared <span class="highlight-title">Image</span> Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayan Guan, Yixuan Wu, Tianzhu Liu, Alex C. Kot, Yanfeng Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible and Infrared Image Fusion (VIF) has garnered significant interest
across a wide range of high-level vision tasks, such as object detection and
semantic segmentation. However, the evaluation of VIF methods remains
challenging due to the absence of ground truth. This paper proposes a
Segmentation-oriented Evaluation Approach (SEA) to assess VIF methods by
incorporating the semantic segmentation task and leveraging segmentation labels
available in latest VIF datasets. Specifically, SEA utilizes universal
segmentation models, capable of handling diverse images and classes, to predict
segmentation outputs from fused images and compare these outputs with
segmentation labels. Our evaluation of recent VIF methods using SEA reveals
that their performance is comparable or even inferior to using visible images
only, despite nearly half of the infrared images demonstrating better
performance than visible images. Further analysis indicates that the two
metrics most correlated to our SEA are the gradient-based fusion metric
$Q_{\text{ABF}}$ and the visual information fidelity metric $Q_{\text{VIFF}}$
in conventional VIF evaluation metrics, which can serve as proxies when
segmentation labels are unavailable. We hope that our evaluation will guide the
development of novel and practical VIF methods. The code has been released in
\url{https://github.com/Yixuan-2002/SEA/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code has been released in
  \url{https://github.com/Yixuan-2002/SEA/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Xie, Weijia Zhang, Zhongdao Wang, Chao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in State Space Models, notably Mamba, have demonstrated
superior performance over the dominant Transformer models, particularly in
reducing the computational complexity from quadratic to linear. Yet,
difficulties in adapting Mamba from language to vision tasks arise due to the
distinct characteristics of visual data, such as the spatial locality and
adjacency within images and large variations in information granularity across
visual tokens. Existing vision Mamba approaches either flatten tokens into
sequences in a raster scan fashion, which breaks the local adjacency of images,
or manually partition tokens into windows, which limits their long-range
modeling and generalization capabilities. To address these limitations, we
present a new vision Mamba model, coined QuadMamba, that effectively captures
local dependencies of varying granularities via quadtree-based image partition
and scan. Concretely, our lightweight quadtree-based scan module learns to
preserve the 2D locality of spatial regions within learned window quadrants.
The module estimates the locality score of each token from their features,
before adaptively partitioning tokens into window quadrants. An omnidirectional
window shifting scheme is also introduced to capture more intact and
informative features across different local regions. To make the discretized
quadtree partition end-to-end trainable, we further devise a sequence masking
strategy based on Gumbel-Softmax and its straight-through gradient estimator.
Extensive experiments demonstrate that QuadMamba achieves state-of-the-art
performance in various vision tasks, including image classification, object
detection, instance segmentation, and semantic segmentation. The code is in
https://github.com/VISIONSJTU/QuadMamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurip2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Pixels to Tokens: Revisiting Object Hallucinations in Large
  <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Shang, Xinyi Zeng, Yutao Zhu, Xiao Yang, Zhengwei Fang, Jingyuan Zhang, Jiawei Chen, Zinan Liu, Yu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in large vision-language models (LVLMs) are a significant
challenge, i.e., generating objects that are not presented in the visual input,
which impairs their reliability. Recent studies often attribute hallucinations
to a lack of understanding of visual input, yet ignore a more fundamental
issue: the model's inability to effectively extract or decouple visual
features. In this paper, we revisit the hallucinations in LVLMs from an
architectural perspective, investigating whether the primary cause lies in the
visual encoder (feature extraction) or the modal alignment module (feature
decoupling). Motivated by our findings on the preliminary investigation, we
propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.
This plug-and-play method can be integrated into various LVLMs, utilizing
adaptive virtual tokens to extract object features from bounding boxes, thereby
addressing hallucinations caused by insufficient decoupling of visual features.
PATCH achieves state-of-the-art performance on multiple multi-modal
hallucination datasets. We hope this approach provides researchers with deeper
insights into the underlying causes of hallucinations in LVLMs, fostering
further advancements and innovation in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transesophageal Echocardiography <span class="highlight-title">Generation</span> using Anatomical Models <span class="chip">MICCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel Oladokun, Musa Abdulkareem, Jurica Šprem, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through automation, deep learning (DL) can enhance the analysis of
transesophageal echocardiography (TEE) images. However, DL methods require
large amounts of high-quality data to produce accurate results, which is
difficult to satisfy. Data augmentation is commonly used to tackle this issue.
In this work, we develop a pipeline to generate synthetic TEE images and
corresponding semantic labels. The proposed data generation pipeline expands on
an existing pipeline that generates synthetic transthoracic echocardiography
images by transforming slices from anatomical models into synthetic images. We
also demonstrate that such images can improve DL network performance through a
left-ventricle semantic segmentation task. For the pipeline's unpaired
image-to-image (I2I) translation section, we explore two generative methods:
CycleGAN and contrastive unpaired translation. Next, we evaluate the synthetic
images quantitatively using the Fr\'echet Inception Distance (FID) Score and
qualitatively through a human perception quiz involving expert cardiologists
and the average researcher.
  In this study, we achieve a dice score improvement of up to 10% when we
augment datasets with our synthetic images. Furthermore, we compare established
methods of assessing unpaired I2I translation and observe a disagreement when
evaluating the synthetic images. Finally, we see which metric better predicts
the generated data's efficacy when used for data augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI2023; DALI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HERM: <span class="highlight-title">Benchmark</span>ing and Enhancing <span class="highlight-title">Multimodal</span> <span class="highlight-title">LLM</span>s for Human-Centric
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keliang Li, Zaifei Yang, Jiahe Zhao, Hongze Shen, Ruibing Hou, Hong Chang, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significant advancements in visual understanding and instruction
following from Multimodal Large Language Models (MLLMs) have opened up more
possibilities for broader applications in diverse and universal human-centric
scenarios. However, existing image-text data may not support the precise
modality alignment and integration of multi-grained information, which is
crucial for human-centric visual understanding. In this paper, we introduce
HERM-Bench, a benchmark for evaluating the human-centric understanding
capabilities of MLLMs. Our work reveals the limitations of existing MLLMs in
understanding complex human-centric scenarios. To address these challenges, we
present HERM-100K, a comprehensive dataset with multi-level human-centric
annotations, aimed at enhancing MLLMs' training. Furthermore, we develop
HERM-7B, a MLLM that leverages enhanced training data from HERM-100K.
Evaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms
existing MLLMs across various human-centric dimensions, reflecting the current
inadequacy of data annotations used in MLLM training for human-centric visual
understanding. This research emphasizes the importance of specialized datasets
and benchmarks in advancing the MLLMs' capabilities for human-centric
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Preserve or To Compress: An In-Depth Study of Connector Selection in
  <span class="highlight-title">Multimodal</span> <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal large language models (MLLMs) have garnered
significant attention from both industry and academia. However, there is still
considerable debate on constructing MLLM architectures, particularly regarding
the selection of appropriate connectors for perception tasks of varying
granularities. This paper systematically investigates the impact of connectors
on MLLM performance. Specifically, we classify connectors into
feature-preserving and feature-compressing types. Utilizing a unified
classification standard, we categorize sub-tasks from three comprehensive
benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained
perception, fine-grained perception, and reasoning, and evaluate the
performance. Our findings reveal that feature-preserving connectors excel in
\emph{fine-grained perception} tasks due to their ability to retain detailed
visual information. In contrast, feature-compressing connectors, while less
effective in fine-grained perception tasks, offer significant speed advantages
and perform comparably in \emph{coarse-grained perception} and \emph{reasoning}
tasks. These insights are crucial for guiding MLLM architecture design and
advancing the optimization of MLLM architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-FMT: <span class="highlight-title">Diffusion</span> Models for Fluorescence Molecular Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Xue, Peng Zhang, Xingyu Liu, Wenjian Wang, Guanglei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fluorescence molecular tomography (FMT) is a real-time, noninvasive optical
imaging technology that plays a significant role in biomedical research.
Nevertheless, the ill-posedness of the inverse problem poses huge challenges in
FMT reconstructions. Previous various deep learning algorithms have been
extensively explored to address the critical issues, but they remain faces the
challenge of high data dependency with poor image quality. In this paper, we,
for the first time, propose a FMT reconstruction method based on a denoising
diffusion probabilistic model (DDPM), termed Diff-FMT, which is capable of
obtaining high-quality reconstructed images from noisy images. Specifically, we
utilize the noise addition mechanism of DDPM to generate diverse training
samples. Through the step-by-step probability sampling mechanism in the inverse
process, we achieve fine-grained reconstruction of the image, avoiding issues
such as loss of image detail that can occur with end-to-end deep-learning
methods. Additionally, we introduce the fluorescence signals as conditional
information in the model training to sample a reconstructed image that is
highly consistent with the input fluorescence signals from the noisy images.
Numerous experimental results show that Diff-FMT can achieve high-resolution
reconstruction images without relying on large-scale datasets compared with
other cutting-edge algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamMesh4D: Video-to-4D <span class="highlight-title">Generation</span> with Sparse-Controlled Gaussian-Mesh
  Hybrid Representation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Li, Yiming Chen, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 2D/3D generative techniques have facilitated the
generation of dynamic 3D objects from monocular videos. Previous methods mainly
rely on the implicit neural radiance fields (NeRF) or explicit Gaussian
Splatting as the underlying representation, and struggle to achieve
satisfactory spatial-temporal consistency and surface appearance. Drawing
inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a
novel framework combining mesh representation with geometric skinning technique
to generate high-quality 4D object from a monocular video. Instead of utilizing
classical texture map for appearance, we bind Gaussian splats to triangle face
of mesh for differentiable optimization of both the texture and mesh vertices.
In particular, DreamMesh4D begins with a coarse mesh obtained through an
image-to-3D generation procedure. Sparse points are then uniformly sampled
across the mesh surface, and are used to build a deformation graph to drive the
motion of the 3D object for the sake of computational efficiency and providing
additional constraint. For each step, transformations of sparse control points
are predicted using a deformation network, and the mesh vertices as well as the
surface Gaussians are deformed via a novel geometric skinning algorithm, which
is a hybrid approach combining LBS (linear blending skinning) and DQS
(dual-quaternion skinning), mitigating drawbacks associated with both
approaches. The static surface Gaussians and mesh vertices as well as the
deformation network are learned via reference view photometric loss, score
distillation loss as well as other regularizers in a two-stage manner.
Extensive experiments demonstrate superior performance of our method.
Furthermore, our method is compatible with modern graphic pipelines, showcasing
its potential in the 3D gaming and film industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Transfer Learning and pre-trained Models for Effective Forest
  Fire Detection: A Case Study of Uttarakhand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hari Prabhat Gupta, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forest fires pose a significant threat to the environment, human life, and
property. Early detection and response are crucial to mitigating the impact of
these disasters. However, traditional forest fire detection methods are often
hindered by our reliability on manual observation and satellite imagery with
low spatial resolution. This paper emphasizes the role of transfer learning in
enhancing forest fire detection in India, particularly in overcoming data
collection challenges and improving model accuracy across various regions. We
compare traditional learning methods with transfer learning, focusing on the
unique challenges posed by regional differences in terrain, climate, and
vegetation. Transfer learning can be categorized into several types based on
the similarity between the source and target tasks, as well as the type of
knowledge transferred. One key method is utilizing pre-trained models for
efficient transfer learning, which significantly reduces the need for extensive
labeled data. We outline the transfer learning process, demonstrating how
researchers can adapt pre-trained models like MobileNetV2 for specific tasks
such as forest fire detection. Finally, we present experimental results from
training and evaluating a deep learning model using the Uttarakhand forest fire
dataset, showcasing the effectiveness of transfer learning in this context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MimicTalk: Mimicking a personalized and expressive 3D talking face in
  minutes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhui Ye, Tianyun Zhong, Yi Ren, Ziyue Jiang, Jiawei Huang, Rongjie Huang, Jinglin Liu, Jinzheng He, Chen Zhang, Zehan Wang, Xize Chen, Xiang Yin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking face generation (TFG) aims to animate a target identity's face to
create realistic talking videos. Personalized TFG is a variant that emphasizes
the perceptual identity similarity of the synthesized result (from the
perspective of appearance and talking style). While previous works typically
solve this problem by learning an individual neural radiance field (NeRF) for
each identity to implicitly store its static and dynamic information, we find
it inefficient and non-generalized due to the per-identity-per-training
framework and the limited training data. To this end, we propose MimicTalk, the
first attempt that exploits the rich knowledge from a NeRF-based
person-agnostic generic model for improving the efficiency and robustness of
personalized TFG. To be specific, (1) we first come up with a person-agnostic
3D TFG model as the base model and propose to adapt it into a specific
identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help
the model learn the personalized static appearance and facial dynamic features;
(3) To generate the facial motion of the personalized talking style, we propose
an in-context stylized audio-to-motion model that mimics the implicit talking
style provided in the reference video without information loss by an explicit
style representation. The adaptation process to an unseen identity can be
performed in 15 minutes, which is 47 times faster than previous
person-dependent methods. Experiments show that our MimicTalk surpasses
previous baselines regarding video quality, efficiency, and expressiveness.
Source code and video samples are available at https://mimictalk.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of <span class="highlight-title">LLM</span>s with
  Situation Puzzles <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Chen, Bowen Zhang, Gang Wang, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While advancements in NLP have significantly improved the performance of
Large Language Models (LLMs) on tasks requiring vertical thinking, their
lateral thinking capabilities remain under-explored and challenging to measure
due to the complexity of assessing creative thought processes and the scarcity
of relevant data. To address these challenges, we introduce SPLAT, a benchmark
leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.
This benchmark, containing 975 graded situation puzzles across three difficulty
levels, employs a new multi-turn player-judge framework instead of the
traditional model-based evaluation, which often necessitates a stronger
evaluation model. This framework simulates an interactive game where the model
(player) asks the evaluation model (judge) questions about an incomplete story
to infer the full scenario. The judge answers based on a detailed reference
scenario or evaluates if the player's predictions align with the reference one.
This approach lessens dependence on more robust evaluation models, enabling the
assessment of state-of-the-art LLMs. The experiments demonstrate that a robust
evaluation model, such as WizardLM-2, closely matches human judgements in both
intermediate question-answering and final scenario accuracy, achieving over 80%
agreement-similar to the agreement levels among humans. Furthermore, applying
data and reasoning processes from our benchmark to other lateral
thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to
performance enhancements. This suggests that our benchmark effectively
evaluates and elicits the lateral thinking abilities of LLMs. Code is available
at: https://github.com/chenqi008/LateralThinking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Impact of Point Cloud Colorization on Semantic
  Segmentation Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinfeng Zhu, Jiaze Cao, Yuanzhi Cai, Lei Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud semantic segmentation, the process of classifying each point into
predefined categories, is essential for 3D scene understanding. While
image-based segmentation is widely adopted due to its maturity, methods relying
solely on RGB information often suffer from degraded performance due to color
inaccuracies. Recent advancements have incorporated additional features such as
intensity and geometric information, yet RGB channels continue to negatively
impact segmentation accuracy when errors in colorization occur. Despite this,
previous studies have not rigorously quantified the effects of erroneous
colorization on segmentation performance. In this paper, we propose a novel
statistical approach to evaluate the impact of inaccurate RGB information on
image-based point cloud segmentation. We categorize RGB inaccuracies into two
types: incorrect color information and similar color information. Our results
demonstrate that both types of color inaccuracies significantly degrade
segmentation accuracy, with similar color errors particularly affecting the
extraction of geometric features. These findings highlight the critical need to
reassess the role of RGB information in point cloud segmentation and its
implications for future algorithm design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 IEEE 8th International Conference on Vision, Image
  and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Computational Pathology Foundation Models for Prostate Cancer
  Grading under Distribution Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik K. Gustafsson, Mattias Rantalainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have recently become a popular research direction within
computational pathology. They are intended to be general-purpose feature
extractors, promising to achieve good performance on a range of downstream
tasks. Real-world pathology image data does however exhibit considerable
variability. Foundation models should be robust to these variations and other
distribution shifts which might be encountered in practice. We evaluate two
computational pathology foundation models: UNI (trained on more than 100,000
whole-slide images) and CONCH (trained on more than 1.1 million image-caption
pairs), by utilizing them as feature extractors within prostate cancer grading
models. We find that while UNI and CONCH perform well relative to baselines,
the absolute performance can still be far from satisfactory in certain
settings. The fact that foundation models have been trained on large and varied
datasets does not guarantee that downstream models always will be robust to
common distribution shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suppress Content Shift: Better <span class="highlight-title">Diffusion</span> Features via Off-the-Shelf
  <span class="highlight-title">Generation</span> Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benyuan Meng, Qianqian Xu, Zitai Wang, Zhiyong Yang, Xiaochun Cao, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are powerful generative models, and this capability can also
be applied to discrimination. The inner activations of a pre-trained diffusion
model can serve as features for discriminative tasks, namely, diffusion
feature. We discover that diffusion feature has been hindered by a hidden yet
universal phenomenon that we call content shift. To be specific, there are
content differences between features and the input image, such as the exact
shape of a certain object. We locate the cause of content shift as one inherent
characteristic of diffusion models, which suggests the broad existence of this
phenomenon in diffusion feature. Further empirical study also indicates that
its negative impact is not negligible even when content shift is not visually
perceivable. Hence, we propose to suppress content shift to enhance the overall
quality of diffusion features. Specifically, content shift is related to the
information drift during the process of recovering an image from the noisy
input, pointing out the possibility of turning off-the-shelf generation
techniques into tools for content shift suppression. We further propose a
practical guideline named GATE to efficiently evaluate the potential benefit of
a technique and provide an implementation of our methodology. Despite the
simplicity, the proposed approach has achieved superior results on various
tasks and datasets, validating its potential as a generic booster for diffusion
features. Our code is available at
https://github.com/Darkbblue/diffusion-content-shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2410.03558</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatMamba: A Matryoshka State Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Shukla, Sai Vemprala, Aditya Kusupati, Ashish Kapoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State Space Models (SSMs) like Mamba2 are a promising alternative to
Transformers, with faster theoretical training and inference times --
especially for long context lengths. Recent work on Matryoshka Representation
Learning -- and its application to Transformer backbones in works like
MatFormer -- showed how to introduce nested granularities of smaller submodels
in one universal elastic model. In this work, we present MatMamba: a state
space model which combines Matryoshka-style learning with Mamba2, by modifying
the block to contain nested dimensions to enable joint training and adaptive
inference. MatMamba allows for efficient and adaptive deployment across various
model sizes. We train a single large MatMamba model and are able to get a
number of smaller nested models for free -- while maintaining or improving upon
the performance of a baseline smaller model trained from scratch. We train
language and image models at a variety of parameter sizes from 35M to 1.4B. Our
results on ImageNet and FineWeb show that MatMamba models scale comparably to
Transformers, while having more efficient inference characteristics. This makes
MatMamba a practically viable option for deploying large-scale models in an
elastic way based on the available inference compute. Code and models are open
sourced at \url{https://github.com/ScaledFoundations/MatMamba}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of different disparity estimation techniques on aerial stereo
  <span class="highlight-title">image</span> <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishan Narayan, Shashi Poddar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of aerial image datasets, dense stereo matching has gained
tremendous progress. This work analyses dense stereo correspondence analysis on
aerial images using different techniques. Traditional methods, optimization
based methods and learning based methods have been implemented and compared
here for aerial images. For traditional methods, we implemented the
architecture of Stereo SGBM while using different cost functions to get an
understanding of their performance on aerial datasets. Analysis of most of the
methods in standard datasets has shown good performance, however in case of
aerial dataset, not much benchmarking is available. Visual qualitative and
quantitative analysis has been carried out for two stereo aerial datasets in
order to compare different cost functions and techniques for the purpose of
depth estimation from stereo images. Using existing pre-trained models, recent
learning based architectures have also been tested on stereo pairs along with
different cost functions in SGBM. The outputs and given ground truth are
compared using MSE, SSIM and other error metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Break the Visual Perception: Adversarial Attacks Targeting Encoded
  Visual Tokens of Large <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Chaohu Liu, Yanqiu Qu, Haoyu Cao, Deqiang Jiang, Linli Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) integrate visual information into large
language models, showcasing remarkable multi-modal conversational capabilities.
However, the visual modules introduces new challenges in terms of robustness
for LVLMs, as attackers can craft adversarial images that are visually clean
but may mislead the model to generate incorrect answers. In general, LVLMs rely
on vision encoders to transform images into visual tokens, which are crucial
for the language models to perceive image contents effectively. Therefore, we
are curious about one question: Can LVLMs still generate correct responses when
the encoded visual tokens are attacked and disrupting the visual information?
To this end, we propose a non-targeted attack method referred to as VT-Attack
(Visual Tokens Attack), which constructs adversarial examples from multiple
perspectives, with the goal of comprehensively disrupting feature
representations and inherent relationships as well as the semantic properties
of visual tokens output by image encoders. Using only access to the image
encoder in the proposed attack, the generated adversarial examples exhibit
transferability across diverse LVLMs utilizing the same image encoder and
generality across different tasks. Extensive experiments validate the superior
attack performance of the VT-Attack over baseline methods, demonstrating its
effectiveness in attacking LVLMs with image encoders, which in turn can provide
guidance on the robustness of LVLMs, particularly in terms of the stability of
the visual feature space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACMMM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier-based Action Recognition for Wildlife Behavior Quantification
  with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Friedhelm Hamann, Suman Ghosh, Ignacio Juarez Martinez, Tom Hart, Alex Kacelnik, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are novel bio-inspired vision sensors that measure pixel-wise
brightness changes asynchronously instead of images at a given frame rate. They
offer promising advantages, namely a high dynamic range, low latency, and
minimal motion blur. Modern computer vision algorithms often rely on artificial
neural network approaches, which require image-like representations of the data
and cannot fully exploit the characteristics of event data. We propose
approaches to action recognition based on the Fourier Transform. The approaches
are intended to recognize oscillating motion patterns commonly present in
nature. In particular, we apply our approaches to a recent dataset of breeding
penguins annotated for "ecstatic display", a behavior where the observed
penguins flap their wings at a certain frequency. We find that our approaches
are both simple and effective, producing slightly lower results than a deep
neural network (DNN) while relying just on a tiny fraction of the parameters
compared to the DNN (five orders of magnitude fewer parameters). They work well
despite the uncontrolled, diverse data present in the dataset. We hope this
work opens a new perspective on event-based processing and action recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniPose6D: Towards Short-Term Object Pose Tracking in Dynamic Scenes
  from Monocular RGB 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Lin, Yipu Zhao, Fu-Jen Chu, Xingyu Chen, Weiyao Wang, Hao Tang, Patricio A. Vela, Matt Feiszli, Kevin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the challenge of short-term object pose tracking in dynamic
environments with monocular RGB input, we introduce a large-scale synthetic
dataset OmniPose6D, crafted to mirror the diversity of real-world conditions.
We additionally present a benchmarking framework for a comprehensive comparison
of pose tracking algorithms. We propose a pipeline featuring an
uncertainty-aware keypoint refinement network, employing probabilistic modeling
to refine pose estimation. Comparative evaluations demonstrate that our
approach achieves performance superior to existing baselines on real datasets,
underscoring the effectiveness of our synthetic dataset and refinement
technique in enhancing tracking precision in dynamic contexts. Our
contributions set a new precedent for the development and assessment of object
pose tracking methodologies in complex scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptual Quality Assessment of Trisoup-Lifting Encoded 3D Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncheng Long, Honglei Su, Qi Liu, Hui Yuan, Wei Gao, Jiarun Song, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-reference bitstream-layer point cloud quality assessment (PCQA) can be
deployed without full decoding at any network node to achieve real-time quality
monitoring. In this work, we develop the first PCQA model dedicated to
Trisoup-Lifting encoded 3D point clouds by analyzing bitstreams without full
decoding. Specifically, we investigate the relationship among texture bitrate
per point (TBPP), texture complexity (TC) and texture quantization parameter
(TQP) while geometry encoding is lossless. Subsequently, we estimate TC by
utilizing TQP and TBPP. Then, we establish a texture distortion evaluation
model based on TC, TBPP and TQP. Ultimately, by integrating this texture
distortion model with a geometry attenuation factor, a function of
trisoupNodeSizeLog2 (tNSL), we acquire a comprehensive NR bitstream-layer PCQA
model named streamPCQ-TL. In addition, this work establishes a database named
WPC6.0, the first and largest PCQA database dedicated to Trisoup-Lifting
encoding mode, encompassing 400 distorted point clouds with both 4 geometric
multiplied by 5 texture distortion levels. Experiment results on M-PCCD,
ICIP2020 and the proposed WPC6.0 database suggest that the proposed
streamPCQ-TL model exhibits robust and notable performance in contrast to
existing advanced PCQA metrics, particularly in terms of computational cost.
The dataset and source code will be publicly released at
\href{https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0}{\textit{https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0}}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">Multimodal</span> <span class="highlight-title">LLM</span> for Detailed and Accurate Video Captioning
  using Multi-Round Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zujun Ma, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos contain a wealth of information, and generating detailed and accurate
descriptions in natural language is a key aspect of video understanding. In
this paper, we present video-SALMONN 2, an advanced audio-visual large language
model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with
paired audio) captioning through directed preference optimization (DPO). We
propose new metrics to evaluate the completeness and accuracy of video
descriptions, which are optimized using DPO. To further improve training, we
introduce a novel multi-round DPO (mrDPO) approach, which involves periodically
updating the DPO reference model, merging and re-initializing the LoRA module
as a proxy for parameter updates after each training round (1,000 steps), and
incorporating guidance from ground-truth video captions to stabilize the
process. To address potential catastrophic forgetting of non-captioning
abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO
LLM by using the captions generated by the mrDPO-trained model as supervised
labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's
captioning accuracy, reducing global and local error rates by 40\% and 20\%,
respectively, while decreasing the repetition rate by 35\%. The final
video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models
such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining
competitive performance to the state-of-the-art on widely used video
question-answering benchmark among models of similar size. Upon acceptance, we
will release the code, model checkpoints, and training and test data. Demos are
available at
\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M${}^{3}$Bench: <span class="highlight-title">Benchmark</span>ing Whole-body Motion <span class="highlight-title">Generation</span> for Mobile
  Manipulation in 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Zhang, Sixu Yan, Muzhi Han, Zaijin Wang, Xinggang Wang, Song-Chun Zhu, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose M^3Bench, a new benchmark for whole-body motion generation for
mobile manipulation tasks. Given a 3D scene context, M^3Bench requires an
embodied agent to understand its configuration, environmental constraints and
task objectives, then generate coordinated whole-body motion trajectories for
object rearrangement tasks. M^3Bench features 30k object rearrangement tasks
across 119 diverse scenes, providing expert demonstrations generated by our
newly developed M^3BenchMaker. This automatic data generation tool produces
coordinated whole-body motion trajectories from high-level task instructions,
requiring only basic scene and robot information. Our benchmark incorporates
various task splits to assess generalization across different dimensions and
leverages realistic physics simulation for trajectory evaluation. Through
extensive experimental analyses, we reveal that state-of-the-art models still
struggle with coordinated base-arm motion while adhering to environment-context
and task-specific constraints, highlighting the need to develop new models that
address this gap. Through M^3Bench, we aim to facilitate future robotics
research towards more adaptive and capable mobile manipulation in diverse,
real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decouple-Then-Merge: Towards Better Training for <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianli Ma, Xuefei Ning, Dongrui Liu, Li Niu, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are trained by learning a sequence of models that reverse
each step of noise corruption. Typically, the model parameters are fully shared
across multiple timesteps to enhance training efficiency. However, since the
denoising tasks differ at each timestep, the gradients computed at different
timesteps may conflict, potentially degrading the overall performance of image
generation. To solve this issue, this work proposes a Decouple-then-Merge
(DeMe) framework, which begins with a pretrained model and finetunes separate
models tailored to specific timesteps. We introduce several improved techniques
during the finetuning stage to promote effective knowledge sharing while
minimizing training interference across timesteps. Finally, after finetuning,
these separate models can be merged into a single model in the parameter space,
ensuring efficient and practical inference. Experimental results show
significant generation quality improvements upon 6 benchmarks including Stable
Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN
Bedroom, and CIFAR10.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning in the Frequency Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Liu, Boyu Diao, Libo Huang, Zijia An, Zhulin An, Yongjun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) is designed to learn new tasks while preserving
existing knowledge. Replaying samples from earlier tasks has proven to be an
effective method to mitigate the forgetting of previously acquired knowledge.
However, the current research on the training efficiency of rehearsal-based
methods is insufficient, which limits the practical application of CL systems
in resource-limited scenarios. The human visual system (HVS) exhibits varying
sensitivities to different frequency components, enabling the efficient
elimination of visually redundant information. Inspired by HVS, we propose a
novel framework called Continual Learning in the Frequency Domain (CLFD). To
our knowledge, this is the first study to utilize frequency domain features to
enhance the performance and efficiency of CL training on edge devices. For the
input features of the feature extractor, CLFD employs wavelet transform to map
the original input image into the frequency domain, thereby effectively
reducing the size of input feature maps. Regarding the output features of the
feature extractor, CLFD selectively utilizes output features for distinct
classes for classification, thereby balancing the reusability and interference
of output features based on the frequency domain similarity of the classes
across various tasks. Optimizing only the input and output features of the
feature extractor allows for seamless integration of CLFD with various
rehearsal-based methods. Extensive experiments conducted in both cloud and edge
environments demonstrate that CLFD consistently improves the performance of
state-of-the-art (SOTA) methods in both precision and training efficiency.
Specifically, CLFD can increase the accuracy of the SOTA CL method by up to
6.83% and reduce the training time by 2.6$\times$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurlIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-RGBT: Open-vocabulary RGB-T Zero-shot Semantic Segmentation in
  Open-world Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Yu, Luojie Yang, Xunjie He, Yi Yang, Yufeng Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation is a critical technique for effective scene
understanding. Traditional RGB-T semantic segmentation models often struggle to
generalize across diverse scenarios due to their reliance on pretrained models
and predefined categories. Recent advancements in Visual Language Models (VLMs)
have facilitated a shift from closed-set to open-vocabulary semantic
segmentation methods. However, these models face challenges in dealing with
intricate scenes, primarily due to the heterogeneity between RGB and thermal
modalities. To address this gap, we present Open-RGBT, a novel open-vocabulary
RGB-T semantic segmentation model. Specifically, we obtain instance-level
detection proposals by incorporating visual prompts to enhance category
understanding. Additionally, we employ the CLIP model to assess image-text
similarity, which helps correct semantic consistency and mitigates ambiguities
in category identification. Empirical evaluations demonstrate that Open-RGBT
achieves superior performance in diverse and challenging real-world scenarios,
even in the wild, significantly advancing the field of RGB-T semantic
segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ETA: Evaluating Then Aligning Safety of Vision Language Models at
  Inference Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ding, Bolian Li, Ru<span class="highlight-author">qi Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have become essential backbones for multimodal
intelligence, yet significant safety challenges limit their real-world
application. While textual inputs are often effectively safeguarded,
adversarial visual inputs can easily bypass VLM defense mechanisms. Existing
defense methods are either resource-intensive, requiring substantial data and
compute, or fail to simultaneously ensure safety and usefulness in responses.
To address these limitations, we propose a novel two-phase inference-time
alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual
contents and output responses to establish a robust safety awareness in
multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep
levels by conditioning the VLMs' generative distribution with an interference
prefix and performing sentence-level best-of-N to search the most harmless and
helpful generation paths. Extensive experiments show that ETA outperforms
baseline methods in terms of harmlessness, helpfulness, and efficiency,
reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%
win-ties in GPT-4 helpfulness evaluation. The code is publicly available at
https://github.com/DripNowhy/ETA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xiao, Zhenzhen Hu, Jia Li, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-video retrieval (TVR) has seen substantial advancements in recent years,
fueled by the utilization of pre-trained models and large language models
(LLMs). Despite these advancements, achieving accurate matching in TVR remains
challenging due to inherent disparities between video and textual modalities
and irregularities in data representation. In this paper, we propose
Text-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the
conventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.
By replacing a single text query with a series of text proxies, TV-ProxyNet not
only broadens the query scope but also achieves a more precise expansion. Each
text proxy is crafted through a refined iterative process, controlled by
mechanisms we term as the director and dash, which regulate the proxy's
direction and distance relative to the original text query. This setup not only
facilitates more precise semantic alignment but also effectively manages the
disparities and noise inherent in multimodal data. Our experiments on three
representative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet
Captions, demonstrate the effectiveness of TV-ProxyNet. The results show an
improvement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved
state-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%
improvement on DiDeMo compared to existing methods, validating our approach's
ability to enhance semantic mapping and reduce error propensity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification
  for Visual Place Recognition with Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Hausler, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a novel joint training method for Visual Place
Recognition (VPR), which simultaneously learns a global descriptor and a pair
classifier for re-ranking. The pair classifier can predict whether a given pair
of images are from the same place or not. The network only comprises Vision
Transformer components for both the encoder and the pair classifier, and both
components are trained using their respective class tokens. In existing VPR
methods, typically the network is initialized using pre-trained weights from a
generic image dataset such as ImageNet. In this work we propose an alternative
pre-training strategy, by using Siamese Masked Image Modelling as a
pre-training task. We propose a Place-aware image sampling procedure from a
collection of large VPR datasets for pre-training our model, to learn visual
features tuned specifically for VPR. By re-using the Mask Image Modelling
encoder and decoder weights in the second stage of training, Pair-VPR can
achieve state-of-the-art VPR performance across five benchmark datasets with a
ViT-B encoder, along with further improvements in localization recall with
larger encoders. The Pair-VPR website is:
https://csiro-robotics.github.io/Pair-VPR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian
  Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Chen, Yingfu Zeng, Haoang Li, Zhitao Deng, Jiafu Yan, Zhenjun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and affordable indoor 3D reconstruction is critical for effective
robot navigation and interaction. Traditional LiDAR-based mapping provides high
precision but is costly, heavy, and power-intensive, with limited ability for
novel view rendering. Vision-based mapping, while cost-effective and capable of
capturing visual data, often struggles with high-quality 3D reconstruction due
to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a
low-altitude camera and single-line LiDAR for high-quality 3D indoor
reconstruction. Our system features Visual Error Construction (VEC) to enhance
sparse point clouds by identifying and correcting areas with insufficient
geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS
initialization method guided by single-line LiDAR, overcoming the limitations
of traditional multi-view setups and enabling effective reconstruction in
resource-constrained environments. Extensive experimental results on our new
Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian
outperforms existing methods, particularly in challenging scenarios. The
project page is available at https://chenlu-china.github.io/ES-Gaussian/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://chenlu-china.github.io/ES-Gaussian/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topologically Faithful Multi-class Segmentation in Medical <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Berger, Nico Stucki, Laurin Lux, Vincent Buergin, Suprosanna Shit, Anna Banaszak, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological accuracy in medical image segmentation is a highly important
property for downstream applications such as network analysis and flow modeling
in vessels or cell counting. Recently, significant methodological advancements
have brought well-founded concepts from algebraic topology to binary
segmentation. However, these approaches have been underexplored in multi-class
segmentation scenarios, where topological errors are common. We propose a
general loss function for topologically faithful multi-class segmentation
extending the recent Betti matching concept, which is based on induced
matchings of persistence barcodes. We project the N-class segmentation problem
to N single-class segmentation tasks, which allows us to use 1-parameter
persistent homology, making training of neural networks computationally
feasible. We validate our method on a comprehensive set of four medical
datasets with highly variant topological characteristics. Our loss formulation
significantly enhances topological correctness in cardiac, cell, artery-vein,
and Circle of Willis segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and
  YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12040v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12040v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjan Sapkota, Zhichao Meng, Martin Churuvija, Xiaoqiang Du, Zenghong Ma, Manoj Karkee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study extensively evaluated You Only Look Once (YOLO) object detection
algorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, and
YOLO11 for green fruit detection in commercial orchards. The research also
validated in-field fruitlet counting using an iPhone and machine vision sensors
across four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.
Among the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-base
outperformed others with mAP@50 scores of 0.933 and 0.935 respectively. In
terms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9
configurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11n
emerged as the fastest model, achieving fastest inference speed of only 2.4 ms,
significantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,
and YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. This
comparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,
offering researchers essential insights to choose the best-suited model for
fruitlet detection and possible automation in commercial orchards. For
real-time automation related work in relevant datasets, we recommend using
YOLO11n due to its high detection and image processing speed. Keywords: YOLO11,
YOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, Fruitlet
Detection, Greenfruit Detection, Green Apple Detection, Agricultural
Automation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shot
Detection
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMD: Contrastive Multi-Modal <span class="highlight-title">Diffusion</span> for Video-Audio Conditional
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Hannes Gamper, Sebastian Braun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a multi-modal diffusion model tailored for the bi-directional
conditional generation of video and audio. We propose a joint contrastive
training loss to improve the synchronization between visual and auditory
occurrences. We present experiments on two datasets to evaluate the efficacy of
our proposed model. The assessment of generation quality and alignment
performance is carried out from various angles, encompassing both objective and
subjective metrics. Our findings demonstrate that the proposed model
outperforms the baseline in terms of quality and generation speed through
introduction of our novel cross-modal easy fusion architectural block.
Furthermore, the incorporation of the contrastive loss results in improvements
in audio-visual alignment, particularly in the high-correlation video-to-audio
generation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defensive Unlearning with Adversarial Training for Robust Concept
  Erasure in <span class="highlight-title">Diffusion</span> Models <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have achieved remarkable success in text-to-image
generation, but they also pose safety risks, such as the potential generation
of harmful content and copyright violations. The techniques of machine
unlearning, also known as concept erasing, have been developed to address these
risks. However, these techniques remain vulnerable to adversarial prompt
attacks, which can prompt DMs post-unlearning to regenerate undesired images
containing concepts (such as nudity) meant to be erased. This work aims to
enhance the robustness of concept erasing by integrating the principle of
adversarial training (AT) into machine unlearning, resulting in the robust
unlearning framework referred to as AdvUnlearn. However, achieving this
effectively and efficiently is highly nontrivial. First, we find that a
straightforward implementation of AT compromises DMs' image generation quality
post-unlearning. To address this, we develop a utility-retaining regularization
on an additional retain set, optimizing the trade-off between concept erasure
robustness and model utility in AdvUnlearn. Moreover, we identify the text
encoder as a more suitable module for robustification compared to UNet,
ensuring unlearning effectiveness. And the acquired text encoder can serve as a
plug-and-play robust unlearner for various DM types. Empirically, we perform
extensive experiments to demonstrate the robustness advantage of AdvUnlearn
across various DM unlearning scenarios, including the erasure of nudity,
objects, and style concepts. In addition to robustness, AdvUnlearn also
achieves a balanced tradeoff with model utility. To our knowledge, this is the
first work to systematically explore robust DM unlearning through AT, setting
it apart from existing methods that overlook robustness in concept erasing.
Codes are available at: https://github.com/OPTML-Group/AdvUnlearn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS'24. Codes are available at
  https://github.com/OPTML-Group/AdvUnlearn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Your Generative Model Detect Out-of-Distribution Covariate Shift? <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christiaan Viviers, Amaan Valiuddin, Francisco Caetano, Lemar Abdi, Lena Filatova, Peter de With, Fons van der Sommen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Out-of-Distribution (OOD) sensory data and covariate distribution
shift aims to identify new test examples with different high-level image
statistics to the captured, normal and In-Distribution (ID) set. Existing OOD
detection literature largely focuses on semantic shift with little-to-no
consensus over covariate shift. Generative models capture the ID data in an
unsupervised manner, enabling them to effectively identify samples that deviate
significantly from this learned distribution, irrespective of the downstream
task. In this work, we elucidate the ability of generative models to detect and
quantify domain-specific covariate shift through extensive analyses that
involves a variety of models. To this end, we conjecture that it is sufficient
to detect most occurring sensory faults (anomalies and deviations in global
signals statistics) by solely modeling high-frequency signal-dependent and
independent details. We propose a novel method, CovariateFlow, for OOD
detection, specifically tailored to covariate heteroscedastic high-frequency
image-components using conditional Normalizing Flows (cNFs). Our results on
CIFAR10 vs. CIFAR10-C and ImageNet200 vs. ImageNet200-C demonstrate the
effectiveness of the method by accurately detecting OOD covariate shift. This
work contributes to enhancing the fidelity of imaging systems and aiding
machine learning models in OOD detection in the presence of covariate shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024, typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Generative Framework for Realistic Lidar Simulation in
  Autonomous Driving Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Haghighi, Mehrdad Dianati, Valentina Donzella, Kurt Debattista
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation models for perception sensors are integral components of
automotive simulators used for the virtual Verification and Validation (V\&V)
of Autonomous Driving Systems (ADS). These models also serve as powerful tools
for generating synthetic datasets to train deep learning-based perception
models. Lidar is a widely used sensor type among the perception sensors for ADS
due to its high precision in 3D environment scanning. However, developing
realistic Lidar simulation models is a significant technical challenge. In
particular, unrealistic models can result in a large gap between the
synthesised and real-world point clouds, limiting their effectiveness in ADS
applications. Recently, deep generative models have emerged as promising
solutions to synthesise realistic sensory data. However, for Lidar simulation,
deep generative models have been primarily hybridised with conventional
algorithms, leaving unified generative approaches largely unexplored in the
literature. Motivated by this research gap, we propose a unified generative
framework to enhance Lidar simulation fidelity. Our proposed framework projects
Lidar point clouds into depth-reflectance images via a lossless transformation,
and employs our novel Controllable Lidar point cloud Generative model, CoLiGen,
to translate the images. We extensively evaluate our CoLiGen model, comparing
it with the state-of-the-art image-to-image translation models using various
metrics to assess the realness, faithfulness, and performance of a downstream
perception model. Our results show that CoLiGen exhibits superior performance
across most metrics. The dataset and source code for this research are
available at https://github.com/hamedhaghighi/CoLiGen.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The BRAVO Semantic Segmentation Challenge Results in UNCV2024 <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Hung Vu, Eduardo Valle, Andrei Bursuc, Tommie Kerssies, Daan de Geus, Gijs Dubbelman, Long Qian, Bingke Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Tomáš Vojíř, Jan Šochman, Jiří Matas, Michael Smith, Frank Ferrie, Shamik Basu, Christos Sakaridis, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the unified BRAVO challenge to benchmark the reliability of
semantic segmentation models under realistic perturbations and unknown
out-of-distribution (OOD) scenarios. We define two categories of reliability:
(1) semantic reliability, which reflects the model's accuracy and calibration
when exposed to various perturbations; and (2) OOD reliability, which measures
the model's ability to detect object classes that are unknown during training.
The challenge attracted nearly 100 submissions from international teams
representing notable research institutions. The results reveal interesting
insights into the importance of large-scale pre-training and minimal
architectural design in developing robust and reliable semantic segmentation
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024 proceeding paper of the BRAVO challenge 2024, see
  https://benchmarks.elsa-ai.eu/?ch=1&com=introduction Corrected numbers in
  Tables 1,3,4,5 and 10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SparseVLM: Visual Token Sparsification for Efficient Vision-Language
  Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In vision-language models (VLMs), visual tokens usually consume a significant
amount of computational overhead, despite their sparser information density
compared to text tokens. To address this, most existing methods learn a network
to prune redundant visual tokens and require additional training data.
Differently, we propose an efficient training-free token optimization mechanism
dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely,
given that visual tokens complement text tokens in VLMs for linguistic
reasoning, we select visual-relevant text tokens to rate the significance of
vision tokens within the self-attention matrix extracted from the VLMs. Then we
progressively prune irrelevant tokens. To maximize sparsity while retaining
essential information, we introduce a rank-based strategy to adaptively
determine the sparsification ratio for each layer, alongside a token recycling
method that compresses pruned tokens into more compact representations.
Experimental results show that our SparseVLM improves the efficiency of various
VLMs across a range of image and video understanding tasks. In particular,
LLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio
of 78% while maintaining 93% of the accuracy. Our code is available at
https://github.com/Gumpest/SparseVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DT<span class="highlight-title">LLM</span>-VLT: Diverse Text <span class="highlight-title">Generation</span> for Visual Language Tracking Based on
  <span class="highlight-title">LLM</span> <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchen Li, Xiaokun Feng, Shiyu Hu, Meiqi Wu, Dailing Zhang, Jing Zhang, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Language Tracking (VLT) enhances single object tracking (SOT) by
integrating natural language descriptions from a video, for the precise
tracking of a specified object. By leveraging high-level semantic information,
VLT guides object tracking, alleviating the constraints associated with relying
on a visual modality. Nevertheless, most VLT benchmarks are annotated in a
single granularity and lack a coherent semantic framework to provide scientific
guidance. Moreover, coordinating human annotators for high-quality annotations
is laborious and time-consuming. To address these challenges, we introduce
DTLLM-VLT, which automatically generates extensive and multi-granularity text
to enhance environmental diversity. (1) DTLLM-VLT generates scientific and
multi-granularity text descriptions using a cohesive prompt framework. Its
succinct and highly adaptable design allows seamless integration into various
visual tracking benchmarks. (2) We select three prominent benchmarks to deploy
our approach: short-term tracking, long-term tracking, and global instance
tracking. We offer four granularity combinations for these benchmarks,
considering the extent and density of semantic information, thereby showcasing
the practicality and versatility of DTLLM-VLT. (3) We conduct comparative
experiments on VLT benchmarks with different text granularities, evaluating and
analyzing the impact of diverse text on tracking performance. Conclusionally,
this work leverages LLM to provide multi-granularity semantic information for
VLT task from efficient and diverse perspectives, enabling fine-grained
evaluation of multi-modal trackers. In the future, we believe this work can be
extended to more datasets to support vision datasets understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR Workshop 2024, Oral Presentation, Best Paper
  Honorable Mention Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DTVLT: A Multi-modal Diverse Text <span class="highlight-title">Benchmark</span> for Visual Language Tracking
  Based on <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language tracking (VLT) has emerged as a cutting-edge research area,
harnessing linguistic data to enhance algorithms with multi-modal inputs and
broadening the scope of traditional single object tracking (SOT) to encompass
video understanding applications. Despite this, most VLT benchmarks still
depend on succinct, human-annotated text descriptions for each video. These
descriptions often fall short in capturing the nuances of video content
dynamics and lack stylistic variety in language, constrained by their uniform
level of detail and a fixed annotation frequency. As a result, algorithms tend
to default to a "memorize the answer" strategy, diverging from the core
objective of achieving a deeper understanding of video content. Fortunately,
the emergence of large language models (LLMs) has enabled the generation of
diverse text. This work utilizes LLMs to generate varied semantic annotations
(in terms of text lengths and granularities) for representative SOT benchmarks,
thereby establishing a novel multi-modal benchmark. Specifically, we (1)
propose a new visual language tracking benchmark with diverse texts, named
DTVLT, based on five prominent VLT and SOT benchmarks, including three
sub-tasks: short-term tracking, long-term tracking, and global instance
tracking. (2) We offer four granularity texts in our benchmark, considering the
extent and density of semantic information. We expect this multi-granular
generation strategy to foster a favorable environment for VLT and video
understanding research. (3) We conduct comprehensive experimental analyses on
DTVLT, evaluating the impact of diverse text on tracking performance and hope
the identified performance bottlenecks of existing algorithms can support
further research in VLT and video understanding. The proposed benchmark,
experimental results and toolkit will be released gradually on
http://videocube.aitestunion.com/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HAHA - a novel approach for animatable human avatar generation
from monocular input videos. The proposed method relies on learning the
trade-off between the use of Gaussian splatting and a textured mesh for
efficient and high fidelity rendering. We demonstrate its efficiency to animate
and render full-body human avatars controlled via the SMPL-X parametric model.
Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh
where it is necessary, like hair and out-of-mesh clothing. This results in a
minimal number of Gaussians being used to represent the full avatar, and
reduced rendering artifacts. This allows us to handle the animation of small
body parts such as fingers that are traditionally disregarded. We demonstrate
the effectiveness of our approach on two open datasets: SnapshotPeople and
X-Humans. Our method demonstrates on par reconstruction quality to the
state-of-the-art on SnapshotPeople, while using less than a third of Gaussians.
HAHA outperforms previous state-of-the-art on novel poses from X-Humans both
quantitatively and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enforcing 3D Topological Constraints in Composite Objects via Implicit
  Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08716v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08716v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu Le, Jingyi Xu, Nicolas Talabot, Jiancheng Yang, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical applications often require accurate 3D representations of complex
organs with multiple parts, such as the heart and spine. Their individual parts
must adhere to specific topological constraints to ensure proper functionality.
Yet, there are very few mechanisms in the deep learning literature to achieve
this goal.
  This paper introduces a novel approach to enforce topological constraints in
3D object reconstruction using deep implicit signed distance functions. Our
method focuses on heart and spine reconstruction but is generalizable to other
applications. We propose a sampling-based technique that effectively checks and
enforces topological constraints between 3D shapes by evaluating signed
distances at randomly sampled points throughout the volume. We demonstrate it
by refining 3D segmentations obtained from the nn-UNet architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Impression for Learning with Distributed Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atrin Arya, Sana Ayromlou, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard deep learning-based classification approaches may not always be
practical in real-world clinical applications, as they require a centralized
collection of all samples. Federated learning (FL) provides a paradigm that can
learn from distributed datasets across clients without requiring them to share
data, which can help mitigate privacy and data ownership issues. In FL,
sub-optimal convergence caused by data heterogeneity is common among data from
different health centers due to the variety in data collection protocols and
patient demographics across centers. Through experimentation in this study, we
show that data heterogeneity leads to the phenomenon of catastrophic forgetting
during local training. We propose FedImpres which alleviates catastrophic
forgetting by restoring synthetic data that represents the global information
as federated impression. To achieve this, we distill the global model resulting
from each communication round. Subsequently, we use the synthetic data
alongside the local data to enhance the generalization of local training.
Extensive experiments show that the proposed method achieves state-of-the-art
performance on both the BloodMNIST and Retina datasets, which contain label
imbalance and domain shift, with an improvement in classification accuracy of
up to 20%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedLSAM: Localize and Segment Anything Model for 3D CT <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14752v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14752v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhui Lei, Xu Wei, Xiaofan Zhang, Kang Li, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in foundation models have shown significant potential in
medical image analysis. However, there is still a gap in models specifically
designed for medical image localization. To address this, we introduce MedLAM,
a 3D medical foundation localization model that accurately identifies any
anatomical part within the body using only a few template scans. MedLAM employs
two self-supervision tasks: unified anatomical mapping (UAM) and multi-scale
similarity (MSS) across a comprehensive dataset of 14,012 CT scans.
Furthermore, we developed MedLSAM by integrating MedLAM with the Segment
Anything Model (SAM). This innovative framework requires extreme point
annotations across three directions on several templates to enable MedLAM to
locate the target anatomical structure in the image, with SAM performing the
segmentation. It significantly reduces the amount of manual annotation required
by SAM in 3D medical imaging scenarios. We conducted extensive experiments on
two 3D datasets covering 38 distinct organs. Our findings are twofold: 1)
MedLAM can directly localize anatomical structures using just a few template
scans, achieving performance comparable to fully supervised models; 2) MedLSAM
closely matches the performance of SAM and its specialized medical adaptations
with manual prompts, while minimizing the need for extensive point annotations
across the entire dataset. Moreover, MedLAM has the potential to be seamlessly
integrated with future 3D SAM models, paving the way for enhanced segmentation
performance. Our code is public at \href{https://github.com/openmedlab/MedLSAM}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MIA 2024. Code is public at https://github.com/openmedlab/MedLSAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IC3M: In-Car <span class="highlight-title">Multimodal</span> Multi-object Monitoring for Abnormal Status of
  Both Driver and Passengers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Fang, Zheng Lin, Senkang Hu, Hangcheng Cao, Yiqin Deng, Xianhao Chen, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, in-car monitoring has emerged as a promising technology for
detecting early-stage abnormal status of the driver and providing timely alerts
to prevent traffic accidents. Although training models with multimodal data
enhances the reliability of abnormal status detection, the scarcity of labeled
data and the imbalance of class distribution impede the extraction of critical
abnormal state features, significantly deteriorating training performance.
Furthermore, missing modalities due to environment and hardware limitations
further exacerbate the challenge of abnormal status identification. More
importantly, monitoring abnormal health conditions of passengers, particularly
in elderly care, is of paramount importance but remains underexplored. To
address these challenges, we introduce our IC3M, an efficient
camera-rotation-based multimodal framework for monitoring both driver and
passengers in a car. Our IC3M comprises two key modules: an adaptive threshold
pseudo-labeling strategy and a missing modality reconstruction. The former
customizes pseudo-labeling thresholds for different classes based on the class
distribution, generating class-balanced pseudo labels to guide model training
effectively, while the latter leverages crossmodality relationships learned
from limited labels to accurately recover missing modalities by distribution
transferring from available modalities. Extensive experimental results
demonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,
precision, and recall while exhibiting superior robustness under limited
labeled data and severe missing modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion and Structure from Event-based Normal Flow <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12239v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12239v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyang Ren, Bangyan Liao, Delei Kong, Jinghang Li, Peidong Liu, Laurent Kneip, Guillermo Gallego, Yi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering the camera motion and scene geometry from visual data is a
fundamental problem in the field of computer vision. Its success in standard
vision is attributed to the maturity of feature extraction, data association
and multi-view geometry. The recent emergence of neuromorphic event-based
cameras places great demands on approaches that use raw event data as input to
solve this fundamental problem. Existing state-of-the-art solutions typically
infer implicitly data association by iteratively reversing the event data
generation process. However, the nonlinear nature of these methods limits their
applicability in real-time tasks, and the constant-motion assumption leads to
unstable results under agile motion. To this end, we rethink the problem
formulation in a way that aligns better with the differential working principle
of event cameras. We show that the event-based normal flow can be used, via the
proposed geometric error term, as an alternative to the full flow in solving a
family of geometric problems that involve instantaneous first-order kinematics
and scene geometry. Furthermore, we develop a fast linear solver and a
continuous-time nonlinear solver on top of the proposed geometric error term.
Experiments on both synthetic and real data show the superiority of our linear
solver in terms of accuracy and efficiency, and indicate its complementary
feature as an initialization method for existing nonlinear solvers. Besides,
our continuous-time non-linear solver exhibits exceptional capability in
accommodating sudden variations in motion since it does not rely on the
constant-motion assumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AUPIMO: Redefining Visual Anomaly Detection <span class="highlight-title">Benchmark</span>s with High Speed
  and Low Tolerance <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01984v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01984v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao P. C. Bertoldo, Dick Ameln, Ashwin Vaidya, Samet Akçay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in visual anomaly detection research have seen AUROC and
AUPRO scores on public benchmark datasets such as MVTec and VisA converge
towards perfect recall, giving the impression that these benchmarks are
near-solved. However, high AUROC and AUPRO scores do not always reflect
qualitative performance, which limits the validity of these metrics in
real-world applications. We argue that the artificial ceiling imposed by the
lack of an adequate evaluation metric restrains progression of the field, and
it is crucial that we revisit the evaluation metrics used to rate our
algorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric
that addresses the shortcomings of AUROC and AUPRO. PIMO retains the
recall-based nature of the existing metrics but introduces two distinctions:
the assignment of curves (and respective area under the curve) is per-image,
and its X-axis relies solely on normal images. Measuring recall per image
simplifies instance score indexing and is more robust to noisy annotations. As
we show, it also accelerates computation and enables the usage of statistical
tests to compare models. By imposing low tolerance for false positives on
normal images, PIMO provides an enhanced model validation procedure and
highlights performance variations across datasets. Our experiments demonstrate
that PIMO offers practical advantages and nuanced performance insights that
redefine anomaly detection benchmarks -- notably challenging the perception
that MVTec AD and VisA datasets have been solved by contemporary models.
Available on GitHub: https://github.com/jpcbertoldo/aupimo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BMVC 2024. Official implementation
  https://github.com/jpcbertoldo/aupimo and integrated in anomalib
  https://github.com/openvinotoolkit/anomalib This research has been conducted
  during Google Summer of Code 2023 (GSoC 2023) at OpenVINO (Intel)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time
  Rendering <span class="chip">SIGGRAPH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting has emerged as a prominent model for constructing 3D
representations from images across diverse domains. However, the efficiency of
the 3D Gaussian Splatting rendering pipeline relies on several simplifications.
Notably, reducing Gaussian to 2D splats with a single view-space depth
introduces popping and blending artifacts during view rotation. Addressing this
issue requires accurate per-pixel depth computation, yet a full per-pixel sort
proves excessively costly compared to a global sort operation. In this paper,
we present a novel hierarchical rasterization approach that systematically
resorts and culls splats with minimal processing overhead. Our software
rasterizer effectively eliminates popping artifacts and view inconsistencies,
as demonstrated through both quantitative and qualitative measurements.
Simultaneously, our method mitigates the potential for cheating view-dependent
effects with popping, ensuring a more authentic representation. Despite the
elimination of cheating, our approach achieves comparable quantitative results
for test images, while increasing the consistency for novel view synthesis in
motion. Due to its design, our hierarchical approach is only 4% slower on
average than the original Gaussian Splatting. Notably, enforcing consistency
enables a reduction in the number of Gaussians by approximately half with
nearly identical quality and view-consistency. Consequently, rendering
performance is nearly doubled, making our approach 1.6x faster than the
original Gaussian Splatting, with a 50% reduction in memory requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2024 (Journal Track); Project Page:
  https://r4dl.github.io/StopThePop/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Window-based Channel Attention for Wavelet-enhanced Learned <span class="highlight-title">Image</span>
  Compression <span class="chip">ACCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Xu, Bowen Hai, Yushun Tang, Zhihai He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned Image Compression (LIC) models have achieved superior rate-distortion
performance than traditional codecs. Existing LIC models use CNN, Transformer,
or Mixed CNN-Transformer as basic blocks. However, limited by the shifted
window attention, Swin-Transformer-based LIC exhibits a restricted growth of
receptive fields, affecting the ability to model large objects for image
compression. To address this issue and improve the performance, we incorporate
window partition into channel attention for the first time to obtain large
receptive fields and capture more global information. Since channel attention
hinders local information learning, it is important to extend existing
attention mechanisms in Transformer codecs to the space-channel attention to
establish multiple receptive fields, being able to capture global correlations
with large receptive fields while maintaining detailed characterization of
local correlations with small receptive fields. We also incorporate the
discrete wavelet transform into our Spatial-Channel Hybrid (SCH) framework for
efficient frequency-dependent down-sampling and further enlarging receptive
fields. Experiment results demonstrate that our method achieves
state-of-the-art performances, reducing BD-rate by 18.54%, 23.98%, 22.33%, and
24.71% on four standard datasets compared to VTM-23.1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV2024 accepted; camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Semantic Equivalence of Tokenization in <span class="highlight-title">Multimodal</span> <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05127v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05127v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated exceptional
capabilities in processing vision-language tasks. One of the crux of MLLMs lies
in vision tokenization, which involves efficiently transforming input visual
signals into feature representations that are most beneficial for LLMs.
However, existing vision tokenizers, essential for semantic alignment between
vision and language, remain problematic. Existing methods aggressively fragment
visual input, corrupting the visual semantic integrity. To address this, this
paper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),
which groups visual features into semantic units via a dynamic clustering
algorithm, flexibly determining the number of tokens based on image complexity.
The resulting vision tokens effectively preserve semantic integrity and capture
both low-frequency and high-frequency visual features. The proposed MLLM
(Setokim) equipped with SeTok significantly demonstrates superior performance
across various tasks, as evidenced by our experimental results. The project
page is at https://chocowu.github.io/SeTok-web/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. The project page:
  https://chocowu.github.io/SeTok-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Mistakes: Iterative Prompt Relabeling for Text-to-<span class="highlight-title">Image</span>
  <span class="highlight-title">Diffusion</span> Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16204v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16204v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Chen, Jiaxin Ge, Tianjun Zhang, Jiaming Liu, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown impressive performance in many domains. However,
the model's capability to follow natural language instructions (e.g., spatial
relationships between objects, generating complex scenes) is still
unsatisfactory. In this work, we propose Iterative Prompt Relabeling (IPR), a
novel algorithm that aligns images to text through iterative image sampling and
prompt relabeling with feedback. IPR first samples a batch of images
conditioned on the text, then relabels the text prompts of unmatched text-image
pairs with classifier feedback. We conduct thorough experiments on SDv2 and
SDXL, testing their capability to follow instructions on spatial relations.
With IPR, we improved up to 15.22% (absolute improvement) on the challenging
spatial relation VISOR benchmark, demonstrating superior performance compared
to previous RL methods. Our code is publicly available at
https://github.com/cxy000000/IPR-RLDF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMSR:Gradient-Guided Mamba for Spectral Reconstruction from RGB <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinying Wang, Zhixiong Huang, Sifan Zhang, Jiawen Zhu, Paolo Gamba, Lin Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mainstream approaches to spectral reconstruction (SR) primarily focus on
designing Convolution- and Transformer-based architectures. However, CNN
methods often face challenges in handling long-range dependencies, whereas
Transformers are constrained by computational efficiency limitations. Recent
breakthroughs in state-space model (e.g., Mamba) has attracted significant
attention due to its near-linear computational efficiency and superior
performance, prompting our investigation into its potential for SR problem. To
this end, we propose the Gradient-guided Mamba for Spectral Reconstruction from
RGB Images, dubbed GMSR-Net. GMSR-Net is a lightweight model characterized by a
global receptive field and linear computational complexity. Its core comprises
multiple stacked Gradient Mamba (GM) blocks, each featuring a tri-branch
structure. In addition to benefiting from efficient global feature
representation by Mamba block, we further innovatively introduce spatial
gradient attention and spectral gradient attention to guide the reconstruction
of spatial and spectral cues. GMSR-Net demonstrates a significant
accuracy-efficiency trade-off, achieving state-of-the-art performance while
markedly reducing the number of parameters and computational burdens. Compared
to existing approaches, GMSR-Net slashes parameters and FLOPS by substantial
margins of 10 times and 20 times, respectively. Code is available at
https://github.com/wxy11-27/GMSR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric
  hybrid solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hala Djeghim, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Désiré Sidibé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit surface representation methods have recently shown impressive
3D reconstruction results. However, existing solutions struggle to reconstruct
urban outdoor scenes due to their large, unbounded, and highly detailed nature.
Hence, to achieve accurate reconstructions, additional supervision data such as
LiDAR, strong geometric priors, and long training times are required. To tackle
such issues, we present SCILLA, a new hybrid implicit surface learning method
to reconstruct large driving scenes from 2D images. SCILLA's hybrid
architecture models two separate implicit fields: one for the volumetric
density and another for the signed distance to the surface. To accurately
represent urban outdoor scenarios, we introduce a novel volume-rendering
strategy that relies on self-supervised probabilistic density estimation to
sample points near the surface and transition progressively from volumetric to
surface representation. Our solution permits a proper and fast initialization
of the signed distance field without relying on any geometric prior on the
scene, compared to concurrent methods. By conducting extensive experiments on
four outdoor driving datasets, we show that SCILLA can learn an accurate and
detailed 3D surface scene representation in various urban scenarios while being
two times faster to train compared to previous state-of-the-art solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Quality of Hallucination <span class="highlight-title">Benchmark</span>s for Large
  <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bei Yan, Jie Zhang, Zheng Yuan, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid progress and outstanding performance of Large
Vision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the
issue of hallucination, i.e., LVLMs tend to generate responses that are
inconsistent with the corresponding visual inputs. To evaluate the degree of
hallucination in LVLMs, previous works have proposed a series of benchmarks
featuring different types of tasks and evaluation metrics. However, we find
that the quality of the existing hallucination benchmarks varies, with some
suffering from problems, e.g., inconsistent evaluation results under repeated
tests, and misalignment with human evaluation. To this end, we propose a
Hallucination benchmark Quality Measurement framework (HQM), which leverages
various indicators to assess the reliability and validity of existing
hallucination benchmarks separately. Specifically, for reliability we explore
test-retest reliability and parallel-forms reliability, while for validity we
examine criterion validity and coverage of hallucination types. Furthermore,
based on the results of our quality measurement, we construct a High-Quality
Hallucination Benchmark (HQH) for LVLMs, which demonstrates superior
reliability and validity under our HQM framework. We conduct an extensive
evaluation of over 10 representative LVLMs, including GPT-4o and
Gemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in
existing models. Our benchmark is publicly available at
https://github.com/HQHBench/HQHBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LISBET: a machine learning model for the automatic segmentation of
  social behavior motifs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Chindemi, Benoit Girard, Camilla Bellone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social behavior is crucial for survival in many animal species, and a heavily
investigated research subject. Current analysis methods generally rely on
measuring animal interaction time or annotating predefined behaviors. However,
these approaches are time consuming, human biased, and can fail to capture
subtle behaviors. Here we introduce LISBET (LISBET Is a Social BEhavior
Transformer), a machine learning model for detecting and segmenting social
interactions. Using self-supervised learning on body tracking data, our model
eliminates the need for extensive human annotation. We tested LISBET in three
scenarios across multiple datasets in mice: supervised behavior classification,
unsupervised motifs segmentation, and unsupervised animal phenotyping.
Additionally, in vivo electrophysiology revealed distinct neural signatures in
the Ventral Tegmental Area corresponding to motifs identified by our model. In
summary, LISBET automates data annotation and reduces human bias in social
behavior research, offering a promising approach to enhance our understanding
of behavior and its neural correlates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Parametric Activation <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Panagiotis Alexandridis, Jiankang Deng, Anh Nguyen, Shan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The activation function plays a crucial role in model optimisation, yet the
optimal choice remains unclear. For example, the Sigmoid activation is the
de-facto activation in balanced classification tasks, however, in imbalanced
classification, it proves inappropriate due to bias towards frequent classes.
In this work, we delve deeper in this phenomenon by performing a comprehensive
statistical analysis in the classification and intermediate layers of both
balanced and imbalanced networks and we empirically show that aligning the
activation function with the data distribution, enhances the performance in
both balanced and imbalanced tasks. To this end, we propose the Adaptive
Parametric Activation (APA) function, a novel and versatile activation function
that unifies most common activation functions under a single formula. APA can
be applied in both intermediate layers and attention layers, significantly
outperforming the state-of-the-art on several imbalanced benchmarks such as
ImageNet-LT, iNaturalist2018, Places-LT, CIFAR100-LT and LVIS and balanced
benchmarks such as ImageNet1K, COCO and V3DET. The code is available at
https://github.com/kostas1515/AGLU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TASAR: Transfer-based Attack on Skeletal Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Ajian Liu, Xingxing Wei, Meng Wang, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeletal sequences, as well-structured representations of human behaviors,
play a vital role in Human Activity Recognition (HAR). The transferability of
adversarial skeletal sequences enables attacks in real-world HAR scenarios,
such as autonomous driving, intelligent surveillance, and human-computer
interactions. However, most existing skeleton-based HAR (S-HAR) attacks are
primarily designed for white-box scenarios and exhibit weak adversarial
transferability. Therefore, they cannot be considered true transfer-based S-HAR
attacks. More importantly, the reason for this failure remains unclear. In this
paper, we study this phenomenon through the lens of loss surface, and find that
its sharpness contributes to the weak transferability in S-HAR. Inspired by
this observation, we assume and empirically validate that smoothening the
rugged loss landscape could potentially improve adversarial transferability in
S-HAR. To this end, we propose the first \textbf{T}ransfer-based
\textbf{A}ttack on \textbf{S}keletal \textbf{A}ction \textbf{R}ecognition,
TASAR. TASAR explores the smoothed model posterior without requiring surrogate
re-training, which is achieved by a new post-train Dual Bayesian optimization
strategy. Furthermore, unlike previous transfer-based attacks that treat each
frame independently and overlook temporal coherence within sequences, TASAR
incorporates motion dynamics into the Bayesian attack gradient, effectively
disrupting the spatial-temporal coherence of S-HARs. To exhaustively evaluate
the effectiveness of existing methods and our method, we build the first
large-scale robust S-HAR benchmark, comprising 7 S-HAR models, 10 attack
methods, 3 S-HAR datasets and 2 defense methods. Extensive results demonstrate
the superiority of TASAR. Our benchmark enables easy comparisons for future
studies, with the code available in the supplementary material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MovieDreamer: Hierarchical <span class="highlight-title">Generation</span> for Coherent Long Visual Sequence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in video generation have primarily leveraged diffusion
models for short-duration content. However, these approaches often fall short
in modeling complex narratives and maintaining character consistency over
extended periods, which is essential for long-form video production like
movies. We propose MovieDreamer, a novel hierarchical framework that integrates
the strengths of autoregressive models with diffusion-based rendering to
pioneer long-duration video generation with intricate plot progressions and
high visual fidelity. Our approach utilizes autoregressive models for global
narrative coherence, predicting sequences of visual tokens that are
subsequently transformed into high-quality video frames through diffusion
rendering. This method is akin to traditional movie production processes, where
complex stories are factorized down into manageable scene capturing. Further,
we employ a multimodal script that enriches scene descriptions with detailed
character information and visual style, enhancing continuity and character
identity across scenes. We present extensive experiments across various movie
genres, demonstrating that our approach not only achieves superior visual and
narrative quality but also effectively extends the duration of generated
content significantly beyond current capabilities. Homepage:
https://aim-uofa.github.io/MovieDreamer/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Lei Zhang, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, Haoyuan Li, Bolin Li, Zhelun Yu, Si Liu, Hongsheng Li, Hao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LLaVA-MoD, a novel framework designed to enable the efficient
training of small-scale Multimodal Language Models (s-MLLM) by distilling
knowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamental
challenges in MLLM distillation. First, we optimize the network structure of
s-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the
language model, striking a balance between computational efficiency and model
expressiveness. Second, we propose a progressive knowledge transfer strategy to
ensure comprehensive knowledge migration. This strategy begins with mimic
distillation, where we minimize the Kullback-Leibler (KL) divergence between
output distributions to enable the student model to emulate the teacher
network's understanding. Following this, we introduce preference distillation
via Direct Preference Optimization (DPO), where the key lies in treating l-MLLM
as the reference model. During this phase, the s-MLLM's ability to discriminate
between superior and inferior examples is significantly enhanced beyond l-MLLM,
leading to a better student that surpasses its teacher, particularly in
hallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoD
outperforms existing models across various multimodal benchmarks while
maintaining a minimal number of activated parameters and low computational
costs. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpasses
Qwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% of
the training data and 23% trainable parameters. These results underscore
LLaVA-MoD's ability to effectively distill comprehensive knowledge from its
teacher model, paving the way for the development of more efficient MLLMs. The
code will be available on: https://github.com/shufangxun/LLaVA-MoD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and
  Long-Range Dependencies for Structural Crack Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Liu, Chen Jia, Fan Shi, Xu Cheng, Mianzhao Wang, Shengyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting cracks with pixel-level precision for key structures is a
significant challenge, as existing methods struggle to effectively integrate
local textures and pixel dependencies of cracks. Furthermore, these methods
often possess numerous parameters and substantial computational requirements,
complicating deployment on edge control devices. In this paper, we propose a
staircase cascaded fusion crack segmentation network (CrackSCF) that generates
high-quality crack segmentation maps using minimal computational resources. We
constructed a staircase cascaded fusion module that effectively captures local
patterns of cracks and long-range dependencies of pixels, and it can suppress
background noise well. To reduce the computational resources required by the
model, we introduced a lightweight convolution block, which replaces all
convolution operations in the network, significantly reducing the required
computation and parameters without affecting the network's performance. To
evaluate our method, we created a challenging benchmark dataset called TUT and
conducted experiments on this dataset and five other public datasets. The
experimental results indicate that our method offers significant advantages
over existing methods, especially in handling background noise interference and
detailed crack segmentation. The F1 and mIoU scores on the TUT dataset are
0.8382 and 0.8473, respectively, achieving state-of-the-art (SOTA) performance
while requiring the least computational resources. The code and dataset is
available at https://github.com/Karl1109/CrackSCF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cell Tracking according to Biological Needs -- Strong Mitosis-aware
  Multi-Hypothesis Tracker with Aleatoric Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15011v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15011v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Kaiser, Maximilian Schier, Bodo Rosenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cell tracking and segmentation assist biologists in extracting insights from
large-scale microscopy time-lapse data. Driven by local accuracy metrics,
current tracking approaches often suffer from a lack of long-term consistency
and the ability to reconstruct lineage trees correctly. To address this issue,
we introduce an uncertainty estimation technique for motion estimation
frameworks and extend the multi-hypothesis tracking framework. Our uncertainty
estimation lifts motion representations into probabilistic spatial densities
using problem-specific test-time augmentations. Moreover, we introduce a novel
mitosis-aware assignment problem formulation that allows multi-hypothesis
trackers to model cell splits and to resolve false associations and mitosis
detections based on long-term conflicts. In our framework, explicit biological
knowledge is modeled in assignment costs. We evaluate our approach on nine
competitive datasets and demonstrate that we outperform the current
state-of-the-art on biologically inspired metrics substantially, achieving
improvements by a factor of approximately 6 and uncover new insights into the
behavior of motion estimation uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Ego: Towards Building Egocentric <span class="highlight-title">Multimodal</span> <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research aims to comprehensively explore building a multimodal
foundation model for egocentric video understanding. To achieve this goal, we
work on three fronts. First, as there is a lack of QA data for egocentric video
understanding, we develop a data engine that efficiently generates 7M
high-quality QA samples for egocentric videos ranging from 30 seconds to one
hour long, based on human-annotated data. This is currently the largest
egocentric QA dataset. Second, we contribute a challenging egocentric QA
benchmark with 629 videos and 7,026 questions to evaluate the models' ability
in recognizing and memorizing visual details across videos of varying lengths.
We introduce a new de-biasing evaluation method to help mitigate the
unavoidable language bias present in the models being evaluated. Third, we
propose a specialized multimodal architecture featuring a novel "Memory Pointer
Prompting" mechanism. This design includes a global glimpse step to gain an
overarching understanding of the entire video and identify key visual
information, followed by a fallback step that utilizes the key visual
information to generate responses. This enables the model to more effectively
comprehend extended video content. With the data, benchmark, and model, we
successfully build MM-Ego, an egocentric multimodal LLM that shows powerful
performance on egocentric video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge
  Conflicts for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Circuit Architectural Priors for Quadruped Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil X. Bhattasali, Venkatesh Pattabiraman, Lerrel Pinto, Grace W. Lindsay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based approaches to quadruped locomotion commonly adopt generic
policy architectures like fully connected MLPs. As such architectures contain
few inductive biases, it is common in practice to incorporate priors in the
form of rewards, training curricula, imitation data, or trajectory generators.
In nature, animals are born with priors in the form of their nervous system's
architecture, which has been shaped by evolution to confer innate ability and
efficient learning. For instance, a horse can walk within hours of birth and
can quickly improve with practice. Such architectural priors can also be useful
in ANN architectures for AI. In this work, we explore the advantages of a
biologically inspired ANN architecture for quadruped locomotion based on neural
circuits in the limbs and spinal cord of mammals. Our architecture achieves
good initial performance and comparable final performance to MLPs, while using
less data and orders of magnitude fewer parameters. Our architecture also
exhibits better generalization to task variations, even admitting deployment on
a physical robot without standard sim-to-real methods. This work shows that
neural circuits can provide valuable architectural priors for locomotion and
encourages future work in other sensorimotor skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Glider: Global and Local Instruction-Driven Expert Router 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingzhi Li, Prateek Yadav, Jaehong Yoon, Jie Peng, Yi-Lin Sung, Mohit Bansal, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of performant pre-trained models has led to a proliferation
of fine-tuned expert models that are specialized to particular domains. This
has enabled the creation of powerful and adaptive routing-based "Model
MoErging" methods with the goal of using expert modules to create an aggregate
system with improved performance or generalization. However, existing MoErging
methods often prioritize generalization to unseen tasks at the expense of
performance on held-in tasks, which limits its practical applicability in
real-world deployment scenarios. We observe that current token-level routing
mechanisms neglect the global semantic context of the input task. This
token-wise independence hinders effective expert selection for held-in tasks,
as routing decisions fail to incorporate the semantic properties of the task.
To address this, we propose, Global and Local Instruction Driven Expert Router
(GLIDER) that integrates a multi-scale routing mechanism, encompassing a
semantic global router and a learned local router. The global router leverages
LLM's advanced reasoning capabilities for semantic-related contexts to enhance
expert selection. Given the input query and LLM, the router generates semantic
task instructions that guide the retrieval of the most relevant experts across
all layers. This global guidance is complemented by a local router that
facilitates token-level routing decisions within each module, enabling finer
control and enhanced performance on unseen tasks. Our experiments using
T5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves
substantially improved held-in performance while maintaining strong
generalization on held-out tasks. We also perform ablations experiments to dive
deeper into the components of GLIDER. Our experiments highlight the importance
of our multi-scale routing that leverages LLM-driven semantic reasoning for
MoErging methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/UNITES-Lab/glider</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Initialization to Rule them All: Fine-tuning via Explained Variance
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are pre-trained on large-scale datasets and then
fine-tuned on a downstream task for a specific application. The most successful
and most commonly used fine-tuning method is to update the pre-trained weights
via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are
usually initialized at random with a uniform rank distribution across model
weights. Recent works focus on weight-driven initialization or learning of
adaptive ranks during training. Both approaches have only been investigated in
isolation, resulting in slow convergence or a uniform rank distribution, in
turn leading to sub-optimal performance. We propose to enhance LoRA by
initializing the new weights in a data-driven manner by computing singular
value decomposition on minibatches of activation vectors. Then, we initialize
the LoRA matrices with the obtained right-singular vectors and re-distribute
ranks among all weight matrices to explain the maximal amount of variance and
continue the standard LoRA fine-tuning procedure. This results in our new
method Explained Variance Adaptation (EVA). We apply EVA to a variety of
fine-tuning tasks ranging from language generation and understanding to image
classification and reinforcement learning. EVA exhibits faster convergence than
competitors and attains the highest average score across a multitude of tasks
per domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages + references and appendix, code available at
  https://github.com/ml-jku/EVA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied <span class="highlight-title">Agent</span> Interface: <span class="highlight-title">Benchmark</span>ing <span class="highlight-title">LLM</span>s for Embodied Decision Making <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to evaluate Large Language Models (LLMs) for embodied decision making.
While a significant body of work has been leveraging LLMs for decision making
in embodied environments, we still lack a systematic understanding of their
performance because they are usually applied in different domains, for
different purposes, and built based on different inputs and outputs.
Furthermore, existing evaluations tend to rely solely on a final success rate,
making it difficult to pinpoint what ability is missing in LLMs and where the
problem lies, which in turn blocks embodied agents from leveraging LLMs
effectively and selectively. To address these limitations, we propose a
generalized interface (Embodied Agent Interface) that supports the
formalization of various types of tasks and input-output specifications of
LLM-based modules. Specifically, it allows us to unify 1) a broad set of
embodied decision-making tasks involving both state and temporally extended
goals, 2) four commonly-used LLM-based modules for decision making: goal
interpretation, subgoal decomposition, action sequencing, and transition
modeling, and 3) a collection of fine-grained metrics which break down
evaluation into various types of errors, such as hallucination errors,
affordance errors, various types of planning errors, etc. Overall, our
benchmark offers a comprehensive assessment of LLMs' performance for different
subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI
systems, and providing insights for effective and selective use of LLMs in
embodied decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at NeurIPS 2024 in the Datasets and
  Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Simplicity Prevails: Rethinking Negative Preference Optimization for <span class="highlight-title">LLM</span>
  Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Rui<span class="highlight-author">qi Zhang</span>, Song Mei, Sijia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the problem of large language model (LLM)
unlearning, aiming to remove unwanted data influences and associated model
capabilities (e.g., copyrighted data or harmful content generation) while
preserving essential model utilities, without the need for retraining from
scratch. Despite the growing need for LLM unlearning, a principled optimization
framework remains lacking. To this end, we revisit the state-of-the-art
approach, negative preference optimization (NPO), and identify the issue of
reference model bias, which could undermine NPO's effectiveness, particularly
when unlearning forget data of varying difficulty. Given that, we propose a
simple yet effective unlearning optimization framework, called SimNPO, showing
that 'simplicity' in removing the reliance on a reference model (through the
lens of simple preference optimization) benefits unlearning. We also provide
deeper insights into SimNPO's advantages, supported by analysis using mixtures
of Markov chains. Furthermore, we present extensive experiments validating
SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU
and MUSE, and robustness against relearning attacks. Codes are available at
https://github.com/OPTML-Group/Unlearn-Simple.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quanda: An Interpretability Toolkit for Training Data Attribution
  Evaluation and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilyara Bareeva, Galip Ümit Yolcu, Anna Hedström, Niklas Schmolenski, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, training data attribution (TDA) methods have emerged as a
promising direction for the interpretability of neural networks. While research
around TDA is thriving, limited effort has been dedicated to the evaluation of
attributions. Similar to the development of evaluation metrics for traditional
feature attribution approaches, several standalone metrics have been proposed
to evaluate the quality of TDA methods across various contexts. However, the
lack of a unified framework that allows for systematic comparison limits trust
in TDA methods and stunts their widespread adoption. To address this research
gap, we introduce Quanda, a Python toolkit designed to facilitate the
evaluation of TDA methods. Beyond offering a comprehensive set of evaluation
metrics, Quanda provides a uniform interface for seamless integration with
existing TDA implementations across different repositories, thus enabling
systematic benchmarking. The toolkit is user-friendly, thoroughly tested,
well-documented, and available as an open-source library on PyPi and under
https://github.com/dilyabareeva/quanda.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructG2I: Synthesizing <span class="highlight-title">Image</span>s from <span class="highlight-title">Multimodal</span> Attributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we approach an overlooked yet critical task Graph2Image:
generating images from multimodal attributed graphs (MMAGs). This task poses
significant challenges due to the explosion in graph size, dependencies among
graph entities, and the need for controllability in graph conditions. To
address these challenges, we propose a graph context-conditioned diffusion
model called InstructG2I. InstructG2I first exploits the graph structure and
multimodal information to conduct informative neighbor sampling by combining
personalized page rank and re-ranking based on vision-language features. Then,
a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary
set of graph prompts to guide the denoising process of diffusion. Finally, we
propose graph classifier-free guidance, enabling controllable generation by
varying the strength of graph guidance and multiple connected edges to a node.
Extensive experiments conducted on three datasets from different domains
demonstrate the effectiveness and controllability of our approach. The code is
available at https://github.com/PeterGriffinJin/InstructG2I.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based
  Multi-Entity Action Recognition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Wen, Mengyuan Liu, Songtao Wu, Beichen Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based multi-entity action recognition is a challenging task aiming
to identify interactive actions or group activities involving multiple diverse
entities. Existing models for individuals often fall short in this task due to
the inherent distribution discrepancies among entity skeletons, leading to
suboptimal backbone optimization. To this end, we introduce a Convex Hull
Adaptive Shift based multi-Entity action recognition method (CHASE), which
mitigates inter-entity distribution gaps and unbiases subsequent backbones.
Specifically, CHASE comprises a learnable parameterized network and an
auxiliary objective. The parameterized network achieves plausible,
sample-adaptive repositioning of skeleton sequences through two key components.
First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new
origin of the coordinate system is within the skeleton convex hull. Second, the
Coefficient Learning Block provides a lightweight parameterization of the
mapping from skeleton sequences to their specific coefficients in convex
combinations. Moreover, to guide the optimization of this network for
discrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean
Discrepancy as the additional objective. CHASE operates as a sample-adaptive
normalization method to mitigate inter-entity distribution discrepancies,
thereby reducing data bias and improving the subsequent classifier's
multi-entity action recognition performance. Extensive experiments on six
datasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and
Volleyball, consistently verify our approach by seamlessly adapting to
single-entity backbones and boosting their performance in multi-entity
scenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Camera-ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Interpreting Visual Information Processing in Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) are powerful tools for processing and
understanding text and images. We study the processing of visual tokens in the
language model component of LLaVA, a prominent VLM. Our approach focuses on
analyzing the localization of object information, the evolution of visual token
representations across layers, and the mechanism of integrating visual
information for predictions. Through ablation studies, we demonstrated that
object identification accuracy drops by over 70\% when object-specific tokens
are removed. We observed that visual token representations become increasingly
interpretable in the vocabulary space across layers, suggesting an alignment
with textual tokens corresponding to image content. Finally, we found that the
model extracts object information from these refined representations at the
last token position for prediction, mirroring the process in text-only language
models for factual association tasks. These findings provide crucial insights
into how VLMs process and integrate visual information, bridging the gap
between our understanding of language and vision models, and paving the way for
more interpretable and controllable multimodal systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Stuffed Mamba: State Collapse and State Capacity of RNN-Based
  Long-Context Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, <span class="highlight-author">Zhiyuan Liu</span>, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One essential advantage of recurrent neural networks (RNNs) over
transformer-based language models is their linear computational complexity
concerning the sequence length, which makes them much faster in handling long
sequences during inference. However, most publicly available RNNs (e.g., Mamba
and RWKV) are trained on sequences with less than 10K tokens, and their
effectiveness in longer contexts remains largely unsatisfying so far. In this
paper, we study the cause of the inability to process long context for RNNs and
suggest critical mitigations. We examine two practical concerns when applying
state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to
inputs longer than the training length and (2) the upper bound of memory
capacity. Addressing the first concern, we first investigate *state collapse*
(SC), a phenomenon that causes severe performance degradation on sequence
lengths not encountered during training. With controlled experiments, we
attribute this to overfitting due to the recurrent state being
overparameterized for the training length. For the second concern, we train a
series of Mamba-2 models on long documents to empirically estimate the
recurrent state capacity in language modeling and passkey retrieval. Then,
three SC mitigation methods are proposed to improve Mamba-2's length
generalizability, allowing the model to process more than 1M tokens without SC.
We also find that the recurrent state capacity in passkey retrieval scales
exponentially to the state size, and we empirically train a Mamba-2 370M with
near-perfect passkey retrieval accuracy on 256K context length. This suggests a
promising future for RNN-based long-context modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cheating Automatic <span class="highlight-title">LLM</span> <span class="highlight-title">Benchmark</span>s: Null Models Achieve High Win Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and
MT-Bench, have become popular for evaluating language models due to their
cost-effectiveness and scalability compared to human evaluation. Achieving high
win rates on these benchmarks can significantly boost the promotional impact of
newly released language models. This promotional benefit may motivate tricks,
such as manipulating model output length or style to game win rates, even
though several mechanisms have been developed to control length and disentangle
style to reduce gameability. Nonetheless, we show that even a "null model" that
always outputs a constant response (irrelevant to input instructions) can cheat
automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on
AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.
Moreover, the crafted cheating outputs are transferable because we assume that
the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are
private and cannot be accessed. While our experiments are primarily
proof-of-concept, an adversary could use LLMs to generate more imperceptible
cheating responses, unethically benefiting from high win rates and promotional
impact. Our findings call for the development of anti-cheating mechanisms for
reliable automatic benchmarks. The code is available at
https://github.com/sail-sg/Cheating-LLM-Benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning: Less Forgetting, More OOD Generalization via
  Adaptive Contrastive Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Rezaei, Mohammad Sabokrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models often suffer from catastrophic forgetting of
previously learned knowledge when learning new classes. Various methods have
been proposed to mitigate this issue. However, rehearsal-based learning, which
retains samples from previous classes, typically achieves good performance but
tends to memorize specific instances, struggling with Out-of-Distribution (OOD)
generalization. This often leads to high forgetting rates and poor
generalization. Surprisingly, the OOD generalization capabilities of these
methods have been largely unexplored. In this paper, we highlight this issue
and propose a simple yet effective strategy inspired by contrastive learning
and data-centric principles to address it. We introduce Adaptive Contrastive
Replay (ACR), a method that employs dual optimization to simultaneously train
both the encoder and the classifier. ACR adaptively populates the replay buffer
with misclassified samples while ensuring a balanced representation of classes
and tasks. By refining the decision boundary in this way, ACR achieves a
balance between stability and plasticity. Our method significantly outperforms
previous approaches in terms of OOD generalization, achieving an improvement of
13.41\% on Split CIFAR-100, 9.91\% on Split Mini-ImageNet, and 5.98\% on Split
Tiny-ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Approach for Auto <span class="highlight-title">Generation</span> of Labeling Functions for Software
  Engineering Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ebube Alor, Ahmad Abdellatif, SayedHassan Khatoonabadi, Emad Shihab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineering (SE) chatbots are increasingly gaining attention for
their role in enhancing development processes. At the core of chatbots are the
Natural Language Understanding platforms (NLUs), which enable them to
comprehend and respond to user queries. Before deploying NLUs, there is a need
to train them with labeled data. However, acquiring such labeled data for SE
chatbots is challenging due to the scarcity of high-quality datasets. This
challenge arises because training SE chatbots requires specialized vocabulary
and phrases not found in typical language datasets. Consequently, chatbot
developers often resort to manually annotating user queries to gather the data
necessary for training effective chatbots, a process that is both
time-consuming and resource-intensive. Previous studies propose approaches to
support chatbot practitioners in annotating users' posed queries. However,
these approaches require human intervention to generate rules, called labeling
functions (LFs), that identify and categorize user queries based on specific
patterns in the data. To address this issue, we propose an approach to
automatically generate LFs by extracting patterns from labeled user queries. We
evaluate the effectiveness of our approach by applying it to the queries of
four diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)
and measure the performance improvement gained from training the NLU on the
queries labeled by the generated LFs. We find that the generated LFs
effectively label data with AUC scores of up to 85.3%, and NLU's performance
improvement of up to 27.2% across the studied datasets. Furthermore, our
results show that the number of LFs used to generate LFs affects the labeling
performance. We believe that our approach can save time and resources in
labeling users' queries, allowing practitioners to focus on core chatbot
functionalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Software Engineering for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collusion Detection with Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Gomes, Jannis Kueck, Mara Mattes, Martin Spindler, Alexey Zaytsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collusion is a complex phenomenon in which companies secretly collaborate to
engage in fraudulent practices. This paper presents an innovative methodology
for detecting and predicting collusion patterns in different national markets
using neural networks (NNs) and graph neural networks (GNNs). GNNs are
particularly well suited to this task because they can exploit the inherent
network structures present in collusion and many other economic problems. Our
approach consists of two phases: In Phase I, we develop and train models on
individual market datasets from Japan, the United States, two regions in
Switzerland, Italy, and Brazil, focusing on predicting collusion in single
markets. In Phase II, we extend the models' applicability through zero-shot
learning, employing a transfer learning approach that can detect collusion in
markets in which training data is unavailable. This phase also incorporates
out-of-distribution (OOD) generalization to evaluate the models' performance on
unseen datasets from other countries and regions. In our empirical study, we
show that GNNs outperform NNs in detecting complex collusive patterns. This
research contributes to the ongoing discourse on preventing collusion and
optimizing detection methodologies, providing valuable guidance on the use of
NNs and GNNs in economic applications to enhance market fairness and economic
welfare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOOSE-Chem: <span class="highlight-title">Large Language Model</span>s for Rediscovering Unseen Chemistry
  Scientific Hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery contributes largely to human society's prosperity, and
recent progress shows that LLMs could potentially catalyze this process.
However, it is still unclear whether LLMs can discover novel and valid
hypotheses in chemistry. In this work, we investigate this central research
question: Can LLMs automatically discover novel and valid chemistry research
hypotheses given only a chemistry research background (consisting of a research
question and/or a background survey), without limitation on the domain of the
research question? After extensive discussions with chemistry experts, we
propose an assumption that a majority of chemistry hypotheses can be resulted
from a research background and several inspirations. With this key insight, we
break the central question into three smaller fundamental questions. In brief,
they are: (1) given a background question, whether LLMs can retrieve good
inspirations; (2) with background and inspirations, whether LLMs can lead to
hypothesis; and (3) whether LLMs can identify good hypotheses to rank them
higher. To investigate these questions, we construct a benchmark consisting of
51 chemistry papers published in Nature, Science, or a similar level in 2024
(all papers are only available online since 2024). Every paper is divided by
chemistry PhD students into three components: background, inspirations, and
hypothesis. The goal is to rediscover the hypothesis, given only the background
and a large randomly selected chemistry literature corpus consisting the ground
truth inspiration papers, with LLMs trained with data up to 2023. We also
develop an LLM-based multi-agent framework that leverages the assumption,
consisting of three stages reflecting the three smaller questions. The proposed
method can rediscover many hypotheses with very high similarity with the ground
truth ones, covering the main innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and Benchmark are available at
  https://github.com/ZonglinY/MOOSE-Chem.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let's Ask GNN: Empowering <span class="highlight-title">Large Language Model</span> for Graph In-Context
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyu Hu, Yichuan Li, Zhengyu Chen, Jingang Wang, Han Liu, Kyumin Lee, Kaize Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world
systems, yet leveraging large language models (LLMs) for TAGs presents unique
challenges due to the gap between sequential text processing and
graph-structured data. We introduce AskGNN, a novel approach that bridges this
gap by leveraging In-Context Learning (ICL) to integrate graph data and
task-specific information into LLMs. AskGNN employs a Graph Neural Network
(GNN)-powered structure-enhanced retriever to select labeled nodes across
graphs, incorporating complex graph structures and their supervision signals.
Our learning-to-retrieve algorithm optimizes the retriever to select example
nodes that maximize LLM performance on graph. Experiments across three tasks
and seven LLMs demonstrate AskGNN's superior effectiveness in graph task
performance, opening new avenues for applying LLMs to graph-structured data
without extensive fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards xAI: Configuring RNN Weights using Domain Knowledge for MIMO
  Receive Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is making a profound impact in the physical layer of wireless
communications. Despite exhibiting outstanding empirical performance in tasks
such as MIMO receive processing, the reasons behind the demonstrated superior
performance improvement remain largely unclear. In this work, we advance the
field of Explainable AI (xAI) in the physical layer of wireless communications
utilizing signal processing principles. Specifically, we focus on the task of
MIMO-OFDM receive processing (e.g., symbol detection) using reservoir computing
(RC), a framework within recurrent neural networks (RNNs), which outperforms
both conventional and other learning-based MIMO detectors. Our analysis
provides a signal processing-based, first-principles understanding of the
corresponding operation of the RC. Building on this fundamental understanding,
we are able to systematically incorporate the domain knowledge of wireless
systems (e.g., channel statistics) into the design of the underlying RNN by
directly configuring the untrained RNN weights for MIMO-OFDM symbol detection.
The introduced RNN weight configuration has been validated through extensive
simulations demonstrating significant performance improvements. This
establishes a foundation for explainable RC-based architectures in MIMO-OFDM
receive processing and provides a roadmap for incorporating domain knowledge
into the design of neural networks for NextG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Decision Transformer: External Memory for In-context
  RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Schmied, Fabian Paischer, Vihang Patil, Markus Hofmarcher, Razvan Pascanu, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is the ability of a model to learn a new task by
observing a few exemplars in its context. While prevalent in NLP, this
capability has recently also been observed in Reinforcement Learning (RL)
settings. Prior in-context RL methods, however, require entire episodes in the
agent's context. Given that complex environments typically lead to long
episodes with sparse rewards, these methods are constrained to simple
environments with short episodes. To address these challenges, we introduce
Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external
memory mechanism to store past experiences from which it retrieves only
sub-trajectories relevant for the current situation. The retrieval component in
RA-DT does not require training and can be entirely domain-agnostic. We
evaluate the capabilities of RA-DT on grid-world environments, robotics
simulations, and procedurally-generated video games. On grid-worlds, RA-DT
outperforms baselines, while using only a fraction of their context length.
Furthermore, we illuminate the limitations of current in-context RL methods on
complex environments and discuss future directions. To facilitate future
research, we release datasets for four of the considered environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReIFE: Re-evaluating Instruction-Following Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic evaluation of instruction following typically involves using
large language models (LLMs) to assess response quality. However, there is a
lack of comprehensive evaluation of these LLM-based evaluators across two
dimensions: the base LLMs and the evaluation protocols. Therefore, we present a
thorough meta-evaluation of instruction following, including 25 base LLMs and
15 recently proposed evaluation protocols, on 4 human-annotated datasets,
assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows
us to identify the best-performing base LLMs and evaluation protocols with a
high degree of robustness. Moreover, our large-scale evaluation reveals: (1)
Base LLM performance ranking remains largely consistent across evaluation
protocols, with less capable LLMs showing greater improvement from protocol
enhancements; (2) Robust evaluation of evaluation protocols requires many base
LLMs with varying capability levels, as protocol effectiveness can depend on
the base LLM used; (3) Evaluation results on different datasets are not always
consistent, so a rigorous evaluation requires multiple datasets with
distinctive features. We release our meta-evaluation suite ReIFE, which
provides the codebase and evaluation result collection for more than 500
LLM-evaluator configurations, to support future research in
instruction-following evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result
  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gentle Introduction and Tutorial on Deep Generative Models in
  Transportation Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongjin Choi, Zhixiong Jin, Seungwoo Ham, Jiwon Kim, Lijun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Generative Models (DGMs) have rapidly advanced in recent years, becoming
essential tools in various fields due to their ability to learn complex data
distributions and generate synthetic data. Their importance in transportation
research is increasingly recognized, particularly for applications like traffic
data generation, prediction, and feature extraction. This paper offers a
comprehensive introduction and tutorial on DGMs, with a focus on their
applications in transportation. It begins with an overview of generative
models, followed by detailed explanations of fundamental models, a systematic
review of the literature, and practical tutorial code to aid implementation.
The paper also discusses current challenges and opportunities, highlighting how
these models can be effectively utilized and further developed in
transportation research. This paper serves as a valuable reference, guiding
researchers and practitioners from foundational knowledge to advanced
applications of DGMs in transportation research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 21 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InAttention: Linear Context Scaling for Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Eisner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  VRAM requirements for transformer models scale quadratically with context
length due to the self-attention mechanism. In this paper we modify the
decoder-only transformer, replacing self-attention with InAttention, which
scales linearly with context length during inference by having tokens attend
only to initial states. Benchmarking shows that InAttention significantly
reduces VRAM usage during inference, enabling handling of long sequences on
consumer GPUs. We corroborate that fine-tuning extends context length
efficiently, improving performance on long sequences without high training
costs. InAttention offers a scalable solution for long-range dependencies in
transformer models, paving the way for further optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Epsilon Net and Piercing Set for Geometric Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujoy Bhore, Devdan Dey, Satyam Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  VC-dimension and $\varepsilon$-nets are key concepts in Statistical Learning
Theory. Intuitively, VC-dimension is a measure of the size of a class of sets.
The famous $\varepsilon$-net theorem, a fundamental result in Discrete
Geometry, asserts that if the VC-dimension of a set system is bounded, then a
small sample exists that intersects all sufficiently large sets.
  In online learning scenarios where data arrives sequentially, the
VC-dimension helps to bound the complexity of the set system, and
$\varepsilon$-nets ensure the selection of a small representative set. This
sampling framework is crucial in various domains, including spatial data
analysis, motion planning in dynamic environments, optimization of sensor
networks, and feature extraction in computer vision, among others. Motivated by
these applications, we study the online $\varepsilon$-net problem for geometric
concepts with bounded VC-dimension. While the offline version of this problem
has been extensively studied, surprisingly, there are no known theoretical
results for the online version to date. We present the first deterministic
online algorithm with an optimal competitive ratio for intervals in
$\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal
competitive ratio for axis-aligned boxes in $\mathbb{R}^d$, for $d\le 3$.
Furthermore, we introduce a novel technique to analyze similar-sized objects of
constant description complexity in $\mathbb{R}^d$, which may be of independent
interest. Next, we focus on the continuous version of this problem, where
ranges of the set system are geometric concepts in $\mathbb{R}^d$ arriving in
an online manner, but the universe is the entire space, and the objective is to
choose a small sample that intersects all the ranges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating the Language Mismatch and Repetition Issues in <span class="highlight-title">LLM</span>-based
  Machine Translation via Model <span class="highlight-title">Editing</span> <span class="chip">EMNLP'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, Ying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently revolutionized the NLP field,
while they still fall short in some specific down-stream tasks. In the work, we
focus on utilizing LLMs to perform machine translation, where we observe that
two patterns of errors frequently occur and drastically affect the translation
quality: language mismatch and repetition. The work sets out to explore the
potential for mitigating these two issues by leveraging model editing methods,
e.g., by locating Feed-Forward Network (FFN) neurons or something that are
responsible for the errors and deactivating them in the inference time. We find
that directly applying such methods either limited effect on the targeted
errors or has significant negative side-effect on the general translation
quality, indicating that the located components may also be crucial for
ensuring machine translation with LLMs on the rails. To this end, we propose to
refine the located components by fetching the intersection of the locating
results under different language settings, filtering out the aforementioned
information that is irrelevant to targeted errors. The experiment results
empirically demonstrate that our methods can effectively reduce the language
mismatch and repetition ratios and meanwhile enhance or keep the general
translation quality in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, EMNLP'2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergent properties with repeated examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        François Charton, Julia Kempe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the performance of transformers as a function of the number of
repetitions of training examples with algorithmically generated datasets. On
three problems of mathematics: the greatest common divisor, modular
multiplication, and matrix eigenvalues, we show that for a fixed number of
training steps, models trained on smaller sets of repeated examples outperform
models trained on larger sets of single-use examples. We also demonstrate that
two-set training - repeated use of a small random subset of examples, along
normal sampling on the rest of the training set - provides for faster learning
and better performance. This highlights that the benefits of repetition can
outweigh those of data diversity. These datasets and problems provide a
controlled setting to shed light on the still poorly understood interplay
between generalization and memorization in deep learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributionally Robust Clustered Federated Learning: A Case Study in
  Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xenia Konti, Hans Riess, Manos Giannopoulos, Yi Shen, Michael J. Pencina, Nicoleta J. Economou-Zavlanos, Michael M. Zavlanos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of heterogeneous data distributions
in cross-silo federated learning by introducing a novel algorithm, which we
term Cross-silo Robust Clustered Federated Learning (CS-RCFL). Our approach
leverages the Wasserstein distance to construct ambiguity sets around each
client's empirical distribution that capture possible distribution shifts in
the local data, enabling evaluation of worst-case model performance. We then
propose a model-agnostic integer fractional program to determine the optimal
distributionally robust clustering of clients into coalitions so that possible
biases in the local models caused by statistically heterogeneous client
datasets are avoided, and analyze our method for linear and logistic regression
models. Finally, we discuss a federated learning protocol that ensures the
privacy of client distributions, a critical consideration, for instance, when
clients are healthcare institutions. We evaluate our algorithm on synthetic and
real-world healthcare data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, Accepted to IEEE CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Contemporary CATE Models Capture Real-World Heterogeneity? Findings
  from a Large-Scale <span class="highlight-title">Benchmark</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haining Yu, Yizhou Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present unexpected findings from a large-scale benchmark study evaluating
Conditional Average Treatment Effect (CATE) estimation algorithms. By running
16 modern CATE models across 43,200 datasets, we find that: (a) 62\% of CATE
estimates have a higher Mean Squared Error (MSE) than a trivial zero-effect
predictor, rendering them ineffective; (b) in datasets with at least one useful
CATE estimate, 80\% still have higher MSE than a constant-effect model; and (c)
Orthogonality-based models outperform other models only 30\% of the time,
despite widespread optimism about their performance. These findings expose
significant limitations in current CATE models and suggest ample opportunities
for further research.
  Our findings stem from a novel application of \textit{observational
sampling}, originally developed to evaluate Average Treatment Effect (ATE)
estimates from observational methods with experiment data. To adapt
observational sampling for CATE evaluation, we introduce a statistical
parameter, $Q$, equal to MSE minus a constant and preserves the ranking of
models by their MSE. We then derive a family of sample statistics, collectively
called $\hat{Q}$, that can be computed from real-world data. We prove that
$\hat{Q}$ is a consistent estimator of $Q$ under mild technical conditions.
When used in observational sampling, $\hat{Q}$ is unbiased and asymptotically
selects the model with the smallest MSE. To ensure the benchmark reflects
real-world heterogeneity, we handpick datasets where outcomes come from field
rather than simulation. By combining the new observational sampling method, new
statistics, and real-world datasets, the benchmark provides a unique
perspective on CATE estimator performance and uncover gaps in capturing
real-world heterogeneity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tri-Level Navigator: <span class="highlight-title">LLM</span>-Empowered Tri-Level Learning for Time Series
  OOD Generalization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengtao Jian, Kai Yang, Yang Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-Distribution (OOD) generalization in machine learning is a burgeoning
area of study. Its primary goal is to enhance the adaptability and resilience
of machine learning models when faced with new, unseen, and potentially
adversarial data that significantly diverges from their original training
datasets. In this paper, we investigate time series OOD generalization via
pre-trained Large Language Models (LLMs). We first propose a novel
\textbf{T}ri-level learning framework for \textbf{T}ime \textbf{S}eries
\textbf{O}OD generalization, termed TTSO, which considers both sample-level and
group-level uncertainties. This formula offers a fresh theoretic perspective
for formulating and analyzing OOD generalization problem. In addition, we
provide a theoretical analysis to justify this method is well motivated. We
then develop a stratified localization algorithm tailored for this tri-level
optimization problem, theoretically demonstrating the guaranteed convergence of
the proposed algorithm. Our analysis also reveals that the iteration complexity
to obtain an $\epsilon$-stationary point is bounded by
O($\frac{1}{\epsilon^{2}}$). Extensive experiments on real-world datasets have
been conducted to elucidate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Estimators of Squared Calibration Errors in Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian G. Gruber, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a mean-squared error-based risk that enables the
comparison and optimization of estimators of squared calibration errors in
practical settings. Improving the calibration of classifiers is crucial for
enhancing the trustworthiness and interpretability of machine learning models,
especially in sensitive decision-making scenarios. Although various calibration
(error) estimators exist in the current literature, there is a lack of guidance
on selecting the appropriate estimator and tuning its hyperparameters. By
leveraging the bilinear structure of squared calibration errors, we reformulate
calibration estimation as a regression problem with independent and identically
distributed (i.i.d.) input pairs. This reformulation allows us to quantify the
performance of different estimators even for the most challenging calibration
criterion, known as canonical calibration. Our approach advocates for a
training-validation-testing pipeline when estimating a calibration error on an
evaluation dataset. We demonstrate the effectiveness of our pipeline by
optimizing existing calibration estimators and comparing them with novel kernel
ridge regression-based estimators on standard image classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Representation Learning in Temporal Data via Single-Parent
  Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Brouillard, Sébastien Lachapelle, Julia Kaltenborn, Yaniv Gurwicz, Dhanya Sridhar, Alexandre Drouin, Peer Nowack, Jakob Runge, David Rolnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific research often seeks to understand the causal structure underlying
high-level variables in a system. For example, climate scientists study how
phenomena, such as El Ni\~no, affect other climate processes at remote
locations across the globe. However, scientists typically collect low-level
measurements, such as geographically distributed temperature readings. From
these, one needs to learn both a mapping to causally-relevant latent variables,
such as a high-level representation of the El Ni\~no phenomenon and other
processes, as well as the causal model over them. The challenge is that this
task, called causal representation learning, is highly underdetermined from
observational data alone, requiring other constraints during learning to
resolve the indeterminacies. In this work, we consider a temporal model with a
sparsity assumption, namely single-parent decoding: each observed low-level
variable is only affected by a single latent variable. Such an assumption is
reasonable in many scientific applications that require finding groups of
low-level variables, such as extracting regions from geographically gridded
measurement data in climate research or capturing brain regions from neural
activity data. We demonstrate the identifiability of the resulting model and
propose a differentiable method, Causal Discovery with Single-parent Decoding
(CDSD), that simultaneously learns the underlying latents and a causal graph
over them. We assess the validity of our theoretical results using simulated
data and showcase the practical validity of our method in an application to
real-world data from the climate science field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Through the Looking Glass: Mirror Schrödinger Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leticia Mattos Da Silva, Silvia Sellán, Justin Solomon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resampling from a target measure whose density is unknown is a fundamental
problem in mathematical statistics and machine learning. A setting that
dominates the machine learning literature consists of learning a map from an
easy-to-sample prior, such as the Gaussian distribution, to a target measure.
Under this model, samples from the prior are pushed forward to generate a new
sample on the target measure, which is often difficult to sample from directly.
In this paper, we propose a new model for conditional resampling called mirror
Schr\"odinger bridges. Our key observation is that solving the Schr\"odinger
bridge problem between a distribution and itself provides a natural way to
produce new samples from conditional distributions, giving in-distribution
variations of an input data point. We show how to efficiently solve this
largely overlooked version of the Schr\"odinger bridge problem. We prove that
our proposed method leads to significant algorithmic simplifications over
existing alternatives, in addition to providing control over in-distribution
variation. Empirically, we demonstrate how these benefits can be leveraged to
produce proximal samples in a number of application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Distribution Matching of Representations via Noise-Injected
  Deep InfoMax 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Butakov, Alexander Sememenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey Frolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep InfoMax (DIM) is a well-established method for self-supervised
representation learning (SSRL) based on maximization of the mutual information
between the input and the output of a deep neural network encoder. Despite the
DIM and contrastive SSRL in general being well-explored, the task of learning
representations conforming to a specific distribution (i.e., distribution
matching, DM) is still under-addressed. Motivated by the importance of DM to
several downstream tasks (including generative modeling, disentanglement,
outliers detection and other), we enhance DIM to enable automatic matching of
learned representations to a selected prior distribution. To achieve this, we
propose injecting an independent noise into the normalized outputs of the
encoder, while keeping the same InfoMax training objective. We show that such
modification allows for learning uniformly and normally distributed
representations, as well as representations of other absolutely continuous
distributions. Our approach is tested on various downstream tasks. The results
indicate a moderate trade-off between the performance on the downstream tasks
and quality of DM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 fugures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span> Density Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhil Premkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the use of diffusion models as neural density estimators. The
current approach to this problem involves converting the generative process to
a smooth flow, known as the Probability Flow ODE. The log density at a given
sample can be obtained by solving the ODE with a black-box solver. We introduce
a new, highly parallelizable method that computes log densities without the
need to solve a flow. Our approach is based on estimating a path integral by
Monte Carlo, in a manner identical to the simulation-free training of diffusion
models. We also study how different training parameters affect the accuracy of
the density calculation, and offer insights into how these models can be made
more scalable and efficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages + references, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Autoencoders Reveal Universal Feature Spaces Across Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones. This makes it
difficult to disentangle and match features across different models. To address
this issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics like Singular Value Canonical Correlation Analysis to
analyze these SAE features across different LLMs. Our experiments reveal
significant similarities in SAE feature spaces across various LLMs, providing
new evidence for feature universality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaRC: Mitigating Graph Structure Shifts during Test-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Bao, Zhichen Zeng, Zhining Liu, Hanghang Tong, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful as they are, graph neural networks (GNNs) are known to be vulnerable
to distribution shifts. Recently, test-time adaptation (TTA) has attracted
attention due to its ability to adapt a pre-trained model to a target domain
without re-accessing the source domain. However, existing TTA algorithms are
primarily designed for attribute shifts in vision tasks, where samples are
independent. These methods perform poorly on graph data that experience
structure shifts, where node connectivity differs between source and target
graphs. We attribute this performance gap to the distinct impact of node
attribute shifts versus graph structure shifts: the latter significantly
degrades the quality of node representations and blurs the boundaries between
different node categories. To address structure shifts in graphs, we propose
AdaRC, an innovative framework designed for effective and efficient adaptation
to structure shifts by adjusting the hop-aggregation parameters in GNNs. To
enhance the representation quality, we design a prediction-informed clustering
loss to encourage the formation of distinct clusters for different node
categories. Additionally, AdaRC seamlessly integrates with existing TTA
algorithms, allowing it to handle attribute shifts effectively while improving
overall performance under combined structure and attribute shifts. We validate
the effectiveness of AdaRC on both synthetic and real-world datasets,
demonstrating its robustness across various combinations of structure and
attribute shifts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosis of Malignant Lymphoma Cancer Using Hybrid Optimized Techniques
  Based on Dense Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salah A. Aly, Ali Bakhiet, Mazen Balat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lymphoma diagnosis, particularly distinguishing between subtypes, is critical
for effective treatment but remains challenging due to the subtle morphological
differences in histopathological images. This study presents a novel hybrid
deep learning framework that combines DenseNet201 for feature extraction with a
Dense Neural Network (DNN) for classification, optimized using the Harris Hawks
Optimization (HHO) algorithm. The model was trained on a dataset of 15,000
biopsy images, spanning three lymphoma subtypes: Chronic Lymphocytic Leukemia
(CLL), Follicular Lymphoma (FL), and Mantle Cell Lymphoma (MCL). Our approach
achieved a testing accuracy of 99.33\%, demonstrating significant improvements
in both accuracy and model interpretability. Comprehensive evaluation using
precision, recall, F1-score, and ROC-AUC underscores the model's robustness and
potential for clinical adoption. This framework offers a scalable solution for
improving diagnostic accuracy and efficiency in oncology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 4 tables, IEEE ICCA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DLGNet: Hyperedge Classification through Directed Line Graphs for
  Chemical Reactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Fiorini, Giulia M. Bovolenta, Stefano Coniglio, Michele Ciavotta, Pietro Morerio, Michele Parrinello, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs and hypergraphs provide powerful abstractions for modeling
interactions among a set of entities of interest and have been attracting a
growing interest in the literature thanks to many successful applications in
several fields. In particular, they are rapidly expanding in domains such as
chemistry and biology, especially in the areas of drug discovery and molecule
generation. One of the areas witnessing the fasted growth is the chemical
reactions field, where chemical reactions can be naturally encoded as directed
hyperedges of a hypergraph. In this paper, we address the chemical reaction
classification problem by introducing the notation of a Directed Line Graph
(DGL) associated with a given directed hypergraph. On top of it, we build the
Directed Line Graph Network (DLGNet), the first spectral-based Graph Neural
Network (GNN) expressly designed to operate on a hypergraph via its DLG
transformation. The foundation of DLGNet is a novel Hermitian matrix, the
Directed Line Graph Laplacian, which compactly encodes the directionality of
the interactions taking place within the directed hyperedges of the hypergraph
thanks to the DLG representation. The Directed Line Graph Laplacian enjoys many
desirable properties, including admitting an eigenvalue decomposition and being
positive semidefinite, which make it well-suited for its adoption within a
spectral-based GNN. Through extensive experiments on chemical reaction
datasets, we show that DGLNet significantly outperforms the existing
approaches, achieving on a collection of real-world datasets an average
relative-percentage-difference improvement of 33.01%, with a maximum
improvement of 37.71%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang, Byeoli Choi, Donghoon Shin, Sung-hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ELMO, a real-time upsampling motion capture framework
designed for a single LiDAR sensor. Modeled as a conditional autoregressive
transformer-based upsampling motion generator, ELMO achieves 60 fps motion
capture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is
the coupling of the self-attention mechanism with thoughtfully designed
embedding modules for motion and point clouds, significantly elevating the
motion quality. To facilitate accurate motion capture, we develop a one-time
skeleton calibration model capable of predicting user skeleton offsets from a
single-frame point cloud. Additionally, we introduce a novel data augmentation
technique utilizing a LiDAR simulator, which enhances global root tracking to
improve environmental understanding. To demonstrate the effectiveness of our
method, we compare ELMO with state-of-the-art methods in both image-based and
point cloud-based motion capture. We further conduct an ablation study to
validate our design principles. ELMO's fast inference time makes it well-suited
for real-time applications, exemplified in our demo video featuring live
streaming and interactive gaming scenarios. Furthermore, we contribute a
high-quality LiDAR-mocap synchronized dataset comprising 20 different subjects
performing a range of motions, which can serve as a valuable resource for
future research. The dataset and evaluation code are available at {\blue
\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Support Vector Boosting Machine (SVBM): Enhancing Classification
  Performance with AdaBoost and Residual Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbo Jacob Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In traditional boosting algorithms, the focus on misclassified training
samples emphasizes their importance based on difficulty during the learning
process. While using a standard Support Vector Machine (SVM) as a weak learner
in an AdaBoost framework can enhance model performance by concentrating on
error samples, this approach introduces significant challenges. Specifically,
SVMs, characterized by their stability and robustness, may require
destabilization to fit the boosting paradigm, which in turn can constrain
performance due to reliance on the weighted results from preceding iterations.
To address these challenges, we propose the Support Vector Boosting Machine
(SVBM), which integrates a novel subsampling process with SVM algorithms and
residual connection techniques. This method updates sample weights by
considering both the current model's predictions and the outputs from prior
rounds, allowing for effective sparsity control. The SVBM framework enhances
the ability to form complex decision boundaries, thereby improving
classification performance. The MATLAB source code for SVBM can be accessed at
https://github.com/junbolian/SVBM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The MATLAB source code for SVBM can be accessed at
  https://github.com/junbolian/SVBM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faithful Interpretation for Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Hu, Tianhao Huang, Lu Yu, Wanyu Lin, Tianhang Zheng, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, attention mechanisms have garnered increasing attention in Graph
Neural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph
Transformers (GTs). It is not only due to the commendable boost in performance
they offer but also its capacity to provide a more lucid rationale for model
behaviors, which are often viewed as inscrutable. However, Attention-based GNNs
have demonstrated instability in interpretability when subjected to various
sources of perturbations during both training and testing phases, including
factors like additional edges or nodes. In this paper, we propose a solution to
this problem by introducing a novel notion called Faithful Graph
Attention-based Interpretation (FGAI). In particular, FGAI has four crucial
properties regarding stability and sensitivity to interpretation and final
output distribution. Built upon this notion, we propose an efficient
methodology for obtaining FGAI, which can be viewed as an ad hoc modification
to the canonical Attention-based GNNs. To validate our proposed solution, we
introduce two novel metrics tailored for graph interpretation assessment.
Experimental results demonstrate that FGAI exhibits superior stability and
preserves the interpretability of attention under various forms of
perturbations and randomness, which makes FGAI a more faithful and reliable
explanation tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Alignment for <span class="highlight-title">Generation</span>: Training <span class="highlight-title">Diffusion</span> Transformers
  Is Easier Than You Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that the denoising process in (generative)
diffusion models can induce meaningful (discriminative) representations inside
the model, though the quality of these representations still lags behind those
learned through recent self-supervised learning methods. We argue that one main
bottleneck in training large-scale diffusion models for generation lies in
effectively learning these representations. Moreover, training can be made
easier by incorporating high-quality external visual representations, rather
than relying solely on the diffusion models to learn them independently. We
study this by introducing a straightforward regularization called
REPresentation Alignment (REPA), which aligns the projections of noisy input
hidden states in denoising networks with clean image representations obtained
from external, pretrained visual encoders. The results are striking: our simple
strategy yields significant improvements in both training efficiency and
generation quality when applied to popular diffusion and flow-based
transformers, such as DiTs and SiTs. For instance, our method can speed up SiT
training by over 17.5$\times$, matching the performance (without
classifier-free guidance) of a SiT-XL model trained for 7M steps in less than
400K steps. In terms of final generation quality, our approach achieves
state-of-the-art results of FID=1.42 using classifier-free guidance with the
guidance interval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Project page: https://sihyun.me/REPA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Bitcoin Market Trends with Enhanced Technical Indicator
  Integration and Classification Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelatif Hafid, Mohamed Rahouti, Linglong Kong, Maad Ebrahim, Mohamed Adel Serhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the high potential for profit, trading has become increasingly
attractive to investors as the cryptocurrency and stock markets rapidly expand.
However, because financial markets are intricate and dynamic, accurately
predicting prices remains a significant challenge. The volatile nature of the
cryptocurrency market makes it even harder for traders and investors to make
decisions. This study presents a machine learning model based on classification
to forecast the direction of the cryptocurrency market, i.e., whether prices
will increase or decrease. The model is trained using historical data and
important technical indicators such as the Moving Average Convergence
Divergence, the Relative Strength Index, and Bollinger Bands. We illustrate our
approach with an empirical study of the closing price of Bitcoin. Several
simulations, including a confusion matrix and Receiver Operating Characteristic
curve, are used to assess the model's performance, and the results show a
buy/sell signal accuracy of over 92%. These findings demonstrate how machine
learning models can assist investors and traders of cryptocurrencies in making
wise/informed decisions in a very volatile market.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral and Rhythm Features for Audio Classification with Deep
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Friedrich Wolf-Monheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are widely used in computer vision. They
can be used not only for conventional digital image material to recognize
patterns, but also for feature extraction from digital imagery representing
spectral and rhythm features extracted from time-domain digital audio signals
for the acoustic classification of sounds. Different spectral and rhythm
feature representations like mel-scaled spectrograms, mel-frequency cepstral
coefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)
chromagrams, constant-Q transform (CQT) chromagrams and chroma energy
normalized statistics (CENS) chromagrams are investigated in terms of the audio
classification performance using a deep convolutional neural network. It can be
clearly shown that the mel-scaled spectrograms and the mel-frequency cepstral
coefficients (MFCCs) perform significantly better then the other spectral and
rhythm features investigated in this research for audio classification tasks
using deep CNNs. The experiments were carried out with the aid of the ESC-50
dataset with 2,000 labeled environmental audio recordings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Exoplanet Mass using Machine Learning on Incomplete <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Lalande, Elizabeth Tasker, Kenji Doya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exoplanet archive is an incredible resource of information on the
properties of discovered extrasolar planets, but statistical analysis has been
limited by the number of missing values. One of the most informative bulk
properties is planet mass, which is particularly challenging to measure with
more than 70\% of discovered planets with no measured value. We compare the
capabilities of five different machine learning algorithms that can utilize
multidimensional incomplete datasets to estimate missing properties for
imputing planet mass. The results are compared when using a partial subset of
the archive with a complete set of six planet properties, and where all planet
discoveries are leveraged in an incomplete set of six and eight planet
properties. We find that imputation results improve with more data even when
the additional data is incomplete, and allows a mass prediction for any planet
regardless of which properties are known. Our favored algorithm is the newly
developed $k$NN$\times$KDE, which can return a probability distribution for the
imputed properties. The shape of this distribution can indicate the algorithm's
level of confidence, and also inform on the underlying demographics of the
exoplanet population. We demonstrate how the distributions can be interpreted
with a series of examples for planets where the discovery was made with either
the transit method, or radial velocity method. Finally, we test the generative
capability of the $k$NN$\times$KDE to create a large synthetic population of
planets based on the archive, and identify potential categories of planets from
groups of properties in the multidimensional space. All codes are Open Source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 14 figures, 1 table. Accepted for publication in the Open
  Journal of Astrophysics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Vulnerability as a Consequence of On-Manifold Inseparibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajdeep Haldar, Yue Xing, Qifan Song, Guang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown theoretically and empirically that redundant data
dimensions are a source of adversarial vulnerability. However, the inverse
doesn't seem to hold in practice; employing dimension-reduction techniques
doesn't exhibit robustness as expected. In this work, we consider
classification tasks and characterize the data distribution as a
low-dimensional manifold, with high/low variance features defining the on/off
manifold direction. We argue that clean training experiences poor convergence
in the off-manifold direction caused by the ill-conditioning in widely used
first-order optimizers like gradient descent. The poor convergence then acts as
a source of adversarial vulnerability when the dataset is inseparable in the
on-manifold direction. We provide theoretical results for logistic regression
and a 2-layer linear network on the considered data distribution. Furthermore,
we advocate using second-order methods that are immune to ill-conditioning and
lead to better robustness. We perform experiments and exhibit tremendous
robustness improvements in clean training through long training and the
employment of second-order methods, corroborating our framework. Additionally,
we find the inclusion of batch-norm layers hinders such robustness gains. We
attribute this to differing implicit biases between traditional and
batch-normalized neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Entailment Learning for Hyperbolic <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avik Pal, Max van Spengler, Guido Maria D'Amely di Melendugno, Alessandro Flaborea, Fabio Galasso, Pascal Mettes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text representation learning forms a cornerstone in vision-language
models, where pairs of images and textual descriptions are contrastively
aligned in a shared embedding space. Since visual and textual concepts are
naturally hierarchical, recent work has shown that hyperbolic space can serve
as a high-potential manifold to learn vision-language representation with
strong downstream performance. In this work, for the first time we show how to
fully leverage the innate hierarchical nature of hyperbolic embeddings by
looking beyond individual image-text pairs. We propose Compositional Entailment
Learning for hyperbolic vision-language models. The idea is that an image is
not only described by a sentence but is itself a composition of multiple object
boxes, each with their own textual description. Such information can be
obtained freely by extracting nouns from sentences and using openly available
localized grounding models. We show how to hierarchically organize images,
image boxes, and their textual descriptions through contrastive and
entailment-based objectives. Empirical evaluation on a hyperbolic
vision-language model trained with millions of image-text pairs shows that the
proposed compositional learning approach outperforms conventional Euclidean
CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot
and retrieval generalization and clearly stronger hierarchical performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 12 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Average Certified Radius is a Poor Metric for Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Sun, Yuhao Mao, Mark Niklas Müller, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomized smoothing is a popular approach for providing certified robustness
guarantees against adversarial attacks, and has become a very active area of
research. Over the past years, the average certified radius (ACR) has emerged
as the single most important metric for comparing methods and tracking progress
in the field. However, in this work, we show that ACR is an exceptionally poor
metric for evaluating robustness guarantees provided by randomized smoothing.
We theoretically show not only that a trivial classifier can have arbitrarily
large ACR, but also that ACR is much more sensitive to improvements on easy
samples than on hard ones. Empirically, we confirm that existing training
strategies that improve ACR reduce the model's robustness on hard samples.
Further, we show that by focusing on easy samples, we can effectively replicate
the increase in ACR. We develop strategies, including explicitly discarding
hard samples, reweighing the dataset with certified radius, and extreme
optimization for easy samples, to achieve state-of-the-art ACR, although these
strategies ignore robustness for the general data distribution. Overall, our
results suggest that ACR has introduced a strong undesired bias to the field,
and better metrics are required to holistically evaluate randomized smoothing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Refinement Protocols for Distributed Distribution Estimation
  under $\ell^p$-Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deheng Yuan, Tao Guo, Zhongyi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider the communication-constrained estimation of discrete distributions
under $\ell^p$ losses, where each distributed terminal holds multiple
independent samples and uses limited number of bits to describe the samples. We
obtain the minimax optimal rates of the problem in most parameter regimes. An
elbow effect of the optimal rates at $p=2$ is clearly identified. To show the
optimal rates, we first design estimation protocols to achieve them. The key
ingredient of these protocols is to introduce adaptive refinement mechanisms,
which first generate rough estimate by partial information and then establish
refined estimate in subsequent steps guided by the rough estimate. The
protocols leverage successive refinement, sample compression and thresholding
methods to achieve the optimal rates in different parameter regimes. The
optimality of the protocols is shown by deriving compatible minimax lower
bounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Degree Distribution based Spiking Graph Networks for Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxu Wang, Siwei Liu, Mengzhu Wang, Shangsong Liang, Nan Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Graph Networks (SGNs) have garnered significant attraction from both
researchers and industry due to their ability to address energy consumption
challenges in graph classification. However, SGNs are only effective for
in-distribution data and cannot tackle out-of-distribution data. In this paper,
we first propose the domain adaptation problem in SGNs, and introduce a novel
framework named Degree-aware Spiking Graph Domain Adaptation for
Classification. The proposed DeSGDA addresses the spiking graph domain
adaptation problem by three aspects: node degree-aware personalized spiking
representation, adversarial feature distribution alignment, and pseudo-label
distillation. First, we introduce the personalized spiking representation
method for generating degree-dependent spiking signals. Specifically, the
threshold of triggering a spike is determined by the node degree, allowing this
personalized approach to capture more expressive information for
classification. Then, we propose the graph feature distribution alignment
module that is adversarially trained using membrane potential against a domain
discriminator. Such an alignment module can efficiently maintain high
performance and low energy consumption in the case of inconsistent
distribution. Additionally, we extract consistent predictions across two spaces
to create reliable pseudo-labels, effectively leveraging unlabeled data to
enhance graph classification performance. Extensive experiments on benchmark
datasets validate the superiority of the proposed DeSGDA compared with
competitive baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise is All You Need: Private Second-Order Convergence of Noisy SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitrii Avdiukhin, Michael Dinitz, Chenglin Fan, Grigory Yaroslavtsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Private optimization is a topic of major interest in machine learning, with
differentially private stochastic gradient descent (DP-SGD) playing a key role
in both theory and practice. Furthermore, DP-SGD is known to be a powerful tool
in contexts beyond privacy, including robustness, machine unlearning, etc.
Existing analyses of DP-SGD either make relatively strong assumptions (e.g.,
Lipschitz continuity of the loss function, or even convexity) or prove only
first-order convergence (and thus might end at a saddle point in the non-convex
setting). At the same time, there has been progress in proving second-order
convergence of the non-private version of ``noisy SGD'', as well as progress in
designing algorithms that are more complex than DP-SGD and do guarantee
second-order convergence. We revisit DP-SGD and show that ``noise is all you
need'': the noise necessary for privacy already implies second-order
convergence under the standard smoothness assumptions, even for non-Lipschitz
loss functions. Hence, we get second-order convergence essentially for free:
DP-SGD, the workhorse of modern private optimization, under minimal assumptions
can be used to find a second-order stationary point.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Shapley Value and Counterfactual Simulations in a Structural Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchan Kwon, Sokbae Lee, Guillaume A. Pouliot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a variant of the Shapley value, the group Shapley value, to
interpret counterfactual simulations in structural economic models by
quantifying the importance of different components. Our framework compares two
sets of parameters, partitioned into multiple groups, and applying group
Shapley value decomposition yields unique additive contributions to the changes
between these sets. The relative contributions sum to one, enabling us to
generate an importance table that is as easily interpretable as a regression
table. The group Shapley value can be characterized as the solution to a
constrained weighted least squares problem. Using this property, we develop
robust decomposition methods to address scenarios where inputs for the group
Shapley value are missing. We first apply our methodology to a simple Roy model
and then illustrate its usefulness by revisiting two published papers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Model Ensemble in Transferable Adversarial Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yao, Zeliang Zhang, Huayi Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model ensemble adversarial attack has become a powerful method for generating
transferable adversarial examples that can target even unknown models, but its
theoretical foundation remains underexplored. To address this gap, we provide
early theoretical insights that serve as a roadmap for advancing model ensemble
adversarial attack. We first define transferability error to measure the error
in adversarial transferability, alongside concepts of diversity and empirical
model ensemble Rademacher complexity. We then decompose the transferability
error into vulnerability, diversity, and a constant, which rigidly explains the
origin of transferability error in model ensemble attack: the vulnerability of
an adversarial example to ensemble components, and the diversity of ensemble
components. Furthermore, we apply the latest mathematical tools in information
theory to bound the transferability error using complexity and generalization
terms, contributing to three practical guidelines for reducing transferability
error: (1) incorporating more surrogate models, (2) increasing their diversity,
and (3) reducing their complexity in cases of overfitting. Finally, extensive
experiments with 54 models validate our theoretical framework, representing a
significant step forward in understanding transferable model ensemble
adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forgetting Through Transforming: Enabling Federated Unlearning via
  Class-Aware Representation Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Guo, Zhen Tian, Minghao Yao, Yong Qi, Saiyu Qi, Yun Li, Jin Song Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Unlearning (FU) enables clients to selectively remove the influence
of specific data from a trained federated learning model, addressing privacy
concerns and regulatory requirements. However, existing FU methods often
struggle to balance effective erasure with model utility preservation,
especially for class-level unlearning in non-IID settings. We propose Federated
Unlearning via Class-aware Representation Transformation (FUCRT), a novel
method that achieves unlearning through class-aware representation
transformation. FUCRT employs two key components: (1) a transformation class
selection strategy to identify optimal forgetting directions, and (2) a
transformation alignment technique using dual class-aware contrastive learning
to ensure consistent transformations across clients. Extensive experiments on
four datasets demonstrate FUCRT's superior performance in terms of erasure
guarantee, model utility preservation, and efficiency. FUCRT achieves complete
(100\%) erasure of unlearning classes while maintaining or improving
performance on remaining classes, outperforming state-of-the-art baselines
across both IID and Non-IID settings. Analysis of the representation space
reveals FUCRT's ability to effectively merge unlearning class representations
with the transformation class from remaining classes, closely mimicking the
model retrained from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement
  Learning and Application in UAV Hovering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihan Qi, Xinsong Yang, Gang Xia, Daniel W. C. Ho, Pengyang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a safety modulator actor-critic (SMAC) method to address
safety constraint and overestimation mitigation in model-free safe
reinforcement learning (RL). A safety modulator is developed to satisfy safety
constraints by modulating actions, allowing the policy to ignore safety
constraint and focus on maximizing reward. Additionally, a distributional
critic with a theoretical update rule for SMAC is proposed to mitigate the
overestimation of Q-values with safety constraints. Both simulation and
real-world scenarios experiments on Unmanned Aerial Vehicles (UAVs) hovering
confirm that the SMAC can effectively maintain safety constraints and
outperform mainstream baseline algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Fine-tuning and Conversion of Pretrained Speech and Language
  Models towards Linear Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian He, Philip N. Garner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architectures such as Linformer and Mamba have recently emerged as
competitive linear time replacements for transformers. However, corresponding
large pretrained models are often unavailable, especially in non-text domains.
To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)
approach that jointly converts a transformer model to a linear time substitute
and fine-tunes it to a target task. We also compare several means to guide the
fine-tuning to optimally retain the desired inference capability from the
original model. The methods differ in their use of the target model and the
trajectory of the parameters. In a series of empirical studies on language
processing, language modeling, and speech processing, we show that CALD can
effectively recover the result of the original model, and that the guiding
strategy contributes to the result. Some reasons for the variation are
suggested.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic metastability in the self-attention model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borjan Geshkovski, Hugo Koubbi, Yury Polyanskiy, Philippe Rigollet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the self-attention model - an interacting particle system on the
unit sphere, which serves as a toy model for Transformers, the deep neural
network architecture behind the recent successes of large language models. We
prove the appearance of dynamic metastability conjectured in [GLPR23] -
although particles collapse to a single cluster in infinite time, they remain
trapped near a configuration of several clusters for an exponentially long
period of time. By leveraging a gradient flow interpretation of the system, we
also connect our result to an overarching framework of slow motion of gradient
flows proposed by Otto and Reznikoff [OR07] in the context of coarsening and
the Allen-Cahn equation. We finally probe the dynamics beyond the exponentially
long period of metastability, and illustrate that, under an appropriate
time-rescaling, the energy reaches its global maximum in finite time and has a
staircase profile, with trajectories manifesting saddle-to-saddle-like
behavior, reminiscent of recent works in the analysis of training dynamics via
gradient descent for two-layer neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning for a Class of Cascade Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shima Rabiei, Sandipan Mishra, Santiago Paternain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work considers the problem of transfer learning in the context of
reinforcement learning. Specifically, we consider training a policy in a
reduced order system and deploying it in the full state system. The motivation
for this training strategy is that running simulations in the full-state system
may take excessive time if the dynamics are complex. While transfer learning
alleviates the computational issue, the transfer guarantees depend on the
discrepancy between the two systems. In this work, we consider a class of
cascade dynamical systems, where the dynamics of a subset of the state-space
influence the rest of the states but not vice-versa. The reinforcement learning
policy learns in a model that ignores the dynamics of these states and treats
them as commanded inputs. In the full-state system, these dynamics are handled
using a classic controller (e.g., a PID). These systems have vast applications
in the control literature and their structure allows us to provide transfer
guarantees that depend on the stability of the inner loop controller. Numerical
experiments on a quadrotor support the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ K-SAM: A Prompting Method Using Pretrained U-Net to Improve Zero Shot
  Performance of SAM on Lung Segmentation in CXR <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Deriche, Mohammad Marufur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical procedures, precise localization of the target area is an
essential step for clinical diagnosis and screening. For many diagnostic
applications, lung segmentation of chest X-ray images is an essential first
step that significantly reduces the image size to speed up the subsequent
analysis. One of the primary difficulties with this task is segmenting the lung
regions covered by dense abnormalities also known as opacities due to diseases
like pneumonia and tuberculosis. SAM has astonishing generalization
capabilities for category agnostic segmentation. In this study we propose an
algorithm to improve zero shot performance of SAM on lung region segmentation
task by automatic prompt selection. Two separate UNet models were trained, one
for predicting lung segments and another for heart segment. Though these
predictions lack fine details around the edges, they provide positive and
negative points as prompt for SAM. Using proposed prompting method zero shot
performance of SAM is evaluated on two benchmark datasets. ViT-l version of the
model achieved slightly better performance compared to other two versions, ViTh
and ViTb. It yields an average Dice score of 95.5 percent and 94.9 percent on
hold out data for two datasets respectively. Though, for most of the images,
SAM did outstanding segmentation, its prediction was way off for some of the
images. After careful inspection it is found that all of these images either
had extreme abnormality or distorted shape. Unlike most of the research
performed so far on lung segmentation from CXR images using SAM, this study
proposes a fully automated prompt selection process only from the input image.
Our finding indicates that using pretrained models for prompt selection can
utilize SAM impressive generalization capability to its full extent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lise Le Boudec, Emmanuel de Bezenac, Louis Serrano, Ramon Daniel Regueiro-Espino, Yuan Yin, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed deep learning often faces optimization challenges due to the
complexity of solving partial differential equations (PDEs), which involve
exploring large solution spaces, require numerous iterations, and can lead to
unstable training. These challenges arise particularly from the
ill-conditioning of the optimization problem, caused by the differential terms
in the loss function. To address these issues, we propose learning a solver,
i.e., solving PDEs using a physics-informed iterative algorithm trained on
data. Our method learns to condition a gradient descent algorithm that
automatically adapts to each PDE instance, significantly accelerating and
stabilizing the optimization process and enabling faster convergence of
physics-aware models. Furthermore, while traditional physics-informed methods
solve for a single PDE instance, our approach addresses parametric PDEs.
Specifically, our method integrates the physical loss gradient with the PDE
parameters to solve over a distribution of PDE parameters, including
coefficients, initial conditions, or boundary conditions. We demonstrate the
effectiveness of our method through empirical experiments on multiple datasets,
comparing training and test-time optimization performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Approach for Cardiac MRI Segmentation based on 3D UNet
  Combined with Papillary Muscle Exclusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narjes Benameur, Ramzi Mahmoudi, Mohamed Deriche, Amira fayouka, Imene Masmoudi, Nessrine Zoghlami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Left ventricular ejection fraction (LVEF) is the most important clinical
parameter of cardiovascular function. The accuracy in estimating this parameter
is highly dependent upon the precise segmentation of the left ventricle (LV)
structure at the end diastole and systole phases. Therefore, it is crucial to
develop robust algorithms for the precise segmentation of the heart structure
during different phases. Methodology: In this work, an improved 3D UNet model
is introduced to segment the myocardium and LV, while excluding papillary
muscles, as per the recommendation of the Society for Cardiovascular Magnetic
Resonance. For the practical testing of the proposed framework, a total of
8,400 cardiac MRI images were collected and analysed from the military hospital
in Tunis (HMPIT), as well as the popular ACDC public dataset. As performance
metrics, we used the Dice coefficient and the F1 score for validation/testing
of the LV and the myocardium segmentation. Results: The data was split into
70%, 10%, and 20% for training, validation, and testing, respectively. It is
worth noting that the proposed segmentation model was tested across three axis
views: basal, medio basal and apical at two different cardiac phases: end
diastole and end systole instances. The experimental results showed a Dice
index of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end
diastolic and systolic phases, respectively. Additionally, clinical evaluation
outcomes revealed a significant difference in the LVEF and other clinical
parameters when the papillary muscles were included or excluded.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Neuron Unleashes Expressivity of ReLU Networks Under Convex
  Relaxation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Mao, Yani Zhang, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural work certification has established itself as a crucial tool for
ensuring the robustness of neural networks. Certification methods typically
rely on convex relaxations of the feasible output set to provide sound bounds.
However, complete certification requires exact bounds, which strongly limits
the expressivity of ReLU networks: even for the simple ``$\max$'' function in
$\mathbb{R}^2$, there does not exist a ReLU network that expresses this
function and can be exactly bounded by single-neuron relaxation methods. This
raises the question whether there exists a convex relaxation that can provide
exact bounds for general continuous piecewise linear functions in
$\mathbb{R}^n$. In this work, we answer this question affirmatively by showing
that (layer-wise) multi-neuron relaxation provides complete certification for
general ReLU networks. Based on this novel result, we show that the
expressivity of ReLU networks is no longer limited under multi-neuron
relaxation. To the best of our knowledge, this is the first positive result on
the completeness of convex relaxations, shedding light on the practice of
certified robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shap-Select: Lightweight Feature Selection Using SHAP Values and
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egor Kraev, Baran Koseoglu, Luca Traverso, Mohammed Topiwalla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature selection is an essential process in machine learning, especially
when dealing with high-dimensional datasets. It helps reduce the complexity of
machine learning models, improve performance, mitigate overfitting, and
decrease computation time. This paper presents a novel feature selection
framework, shap-select. The framework conducts a linear or logistic regression
of the target on the Shapley values of the features, on the validation set, and
uses the signs and significance levels of the regression coefficients to
implement an efficient heuristic for feature selection in tabular regression
and classification tasks. We evaluate shap-select on the Kaggle credit card
fraud dataset, demonstrating its effectiveness compared to established methods
such as Recursive Feature Elimination (RFE), HISEL (a mutual information-based
feature selection method), Boruta and a simpler Shapley value-based method. Our
findings show that shap-select combines interpretability, computational
efficiency, and performance, offering a robust solution for feature selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Hu, Hengxiang Zhang, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-parameterized models are typically vulnerable to membership inference
attacks, which aim to determine whether a specific sample is included in the
training of a given model. Previous Weight regularizations (e.g., L1
regularization) typically impose uniform penalties on all parameters, leading
to a suboptimal tradeoff between model utility and privacy. In this work, we
first show that only a small fraction of parameters substantially impact the
privacy risk. In light of this, we propose Privacy-aware Sparsity Tuning
(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties
to different parameters. Our key idea behind PAST is to promote sparsity in
parameters that significantly contribute to privacy leakage. In particular, we
construct the adaptive weight for each parameter based on its privacy
sensitivity, i.e., the gradient of the loss gap with respect to the parameter.
Using PAST, the network shrinks the loss gap between members and non-members,
leading to strong resistance to privacy attacks. Extensive experiments
demonstrate the superiority of PAST, achieving a state-of-the-art balance in
the privacy-utility trade-off.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A neural network-based approach to hybrid systems identification for
  control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Fabiani, Bartolomeo Stellato, Daniele Masti, Paul J. Goulart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of designing a machine learning-based model of an
unknown dynamical system from a finite number of (state-input)-successor state
data points, such that the model obtained is also suitable for optimal control
design. We adopt a neural network (NN) architecture that, once suitably
trained, yields a hybrid system with continuous piecewise-affine (PWA) dynamics
that is differentiable with respect to the network's parameters, thereby
enabling the use of derivative-based training procedures. We show that a
careful choice of our NN's weights produces a hybrid system model with
structural properties that are highly favorable when used as part of a finite
horizon optimal control problem (OCP). Specifically, we rely on available
results to establish that optimal solutions with strong local optimality
guarantees can be computed via nonlinear programming (NLP), in contrast to
classical OCPs for general hybrid systems which typically require mixed-integer
optimization. Besides being well-suited for optimal control design, numerical
simulations illustrate that our NN-based technique enjoys very similar
performance to state-of-the-art system identification methods for hybrid
systems and it is competitive on nonlinear benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The FIX <span class="highlight-title">Benchmark</span>: Extracting Features Interpretable to eXperts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helen Jin, Shreya Havaldar, Chaehyeon Kim, Anton Xue, Weiqiu You, Helen Qu, Marco Gatti, Daniel A Hashimoto, Bhuvnesh Jain, Amin Madani, Masao Sako, Lyle Ungar, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature-based methods are commonly used to explain model predictions, but
these methods often implicitly assume that interpretable features are readily
available. However, this is often not the case for high-dimensional data, and
it can be hard even for domain experts to mathematically specify which features
are important. Can we instead automatically extract collections or groups of
features that are aligned with expert knowledge? To address this gap, we
present FIX (Features Interpretable to eXperts), a benchmark for measuring how
well a collection of features aligns with expert knowledge. In collaboration
with domain experts, we propose FIXScore, a unified expert alignment measure
applicable to diverse real-world settings across cosmology, psychology, and
medicine domains in vision, language and time series data modalities. With
FIXScore, we find that popular feature-based explanation methods have poor
alignment with expert-specified knowledge, highlighting the need for new
methods that can better identify features interpretable to experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private prediction for large-scale synthetic text <span class="highlight-title">generation</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for generating differentially private synthetic text
using large language models (LLMs), via private prediction. In the private
prediction framework, we only require the output synthetic data to satisfy
differential privacy guarantees. This is in contrast to approaches that train a
generative model on potentially sensitive user-supplied source data and seek to
ensure the model itself is safe to release.
  We prompt a pretrained LLM with source data, but ensure that next-token
predictions are made with differential privacy guarantees. Previous work in
this paradigm reported generating a small number of examples (<10) at
reasonable privacy levels, an amount of data that is useful only for downstream
in-context learning or prompting. In contrast, we make changes that allow us to
generate thousands of high-quality synthetic data points, greatly expanding the
set of potential applications. Our improvements come from an improved privacy
analysis and a better private selection mechanism, which makes use of the
equivalence between the softmax layer for sampling tokens in LLMs and the
exponential mechanism. Furthermore, we introduce a novel use of public
predictions via the sparse vector technique, in which we do not pay privacy
costs for tokens that are predictable without sensitive data; we find this to
be particularly effective for structured data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages; updated figure + some new experiments from EMNLP 2024
  findings camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topologically Faithful Multi-class Segmentation in Medical <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Berger, Nico Stucki, Laurin Lux, Vincent Buergin, Suprosanna Shit, Anna Banaszak, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological accuracy in medical image segmentation is a highly important
property for downstream applications such as network analysis and flow modeling
in vessels or cell counting. Recently, significant methodological advancements
have brought well-founded concepts from algebraic topology to binary
segmentation. However, these approaches have been underexplored in multi-class
segmentation scenarios, where topological errors are common. We propose a
general loss function for topologically faithful multi-class segmentation
extending the recent Betti matching concept, which is based on induced
matchings of persistence barcodes. We project the N-class segmentation problem
to N single-class segmentation tasks, which allows us to use 1-parameter
persistent homology, making training of neural networks computationally
feasible. We validate our method on a comprehensive set of four medical
datasets with highly variant topological characteristics. Our loss formulation
significantly enhances topological correctness in cardiac, cell, artery-vein,
and Circle of Willis segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data
  Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinayak Arannil, Neha Narwal, Sourav Sanjukta Bhabesh, Sai Nikhil Thirandas, Darren Yow-Bang Wang, Graham Horwood, Alex Anto Chirayath, Gouri Pandeshwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable ability to generalize
effectively across numerous industry domains while executing a range of tasks.
Many of these competencies are obtained from the data utilized during the
pre-training phase of the Language Models (LMs). However, these models exhibit
limitations when tasked with performing in specialized or low-resource industry
domains. More recent approaches use LLMs for generating domain-specific
synthetic data but most often they lack in truthfulness and complexity.
Alternatively, in cases where domain data is available like healthcare and
finance most of the LMs are proprietary necessitating the need for a scalable
method to curate real world industry specific pre-training data. In this work,
we propose an automated and scalable framework - DoPAMine:Domain-specific
Pre-training Adaptation from seed-guided data Mining, to mine domain specific
training data from a large data corpus for domain adaptation of a LM. The
framework leverages the parametric knowledge of a LLM to generate diverse and
representative seed data tailored to a specific domain which is then used to
mine real world data from a large data corpus like Common Crawl. We evaluated
our framework's performance in the continual pre-training (CPT) setting by
training two domain specific 7B parameter LMs in healthcare and finance with
data mined via DoPAMine. Our experiments show that DoPAMine boosts the
performance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and
5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and
PubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings
respectively on finance tasks from FiQA-SA, FPB and Headlines datasets when
compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Population Transformer: Learning Population-level Representations of
  Neural Activity <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, Boris Katz, Andrei Barbu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a self-supervised framework that learns population-level codes for
arbitrary ensembles of neural recordings at scale. We address two key
challenges in scaling models with neural time-series data: sparse and variable
electrode distribution across subjects and datasets. The Population Transformer
(PopT) stacks on top of pretrained representations and enhances downstream
decoding by enabling learned aggregation of multiple spatially-sparse data
channels. The pretrained PopT lowers the amount of data required for downstream
decoding experiments, while increasing accuracy, even on held-out subjects and
tasks. Compared to end-to-end methods, this approach is computationally
lightweight and more interpretable, while still retaining competitive
performance. We further show how our framework is generalizable to multiple
time-series embeddings and neural data modalities. Beyond decoding, we
interpret the pretrained PopT and fine-tuned models to show how they can be
used to extract neuroscience insights from massive amounts of data. We release
our code as well as a pretrained PopT to enable off-the-shelf improvements in
multi-channel intracranial data decoding and interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 11 figures, submitted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMD: Contrastive Multi-Modal <span class="highlight-title">Diffusion</span> for Video-Audio Conditional
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Hannes Gamper, Sebastian Braun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a multi-modal diffusion model tailored for the bi-directional
conditional generation of video and audio. We propose a joint contrastive
training loss to improve the synchronization between visual and auditory
occurrences. We present experiments on two datasets to evaluate the efficacy of
our proposed model. The assessment of generation quality and alignment
performance is carried out from various angles, encompassing both objective and
subjective metrics. Our findings demonstrate that the proposed model
outperforms the baseline in terms of quality and generation speed through
introduction of our novel cross-modal easy fusion architectural block.
Furthermore, the incorporation of the contrastive loss results in improvements
in audio-visual alignment, particularly in the high-correlation video-to-audio
generation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05649v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05649v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongzhou Liao, Barnabás Póczos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have become important tools for machine learning
on graph-structured data. In this paper, we explore the synergistic combination
of graph encoding, graph rewiring, and graph attention, by introducing Graph
Attention with Stochastic Structures (GRASS), a novel GNN architecture. GRASS
utilizes relative random walk probabilities (RRWP) encoding and a novel
decomposed variant (D-RRWP) to efficiently capture structural information. It
rewires the input graph by superimposing a random regular graph to enhance
long-range information propagation. It also employs a novel additive attention
mechanism tailored for graph-structured data. Our empirical evaluations
demonstrate that GRASS achieves state-of-the-art performance on multiple
benchmark datasets, including a 20.3% reduction in mean absolute error on the
ZINC dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Poincaré Inequality and Consistency Results for Signal Sampling on
  Large Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thien Le, Luana Ruiz, Stefanie Jegelka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale graph machine learning is challenging as the complexity of
learning models scales with the graph size. Subsampling the graph is a viable
alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.
Existing graph sampling techniques require not only computing the spectra of
large matrices but also repeating these computations when the graph changes,
e.g., grows. In this paper, we introduce a signal sampling theory for a type of
graph limit -- the graphon. We prove a Poincar\'e inequality for graphon
signals and show that complements of node subsets satisfying this inequality
are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting
connections with spectral clustering and Gaussian elimination, we prove that
such sampling sets are consistent in the sense that unique sampling sets on a
convergent graph sequence converge to unique sampling sets on the graphon. We
then propose a related graphon signal sampling algorithm for large graphs, and
demonstrate its good empirical performance on graph machine learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14432v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14432v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Ahmed Jellouli, Geovani Rizk, John Stephan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byzantine-resilient distributed machine learning seeks to achieve robust
learning performance in the presence of misbehaving or adversarial workers.
While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD)
methods were proven theoretically optimal, their empirical success has often
relied on pre-aggregation gradient clipping. However, the currently considered
static clipping strategy exhibits mixed results: improving robustness against
some attacks while being ineffective or detrimental against others. We address
this gap by proposing a principled adaptive clipping strategy, termed Adaptive
Robust Clipping (ARC). We show that ARC consistently enhances the empirical
robustness of SOTA Robust-DGD methods, while preserving the theoretical
robustness guarantees. Our analysis shows that ARC provably improves the
asymptotic convergence guarantee of Robust-DGD in the case when the model is
well-initialized. We validate this theoretical insight through an exhaustive
set of experiments on benchmark image classification tasks. We observe that the
improvement induced by ARC is more pronounced in highly heterogeneous and
adversarial settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s learn governing principles of dynamical systems, revealing an
  in-context neural scaling law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00795v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00795v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained large language models (LLMs) are surprisingly effective at
performing zero-shot tasks, including time-series forecasting. However,
understanding the mechanisms behind such capabilities remains highly
challenging due to the complexity of the models. We study LLMs' ability to
extrapolate the behavior of dynamical systems whose evolution is governed by
principles of physical interest. Our results show that LLaMA 2, a language
model trained primarily on texts, achieves accurate predictions of dynamical
system time series without fine-tuning or prompt engineering. Moreover, the
accuracy of the learned physical rules increases with the length of the input
context window, revealing an in-context version of neural scaling law. Along
the way, we present a flexible and efficient algorithm for extracting
probability density functions of multi-digit numbers directly from LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  Prompts Do Not Improve Performances of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of
its default system prompt. Despite current practices of adding personas to
system prompts, it remains unclear how different personas affect a model's
performance on objective tasks. In this study, we present a systematic
evaluation of personas in system prompts. We curate a list of 162 roles
covering 6 types of interpersonal relationships and 8 domains of expertise.
Through extensive analysis of 4 popular families of LLMs and 2,410 factual
questions, we demonstrate that adding personas in system prompts does not
improve model performance across a range of questions compared to the control
setting where no persona is added. Nevertheless, further analysis suggests that
the gender, type, and domain of the persona can all influence the resulting
prediction accuracies. We further experimented with a list of persona search
strategies and found that, while aggregating results from the best persona for
each question significantly improves prediction accuracy, automatically
identifying the best persona is challenging, with predictions often performing
no better than random selection. Overall, our findings suggest that while
adding a persona may lead to performance gains in certain settings, the effect
of each persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Your Generative Model Detect Out-of-Distribution Covariate Shift? <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christiaan Viviers, Amaan Valiuddin, Francisco Caetano, Lemar Abdi, Lena Filatova, Peter de With, Fons van der Sommen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Out-of-Distribution (OOD) sensory data and covariate distribution
shift aims to identify new test examples with different high-level image
statistics to the captured, normal and In-Distribution (ID) set. Existing OOD
detection literature largely focuses on semantic shift with little-to-no
consensus over covariate shift. Generative models capture the ID data in an
unsupervised manner, enabling them to effectively identify samples that deviate
significantly from this learned distribution, irrespective of the downstream
task. In this work, we elucidate the ability of generative models to detect and
quantify domain-specific covariate shift through extensive analyses that
involves a variety of models. To this end, we conjecture that it is sufficient
to detect most occurring sensory faults (anomalies and deviations in global
signals statistics) by solely modeling high-frequency signal-dependent and
independent details. We propose a novel method, CovariateFlow, for OOD
detection, specifically tailored to covariate heteroscedastic high-frequency
image-components using conditional Normalizing Flows (cNFs). Our results on
CIFAR10 vs. CIFAR10-C and ImageNet200 vs. ImageNet200-C demonstrate the
effectiveness of the method by accurately detecting OOD covariate shift. This
work contributes to enhancing the fidelity of imaging systems and aiding
machine learning models in OOD detection in the presence of covariate shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024, typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbolic Recovery of Differential Equations: The Identifiability Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08342v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08342v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Scholl, Aras Bacho, Holger Boche, Gitta Kutyniok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic recovery of differential equations is the ambitious attempt at
automating the derivation of governing equations with the use of machine
learning techniques. In contrast to classical methods which assume the
structure of the equation to be known and focus on the estimation of specific
parameters, these algorithms aim to learn the structure and the parameters
simultaneously. While the uniqueness and, therefore, the identifiability of
parameters of governing equations are a well-addressed problem in the field of
parameter estimation, it has not been investigated for symbolic recovery.
However, this problem should be even more present in this field since the
algorithms aim to cover larger spaces of governing equations. In this paper, we
investigate under which conditions a solution of a differential equation does
not uniquely determine the equation itself. For various classes of differential
equations, we provide both necessary and sufficient conditions for a function
to uniquely determine the corresponding differential equation. We then use our
results to devise numerical algorithms aiming to determine whether a function
solves a differential equation uniquely. Finally, we provide extensive
numerical experiments showing that our algorithms can indeed guarantee the
uniqueness of the learned governing differential equation, without assuming any
knowledge about the analytic form of function, thereby ensuring the reliability
of the learned equation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Generative Framework for Realistic Lidar Simulation in
  Autonomous Driving Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Haghighi, Mehrdad Dianati, Valentina Donzella, Kurt Debattista
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation models for perception sensors are integral components of
automotive simulators used for the virtual Verification and Validation (V\&V)
of Autonomous Driving Systems (ADS). These models also serve as powerful tools
for generating synthetic datasets to train deep learning-based perception
models. Lidar is a widely used sensor type among the perception sensors for ADS
due to its high precision in 3D environment scanning. However, developing
realistic Lidar simulation models is a significant technical challenge. In
particular, unrealistic models can result in a large gap between the
synthesised and real-world point clouds, limiting their effectiveness in ADS
applications. Recently, deep generative models have emerged as promising
solutions to synthesise realistic sensory data. However, for Lidar simulation,
deep generative models have been primarily hybridised with conventional
algorithms, leaving unified generative approaches largely unexplored in the
literature. Motivated by this research gap, we propose a unified generative
framework to enhance Lidar simulation fidelity. Our proposed framework projects
Lidar point clouds into depth-reflectance images via a lossless transformation,
and employs our novel Controllable Lidar point cloud Generative model, CoLiGen,
to translate the images. We extensively evaluate our CoLiGen model, comparing
it with the state-of-the-art image-to-image translation models using various
metrics to assess the realness, faithfulness, and performance of a downstream
perception model. Our results show that CoLiGen exhibits superior performance
across most metrics. The dataset and source code for this research are
available at https://github.com/hamedhaghighi/CoLiGen.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The BRAVO Semantic Segmentation Challenge Results in UNCV2024 <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Hung Vu, Eduardo Valle, Andrei Bursuc, Tommie Kerssies, Daan de Geus, Gijs Dubbelman, Long Qian, Bingke Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Tomáš Vojíř, Jan Šochman, Jiří Matas, Michael Smith, Frank Ferrie, Shamik Basu, Christos Sakaridis, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the unified BRAVO challenge to benchmark the reliability of
semantic segmentation models under realistic perturbations and unknown
out-of-distribution (OOD) scenarios. We define two categories of reliability:
(1) semantic reliability, which reflects the model's accuracy and calibration
when exposed to various perturbations; and (2) OOD reliability, which measures
the model's ability to detect object classes that are unknown during training.
The challenge attracted nearly 100 submissions from international teams
representing notable research institutions. The results reveal interesting
insights into the importance of large-scale pre-training and minimal
architectural design in developing robust and reliable semantic segmentation
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024 proceeding paper of the BRAVO challenge 2024, see
  https://benchmarks.elsa-ai.eu/?ch=1&com=introduction Corrected numbers in
  Tables 1,3,4,5 and 10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KID-PPG: Knowledge Informed Deep Learning for Extracting Heart Rate from
  a Smartwatch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate extraction of heart rate from photoplethysmography (PPG) signals
remains challenging due to motion artifacts and signal degradation. Although
deep learning methods trained as a data-driven inference problem offer
promising solutions, they often underutilize existing knowledge from the
medical and signal processing community. In this paper, we address three
shortcomings of deep learning models: motion artifact removal, degradation
assessment, and physiologically plausible analysis of the PPG signal. We
propose KID-PPG, a knowledge-informed deep learning model that integrates
expert knowledge through adaptive linear filtering, deep probabilistic
inference, and data augmentation. We evaluate KID-PPG on the PPGDalia dataset,
achieving an average mean absolute error of 2.85 beats per minute, surpassing
existing reproducible methods. Our results demonstrate a significant
performance improvement in heart rate tracking through the incorporation of
prior knowledge into deep learning models. This approach shows promise in
enhancing various biomedical applications by incorporating existing expert
knowledge in deep learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Stability Principle for Learning under Non-Stationarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18304v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18304v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpiao Huang, Kaizheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a versatile framework for statistical learning in non-stationary
environments. In each time period, our approach applies a stability principle
to select a look-back window that maximizes the utilization of historical data
while keeping the cumulative bias within an acceptable range relative to the
stochastic error. Our theory and numerical experiments showcase the adaptivity
of this approach to unknown non-stationarity. We prove regret bounds that are
minimax optimal up to logarithmic factors when the population losses are
strongly convex, or Lipschitz only. At the heart of our analysis lie two novel
components: a measure of similarity between functions and a segmentation
technique for dividing the non-stationary data sequence into quasi-stationary
pieces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>65 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Directly Handling Missing Data in Linear Discriminant Analysis for
  Enhancing Classification Accuracy and Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00710v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00710v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan L. Vo, Uyen Dang, Thu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the adoption of Artificial Intelligence (AI) models expands into critical
real-world applications, ensuring the explainability of these models becomes
paramount, particularly in sensitive fields such as medicine and finance.
Linear Discriminant Analysis (LDA) remains a popular choice for classification
due to its interpretable nature, derived from its capacity to model class
distributions and enhance class separation through linear combinations of
features. However, real-world datasets often suffer from incomplete data,
posing substantial challenges for both classification accuracy and model
interpretability. In this paper, we introduce a novel and robust classification
method, termed Weighted missing Linear Discriminant Analysis (WLDA), which
extends LDA to handle datasets with missing values without the need for
imputation. Our approach innovatively incorporates a weight matrix that
penalizes missing entries, thereby refining parameter estimation directly on
incomplete data. This methodology not only preserves the interpretability of
LDA but also significantly enhances classification performance in scenarios
plagued by missing data. We conduct an in-depth theoretical analysis to
establish the properties of WLDA and thoroughly evaluate its explainability.
Experimental results across various datasets demonstrate that WLDA consistently
outperforms traditional methods, especially in challenging environments where
missing values are prevalent in both training and test datasets. This
advancement provides a critical tool for improving classification accuracy and
maintaining model transparency in the face of incomplete data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn while Unlearn: An Iterative Unlearning Framework for Generative
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Tang, Ye Liu, Xukai Liu, Kai Zhang, Yanghai Zhang, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in machine learning, particularly in Natural Language
Processing (NLP), have led to the development of sophisticated models trained
on extensive datasets, yet raising concerns about the potential leakage of
sensitive information. In response, regulatory measures such as the European
Union's General Data Protection Regulation (GDPR) have driven increasing
interest in Machine Unlearning techniques, which enable models to selectively
forget specific data entries. Early approaches primarily relied on
pre-processing methods, while more recent research has shifted towards
training-based unlearning techniques. Despite their effectiveness, most
existing methods require access to the original training data, which is often
inaccessible. Additionally, directly applying unlearning techniques bear the
cost of undermining the model's expressive capabilities. To address these
challenges, we introduce the Iterative Contrastive Unlearning (ICU) framework,
which consists of three core components: A Knowledge Unlearning Induction
module designed to remove specific knowledge through an unlearning loss; A
Contrastive Learning Enhancement module to preserve the model's expressive
capabilities against the pure unlearning goal; And an Iterative Unlearning
Refinement module that dynamically assess the unlearning extent on specific
data pieces and make iterative update. Experimental results demonstrate the
efficacy of our ICU method in unlearning sensitive information while
maintaining the model's overall performance, offering a promising solution for
privacy-conscious machine learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaitor: Learning a Unified Representation Across Gaits for Real-World
  Quadruped Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander L. Mitchell, Wolfgang Merkt, Aristotelis Papatheodorou, Ioannis Havoutis, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current state-of-the-art in quadruped locomotion is able to produce a
variety of complex motions. These methods either rely on switching between a
discrete set of skills or learn a distribution across gaits using complex
black-box models. Alternatively, we present Gaitor, which learns a disentangled
and 2D representation across locomotion gaits. This learnt representation forms
a planning space for closed-loop control delivering continuous gait transitions
and perceptive terrain traversal. Gaitor's latent space is readily
interpretable and we discover that during gait transitions, novel unseen gaits
emerge. The latent space is disentangled with respect to footswing heights and
lengths. This means that these gait characteristics can be varied independently
in the 2D latent representation. Together with a simple terrain encoding and a
learnt planner operating in the latent space, Gaitor can take motion commands
including desired gait type and swing characteristics all while reacting to
uneven terrain. We evaluate Gaitor in both simulation and the real world on the
ANYmal C platform. To the best of our knowledge, this is the first work
learning a unified and interpretable latent space for multiple gaits, resulting
in continuous blending between different locomotion modes on a real quadruped
robot. An overview of the methods and results in this paper is found at
https://youtu.be/eVFQbRyilCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 2 tables, Accepted to CoRL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Automated Optimisation of Hyperparameters and Reward Shape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Dierkes, Emma Cramer, Holger H. Hoos, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant progress in deep reinforcement learning (RL) in
recent years. Nevertheless, finding suitable hyperparameter configurations and
reward functions remains challenging even for experts, and performance heavily
relies on these design choices. Also, most RL research is conducted on known
benchmarks where knowledge about these choices already exists. However, novel
practical applications often pose complex tasks for which no prior knowledge
about good hyperparameters and reward functions is available, thus
necessitating their derivation from scratch. Prior work has examined
automatically tuning either hyperparameters or reward functions individually.
We demonstrate empirically that an RL algorithm's hyperparameter configurations
and reward function are often mutually dependent, meaning neither can be fully
optimised without appropriate values for the other. We then propose a
methodology for the combined optimisation of hyperparameters and the reward
function. Furthermore, we include a variance penalty as an optimisation
objective to improve the stability of learned policies. We conducted extensive
experiments using Proximal Policy Optimisation and Soft Actor-Critic on four
environments. Our results show that combined optimisation significantly
improves over baseline performance in half of the environments and achieves
competitive performance in the others, with only a minor increase in
computational costs. This suggests that combined optimisation should be best
practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Reinforcement Learning Journal 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying Quantum Autoencoders for Time Series Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Frehner, Kurt Stockinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is an important problem with applications in various
domains such as fraud detection, pattern recognition or medical diagnosis.
Several algorithms have been introduced using classical computing approaches.
However, using quantum computing for solving anomaly detection problems in time
series data is a widely unexplored research field.
  This paper explores the application of quantum autoencoders to time series
anomaly detection. We investigate two primary techniques for classifying
anomalies: (1) Analyzing the reconstruction error generated by the quantum
autoencoder and (2) latent representation analysis. Our simulated experimental
results, conducted across various ansaetze, demonstrate that quantum
autoencoders consistently outperform classical deep learning-based autoencoders
across multiple datasets. Specifically, quantum autoencoders achieve superior
anomaly detection performance while utilizing 60-230 times fewer parameters and
requiring five times fewer training iterations. In addition, we implement our
quantum encoder on real quantum hardware. Our experimental results demonstrate
that quantum autoencoders achieve anomaly detection performance on par with
their simulated counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Impression for Learning with Distributed Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atrin Arya, Sana Ayromlou, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard deep learning-based classification approaches may not always be
practical in real-world clinical applications, as they require a centralized
collection of all samples. Federated learning (FL) provides a paradigm that can
learn from distributed datasets across clients without requiring them to share
data, which can help mitigate privacy and data ownership issues. In FL,
sub-optimal convergence caused by data heterogeneity is common among data from
different health centers due to the variety in data collection protocols and
patient demographics across centers. Through experimentation in this study, we
show that data heterogeneity leads to the phenomenon of catastrophic forgetting
during local training. We propose FedImpres which alleviates catastrophic
forgetting by restoring synthetic data that represents the global information
as federated impression. To achieve this, we distill the global model resulting
from each communication round. Subsequently, we use the synthetic data
alongside the local data to enhance the generalization of local training.
Extensive experiments show that the proposed method achieves state-of-the-art
performance on both the BloodMNIST and Retina datasets, which contain label
imbalance and domain shift, with an improvement in classification accuracy of
up to 20%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear
  Diffusive Parametric PDEs on Multiple Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shane E. Loeffler, Zan Ahmad, Syed Yusuf Ali, Carolyna Yamamoto, Dan M. Popescu, Alana Yee, Yash Lal, Natalia Trayanova, Mauro Maggioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting time-dependent dynamics of complex systems governed by non-linear
partial differential equations (PDEs) with varying parameters and domains is a
challenging task motivated by applications across various fields. We introduce
a novel family of neural operators based on our Graph Fourier Neural Kernels,
designed to learn solution generators for nonlinear PDEs in which the
highest-order term is diffusive, across multiple domains and parameters. G-FuNK
combines components that are parameter- and domain-adapted with others that are
not. The domain-adapted components are constructed using a weighted graph on
the discretized domain, where the graph Laplacian approximates the
highest-order diffusive term, ensuring boundary condition compliance and
capturing the parameter and domain-specific behavior. Meanwhile, the learned
components transfer across domains and parameters using our variant Fourier
Neural Operators. This approach naturally embeds geometric and directional
information, improving generalization to new test domains without need for
retraining the network. To handle temporal dynamics, our method incorporates an
integrated ODE solver to predict the evolution of the system. Experiments show
G-FuNK's capability to accurately approximate heat, reaction diffusion, and
cardiac electrophysiology equations across various geometries and anisotropic
diffusivity fields. G-FuNK achieves low relative errors on unseen domains and
fiber fields, significantly accelerating predictions compared to traditional
finite-element solvers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher M. Ackerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation engineering is becoming increasingly popular as a means of online
control of large language models (LLMs). In this work, I extend the idea of
active steering with vectors that represent a behavioral direction of interest
to tuning those vectors directly into the model, obviating the need for online
control. First, I identify activation vectors related to honesty in an
open-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can
be made more or less honest by adding positive or negative multiples of these
vectors to residual stream activations during generation. Then, I show that a
similar effect can be achieved by fine-tuning the vectors directly into the
model, by use of a dual loss function based on the cosine similarity of
residual stream activations to the vectors combined with a standard token-based
loss ("representation tuning"). Finally, I compare the generations in response
to honesty-probing prompts from the resulting models to those from models
fine-tuned with a token-based loss alone, and to those from the untuned model
subjected to online steering. Overall, fine-tuning the vectors into the models
using the cosine similarity plus token loss showed a stronger effect than
online steering, and generalized better than using the standard loss,
suggesting the potential utility of this approach as a safety measure. Code and
data are available at https://github.com/cma1114/representation_tuning; tuned
models are available at https://huggingface.co/collections/cackerman/
representation-tuning-66da1e5ab41cd1b824687d9f.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Deep Model-Based Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Rio, Merwan Barlier, Igor Colin, Albert Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address private deep offline reinforcement learning (RL), where the goal
is to train a policy on standard control tasks that is differentially private
(DP) with respect to individual trajectories in the dataset. To achieve this,
we introduce PriMORL, a model-based RL algorithm with formal differential
privacy guarantees. PriMORL first learns an ensemble of trajectory-level DP
models of the environment from offline data. It then optimizes a policy on the
penalized private model, without any further interaction with the system or
access to the dataset. In addition to offering strong theoretical foundations,
we demonstrate empirically that PriMORL enables the training of private RL
agents on offline continuous control tasks with deep function approximations,
whereas current methods are limited to simpler tabular and linear Markov
Decision Processes (MDPs). We furthermore outline the trade-offs involved in
achieving privacy in this setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IC3M: In-Car <span class="highlight-title">Multimodal</span> Multi-object Monitoring for Abnormal Status of
  Both Driver and Passengers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Fang, Zheng Lin, Senkang Hu, Hangcheng Cao, Yiqin Deng, Xianhao Chen, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, in-car monitoring has emerged as a promising technology for
detecting early-stage abnormal status of the driver and providing timely alerts
to prevent traffic accidents. Although training models with multimodal data
enhances the reliability of abnormal status detection, the scarcity of labeled
data and the imbalance of class distribution impede the extraction of critical
abnormal state features, significantly deteriorating training performance.
Furthermore, missing modalities due to environment and hardware limitations
further exacerbate the challenge of abnormal status identification. More
importantly, monitoring abnormal health conditions of passengers, particularly
in elderly care, is of paramount importance but remains underexplored. To
address these challenges, we introduce our IC3M, an efficient
camera-rotation-based multimodal framework for monitoring both driver and
passengers in a car. Our IC3M comprises two key modules: an adaptive threshold
pseudo-labeling strategy and a missing modality reconstruction. The former
customizes pseudo-labeling thresholds for different classes based on the class
distribution, generating class-balanced pseudo labels to guide model training
effectively, while the latter leverages crossmodality relationships learned
from limited labels to accurately recover missing modalities by distribution
transferring from available modalities. Extensive experimental results
demonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,
precision, and recall while exhibiting superior robustness under limited
labeled data and severe missing modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12489v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12489v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin-Chun Li, Jin-Lin Tang, Bo Zhang, Lan Li, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the loss landscape offers insights into the inherent principles of
deep neural networks (DNNs). Recent work suggests an additional asymmetry of
the valley beyond the flat and sharp ones, yet without thoroughly examining its
causes or implications. Our study methodically explores the factors affecting
the symmetry of DNN valleys, encompassing (1) the dataset, network
architecture, initialization, and hyperparameters that influence the
convergence point; and (2) the magnitude and direction of the noise for 1D
visualization. Our major observation shows that the {\it degree of sign
consistency} between the noise and the convergence point is a critical
indicator of valley symmetry. Theoretical insights from the aspects of ReLU
activation and softmax function could explain the interesting phenomenon. Our
discovery propels novel understanding and applications in the scenario of Model
Fusion: (1) the efficacy of interpolating separate models significantly
correlates with their sign consistency ratio, and (2) imposing sign alignment
during federated learning emerges as an innovative approach for model parameter
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Dominici, Pietro Barbiero, Francesco Giannini, Martin Gjoreski, Giuseppe Marra, Marc Langheinrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current deep learning models are not designed to simultaneously address three
fundamental questions: predict class labels to solve a given classification
task (the "What?"), simulate changes in the situation to evaluate how this
impacts class predictions (the "How?"), and imagine how the scenario should
change to result in different class predictions (the "Why not?"). The inability
to answer these questions represents a crucial gap in deploying reliable AI
agents, calibrating human trust, and improving human-machine interaction. To
bridge this gap, we introduce CounterFactual Concept Bottleneck Models
(CF-CBMs), a class of models designed to efficiently address the above queries
all at once without the need to run post-hoc searches. Our experimental results
demonstrate that CF-CBMs: achieve classification accuracy comparable to
black-box models and existing CBMs ("What?"), rely on fewer important concepts
leading to simpler explanations ("How?"), and produce interpretable,
concept-based counterfactuals ("Why not?"). Additionally, we show that training
the counterfactual generator jointly with the CBM leads to two key
improvements: (i) it alters the model's decision-making process, making the
model rely on fewer important concepts (leading to simpler explanations), and
(ii) it significantly increases the causal effect of concept interventions on
class predictions, making the model more responsive to these changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance
Quesetion Answering (QA) performance of Large Language Models (LLMs), yet
structured KG verbalization remains challengin. Existing methods, such as
triple-form or free-form textual conversion of triple-form facts, encounter
several issues. These include reduced evidence density due to duplicated
entities or relationships, and reduced evidence clarity due to an inability to
emphasize crucial evidence. To address these issues, we propose EFSum, an
Evidence-focused Fact Summarization framework for enhanced QA with
knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer
through distillation and preference alignment. Our extensive experiments show
that EFSum improves LLM's zero-shot QA performance, and it is possible to
ensure both the helpfulness and faithfulness of the summary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Byzantine-Resilience of Distillation-Based Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christophe Roux, Max Zimmer, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) algorithms using Knowledge Distillation (KD) have
received increasing attention due to their favorable properties with respect to
privacy, non-i.i.d. data and communication cost. These methods depart from
transmitting model parameters and instead communicate information about a
learning task by sharing predictions on a public dataset. In this work, we
study the performance of such approaches in the byzantine setting, where a
subset of the clients act in an adversarial manner aiming to disrupt the
learning process. We show that KD-based FL algorithms are remarkably resilient
and analyze how byzantine clients can influence the learning process. Based on
these insights, we introduce two new byzantine attacks and demonstrate their
ability to break existing byzantine-resilient methods. Additionally, we propose
a novel defence method which enhances the byzantine resilience of KD-based FL
algorithms. Finally, we provide a general framework to obfuscate attacks,
making them significantly harder to detect, thereby improving their
effectiveness. Our findings serve as an important building block in the
analysis of byzantine FL, contributing through the development of new attacks
and new defence mechanisms, further advancing the robustness of KD-based FL
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Concept Graph Models: Beyond Causal Opacity in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Dominici, Pietro Barbiero, Mateo Espinosa Zarlenga, Alberto Termine, Martin Gjoreski, Giuseppe Marra, Marc Langheinrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal opacity denotes the difficulty in understanding the "hidden" causal
structure underlying the decisions of deep neural network (DNN) models. This
leads to the inability to rely on and verify state-of-the-art DNN-based
systems, especially in high-stakes scenarios. For this reason, circumventing
causal opacity in DNNs represents a key open challenge at the intersection of
deep learning, interpretability, and causality. This work addresses this gap by
introducing Causal Concept Graph Models (Causal CGMs), a class of interpretable
models whose decision-making process is causally transparent by design. Our
experiments show that Causal CGMs can: (i) match the generalisation performance
of causally opaque models, (ii) enable human-in-the-loop corrections to
mispredicted intermediate reasoning steps, boosting not just downstream
accuracy after corrections but also the reliability of the explanations
provided for specific instances, and (iii) support the analysis of
interventional and counterfactual scenarios, thereby improving the model's
causal interpretability and supporting the effective verification of its
reliability and fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Noise Robustness of In-Context Learning for Text <span class="highlight-title">Generation</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfu Gao, Feipeng Zhang, Wenyu Jiang, Jun Shu, Feng Zheng, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive performance on downstream
tasks by in-context learning (ICL), which heavily relies on the quality of
demonstrations selected from a large set of annotated examples. Recent works
claim that in-context learning is robust to noisy demonstrations in text
classification. In this work, we show that, on text generation tasks, noisy
annotations significantly hurt the performance of in-context learning. To
circumvent the issue, we propose a simple and effective approach called Local
Perplexity Ranking (LPR), which replaces the "noisy" candidates with their
nearest neighbors that are more likely to be clean. Our method is motivated by
analyzing the perplexity deviation caused by noisy labels and decomposing
perplexity into inherent perplexity and matching perplexity. Our key idea
behind LPR is thus to decouple the matching perplexity by performing the
ranking among the neighbors in semantic space. Our approach can prevent the
selected demonstrations from including mismatched input-label pairs while
preserving the effectiveness of the original selection methods. Extensive
experiments demonstrate the effectiveness of LPR, improving the EM score by up
to 18.75 on common benchmarks with noisy annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Training of Grid-Dependent Physics-Informed Kolmogorov-Arnold
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spyros Rigas, Michalis Papachristou, Theofilos Papadopoulos, Fotios Anagnostopoulos, Georgios Alexandridis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) have emerged as a robust framework
for solving Partial Differential Equations (PDEs) by approximating their
solutions via neural networks and imposing physics-based constraints on the
loss function. Traditionally, Multilayer Perceptrons (MLPs) have been the
neural network of choice, with significant progress made in optimizing their
training. Recently, Kolmogorov-Arnold Networks (KANs) were introduced as a
viable alternative, with the potential of offering better interpretability and
efficiency while requiring fewer parameters. In this paper, we present a fast
JAX-based implementation of grid-dependent Physics-Informed Kolmogorov-Arnold
Networks (PIKANs) for solving PDEs, achieving up to 84 times faster training
times than the original KAN implementation. We propose an adaptive training
scheme for PIKANs, introducing an adaptive state transition technique to avoid
loss function peaks between grid extensions, and a methodology for designing
PIKANs with alternative basis functions. Through comparative experiments, we
demonstrate that the adaptive features significantly enhance solution accuracy,
decreasing the L^2 error relative to the reference solution by up to 43.02%.
For the studied PDEs, our methodology approaches or surpasses the results
obtained from architectures that utilize up to 8.5 times more parameters,
highlighting the potential of adaptive, grid-dependent PIKANs as a superior
alternative in scientific and engineering applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Regression over Averaged Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Bertsimas, Yu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new formulation of robust regression by integrating all
realizations of the uncertainty set and taking an averaged approach to obtain
the optimal solution for the ordinary least squares regression problem. We show
that this formulation recovers ridge regression exactly and establishes the
missing link between robust optimization and the mean squared error approaches
for existing regression problems. We further demonstrate that the condition of
this equivalence relies on the geometric properties of the defined uncertainty
set. We provide exact, closed-form, in some cases, analytical solutions to the
equivalent regularization strength under uncertainty sets induced by $\ell_p$
norm, Schatten $p$-norm, and general polytopes. We then show in synthetic
datasets with different levels of uncertainties, a consistent improvement of
the averaged formulation over the existing worst-case formulation in
out-of-sample performance. In real-world regression problems obtained from UCI
datasets, similar improvements are seen in the out-of-sample datasets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongshuai Duan, Honglei Su, Qi Liu, Hui Yuan, Wei Gao, Jiarun Song, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-reference bitstream-layer point cloud quality assessment (PCQA) can be
deployed without full decoding at any network node to achieve real-time quality
monitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT
encoding mode. First, to address the issue that existing PCQA databases have a
small scale and limited distortion levels, we establish the WPC5.0 database
which is the first one dedicated to Octree-RAHT encoding mode with a scale of
400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude
distortion levels. Then, we propose the first PCQA model dedicated to
Octree-RAHT encoding mode by parsing PC bitstreams without full decoding. The
model introduces texture bitrate (TBPP) to predict texture complexity (TC) and
further derives the texture distortion factor. In addition, the Geometric
Quantization Parameter (PQS) is used to estimate the geometric distortion
factor, which is then integrated into the model along with the texture
distortion factor to obtain the proposed PCQA model named streamPCQ-OR. The
proposed model has been compared with other advanced PCQA methods on the
WPC5.0, BASICS and M-PCCD databases, and experimental results show that our
model has excellent performance while having very low computational complexity,
providing a reliable choice for time-critical applications. To facilitate
subsequent research, the database and source code will be publicly released at
https://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Impact of Point Cloud Colorization on Semantic
  Segmentation Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinfeng Zhu, Jiaze Cao, Yuanzhi Cai, Lei Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud semantic segmentation, the process of classifying each point into
predefined categories, is essential for 3D scene understanding. While
image-based segmentation is widely adopted due to its maturity, methods relying
solely on RGB information often suffer from degraded performance due to color
inaccuracies. Recent advancements have incorporated additional features such as
intensity and geometric information, yet RGB channels continue to negatively
impact segmentation accuracy when errors in colorization occur. Despite this,
previous studies have not rigorously quantified the effects of erroneous
colorization on segmentation performance. In this paper, we propose a novel
statistical approach to evaluate the impact of inaccurate RGB information on
image-based point cloud segmentation. We categorize RGB inaccuracies into two
types: incorrect color information and similar color information. Our results
demonstrate that both types of color inaccuracies significantly degrade
segmentation accuracy, with similar color errors particularly affecting the
extraction of geometric features. These findings highlight the critical need to
reassess the role of RGB information in point cloud segmentation and its
implications for future algorithm design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 IEEE 8th International Conference on Vision, Image
  and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Evaluation in Multimedia Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loris Sauter, Ralph Gasser, Heiko Schuldt, Abraham Bernstein, Luca Rossetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance evaluation in multimedia retrieval, as in the information
retrieval domain at large, relies heavily on retrieval experiments, employing a
broad range of techniques and metrics. These can involve human-in-the-loop and
machine-only settings for the retrieval process itself and the subsequent
verification of results. Such experiments can be elaborate and
use-case-specific, which can make them difficult to compare or replicate. In
this paper, we present a formal model to express all relevant aspects of such
retrieval experiments, as well as a flexible open-source evaluation
infrastructure that implements the model. These contributions intend to make a
step towards lowering the hurdles for conducting retrieval experiments and
improving their reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xiao, Zhenzhen Hu, Jia Li, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-video retrieval (TVR) has seen substantial advancements in recent years,
fueled by the utilization of pre-trained models and large language models
(LLMs). Despite these advancements, achieving accurate matching in TVR remains
challenging due to inherent disparities between video and textual modalities
and irregularities in data representation. In this paper, we propose
Text-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the
conventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.
By replacing a single text query with a series of text proxies, TV-ProxyNet not
only broadens the query scope but also achieves a more precise expansion. Each
text proxy is crafted through a refined iterative process, controlled by
mechanisms we term as the director and dash, which regulate the proxy's
direction and distance relative to the original text query. This setup not only
facilitates more precise semantic alignment but also effectively manages the
disparities and noise inherent in multimodal data. Our experiments on three
representative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet
Captions, demonstrate the effectiveness of TV-ProxyNet. The results show an
improvement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved
state-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%
improvement on DiDeMo compared to existing methods, validating our approach's
ability to enhance semantic mapping and reduce error propensity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMD: Contrastive Multi-Modal <span class="highlight-title">Diffusion</span> for Video-Audio Conditional
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Hannes Gamper, Sebastian Braun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a multi-modal diffusion model tailored for the bi-directional
conditional generation of video and audio. We propose a joint contrastive
training loss to improve the synchronization between visual and auditory
occurrences. We present experiments on two datasets to evaluate the efficacy of
our proposed model. The assessment of generation quality and alignment
performance is carried out from various angles, encompassing both objective and
subjective metrics. Our findings demonstrate that the proposed model
outperforms the baseline in terms of quality and generation speed through
introduction of our novel cross-modal easy fusion architectural block.
Furthermore, the incorporation of the contrastive loss results in improvements
in audio-visual alignment, particularly in the high-correlation video-to-audio
generation task.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-08T00:00:00Z">2024-10-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POLIPHONE: A <span class="highlight-title">Dataset</span> for Smartphone Model Identification from Audio
  Recordings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Salvi, Daniele Ugo Leonzio, Antonio Giganti, Claudio Eutizi, Sara Mandelli, Paolo Bestagini, Stefano Tubaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When dealing with multimedia data, source attribution is a key challenge from
a forensic perspective. This task aims to determine how a given content was
captured, providing valuable insights for various applications, including legal
proceedings and integrity investigations. The source attribution problem has
been addressed in different domains, from identifying the camera model used to
capture specific photographs to detecting the synthetic speech generator or
microphone model used to create or record given audio tracks. Recent
advancements in this area rely heavily on machine learning and data-driven
techniques, which often outperform traditional signal processing-based methods.
  However, a drawback of these systems is their need for large volumes of
training data, which must reflect the latest technological trends to produce
accurate and reliable predictions. This presents a significant challenge, as
the rapid pace of technological progress makes it difficult to maintain
datasets that are up-to-date with real-world conditions. For instance, in the
task of smartphone model identification from audio recordings, the available
datasets are often outdated or acquired inconsistently, making it difficult to
develop solutions that are valid beyond a research environment. In this paper
we present POLIPHONE, a dataset for smartphone model identification from audio
recordings. It includes data from 20 recent smartphones recorded in a
controlled environment to ensure reproducibility and scalability for future
research. The released tracks contain audio data from various domains (i.e.,
speech, music, environmental sounds), making the corpus versatile and
applicable to a wide range of use cases. We also present numerous experiments
to benchmark the proposed dataset using a state-of-the-art classifier for
smartphone model identification from audio recordings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Scalable <span class="highlight-title">Image</span> Feature Compression: A Content-Adaptive and
  <span class="highlight-title">Diffusion</span>-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sha Guo, Zhuo Chen, Yang Zhao, Ning Zhang, Xiaotong Li, Lingyu Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional image codecs emphasize signal fidelity and human perception,
often at the expense of machine vision tasks. Deep learning methods have
demonstrated promising coding performance by utilizing rich semantic embeddings
optimized for both human and machine vision. However, these compact embeddings
struggle to capture fine details such as contours and textures, resulting in
imperfect reconstructions. Furthermore, existing learning-based codecs lack
scalability. To address these limitations, this paper introduces a
content-adaptive diffusion model for scalable image compression. The proposed
method encodes fine textures through a diffusion process, enhancing perceptual
quality while preserving essential features for machine vision tasks. The
approach employs a Markov palette diffusion model combined with widely used
feature extractors and image generators, enabling efficient data compression.
By leveraging collaborative texture-semantic feature extraction and
pseudo-label generation, the method accurately captures texture information. A
content-adaptive Markov palette diffusion model is then applied to represent
both low-level textures and high-level semantic content in a scalable manner.
This framework offers flexible control over compression ratios by selecting
intermediate diffusion states, eliminating the need for retraining deep
learning models at different operating points. Extensive experiments
demonstrate the effectiveness of the proposed framework in both image
reconstruction and downstream machine vision tasks such as object detection,
segmentation, and facial landmark detection, achieving superior perceptual
quality compared to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resolution limit of the eye: how many pixels can we see? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maliha Ashraf, Alexandre Chapiro, Rafał K. Mantiuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large engineering efforts go towards improving the resolution of mobile,
AR and VR displays, it is important to know the maximum resolution at which
further improvements bring no noticeable benefit. This limit is often referred
to as the "retinal resolution", although the limiting factor may not
necessarily be attributed to the retina. To determine the ultimate resolution
at which an image appears sharp to our eyes with no perceivable blur, we
created an experimental setup with a sliding display, which allows for
continuous control of the resolution. The lack of such control was the main
limitation of the previous studies. We measure achromatic (black-white) and
chromatic (red-green and yellow-violet) resolution limits for foveal vision,
and at two eccentricities (10 and 20 deg). Our results demonstrate that the
resolution limit is higher than what was previously believed, reaching 94
pixels-per-degree (ppd) for foveal achromatic vision, 89 ppd for red-green
patterns, and 53 ppd for yellow-violet patterns. We also observe a much larger
drop in the resolution limit for chromatic patterns (red-green and
yellow-violet) than for achromatic. Our results set the north star for display
development, with implications for future imaging, rendering and video coding
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main document: 12 pages, 4 figures, 1 table. Supplementary: 14 pages,
  12 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Gaussian Data Augmentation in Feature Space for One-shot Object
  Detection in Manga 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takara Taniguchi, Ryosuke Furuta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle one-shot object detection in Japanese Manga. The rising global
popularity of Japanese manga has made the object detection of character faces
increasingly important, with potential applications such as automatic
colorization. However, obtaining sufficient data for training conventional
object detectors is challenging due to copyright restrictions. Additionally,
new characters appear every time a new volume of manga is released, making it
impractical to re-train object detectors each time to detect these new
characters. Therefore, one-shot object detection, where only a single query
(reference) image is required to detect a new character, is an essential task
in the manga industry. One challenge with one-shot object detection in manga is
the large variation in the poses and facial expressions of characters in target
images, despite having only one query image as a reference. Another challenge
is that the frequency of character appearances follows a long-tail
distribution. To overcome these challenges, we propose a data augmentation
method in feature space to increase the variation of the query. The proposed
method augments the feature from the query by adding Gaussian noise, with the
noise variance at each channel learned during training. The experimental
results show that the proposed method improves the performance for both seen
and unseen classes, surpassing data augmentation methods in image space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding is All You Need? Dual Temporal Grounding for Video Dialog 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Qin, Wei Ji, Xinze Lan, Hao Fei, Xun Yang, Dan Guo, Roger Zimmermann, Lizi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of video dialog response generation, the understanding of video
content and the temporal nuances of conversation history are paramount. While a
segment of current research leans heavily on large-scale pretrained
visual-language models and often overlooks temporal dynamics, another delves
deep into spatial-temporal relationships within videos but demands intricate
object trajectory pre-extractions and sidelines dialog temporal dynamics. This
paper introduces the Dual Temporal Grounding-enhanced Video Dialog model
(DTGVD), strategically designed to merge the strengths of both dominant
approaches. It emphasizes dual temporal relationships by predicting dialog
turn-specific temporal regions, filtering video content accordingly, and
grounding responses in both video and dialog contexts. One standout feature of
DTGVD is its heightened attention to chronological interplay. By recognizing
and acting upon the dependencies between different dialog turns, it captures
more nuanced conversational dynamics. To further bolster the alignment between
video and dialog temporal dynamics, we've implemented a list-wise contrastive
learning strategy. Within this framework, accurately grounded turn-clip
pairings are designated as positive samples, while less precise pairings are
categorized as negative. This refined classification is then funneled into our
holistic end-to-end response generation mechanism. Evaluations using
AVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our
methodology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIA-OVD: Shape-Invariant Adapter for Bridging the <span class="highlight-title">Image</span>-Region Gap in
  Open-Vocabulary Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zishuo Wang, Wenhao Zhou, Jinglin Xu, Yuxin Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary detection (OVD) aims to detect novel objects without
instance-level annotations to achieve open-world object detection at a lower
cost. Existing OVD methods mainly rely on the powerful open-vocabulary
image-text alignment capability of Vision-Language Pretrained Models (VLM) such
as CLIP. However, CLIP is trained on image-text pairs and lacks the perceptual
ability for local regions within an image, resulting in the gap between image
and region representations. Directly using CLIP for OVD causes inaccurate
region classification. We find the image-region gap is primarily caused by the
deformation of region feature maps during region of interest (RoI) extraction.
To mitigate the inaccurate region classification in OVD, we propose a new
Shape-Invariant Adapter named SIA-OVD to bridge the image-region gap in the OVD
task. SIA-OVD learns a set of feature adapters for regions with different
shapes and designs a new adapter allocation mechanism to select the optimal
adapter for each region. The adapted region representations can align better
with text representations learned by CLIP. Extensive experiments demonstrate
that SIA-OVD effectively improves the classification accuracy for regions by
addressing the gap between images and regions caused by shape deformation.
SIA-OVD achieves substantial improvements over representative methods on the
COCO-OVD benchmark. The code is available at
https://github.com/PKU-ICST-MIPL/SIA-OVD_ACMMM2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-07T00:00:00Z">2024-10-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">101</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Advisor: Dynamic Data Curation for Safety Alignment of Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Ninareh Mehrabi, Palash Goyal, Rahul Gupta, Kai-Wei Chang, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is a crucial element in large language model (LLM) alignment. Recent
studies have explored using LLMs for efficient data collection. However,
LLM-generated data often suffers from quality issues, with underrepresented or
absent aspects and low-quality datapoints. To address these problems, we
propose Data Advisor, an enhanced LLM-based method for generating data that
takes into account the characteristics of the desired dataset. Starting from a
set of pre-defined principles in hand, Data Advisor monitors the status of the
generated data, identifies weaknesses in the current dataset, and advises the
next iteration of data generation accordingly. Data Advisor can be easily
integrated into existing data generation methods to enhance data quality and
coverage. Experiments on safety alignment of three representative LLMs (i.e.,
Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in
enhancing model safety against various fine-grained safety issues without
sacrificing model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/DataAdvisor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Partially-Defined Events in <span class="highlight-title">Multimodal</span> Data <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Sanders, Reno Kriz, David Etter, Hannah Recknor, Alexander Martin, Cameron Carpenter, Jingyang Lin, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How are we able to learn about complex current events just from short
snippets of video? While natural language enables straightforward ways to
represent under-specified, partially observable events, visual data does not
facilitate analogous methods and, consequently, introduces unique challenges in
event understanding. With the growing prevalence of vision-capable AI agents,
these systems must be able to model events from collections of unstructured
video data. To tackle robust event modeling in multimodal settings, we
introduce a multimodal formulation for partially-defined events and cast the
extraction of these events as a three-stage span retrieval task. We propose a
corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours
of densely annotated current event videos and 1,168 text documents, containing
22.8K labeled event-centric entities. We propose a collection of LLM-driven
approaches to the task of multimodal event analysis, and evaluate them on
MultiVENT-G. Results illustrate the challenges that abstract event
understanding poses and demonstrates promise in event-centric video-language
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 9 pages; 2024 EMNLP Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers
  in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is essential for deploying Large Language Models (LLMs) by
enhancing memory efficiency and inference speed. Existing methods for
activation quantization mainly address channel-wise outliers, often neglecting
token-wise outliers, leading to reliance on costly per-token dynamic
quantization. To address this, we introduce PrefixQuant, a novel technique that
isolates outlier tokens offline without re-training. Specifically, PrefixQuant
identifies high-frequency outlier tokens and prefixes them in the KV cache,
preventing the generation of outlier tokens during inference and simplifying
quantization. To our knowledge, PrefixQuant is the first to enable efficient
per-tensor static quantization to outperform expensive per-token dynamic
quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and
4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization
achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5
common-sense reasoning tasks, outperforming previous per-token dynamic
quantization methods like QuaRot with 0.98 perplexity improvement and +5.98
points accuracy. Additionally, the inference speed of W4A4 quantized models
using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot
models by 1.2x to 1.3x. Our code is available at
\url{https://github.com/ChenMnZ/PrefixQuant}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A PTQ method to significantly boost the performance of static
  activation quantization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TurtleBench: Evaluating Top Language Models via Real-World Yes/No
  Puzzles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingchen Yu, Shichao Song, Ke Fang, Yunfeng Shi, Zifan Zheng, Hanyu Wang, Simin Niu, Zhiyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the application of Large Language Models (LLMs) expands, the demand for
reliable evaluations increases. Existing LLM evaluation benchmarks primarily
rely on static datasets, making it challenging to assess model performance in
dynamic interactions with users. Moreover, these benchmarks often depend on
specific background knowledge, complicating the measurement of a model's
logical reasoning capabilities. Other dynamic evaluation methods based on
strong models or manual efforts may introduce biases and incur high costs and
time demands, hindering large-scale application. To address these issues, we
propose TurtleBench. TurtleBench collects real user guesses from our online
Turtle Soup Puzzle platform that we developed. This approach allows for the
relatively dynamic generation of evaluation datasets, mitigating the risk of
model cheating while aligning assessments more closely with genuine user needs
for reasoning capabilities, thus enhancing the reliability of evaluations.
TurtleBench includes 1,532 user guesses along with the correctness of guesses
after annotation. Using this dataset, we thoroughly evaluated nine of the most
advanced LLMs available today. Notably, the OpenAI o1 series models did not
achieve leading results in these evaluations. We propose several hypotheses for
further research, such as "the latent reasoning of o1 utilizes trivial
Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides
reasoning benefits but also incurs noise costs."
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer tends to overallocate attention to irrelevant context. In this
work, we introduce Diff Transformer, which amplifies attention to the relevant
context while canceling noise. Specifically, the differential attention
mechanism calculates attention scores as the difference between two separate
softmax attention maps. The subtraction cancels noise, promoting the emergence
of sparse attention patterns. Experimental results on language modeling show
that Diff Transformer outperforms Transformer in various settings of scaling up
model size and training tokens. More intriguingly, it offers notable advantages
in practical applications, such as long-context modeling, key information
retrieval, hallucination mitigation, in-context learning, and reduction of
activation outliers. By being less distracted by irrelevant context, Diff
Transformer can mitigate hallucination in question answering and text
summarization. For in-context learning, Diff Transformer not only enhances
accuracy but is also more robust to order permutation, which was considered as
a chronic robustness issue. The results position Diff Transformer as a highly
effective and promising architecture to advance large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLEE: A Unified Framework and <span class="highlight-title">Benchmark</span> for Language-based Economic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show significant potential in economic and
strategic interactions, where communication via natural language is often
prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic
human behavior? Do they tend to reach an efficient and fair outcome? What is
the role of natural language in the strategic interaction? How do
characteristics of the economic environment influence these dynamics? These
questions become crucial concerning the economic and societal implications of
integrating LLM-based agents into real-world data-driven systems, such as
online retail platforms and recommender systems. While the ML community has
been exploring the potential of LLMs in such multi-agent setups, varying
assumptions, design choices and evaluation criteria across studies make it
difficult to draw robust and meaningful conclusions. To address this, we
introduce a benchmark for standardizing research on two-player, sequential,
language-based games. Inspired by the economic literature, we define three base
families of games with consistent parameterization, degrees of freedom and
economic measures to evaluate agents' performance (self-gain), as well as the
game outcome (efficiency and fairness). We develop an open-source framework for
interaction simulation and analysis, and utilize it to collect a dataset of LLM
vs. LLM interactions across numerous game configurations and an additional
dataset of human vs. LLM interactions. Through extensive experimentation, we
demonstrate how our framework and dataset can be used to: (i) compare the
behavior of LLM-based agents to human players in various economic contexts;
(ii) evaluate agents in both individual and collective performance measures;
and (iii) quantify the effect of the economic characteristics of the
environments on the behavior of agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Micro-Narratives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mourad Heddaya, Qingcheng Zeng, Chenhao Tan, Rob Voigt, Alexander Zentefis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Workshop on Narrative Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To induce desired behaviors in large language models (LLMs) for
interaction-driven tasks, the instruction-tuning stage typically trains LLMs on
instruction-response pairs using the next-token prediction (NTP) loss. Previous
work aiming to improve instruction-tuning performance often emphasizes the need
for higher-quality supervised fine-tuning (SFT) datasets, which typically
involves expensive data filtering with proprietary LLMs or labor-intensive data
generation by human annotators. However, these approaches do not fully leverage
the datasets' intrinsic properties, resulting in high computational and labor
costs, thereby limiting scalability and performance gains. In this paper, we
propose SFTMix, a novel recipe that elevates instruction-tuning performance
beyond the conventional NTP paradigm, without the need for well-curated
datasets. Observing that LLMs exhibit uneven confidence across the semantic
representation space, we argue that examples with different confidence levels
should play distinct roles during the instruction-tuning process. Based on this
insight, SFTMix leverages training dynamics to identify examples with varying
confidence levels, then applies a Mixup-based regularization to mitigate
overfitting on confident examples while propagating supervision signals to
improve learning on relatively unconfident ones. This approach enables SFTMix
to significantly outperform NTP across a wide range of instruction-following
and healthcare domain-specific SFT tasks, demonstrating its adaptability to
diverse LLM families and scalability to datasets of any size. Comprehensive
ablation studies further verify the robustness of SFTMix's design choices,
underscoring its versatility in consistently enhancing performance across
different LLMs and datasets in broader natural language processing
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Digital World as Humans Do: Universal Visual Grounding
  for GUI <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly take
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TuneVLSeg: Prompt Tuning <span class="highlight-title">Benchmark</span> for Vision-Language Segmentation
  Models <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabin Adhikari, Safal Thapaliya, Manish Dhakal, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have shown impressive performance in vision
tasks, but adapting them to new domains often requires expensive fine-tuning.
Prompt tuning techniques, including textual, visual, and multimodal prompting,
offer efficient alternatives by leveraging learnable prompts. However, their
application to Vision-Language Segmentation Models (VLSMs) and evaluation under
significant domain shifts remain unexplored. This work presents an open-source
benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal
prompt tuning techniques into VLSMs, making prompt tuning usable for downstream
segmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt
tuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$
different combinations. We test various prompt tuning on $8$ diverse medical
datasets, including $3$ radiology datasets (breast tumor, echocardiograph,
chest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin
cancer), and two natural domain segmentation datasets. Our study found that
textual prompt tuning struggles under significant domain shifts, from
natural-domain images to medical data. Furthermore, visual prompt tuning, with
fewer hyperparameters than multimodal prompt tuning, often achieves performance
competitive to multimodal approaches, making it a valuable first attempt. Our
work advances the understanding and applicability of different prompt-tuning
techniques for robust domain-specific segmentation. The source code is
available at https://github.com/naamiinepal/tunevlseg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACCV 2024 (oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CasiMedicos-Arg: A Medical Question Answering <span class="highlight-title">Dataset</span> Annotated with
  Explanatory Argumentative Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        katerina Sviridova, Anar Yeginbergen, Ainara Estarrona, Elena Cabrio, Serena Villata, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explaining Artificial Intelligence (AI) decisions is a major challenge
nowadays in AI, in particular when applied to sensitive scenarios like medicine
and law. However, the need to explain the rationale behind decisions is a main
issue also for human-based deliberation as it is important to justify
\textit{why} a certain decision has been taken. Resident medical doctors for
instance are required not only to provide a (possibly correct) diagnosis, but
also to explain how they reached a certain conclusion. Developing new tools to
aid residents to train their explanation skills is therefore a central
objective of AI in education. In this paper, we follow this direction, and we
present, to the best of our knowledge, the first multilingual dataset for
Medical Question Answering where correct and incorrect diagnoses for a clinical
case are enriched with a natural language explanation written by doctors. These
explanations have been manually annotated with argument components (i.e.,
premise, claim) and argument relations (i.e., attack, support), resulting in
the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases
in four languages (English, Spanish, French, Italian) with explanations, where
we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106
attack relations. We conclude by showing how competitive baselines perform over
this challenging dataset for the argument mining task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cookbook: A framework for improving <span class="highlight-title">LLM</span> generative abilities via
  programmatic data generating templates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avanika Narayan, Mayee F. Chen, Kush Bhatia, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) on instruction datasets is a common
way to improve their generative capabilities. However, instruction datasets can
be expensive and time-consuming to manually curate, and while LLM-generated
data is less labor-intensive, it may violate user privacy agreements or terms
of service of LLM providers. Therefore, we seek a way of constructing
instruction datasets with samples that are not generated by humans or LLMs but
still improve LLM generative capabilities. In this work, we introduce Cookbook,
a framework that programmatically generates training data consisting of simple
patterns over random tokens, resulting in a scalable, cost-effective approach
that avoids legal and privacy issues. First, Cookbook uses a template -- a data
generating Python function -- to produce training data that encourages the
model to learn an explicit pattern-based rule that corresponds to a desired
task. We find that fine-tuning on Cookbook-generated data is able to improve
performance on its corresponding task by up to 52.7 accuracy points. Second,
since instruction datasets improve performance on multiple downstream tasks
simultaneously, Cookbook algorithmically learns how to mix data from various
templates to optimize performance on multiple tasks. On the standard multi-task
GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated
dataset attains the best accuracy on average compared to other 7B parameter
instruction-tuned models and is the best performing model on 3 out of 8 tasks.
Finally, we analyze when and why Cookbook improves performance and present a
metric that allows us to verify that the improvement is largely explained by
the model's generations adhering better to template rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Model <span class="highlight-title">Benchmark</span>ing with Only a Few Observations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we precisely estimate a large language model's (LLM) accuracy on
questions belonging to a specific topic within a larger question-answering
dataset? The standard direct estimator, which averages the model's accuracy on
the questions in each subgroup, may exhibit high variance for subgroups
(topics) with small sample sizes. Synthetic regression modeling, which
leverages the model's accuracy on questions about other topics, may yield
biased estimates that are too unreliable for large subgroups. We prescribe a
simple yet effective solution: an empirical Bayes (EB) estimator that balances
direct and regression estimates for each subgroup separately, improving the
precision of subgroup-level estimates of model performance. Our experiments on
multiple datasets show that this approach consistently provides more precise
estimates of the LLM performance compared to the direct and regression
approaches, achieving substantial reductions in the mean squared error.
Confidence intervals for EB estimates also have near-nominal coverage and are
narrower compared to those for the direct estimator. Additional experiments on
tabular and vision data validate the benefits of this EB approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density estimation with <span class="highlight-title">LLM</span>s: a geometric investigation of in-context
  learning trajectories <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable emergent abilities to
perform in-context learning across various tasks, including time series
forecasting. This work investigates LLMs' ability to estimate probability
density functions (PDFs) from data observed in-context; such density estimation
(DE) is a fundamental task underlying many probabilistic modeling problems. We
leverage the Intensive Principal Component Analysis (InPCA) to visualize and
analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is
that these LLMs all follow similar learning trajectories in a low-dimensional
InPCA space, which are distinct from those of traditional density estimation
methods like histograms and Gaussian kernel density estimation (KDE). We
interpret the LLaMA in-context DE process as a KDE with an adaptive kernel
width and shape. This custom kernel model captures a significant portion of
LLaMA's behavior despite having only two parameters. We further speculate on
why LLaMA's kernel width and shape differs from classical algorithms, providing
insights into the mechanism of in-context probabilistic reasoning in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving
  Vision-Linguistic Compositionality <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, In So Kweon, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new method to enhance compositional understanding
in pre-trained vision and language models (VLMs) without sacrificing
performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches
often improve compositional reasoning at the cost of degrading multi-modal
capabilities, primarily due to the use of global hard negative (HN) loss, which
contrasts global representations of images and texts. This global HN loss
pushes HN texts that are highly similar to the original ones, damaging the
model's multi-modal representations. To overcome this limitation, we propose
Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard
negative loss and selective calibrated regularization. These innovations
provide fine-grained negative supervision while preserving the model's
representational integrity. Our extensive evaluations across diverse benchmarks
for both compositionality and multi-modal tasks show that FSC-CLIP not only
achieves compositionality on par with state-of-the-art models but also retains
strong multi-modal capabilities. Code is available at:
https://github.com/ytaek-oh/fsc-clip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Long, Main). Project page:
  https://ytaek-oh.github.io/fsc-clip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Studying and Mitigating Biases in Sign Language Understanding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine Atwell, Danielle Bragg, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that the benefits of sign language technologies are distributed
equitably among all community members is crucial. Thus, it is important to
address potential biases and inequities that may arise from the design or use
of these resources. Crowd-sourced sign language datasets, such as the ASL
Citizen dataset, are great resources for improving accessibility and preserving
linguistic diversity, but they must be used thoughtfully to avoid reinforcing
existing biases.
  In this work, we utilize the rich information about participant demographics
and lexical features present in the ASL Citizen dataset to study and document
the biases that may result from models trained on crowd-sourced sign datasets.
Further, we apply several bias mitigation techniques during model training, and
find that these techniques reduce performance disparities without decreasing
accuracy. With the publication of this work, we release the demographic
information about the participants in the ASL Citizen dataset to encourage
future bias mitigation work in this space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RevisEval: Improving <span class="highlight-title">LLM</span>-as-a-Judge via Response-Adapted References 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Zhang, Yufei Wang, Tiezheng YU, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With significant efforts in recent studies, LLM-as-a-Judge has become a
cost-effective alternative to human evaluation for assessing the text
generation quality in a wide range of tasks. However, there still remains a
reliability gap between LLM-as-a-Judge and human evaluation. One important
reason is the lack of guided oracles in the evaluation process. Motivated by
the role of reference pervasively used in classic text evaluation, we introduce
RevisEval, a novel text generation evaluation paradigm via the response-adapted
references. RevisEval is driven by the key observation that an ideal reference
should maintain the necessary relevance to the response to be evaluated.
Specifically, RevisEval leverages the text revision capabilities of large
language models (LLMs) to adaptively revise the response, then treat the
revised text as the reference (response-adapted reference) for the subsequent
evaluation. Extensive experiments demonstrate that RevisEval outperforms
traditional reference-free and reference-based evaluation paradigms that use
LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.
More importantly, our response-adapted references can further boost the
classical text metrics, e.g., BLEU and BERTScore, compared to traditional
references and even rival the LLM-as-a-Judge. A detailed analysis is also
conducted to confirm RevisEval's effectiveness in bias reduction, the impact of
inference cost, and reference relevance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss
  Landscape Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training language models currently requires pre-determining a fixed compute
budget because the typical cosine learning rate schedule depends on the total
number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a
constant learning rate to produce a main branch of iterates that can in
principle continue indefinitely without a pre-specified compute budget. Then,
given any compute budget, one can branch out from the main branch at a proper
at any time with a rapidly decaying learning rate to produce a strong model.
Empirically, WSD generates a non-traditional loss curve: the loss remains
elevated during the stable phase but sharply declines during the decay phase.
Towards explaining this phenomenon, we conjecture that pretraining loss
exhibits a river valley landscape, which resembles a deep valley with a river
at its bottom. Under this assumption, we show that during the stable phase, the
iterate undergoes large oscillations due to the high learning rate, yet it
progresses swiftly along the river. During the decay phase, the rapidly
dropping learning rate minimizes the iterate's oscillations, moving it closer
to the river and revealing true optimization progress. Therefore, the sustained
high learning rate phase and fast decaying phase are responsible for progress
in the river and the mountain directions respectively, and are both critical.
Our analysis predicts phenomenons consistent with empirical observations and
shows that this landscape can emerge from pretraining on a simple bi-gram
dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that
reuses previous checkpoints' decay phases and keeps only one main branch, where
we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and
Cyclic-Cosine in obtaining multiple language model checkpoints across various
compute budgets in a single run for parameters scaling from 0.1B to 1.2B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages,13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Correlation: Interpretable Evaluation of Machine Translation
  Metrics <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Perrella, Lorenzo Proietti, Pere-Lluís Huguet Cabot, Edoardo Barba, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) evaluation metrics assess translation quality
automatically. Recently, researchers have employed MT metrics for various new
use cases, such as data filtering and translation re-ranking. However, most MT
metrics return assessments as scalar scores that are difficult to interpret,
posing a challenge to making informed design choices. Moreover, MT metrics'
capabilities have historically been evaluated using correlation with human
judgment, which, despite its efficacy, falls short of providing intuitive
insights into metric performance, especially in terms of new metric use cases.
To address these issues, we introduce an interpretable evaluation framework for
MT metrics. Within this framework, we evaluate metrics in two scenarios that
serve as proxies for the data filtering and translation re-ranking use cases.
Furthermore, by measuring the performance of MT metrics using Precision,
Recall, and F-score, we offer clearer insights into their capabilities than
correlation with human judgments. Finally, we raise concerns regarding the
reliability of manually curated data following the Direct Assessments+Scalar
Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with
Multidimensional Quality Metrics (MQM) annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference. 26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Equity in <span class="highlight-title">Large Language Model</span>s for Medical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelyu Ji, Wenhe Ma, Sonish Sivarajkumar, Hang Zhang, Eugene Mathew Sadhu, Zhuochun Li, Xizhi Wu, Shyam Visweswaran, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have highlighted the potential of large language models
(LLMs) in medical applications, notably in automating Clinical Trial Matching
for translational research and providing medical question-answering for
clinical decision support. However, our study reveals significant inequities in
the use of LLMs, particularly for individuals from specific racial, gender, and
underrepresented groups influenced by social determinants of health. These
disparities could worsen existing health inequities if LLMs are broadly adopted
in healthcare. To address this, we propose and evaluate a novel framework,
EquityGuard, designed to detect and mitigate biases in LLM-based medical
applications. EquityGuard incorporates a Bias Detection Mechanism capable of
identifying and correcting unfair predictions, thus enhancing outcomes and
promoting equity across diverse population groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Reasoning</span>Rank: Teaching Student Models to Rank through <span class="highlight-title">Reasoning</span>-Based
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking documents based on their relevance to a given query is critical in
information retrieval. Traditional reranking methods often focus on improving
the initial rankings but lack transparency, failing to explain why one document
is ranked higher. In this paper, we introduce ReasoningRank, a novel reranking
approach that enhances clarity by generating two types of reasoning: explicit
reasoning, which explains how a document addresses the query, and comparison
reasoning, which justifies the relevance of one document over another. We
leverage large language models (LLMs) as teacher models to generate these
explanations and distill this knowledge into smaller, more resource-efficient
student models. While the student models may not outperform LLMs in speed, they
significantly reduce the computational burden by requiring fewer resources,
making them more suitable for large-scale or resource-constrained settings.
These student models are trained to both generate meaningful reasoning and
rerank documents, achieving competitive performance across multiple datasets,
including MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank
improves reranking accuracy and provides valuable insights into the
decision-making process, offering a structured and interpretable solution for
reranking tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Inference for <span class="highlight-title">Large Language Model</span>-based Generative
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Lin, Chaoqun Yang, Wenjie Wang, Yongqi Li, Cunxiao Du, Fuli Feng, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based generative recommendation has achieved
notable success, yet its practical deployment is costly particularly due to
excessive inference latency caused by autoregressive decoding. For lossless LLM
decoding acceleration, Speculative Decoding (SD) has emerged as a promising
solution. However, applying SD to generative recommendation presents unique
challenges due to the requirement of generating top-K items (i.e., K distinct
token sequences) as a recommendation list by beam search. This leads to more
stringent verification in SD, where all the top-K sequences from the target LLM
must be successfully drafted by the draft model at each decoding step. To
alleviate this, we consider 1) boosting top-K sequence alignment between the
draft model and the target LLM, and 2) relaxing the verification strategy to
reduce trivial LLM calls. To this end, we propose an alignment framework named
AtSpeed, which presents the AtSpeed-S optimization objective for top-K
alignment under the strict top-K verification. Moreover, we introduce a relaxed
sampling verification strategy that allows high-probability non-top-K drafted
sequences to be accepted, significantly reducing LLM calls. Correspondingly, we
propose AtSpeed-R for top-K alignment under this relaxed sampling verification.
Empirical results on two real-world datasets demonstrate that AtSpeed
significantly accelerates LLM-based generative recommendation, e.g., near 2x
speedup under strict top-K verification and up to 2.5 speedup under relaxed
sampling verification. The codes and datasets will be released in the near
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Interplay of Parametric and Non-parametric Memory in
  Retrieval-augmented Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Farahani, Richard Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative language models often struggle with specialized or less-discussed
knowledge. A potential solution is found in Retrieval-Augmented Generation
(RAG) models which act like retrieving information before generating responses.
In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides
between what it already knows (parametric) and what it retrieves
(non-parametric). We use causal mediation analysis and controlled experiments
to examine how internal representations influence information processing. Our
findings disentangle the effects of parametric knowledge and the retrieved
context. They indicate that in cases where the model can choose between both
types of information (parametric and non-parametric), it relies more on the
context than the parametric knowledge. Furthermore, the analysis investigates
the computations involved in \emph{how} the model uses the information from the
context. We find that multiple mechanisms are active within the model and can
be detected with mediation analysis: first, the decision of \emph{whether the
context is relevant}, and second, how the encoder computes output
representations to support copying when relevant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM2Vec: Training <span class="highlight-title">Vision-Language Model</span>s for Massive <span class="highlight-title">Multimodal</span>
  Embedding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models have been crucial in enabling various downstream tasks such
as semantic similarity, information retrieval, and clustering. Recently, there
has been a surge of interest in developing universal text embedding models that
can generalize across tasks (e.g., MTEB). However, progress in learning
universal multimodal embedding models has been relatively slow despite their
importance. In this work, we aim to explore the potential for building
universal embeddings capable of handling a wide range of downstream tasks. Our
contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),
which covers 4 meta-tasks (i.e. classification, visual question answering,
multimodal retrieval, and visual grounding) and 36 datasets, including 20
training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->
Vector), a contrastive training framework that converts any state-of-the-art
vision-language model into an embedding model via training on MMEB. Unlike
previous models such as CLIP and BLIP, VLM2Vec can process any combination of
images and text to generate a fixed-dimensional vector based on task
instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate
them on MMEB's evaluation split. Our results show that \model achieves an
absolute average improvement of 10% to 20% over existing multimodal embedding
models on both in-distribution and out-of-distribution datasets in MMEB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTC-GMM: CTC guided modality matching for fast and accurate streaming
  speech translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhao, Jinyu Li, Ruchao Fan, Matt Post
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models for streaming speech translation (ST) can achieve high accuracy and
low latency if they're developed with vast amounts of paired audio in the
source language and written text in the target language. Yet, these text labels
for the target language are often pseudo labels due to the prohibitive cost of
manual ST data labeling. In this paper, we introduce a methodology named
Connectionist Temporal Classification guided modality matching (CTC-GMM) that
enhances the streaming ST model by leveraging extensive machine translation
(MT) text data. This technique employs CTC to compress the speech sequence into
a compact embedding sequence that matches the corresponding text sequence,
allowing us to utilize matched {source-target} language text pairs from the MT
corpora to refine the streaming ST model further. Our evaluations with FLEURS
and CoVoST2 show that the CTC-GMM approach can increase translation accuracy
relatively by 13.9% and 6.4% respectively, while also boosting decoding speed
by 59.7% on GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Spoken Language Technology Workshop (SLT 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparsePO: Controlling Preference Alignment of <span class="highlight-title">LLM</span>s via Sparse Token
  Masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) has proven an effective step for aligning
language models to human-desired behaviors. Current variants, following the
offline Direct Preference Optimization objective, have focused on a strict
setting where all tokens are contributing signals of KL divergence and rewards
to the loss function. However, human preference is not affected by each word in
a sequence equally but is often dependent on specific words or phrases, e.g.
existence of toxic terms leads to non-preferred responses. Based on this
observation, we argue that not all tokens should be weighted equally during PO
and propose a flexible objective termed SparsePO, that aims to automatically
learn to weight the KL divergence and reward corresponding to each token during
PO training. We propose two different variants of weight-masks that can either
be derived from the reference model itself or learned on the fly. Notably, our
method induces sparsity in the learned masks, allowing the model to learn how
to best weight reward and KL divergence contributions at the token level,
learning an optimal level of mask sparsity. Extensive experiments on multiple
domains, including sentiment control, dialogue, text summarization and
text-to-code generation, illustrate that our approach assigns meaningful
weights to tokens according to the target task, generates more responses with
the desired preference and improves reasoning tasks by up to 2 percentage
points compared to other token- and response-level PO methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 papges, 9 figures, 5 tables. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating <span class="highlight-title">large language model</span>s for their competence in extracting
  grammatically sound sentences from transcribed noisy utterances <span class="chip">CoNLL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Wróblewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selectively processing noisy utterances while effectively disregarding
speech-specific elements poses no considerable challenge for humans, as they
exhibit remarkable cognitive abilities to separate semantically significant
content from speech-specific noise (i.e. filled pauses, disfluencies, and
restarts). These abilities may be driven by mechanisms based on acquired
grammatical rules that compose abstract syntactic-semantic structures within
utterances. Segments without syntactic and semantic significance are
consistently disregarded in these structures. The structures, in tandem with
lexis, likely underpin language comprehension and thus facilitate effective
communication. In our study, grounded in linguistically motivated experiments,
we investigate whether large language models (LLMs) can effectively perform
analogical speech comprehension tasks. In particular, we examine the ability of
LLMs to extract well-structured utterances from transcriptions of noisy
dialogues. We conduct two evaluation experiments in the Polish language
scenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of
data contamination. Our results show that not all extracted utterances are
correctly structured, indicating that either LLMs do not fully acquire
syntactic-semantic rules or they acquire them but cannot apply them
effectively. We conclude that the ability of LLMs to comprehend noisy
utterances is still relatively superficial compared to human proficiency in
processing them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoNLL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explanation sensitivity to the randomness of <span class="highlight-title">large language model</span>s: the
  case of journalistic text classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremie Bogaert, Marie-Catherine de Marneffe, Antonin Descampe, Louis Escouflaire, Cedrick Fairon, Francois-Xavier Standaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) perform very well in several natural language
processing tasks but raise explainability challenges. In this paper, we examine
the effect of random elements in the training of LLMs on the explainability of
their predictions. We do so on a task of opinionated journalistic text
classification in French. Using a fine-tuned CamemBERT model and an explanation
method based on relevance propagation, we find that training with different
random seeds produces models with similar accuracy but variable explanations.
We therefore claim that characterizing the explanations' statistical
distribution is needed for the explainability of LLMs. We then explore a
simpler model based on textual features which offers stable explanations but is
less accurate. Hence, this simpler model corresponds to a different tradeoff
between accuracy and explainability. We show that it can be improved by
inserting features derived from CamemBERT's explanations. We finally discuss
new research directions suggested by our results, in particular regarding the
origin of the sensitivity observed in the training randomness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a faithful translation of a paper which was
  peer-reviewed and published in the French journal Traitement Automatique des
  Langues, n. 64</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Science<span class="highlight-title">Agent</span>Bench: Toward Rigorous Assessment of Language <span class="highlight-title">Agent</span>s for
  Data-Driven Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements of language language models (LLMs) have piqued growing
interest in developing LLM-based language agents to automate scientific
discovery end-to-end, which has sparked both excitement and skepticism about
the true capabilities of such agents. In this work, we argue that for an agent
to fully automate scientific discovery, it must be able to complete all
essential tasks in the workflow. Thus, we call for rigorous assessment of
agents on individual tasks in a scientific workflow before making bold claims
on end-to-end automation. To this end, we present ScienceAgentBench, a new
benchmark for evaluating language agents for data-driven scientific discovery.
To ensure the scientific authenticity and real-world relevance of our
benchmark, we extract 102 tasks from 44 peer-reviewed publications in four
disciplines and engage nine subject matter experts to validate them. We unify
the target output for every task to a self-contained Python program file and
employ an array of evaluation metrics to examine the generated programs,
execution results, and costs. Each task goes through multiple rounds of manual
validation by annotators and subject matter experts to ensure its annotation
quality and scientific plausibility. We also propose two effective strategies
to mitigate data contamination concerns. Using our benchmark, we evaluate five
open-weight and proprietary LLMs, each with three frameworks: direct prompting,
OpenHands, and self-debug. Given three attempts for each task, the
best-performing agent can only solve 32.4% of the tasks independently and 34.3%
with expert-provided knowledge. These results underscore the limited capacities
of current language agents in generating code for data-driven discovery, let
alone end-to-end automation for scientific research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense
  Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Maria Molfese, Simone Conia, Riccardo Orlando, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Models (LLMs) have shown strong reasoning capabilities
in commonsense question answering benchmarks, but the process underlying their
success remains largely opaque. As a consequence, recent approaches have
equipped LLMs with mechanisms for knowledge retrieval, reasoning and
introspection, not only to improve their capabilities but also to enhance the
interpretability of their outputs. However, these methods require additional
training, hand-crafted templates or human-written explanations. To address
these issues, we introduce ZEBRA, a zero-shot question answering framework that
combines retrieval, case-based reasoning and introspection and dispenses with
the need for additional training of the LLM. Given an input question, ZEBRA
retrieves relevant question-knowledge pairs from a knowledge base and generates
new knowledge by reasoning over the relationships in these pairs. This
generated knowledge is then used to answer the input question, improving the
model's performance and interpretability. We evaluate our approach across 8
well-established commonsense reasoning benchmarks, demonstrating that ZEBRA
consistently outperforms strong LLMs and previous knowledge integration
approaches, achieving an average accuracy improvement of up to 4.5 points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TidalDecode: Fast and Accurate <span class="highlight-title">LLM</span> Decoding with Position Persistent
  Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have driven significant advancements across
diverse NLP tasks, with long-context models gaining prominence for handling
extended inputs. However, the expanding key-value (KV) cache size required by
Transformer architectures intensifies the memory constraints, particularly
during the decoding phase, creating a significant bottleneck. Existing sparse
attention mechanisms designed to address this bottleneck have two limitations:
(1) they often fail to reliably identify the most relevant tokens for
attention, and (2) they overlook the spatial coherence of token selection
across consecutive Transformer layers, which can lead to performance
degradation and substantial overhead in token selection. This paper introduces
TidalDecode, a simple yet effective algorithm and system for fast and accurate
LLM decoding through position persistent sparse attention. TidalDecode
leverages the spatial coherence of tokens selected by existing sparse attention
methods and introduces a few token selection layers that perform full attention
to identify the tokens with the highest attention scores, while all other
layers perform sparse attention with the pre-selected tokens. This design
enables TidalDecode to substantially reduce the overhead of token selection for
sparse attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely
matches the generative performance of full attention methods while reducing the
LLM decoding latency by up to 2.1x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initialization of <span class="highlight-title">Large Language Model</span>s via Reparameterization to
  Mitigate Loss Spikes <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Nishida, Kyosuke Nishida, Kuniko Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loss spikes, a phenomenon in which the loss value diverges suddenly, is a
fundamental issue in the pre-training of large language models. This paper
supposes that the non-uniformity of the norm of the parameters is one of the
causes of loss spikes. Here, in training of neural networks, the scale of the
gradients is required to be kept constant throughout the layers to avoid the
vanishing and exploding gradients problem. However, to meet these requirements
in the Transformer model, the norm of the model parameters must be non-uniform,
and thus, parameters whose norm is smaller are more sensitive to the parameter
update. To address this issue, we propose a novel technique, weight scaling as
reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter
matrix and adjusts it to the value satisfying the requirements. Because of the
gate parameter, WeSaR sets the norm of the original parameters uniformly, which
results in stable training. Experimental results with the Transformer decoders
consisting of 130 million, 1.3 billion, and 13 billion parameters showed that
WeSaR stabilizes and accelerates training and that it outperformed compared
methods including popular initialization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A test suite of prompt injection attacks for <span class="highlight-title">LLM</span>-based machine
  translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Valerio Miceli-Barone, Zhifan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based NLP systems typically work by embedding their input data into
prompt templates which contain instructions and/or in-context examples,
creating queries which are submitted to a LLM, and then parsing the LLM
response in order to generate the system outputs. Prompt Injection Attacks
(PIAs) are a type of subversion of these systems where a malicious user crafts
special inputs which interfere with the prompt templates, causing the LLM to
respond in ways unintended by the system designer.
  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based
machine translation. Specifically, the task is to translate questions from the
TruthfulQA test suite, where an adversarial prompt is prepended to the
questions, instructing the system to ignore the translation instruction and
answer the questions instead.
  In this test suite, we extend this approach to all the language pairs of the
WMT 2024 General Machine Translation task. Moreover, we include additional
attack formats in addition to the one originally studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Named Clinical Entity Recognition <span class="highlight-title">Benchmark</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wadood M Abdul, Marco AF Pimentel, Muhammad Umar Salman, Tathagata Raha, Clément Christophe, Praveen K Kanithi, Nasir Hayat, Ronnie Rajan, Shadab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report introduces a Named Clinical Entity Recognition
Benchmark for evaluating language models in healthcare, addressing the crucial
natural language processing (NLP) task of extracting structured information
from clinical narratives to support applications like automated coding,
clinical trial cohort identification, and clinical decision support.
  The leaderboard provides a standardized platform for assessing diverse
language models, including encoder and decoder architectures, on their ability
to identify and classify clinical entities across multiple medical domains. A
curated collection of openly available clinical datasets is utilized,
encompassing entities such as diseases, symptoms, medications, procedures, and
laboratory measurements. Importantly, these entities are standardized according
to the Observational Medical Outcomes Partnership (OMOP) Common Data Model,
ensuring consistency and interoperability across different healthcare systems
and datasets, and a comprehensive evaluation of model performance. Performance
of models is primarily assessed using the F1-score, and it is complemented by
various assessment modes to provide comprehensive insights into model
performance. The report also includes a brief analysis of models evaluated to
date, highlighting observed trends and limitations.
  By establishing this benchmarking framework, the leaderboard aims to promote
transparency, facilitate comparative analyses, and drive innovation in clinical
entity recognition tasks, addressing the need for robust evaluation methods in
healthcare NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">LLM</span>s plan paths with extra hints from solvers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wu, Sayan Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in natural
language processing, mathematical problem solving, and tasks related to program
synthesis. However, their effectiveness in long-term planning and higher-order
reasoning has been noted to be limited and fragile. This paper explores an
approach for enhancing LLM performance in solving a classical robotic planning
task by integrating solver-generated feedback. We explore four different
strategies for providing feedback, including visual feedback, we utilize
fine-tuning, and we evaluate the performance of three different LLMs across a
10 standard and 100 more randomly generated planning problems. Our results
suggest that the solver-generated feedback improves the LLM's ability to solve
the moderately difficult problems, but the harder problems still remain out of
reach. The study provides detailed analysis of the effects of the different
hinting strategies and the different planning tendencies of the evaluated LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEPT: Decoupled Embeddings for Pre-training Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Model pre-training benefits from a broader data mixture to enhance
performance across domains and languages. However, training on such
heterogeneous text corpora is complex, requiring extensive and cost-intensive
efforts. Since these data sources vary in lexical, syntactic, and semantic
aspects, they cause negative interference or the "curse of multilinguality". We
propose a novel pre-training framework to alleviate this curse. Our method,
DEPT, decouples the embedding layers from the transformer body while
simultaneously training the latter in multiple contexts. DEPT enables the model
to train without being bound to a shared global vocabulary. DEPT: (1) can train
robustly and effectively under significant data heterogeneity, (2) reduces the
parameter count of the token embeddings by up to 80% and the communication
costs by 675x for billion-scale models (3) enhances model generalization and
plasticity in adapting to new languages and domains, and (4) allows training
with custom optimized vocabulary per data source. We prove DEPT's potential by
performing the first vocabulary-agnostic federated multilingual pre-training of
a 1.3 billion-parameter model across high and low-resource languages, reducing
its parameter count by 409 million.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Biased Assessment of Expert Finding Systems <span class="chip">RecSys</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large organisations, identifying experts on a given topic is crucial in
leveraging the internal knowledge spread across teams and departments.
So-called enterprise expert retrieval systems automatically discover and
structure employees' expertise based on the vast amount of heterogeneous data
available about them and the work they perform. Evaluating these systems
requires comprehensive ground truth expert annotations, which are hard to
obtain. Therefore, the annotation process typically relies on automated
recommendations of knowledge areas to validate. This case study provides an
analysis of how these recommendations can impact the evaluation of expert
finding systems. We demonstrate on a popular benchmark that system-validated
annotations lead to overestimated performance of traditional term-based
retrieval models and even invalidate comparisons with more recent neural
methods. We also augment knowledge areas with synonyms to uncover a strong bias
towards literal mentions of their constituent words. Finally, we propose
constraints to the annotation process to prevent these biased evaluations, and
show that this still allows annotation suggestions of high utility. These
findings should inform benchmark creation or selection for expert finding, to
guarantee meaningful comparison of methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 4th Workshop on Recommender Systems for Human
  Resources (RecSys in HR 2024) as part of RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ski<span class="highlight-title">llM</span>atch: Evaluating Self-supervised Learning of Skill Relatedness <span class="chip">ECML-PKDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens-Joris Decorte, Jeroen Van Hautte, Thomas Demeester, Chris Develder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling the relationships between skills is a crucial part of
human resources processes such as recruitment and employee development. Yet, no
benchmarks exist to evaluate such methods directly. We construct and release
SkillMatch, a benchmark for the task of skill relatedness, based on expert
knowledge mining from millions of job ads. Additionally, we propose a scalable
self-supervised learning technique to adapt a Sentence-BERT model based on
skill co-occurrence in job ads. This new method greatly surpasses traditional
models for skill relatedness as measured on SkillMatch. By releasing SkillMatch
publicly, we aim to contribute a foundation for research towards increased
accuracy and transparency of skill-based recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International workshop on AI for Human Resources and
  Public Employment Services (AI4HR&PES) as part of ECML-PKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Rigour of Scientific Writing: Criteria, Analysis, and Insights <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rigour is crucial for scientific research as it ensures the reproducibility
and validity of results and findings. Despite its importance, little work
exists on modelling rigour computationally, and there is a lack of analysis on
whether these criteria can effectively signal or measure the rigour of
scientific papers in practice. In this paper, we introduce a bottom-up,
data-driven framework to automatically identify and define rigour criteria and
assess their relevance in scientific writing. Our framework includes rigour
keyword extraction, detailed rigour definition generation, and salient criteria
identification. Furthermore, our framework is domain-agnostic and can be
tailored to the evaluation of scientific rigour for different areas,
accommodating the distinct salient criteria across fields. We conducted
comprehensive experiments based on datasets collected from two high impact
venues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the
effectiveness of our framework in modelling rigour. In addition, we analyse
linguistic patterns of rigour, revealing that framing certainty is crucial for
enhancing the perception of scientific rigour, while suggestion certainty and
probability uncertainty diminish it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Findings at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation Scaling for Steering and Interpreting Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Stoehr, Kevin Du, Vésteinn Snæbjarnarson, Robert West, Ryan Cotterell, Aaron Schein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the prompt "Rome is in", can we steer a language model to flip its
prediction of an incorrect token "France" to a correct token "Italy" by only
multiplying a few relevant activation vectors with scalars? We argue that
successfully intervening on a model is a prerequisite for interpreting its
internal workings. Concretely, we establish a three-term objective: a
successful intervention should flip the correct with the wrong token and vice
versa (effectiveness), and leave other tokens unaffected (faithfulness), all
while being sparse (minimality). Using gradient-based optimization, this
objective lets us learn (and later evaluate) a specific kind of efficient and
interpretable intervention: activation scaling only modifies the signed
magnitude of activation vectors to strengthen, weaken, or reverse the steering
directions already encoded in the model. On synthetic tasks, this intervention
performs comparably with steering vectors in terms of effectiveness and
faithfulness, but is much more minimal allowing us to pinpoint interpretable
model components. We evaluate activation scaling from different angles, compare
performance on different datasets, and make activation scalars a learnable
function of the activation vectors themselves to generalize to varying-length
prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of the Association for Computational Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent Classification for Bank Chatbots through <span class="highlight-title">LLM</span> Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bibiána Lajčinová, Patrik Valábek, Michal Spišiak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the application of large language models (LLMs) for
intent classification within a chatbot with predetermined responses designed
for banking industry websites. Specifically, the research examines the
effectiveness of fine-tuning SlovakBERT compared to employing multilingual
generative models, such as Llama 8b instruct and Gemma 7b instruct, in both
their pre-trained and fine-tuned versions. The findings indicate that
SlovakBERT outperforms the other models in terms of in-scope accuracy and
out-of-scope false positive rate, establishing it as the benchmark for this
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, no figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Grammar Induction for Language Understanding and <span class="highlight-title">Generation</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushi Kai, Shengyuan Hou, Yusheng Huang, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grammar induction has made significant progress in recent years. However, it
is not clear how the application of induced grammar could enhance practical
performance in downstream tasks. In this work, we introduce an unsupervised
grammar induction method for language understanding and generation. We
construct a grammar parser to induce constituency structures and dependency
relations, which is simultaneously trained on downstream tasks without
additional syntax annotations. The induced grammar features are subsequently
incorporated into Transformer as a syntactic mask to guide self-attention. We
evaluate and apply our method to multiple machine translation tasks and natural
language understanding tasks. Our method demonstrates superior performance
compared to the original Transformer and other models enhanced with external
parsers. Experimental results indicate that our method is effective in both
from-scratch and pre-trained scenarios. Additionally, our research highlights
the contribution of explicitly modeling the grammatical structure of texts to
neural network models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationale-Aware Answer Verification by Pairwise Self-Evaluation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akira Kawabata, Saku Sugawara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answer verification identifies correct solutions among candidates generated
by large language models (LLMs). Current approaches typically train verifier
models by labeling solutions as correct or incorrect based solely on whether
the final answer matches the gold answer. However, this approach neglects any
flawed rationale in the solution yielding the correct answer, undermining the
verifier's ability to distinguish between sound and flawed rationales. We
empirically show that in StrategyQA, only 19% of LLM-generated solutions with
correct answers have valid rationales, thus leading to an unreliable verifier.
Furthermore, we demonstrate that training a verifier on valid rationales
significantly improves its ability to distinguish valid and flawed rationale.
To make a better verifier without extra human supervision, we introduce REPS
(Rationale Enhancement through Pairwise Selection), a method for selecting
valid rationales from candidates by iteratively applying pairwise
self-evaluation using the same LLM that generates the solutions. Verifiers
trained on solutions selected by REPS outperform those trained using
conventional training methods on three reasoning benchmarks (ARC-Challenge,
DROP, and StrategyQA). Our results suggest that training reliable verifiers
requires ensuring the validity of rationales in addition to the correctness of
the final answers, which would be critical for models assisting humans in
solving complex reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ As Simple as Fine-tuning: <span class="highlight-title">LLM</span> Alignment via Bidirectional Negative
  Feedback Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Mao, Feng-Lin Li, Huimin Xu, Wei Zhang, Wang Chen, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has emerged as a more computationally
efficient alternative to Reinforcement Learning from Human Feedback (RLHF) with
Proximal Policy Optimization (PPO), eliminating the need for reward models and
online sampling. Despite these benefits, DPO and its variants remain sensitive
to hyper-parameters and prone to instability, particularly on mathematical
datasets. We argue that these issues arise from the unidirectional
likelihood-derivative negative feedback inherent in the log-likelihood loss
function. To address this, we propose a novel LLM alignment loss that
establishes a stable Bidirectional Negative Feedback (BNF) during optimization.
Our proposed BNF loss eliminates the need for pairwise contrastive losses and
does not require any extra tunable hyper-parameters or pairwise preference
data, streamlining the alignment pipeline to be as simple as supervised
fine-tuning. We conduct extensive experiments across two challenging QA
benchmarks and four reasoning benchmarks. The experimental results show that
BNF achieves comparable performance to the best methods on QA benchmarks, while
its performance decrease on the four reasoning benchmarks is significantly
lower compared to the best methods, thus striking a better balance between
value alignment and reasoning ability. In addition, we further validate the
performance of BNF on non-pairwise datasets, and conduct in-depth analysis of
log-likelihood and logit shifts across different preference optimization
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MINER: Mining the Underlying Pattern of Modality-Specific Neurons in
  <span class="highlight-title">Multimodal</span> <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaichen Huang, Jiahao Huo, Yibo Yan, Kun Wang, Yutao Yue, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal large language models (MLLMs) have significantly
advanced, integrating more modalities into diverse applications. However, the
lack of explainability remains a major barrier to their use in scenarios
requiring decision transparency. Current neuron-level explanation paradigms
mainly focus on knowledge localization or language- and domain-specific
analyses, leaving the exploration of multimodality largely unaddressed. To
tackle these challenges, we propose MINER, a transferable framework for mining
modality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1)
modality separation, (2) importance score calculation, (3) importance score
aggregation, (4) modality-specific neuron selection. Extensive experiments
across six benchmarks and two representative MLLMs show that (I) deactivating
ONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for
Qwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly
converge in the lower layers, (III) MSNs influence how key information from
various modalities converges to the last token, (IV) two intriguing phenomena
worth further investigation, i.e., semantic probing and semantic telomeres. The
source code is available at this URL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LPZero: Language Model Zero-cost Proxy Search from Zero 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijie Dong, Lujun Li, Xiang Liu, Zhenheng Tang, Xuebo Liu, Qiang Wang, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of the outstanding performance, Neural Architecture Search (NAS) is
criticized for massive computation. Recently, Zero-shot NAS has emerged as a
promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce
computational demands. Despite this, existing ZC proxies heavily rely on expert
knowledge and incur significant trial-and-error costs. Particularly in NLP
tasks, most existing ZC proxies fail to surpass the performance of the naive
baseline. To address these challenges, we introduce a novel framework,
\textbf{LPZero}, which is the first to automatically design ZC proxies for
various tasks, achieving higher ranking consistency than human-designed
proxies. Specifically, we model the ZC proxy as a symbolic equation and
incorporate a unified proxy search space that encompasses existing ZC proxies,
which are composed of a predefined set of mathematical symbols. To
heuristically search for the best ZC proxy, LPZero incorporates genetic
programming to find the optimal symbolic composition. We propose a
\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates
unpromising proxies, thereby mitigating the risk of proxy degradation.
Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's
superior ranking ability and performance on downstream tasks compared to
current approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 10 appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAPE V2: Process Attention Score as Feature Map for Length Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism is a fundamental component of the Transformer model,
contributing to interactions among distinct tokens, in contrast to earlier
feed-forward neural networks. In general, the attention scores are determined
simply by the key-query products. However, this work's occasional trial
(combining DAPE and NoPE) of including additional MLPs on attention scores
without position encoding indicates that the classical key-query multiplication
may limit the performance of Transformers. In this work, we conceptualize
attention as a feature map and apply the convolution operator (for neighboring
attention scores across different heads) to mimic the processing methods in
computer vision. Specifically, the main contribution of this paper is
identifying and interpreting the Transformer length extrapolation problem as a
result of the limited expressiveness of the naive query and key dot product,
and we successfully translate the length extrapolation issue into a
well-understood feature map processing problem. The novel insight, which can be
adapted to various attention-related models, reveals that the current
Transformer architecture has the potential for further evolution. Extensive
experiments demonstrate that treating attention as a feature map and applying
convolution as a processing method significantly enhances Transformer
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report. arXiv admin note: text overlap with arXiv:2405.14722</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing the Under-Represented: Cultural and Core Capability
  <span class="highlight-title">Benchmark</span>s for Developing Thai <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Kim, Sukyung Lee, Yungi Kim, Attapol Rutherford, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has highlighted the
need for robust evaluation frameworks that assess their core capabilities, such
as reasoning, knowledge, and commonsense, leading to the inception of certain
widely-used benchmark suites such as the H6 benchmark. However, these benchmark
suites are primarily built for the English language, and there exists a lack
thereof for under-represented languages, in terms of LLM development, such as
Thai. On the other hand, developing LLMs for Thai should also include enhancing
the cultural understanding as well as core capabilities. To address these dual
challenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai
Cultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough
evaluation of various LLMs with multi-lingual capabilities, we provide a
comprehensive analysis of the proposed benchmarks and how they contribute to
Thai LLM development. Furthermore, we will make both the datasets and
evaluation code publicly available to encourage further research and
development for Thai LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GARLIC: <span class="highlight-title">LLM</span>-Guided Dynamic Progress Control with Hierarchical Weighted
  Graph for Long Document QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Wang, Yanzheng Xiang, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mDPO: Conditional Preference Optimization for <span class="highlight-title">Multimodal</span> Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/mDPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Membership Inference Attacks on <span class="highlight-title">LLM</span>s are Rushing Nowhere (and How
  to Fix It) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whether LLMs memorize their training data and what this means, from privacy
leakage to detecting copyright violations -- has become a rapidly growing area
of research over the last two years. In recent months, more than 10 new methods
have been proposed to perform Membership Inference Attacks (MIAs) against LLMs.
Contrary to traditional MIAs which rely on fixed -- but randomized -- records
or models, these methods are mostly evaluated on datasets collected post-hoc.
Sets of members and non-members, used to evaluate the MIA, are constructed
using informed guesses after the release of a model. This lack of randomization
raises concerns of a distribution shift between members and non-members. In the
first part, we review the literature on MIAs against LLMs. While most work
focuses on sequence-level MIAs evaluated in post-hoc setups, we show that a
range of target models, motivations and units of interest have been considered
in the literature. We then quantify distribution shifts present in the 6
datasets used in the literature, ranging from books to papers, using a bag of
word classifier. Our analysis reveals that all of them suffer from severe
distribution shifts. This challenges the validity of using such setups to
measure LLM memorization and may undermine the benchmarking of recently
proposed methods. Yet, all hope might not be lost. In the second part, we
introduce important considerations to properly evaluate MIAs against LLMs and
discuss potential ways forward: randomized test splits, injections of
randomized (unique) sequences, randomized finetuning, and post-hoc control
methods. While each option comes with its advantages and limitations, we
believe they collectively provide solid grounds to guide the development of MIA
methods and study LLM memorization. We conclude by proposing comprehensive,
easy-to-use benchmarks for sequence- and document-level MIAs against LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMLU-Pro: A More Robust and Challenging Multi-Task Language
  Understanding <span class="highlight-title">Benchmark</span> (Published at NeurIPS 2024 Track <span class="highlight-title">Dataset</span>s and
  <span class="highlight-title">Benchmark</span>s) <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01574v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01574v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of large-scale language models, benchmarks like the Massive
Multitask Language Understanding (MMLU) have been pivotal in pushing the
boundaries of what AI can achieve in language comprehension and reasoning
across diverse domains. However, as models continue to improve, their
performance on these benchmarks has begun to plateau, making it increasingly
difficult to discern differences in model capabilities. This paper introduces
MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven
MMLU benchmark by integrating more challenging, reasoning-focused questions and
expanding the choice set from four to ten options. Additionally, MMLU-Pro
eliminates the trivial and noisy questions in MMLU. Our experimental results
show that MMLU-Pro not only raises the challenge, causing a significant drop in
accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability
under varying prompts. With 24 different prompt styles tested, the sensitivity
of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in
MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)
reasoning achieved better performance on MMLU-Pro compared to direct answering,
which is in stark contrast to the findings on the original MMLU, indicating
that MMLU-Pro includes more complex reasoning questions. Our assessments
confirm that MMLU-Pro is a more discriminative benchmark to better track
progress in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at NeurIPS 2024 Track
  Datasets and Benchmarks (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigCodeBench: <span class="highlight-title">Benchmark</span>ing Code <span class="highlight-title">Generation</span> with Diverse Function Calls
  and Complex Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15877v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15877v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task automation has been greatly empowered by the recent advances in Large
Language Models (LLMs) via Python code, where the tasks ranging from software
engineering development to general-purpose reasoning. While current benchmarks
have shown that LLMs can solve tasks using programs like human developers, the
majority of their evaluations are limited to short and self-contained
algorithmic tasks or standalone function calls. Solving challenging and
practical requires the capability of utilizing diverse function calls as tools
to efficiently implement functionalities like data analysis and web
development. In addition, using multiple tools to solve a task needs
compositional reasoning by accurately understanding complex instructions.
Fulfilling both of these characteristics can pose a great challenge for LLMs.To
assess how well LLMs can solve challenging and practical tasks via programs, we
introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple
function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained
tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with
an average branch coverage of 99%. In addition, we propose a
natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that
automatically transforms the original docstrings into short instructions only
with essential information. Our extensive evaluation of 60 LLMs shows that LLMs
are not yet capable of following complex instructions to use function calls
precisely, with scores up to 60%, significantly lower than the human
performance of 97%. The results underscore the need for further advancements in
this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 14 figures, 7 tables, built with love by the BigCode
  community :)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Black-box <span class="highlight-title">LLM</span>s with Medical Textbooks for Biomedical Question
  Answering (Published in Findings of EMNLP 2024) <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale language models (LLMs) like ChatGPT have demonstrated impressive
abilities in generating responses based on human instructions. However, their
use in the medical field can be challenging due to their lack of specific,
in-depth knowledge. In this study, we present a system called LLMs Augmented
with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in
specialized domains. LLM-AMT integrates authoritative medical textbooks into
the LLMs' framework using plug-and-play modules. These modules include a Query
Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,
they incorporate authoritative medical knowledge. Additionally, an LLM Reader
aids in contextual understanding. Our experimental results on three medical QA
tasks demonstrate that LLMAMT significantly improves response quality, with
accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the
base model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on
a massive amount of medical corpus by 2-3%. We found that despite being 100x
smaller in size, medical textbooks as a retrieval corpus is proven to be a more
effective knowledge database than Wikipedia in the medical domain, boosting
performance by 7.8%-13.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation alignment: Comparing <span class="highlight-title">LLM</span> and human annotations of
  conversational safety <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06369v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06369v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajiv Movva, Pang Wei Koh, Emma Pierson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do LLMs align with human perceptions of safety? We study this question via
annotation alignment, the extent to which LLMs and humans agree when annotating
the safety of user-chatbot conversations. We leverage the recent DICES dataset
(Aroyo et al., 2023), in which 350 conversations are each rated for safety by
112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson
correlation of $r = 0.59$ with the average annotator rating, \textit{higher}
than the median annotator's correlation with the average ($r=0.51$). We show
that larger datasets are needed to resolve whether LLMs exhibit disparities in
how well they correlate with different demographic groups. Also, there is
substantial idiosyncratic variation in correlation within groups, suggesting
that race & gender do not fully capture differences in alignment. Finally, we
find that GPT-4 cannot predict when one demographic group finds a conversation
more unsafe than another.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Main). Main text contains 6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creative Beam Search: <span class="highlight-title">LLM</span>-as-a-Judge For Improving Response <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are revolutionizing several areas, including artificial
creativity. However, the process of generation in machines profoundly diverges
from that observed in humans. In particular, machine generation is
characterized by a lack of intentionality and an underlying creative process.
We propose a method called Creative Beam Search that uses Diverse Beam Search
and LLM-as-a-Judge to perform response generation and response validation. The
results of a qualitative experiment show how our approach can provide better
output than standard sampling techniques. We also show that the response
validation step is a necessary complement to the response generation step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a short paper at the 15th International Conference on
  Computational Creativity (ICCC'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For <span class="highlight-title">Generation</span> Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Usage-centric Take on Intent Understanding in E-Commerce <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Zhou, Tianyi Li, Pavlos Vougiouklis, Mark Steedman, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying and understanding user intents is a pivotal task for E-Commerce.
Despite its essential role in product recommendation and business user
profiling analysis, intent understanding has not been consistently defined or
accurately benchmarked. In this paper, we focus on predicative user intents as
"how a customer uses a product", and pose intent understanding as a natural
language reasoning task, independent of product ontologies. We identify two
weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:
category-rigidity and property-ambiguity. They limit its ability to strongly
align user intents with products having the most desirable property, and to
recommend useful products across diverse categories. Following these
observations, we introduce a Product Recovery Benchmark featuring a novel
evaluation framework and an example dataset. We further validate the above
FolkScope weaknesses on this benchmark. Our code and dataset are available at
https://github.com/stayones/Usgae-Centric-Intent-Understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acepted by EMNLP 2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Model-Agnostic Multi-Group Equivariant Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razan Baltaji, Sourya Basu, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing model-agnostic group equivariant networks, such as equitune
(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be
computationally expensive for large product groups. We address this problem by
providing efficient model-agnostic equivariant designs for two related
problems: one where the network has multiple inputs each with potentially
different groups acting on them, and another where there is a single input but
the group acting on it is a large product group. For the first design, we
initially consider a linear model and characterize the entire equivariant space
that satisfies this constraint. This characterization gives rise to a novel
fusion layer between different channels that satisfies an invariance-symmetry
(IS) constraint, which we call an IS layer. We then extend this design beyond
linear models, similar to equitune, consisting of equivariant and IS layers. We
also show that the IS layer is a universal approximator of invariant-symmetric
functions. Inspired by the first design, we use the notion of the IS property
to design a second efficient model-agnostic equivariant design for large
product groups acting on a single input. For the first design, we provide
experiments on multi-image classification where each view is transformed
independently with transformations such as rotations. We find equivariant
models are robust to such transformations and perform competitively otherwise.
For the second design, we consider three applications: language
compositionality on the SCAN dataset to product groups; fairness in natural
language generation from GPT-2 to address intersectionality; and robust
zero-shot image classification with CLIP. Overall, our methods are simple and
general, competitive with equitune and its variants, while also being
computationally more efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  Prompts Do Not Improve Performances of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses "You are a helpful assistant" as part of its
default system prompt. Despite current practices of adding personas to system
prompts, it remains unclear how different personas affect a model's performance
on objective tasks. In this study, we present a systematic evaluation of
personas in system prompts. We curate a list of 162 roles covering 6 types of
interpersonal relationships and 8 domains of expertise. Through extensive
analysis of 4 popular families of LLMs and 2,410 factual questions, we
demonstrate that adding personas in system prompts does not improve model
performance across a range of questions compared to the control setting where
no persona is added. Nevertheless, further analysis suggests that the gender,
type, and domain of the persona can all influence the resulting prediction
accuracies. We further experimented with a list of persona search strategies
and found that, while aggregating results from the best persona for each
question significantly improves prediction accuracy, automatically identifying
the best persona is challenging, with predictions often performing no better
than random selection. Overall, our findings suggest that while adding a
persona may lead to performance gains in certain settings, the effect of each
persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Instruction-Following Through Minimum Bayes Risk 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Pakazad, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose LLM judges capable of human-level evaluation provide not only
a scalable and accurate way of evaluating instruction-following LLMs but also
new avenues for supervising and improving their performance. One promising way
of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)
decoding, which uses a reference-based evaluator to select a high-quality
output from amongst a set of candidate outputs. In the first part of this work,
we explore using MBR decoding as a method for improving the test-time
performance of instruction-following LLMs. We find that MBR decoding with
reference-based LLM judges substantially improves over greedy decoding,
best-of-N decoding with reference-free judges and MBR decoding with lexical and
embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent
across LLMs with up to 70B parameters, demonstrating that smaller LLM judges
can be used to supervise much larger LLMs. Then, seeking to retain the
improvements from MBR decoding while mitigating additional test-time costs, we
explore iterative self-training on MBR-decoded outputs. We find that
self-training using Direct Preference Optimisation leads to significant
performance gains, such that the self-trained models with greedy decoding
generally match and sometimes exceed the performance of their base models with
MBR decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation noising effectively prevents harmful fine-tuning on <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Releasing open-source large language models (LLMs) presents a dual-use risk
since bad actors can easily fine-tune these models for harmful purposes. Even
without the open release of weights, weight stealing and fine-tuning APIs make
closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety
measures like preventing jailbreaks and improving safety guardrails are
important, such measures can easily be reversed through fine-tuning. In this
work, we propose Representation Noising (RepNoise), a defence mechanism that is
effective even when attackers have access to the weights. RepNoise works by
removing information about harmful representations such that it is difficult to
recover them during fine-tuning. Importantly, our defence is also able to
generalize across different subsets of harm that have not been seen during the
defence process as long as they are drawn from the same distribution of the
attack set. Our method does not degrade the general capability of LLMs and
retains the ability to train the model on harmless tasks. We provide empirical
evidence that the effectiveness of our defence lies in its "depth": the degree
to which information about harmful representations is removed across all layers
of the LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPs 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Bias Probing: Fairness <span class="highlight-title">Benchmark</span>ing for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09090v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09090v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the impact of social biases in language models has been recognized,
prior methods for bias evaluation have been limited to binary association tests
on small datasets, limiting our understanding of bias complexities. This paper
proposes a novel framework for probing language models for social biases by
assessing disparate treatment, which involves treating individuals differently
according to their affiliation with a sensitive demographic group. We curate
SoFa, a large-scale benchmark designed to address the limitations of existing
fairness collections. SoFa expands the analysis beyond the binary comparison of
stereotypical versus anti-stereotypical identities to include a diverse range
of identities and stereotypes. Comparing our methodology with existing
benchmarks, we reveal that biases within language models are more nuanced than
acknowledged, indicating a broader scope of encoded biases than previously
recognized. Benchmarking LMs on SoFa, we expose how identities expressing
different religions lead to the most pronounced disparate treatments across all
models. Finally, our findings indicate that real-life adversities faced by
various groups such as women and people with disabilities are mirrored in the
behavior of these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better
  Together <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilara Soylu, Christopher Potts, Omar Khattab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAC$^2$E: Better Understanding <span class="highlight-title">Large Language Model</span> Capabilities by
  Dissociating Language and Cognition <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqiang Wang, Lingfei Wu, Tengfei Ma, Bang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are primarily evaluated by overall performance
on various text understanding and generation tasks. However, such a paradigm
fails to comprehensively differentiate the fine-grained language and cognitive
skills, rendering the lack of sufficient interpretation to LLMs' capabilities.
In this paper, we present FAC$^2$E, a framework for Fine-grAined and
Cognition-grounded LLMs' Capability Evaluation. Specifically, we formulate
LLMs' evaluation in a multi-dimensional and explainable manner by dissociating
the language-related capabilities and the cognition-related ones. Besides,
through extracting the intermediate reasoning from LLMs, we further break down
the process of applying a specific capability into three sub-steps: recalling
relevant knowledge, utilizing knowledge, and solving problems. Finally,
FAC$^2$E evaluates each sub-step of each fine-grained capability, providing a
two-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common
shortfall in knowledge utilization among models and propose a straightforward,
knowledge-enhanced method to mitigate this issue. Our results not only showcase
promising performance enhancements but also highlight a direction for future
LLM advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Invasive Suicide Risk Prediction Through Speech Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Amiriparian, Maurice Gerczuk, Justina Lutz, Wolfgang Strube, Irina Papazova, Alkomiet Hasan, Alexander Kathan, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The delayed access to specialized psychiatric assessments and care for
patients at risk of suicidal tendencies in emergency departments creates a
notable gap in timely intervention, hindering the provision of adequate mental
health support during critical situations. To address this, we present a
non-invasive, speech-based approach for automatic suicide risk assessment. For
our study, we collected a novel speech recording dataset from $20$ patients. We
extract three sets of features, including wav2vec, interpretable speech and
acoustic features, and deep learning-based spectral representations. We proceed
by conducting a binary classification to assess suicide risk in a
leave-one-subject-out fashion. Our most effective speech model achieves a
balanced accuracy of $66.2\,\%$. Moreover, we show that integrating our speech
model with a series of patients' metadata, such as the history of suicide
attempts or access to firearms, improves the overall result. The metadata
integration yields a balanced accuracy of $94.4\,\%$, marking an absolute
improvement of $28.2\,\%$, demonstrating the efficacy of our proposed
approaches for automatic suicide risk assessment in emergency medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Native Design Bias: Studying the Impact of English Nativeness on
  Language Model Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manon Reusens, Philipp Borchert, Jochen De Weerdt, Bart Baesens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at providing information acquired during
pretraining on large-scale corpora and following instructions through user
prompts. This study investigates whether the quality of LLM responses varies
depending on the demographic profile of users. Considering English as the
global lingua franca, along with the diversity of its dialects among speakers
of different native languages, we explore whether non-native English speakers
receive lower-quality or even factually incorrect responses from LLMs more
frequently. Our results show that performance discrepancies occur when LLMs are
prompted by native versus non-native English speakers and persist when
comparing native speakers from Western countries with others. Additionally, we
find a strong anchoring effect when the model recognizes or is made aware of
the user's nativeness, which further degrades the response quality when
interacting with non-native speakers. Our analysis is based on a newly
collected dataset with over 12,000 unique annotations from 124 annotators,
including information on their native language and English proficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UPCS: Unbiased Persona Construction for Dialogue <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuiyun Chen, Yanbin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narrative systems, such as dialogue and storytelling systems, often utilize
persona profiles to enhance personalized interactions. Existing persona
profiles frequently exhibit biases, posing risks to system integrity and
fairness. To address this, we introduce the UPCS framework, which categorizes
character descriptions into eight dimensions, including bias mitigation
strategies. Experimental results demonstrate UPCS's superiority in accuracy,
diversity, bias elimination, and user satisfaction, marking a significant
advancement in persona construction for reliable narrative systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity Over Size: On the Effect of Sample and Topic Sizes for
  Topic-Dependent Argument Mining <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11472v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11472v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Schiller, Johannes Daxenberger, Andreas Waldis, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Argument Mining, that is extracting and classifying argument
components for a specific topic from large document sources, is an inherently
difficult task for machine learning models and humans alike, as large Argument
Mining datasets are rare and recognition of argument components requires expert
knowledge. The task becomes even more difficult if it also involves stance
detection of retrieved arguments. In this work, we investigate the effect of
Argument Mining dataset composition in few- and zero-shot settings. Our
findings show that, while fine-tuning is mandatory to achieve acceptable model
performance, using carefully composed training samples and reducing the
training sample size by up to almost 90% can still yield 95% of the maximum
performance. This gain is consistent across three Argument Mining tasks on
three different datasets. We also publish a new dataset for future
benchmarking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KV-Compress: Paged KV-Cache Compression with Variable Compression Rates
  per Attention Head 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Rehg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context lengths of Large Language Models (LLMs) have exploded in recent
years, with 128k-token context becoming a standard and million-token context
becoming a reality. Efficiently supporting long-context inference remains
challenging as the memory that must be allocated in key-value (KV) cache for a
generation scales with its context length, limiting the number of long-context
requests that can be served concurrently under a given memory budget. KV cache
compression can mitigate this issue by removing under-utilized KVs from each
attention head's cache and reducing its memory footprint. Higher theoretical
compression rates can be achieved when the number of removed KVs varies across
attention heads, but application of such a strategy within existing inference
frameworks adds fragmentation and cannot realize the theoretical compression
rates in physical memory. We introduce KV-Compress, a novel compression method
that evicts contiguous KV blocks within a PagedAttention framework, reducing
the memory footprint of the KV cache proportionally to this theoretical
compression rate. Our method achieves state-of-the-art performance on LongBench
for both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the
total number of compressed KVs by 4x compared with prior methods. Evaluations
on Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression
rates up to 8x with negligible impact on performance, and up to 64x while
retaining over 90% of full-cache performance for all but three of the suite's
subsets. We benchmark an integration of our method with vLLM that increases
total throughput by up to 5.18x by enabling larger decoding batches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Intelligence: A Framework for Certifying Knowledge
  Comprehension in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge comprehension capability is an important aspect of human
intelligence. As Large Language Models (LLMs) are being envisioned as
superhuman agents, it is crucial for them to be proficient at knowledge
comprehension. However, existing benchmarking studies do not provide
consistent, generalizable, and formal guarantees on the knowledge comprehension
capabilities of LLMs. In this work, we propose the first framework to certify
knowledge comprehension in LLMs with formal probabilistic guarantees. Our
certificates are quantitative -- they consist of high-confidence, tight bounds
on the probability that a target LLM gives the correct answer on any knowledge
comprehension prompt sampled from a distribution. We design and certify novel
specifications that precisely represent distributions of knowledge
comprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for
specifications over the Wikidata5m knowledge graph. We find that the knowledge
comprehension capability improves significantly with scaling the size of the
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First Heuristic Then Rational: Dynamic Use of Heuristics in Language
  Model <span class="highlight-title">Reasoning</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoichi Aoki, Keito Kudo, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step reasoning instruction, such as chain-of-thought prompting, is
widely adopted to explore better language models (LMs) performance. We report
on the systematic strategy that LMs employ in such a multi-step reasoning
process. Our controlled experiments reveal that LMs rely more heavily on
heuristics, such as lexical overlap, in the earlier stages of reasoning, where
more reasoning steps remain to reach a goal. Conversely, their reliance on
heuristics decreases as LMs progress closer to the final answer through
multiple reasoning steps. This suggests that LMs can backtrack only a limited
number of future steps and dynamically combine heuristic strategies with
rationale ones in tasks involving multi-step reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s Know More Than They Show: On the Intrinsic Representation of <span class="highlight-title">LLM</span>
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often produce errors, including factual
inaccuracies, biases, and reasoning failures, collectively referred to as
"hallucinations". Recent studies have demonstrated that LLMs' internal states
encode information regarding the truthfulness of their outputs, and that this
information can be utilized to detect errors. In this work, we show that the
internal representations of LLMs encode much more information about
truthfulness than previously recognized. We first discover that the
truthfulness information is concentrated in specific tokens, and leveraging
this property significantly enhances error detection performance. Yet, we show
that such error detectors fail to generalize across datasets, implying that --
contrary to prior claims -- truthfulness encoding is not universal but rather
multifaceted. Next, we show that internal representations can also be used for
predicting the types of errors the model is likely to make, facilitating the
development of tailored mitigation strategies. Lastly, we reveal a discrepancy
between LLMs' internal encoding and external behavior: they may encode the
correct answer, yet consistently generate an incorrect one. Taken together,
these insights deepen our understanding of LLM errors from the model's internal
perspective, which can guide future research on enhancing error analysis and
mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructLM: Towards Building Generalist Models for Structured Knowledge
  Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16671v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16671v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured data sources, such as tables, graphs, and databases, are
ubiquitous knowledge sources. Despite the demonstrated capabilities of large
language models (LLMs) on plain text, their proficiency in interpreting and
utilizing structured data remains limited. Our investigation reveals a notable
deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags
behind state-of-the-art (SoTA) model by an average of 35%. To augment the
Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a
comprehensive instruction tuning dataset comprising 1.1 million examples.
Utilizing this dataset, we train a series of models, referred to as StructLM,
based on the Mistral and the CodeLlama model family, ranging from 7B to 34B
parameters. Our StructLM series surpasses task-specific models on 16 out of 18
evaluated datasets and establishes new SoTA performance on 8 SKG tasks.
Furthermore, StructLM demonstrates strong generalization across 6 novel
held-out SKG tasks, outperforming TableLlama by an average of 35\% and Flan-UL2
20B by an average of 10\%. Contrary to expectations, we observe that scaling
model size offers marginal benefits, with StructLM-34B showing only slight
improvements over StructLM-7B. This suggests that structured knowledge
grounding is still a challenging task and requires more innovative design to
push to a new level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model <span class="highlight-title">Editing</span> of
  <span class="highlight-title">Large Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-MoE: Towards Compositional <span class="highlight-title">Large Language Model</span>s with
  Self-Specialized Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Self-MoE, an approach that transforms a monolithic LLM into a
compositional, modular system of self-specialized experts, named MiXSE (MiXture
of Self-specialized Experts). Our approach leverages self-specialization, which
constructs expert modules using self-generated synthetic data, each equipping a
shared base LLM with distinct domain-specific capabilities, activated via
self-optimized routing. This allows for dynamic and capability-specific
handling of various target tasks, enhancing overall capabilities, without
extensive human-labeled data and added parameters. Our empirical results reveal
that specializing LLMs may exhibit potential trade-offs in performances on
non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial
improvements (6.5%p on average) over the base LLM across diverse benchmarks
such as knowledge, reasoning, math, and coding. It also consistently
outperforms other methods, including instance merging and weight merging, while
offering better flexibility and interpretability by design with semantic
experts and routing. Our findings highlight the critical role of modularity,
the applicability of Self-MoE to multiple base LLMs, and the potential of
self-improvement in achieving efficient, scalable, and adaptable systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances
  Retrieval-Augmented <span class="highlight-title">Generation</span> with Zero Inference Overhead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tan, Yining Qian, Ang Lv, Hongzhan Lin, Songhao Wu, Yongbo Wang, Feng Wang, Jingtong Wu, Xin Lu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) enhanced with retrieval-augmented generation
(RAG) have introduced a new paradigm for web search. However, the limited
context awareness of LLMs degrades their performance on RAG tasks. Existing
methods to enhance context awareness are often inefficient, incurring time or
memory overhead during inference, and many are tailored to specific position
embeddings. In this paper, we propose Position-Embedding-Agnostic attention
Re-weighting (PEAR), which enhances the context awareness of LLMs with zero
inference overhead. Specifically, on a proxy task focused on context copying,
we first detect heads which suppress the models' context awareness thereby
diminishing RAG performance. To weaken the impact of these heads, we re-weight
their outputs with learnable coefficients. The LLM (with frozen parameters) is
optimized by adjusting these coefficients to minimize loss on the proxy task.
As a result, the coefficients are optimized to values less than one, thereby
reducing their tendency to suppress RAG performance. During inference, the
optimized coefficients are fixed to re-weight these heads, regardless of the
specific task at hand. Our proposed PEAR offers two major advantages over
previous approaches: (1) It introduces zero additional inference overhead in
terms of memory usage or inference time, while outperforming competitive
baselines in accuracy and efficiency across various RAG tasks. (2) It is
independent of position embedding algorithms, ensuring broader applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WellDunn: On the Robustness and Explainability of Language Models and
  <span class="highlight-title">Large Language Model</span>s in Identifying Wellness Dimensions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12058v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12058v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, Manas Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are being proposed for mental health applications where
the heightened risk of adverse outcomes means predictive performance may not be
a sufficient litmus test of a model's utility in clinical practice. A model
that can be trusted for practice should have a correspondence between
explanation and clinical determination, yet no prior research has examined the
attention fidelity of these models and their effect on ground truth
explanations. We introduce an evaluation design that focuses on the robustness
and explainability of LMs in identifying Wellness Dimensions (WDs). We focus on
two existing mental health and well-being datasets: (a) Multi-label
Classification-based MultiWD, and (b) WellXplain for evaluating attention
mechanism veracity against expert-labeled explanations. The labels are based on
Halbert Dunn's theory of wellness, which gives grounding to our evaluation. We
reveal four surprising results about LMs/LLMs: (1) Despite their human-like
capabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on
WellXplain fails to deliver any remarkable improvements in performance or
explanations. (2) Re-examining LMs' predictions based on a confidence-oriented
loss function reveals a significant performance drop. (3) Across all LMs/LLMs,
the alignment between attention and explanations remains low, with LLMs scoring
a dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific
knowledge and undervalue explanations, causing these discrepancies. This study
highlights the need for further research into their consistency and
explanations in mental health and well-being.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in BlackboxNLP @ EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Can Transformers Count to n? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilad Yehudai, Haim Kaplan, Asma Ghandeharioun, Mor Geva, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models based on the transformer architectures can solve highly
complex tasks. But are there simple tasks that such models cannot solve? Here
we focus on very simple counting tasks, that involve counting how many times a
token in the vocabulary have appeared in a string. We show that if the
dimension of the transformer state is linear in the context length, this task
can be solved. However, the solution we propose does not scale beyond this
limit, and we provide theoretical arguments for why it is likely impossible for
a size limited transformer to implement this task. Our empirical results
demonstrate the same phase-transition in performance, as anticipated by the
theoretical argument. Our results demonstrate the importance of understanding
how transformers can solve simple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Is More Than Compression <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is a foundational step in natural language processing (NLP)
tasks, bridging raw text and language models. Existing tokenization approaches
like Byte-Pair Encoding (BPE) originate from the field of data compression, and
it has been suggested that the effectiveness of BPE stems from its ability to
condense text into a relatively small number of tokens. We test the hypothesis
that fewer tokens lead to better downstream performance by introducing
PathPiece, a new tokenizer that segments a document's text into the minimum
number of tokens for a given vocabulary. Through extensive experimentation we
find this hypothesis not to be the case, casting doubt on the understanding of
the reasons for effective tokenization. To examine which other factors play a
role, we evaluate design decisions across all three phases of tokenization:
pre-tokenization, vocabulary construction, and segmentation, offering new
insights into the design of effective tokenizers. Specifically, we illustrate
the importance of pre-tokenization and the benefits of using BPE to initialize
vocabulary construction. We train 64 language models with varying tokenization,
ranging in size from 350M to 2.4B parameters, all of which are made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComplexTempQA: A Large-Scale <span class="highlight-title">Dataset</span> for Complex Temporal Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Gruber, Abdelrahman Abdallah, Michael Färber, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ComplexTempQA, a large-scale dataset consisting of over 100
million question-answer pairs designed to tackle the challenges in temporal
question answering. ComplexTempQA significantly surpasses existing benchmarks
like HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from
Wikipedia and Wikidata, the dataset covers questions spanning over two decades
and offers an unmatched breadth of topics. We introduce a unique taxonomy that
categorizes questions as attributes, comparisons, and counting questions, each
revolving around events, entities, and time periods. One standout feature of
ComplexTempQA is the high complexity of its questions, which demand effective
capabilities for answering such as across-time comparison, temporal
aggregation, and multi-hop reasoning involving temporal event ordering and
entity recognition. Additionally, each question is accompanied by detailed
metadata, including specific time scopes, allowing for comprehensive evaluation
and enhancement of the temporal reasoning abilities of large language models.
ComplexTempQA serves both as a testing ground for developing sophisticated AI
models and as a foundation for advancing research in question answering,
information retrieval, and language understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconstruct Your Previous Conversations! Comprehensively Investigating
  Privacy Leakage Risks in Conversations with <span class="highlight-title">GPT</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advancements have recently been made in large language models
represented by GPT models. Users frequently have multi-round private
conversations with cloud-hosted GPT models for task optimization. Yet, this
operational paradigm introduces additional attack surfaces, particularly in
custom GPTs and hijacked chat sessions. In this paper, we introduce a
straightforward yet potent Conversation Reconstruction Attack. This attack
targets the contents of previous conversations between GPT models and benign
users, i.e., the benign users' input contents during their interaction with GPT
models. The adversary could induce GPT models to leak such contents by querying
them with designed malicious prompts. Our comprehensive examination of privacy
risks during the interactions with GPT models under this attack reveals GPT-4's
considerable resilience. We present two advanced attacks targeting improved
reconstruction of past conversations, demonstrating significant privacy leakage
across all models under these advanced techniques. Evaluating various defense
mechanisms, we find them ineffective against these attacks. Our findings
highlight the ease with which privacy can be compromised in interactions with
GPT models, urging the community to safeguard against potential abuses of these
models' capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024. 14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Question Decomposition on <span class="highlight-title">Multimodal</span> <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Vera Marjanović, Haeun Yu, Pepa Atanasova, Maria Maistro, Christina Lioma, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-intensive language understanding tasks require Language Models
(LMs) to integrate relevant context, mitigating their inherent weaknesses, such
as incomplete or outdated knowledge. However, conflicting knowledge can be
present in the LM's parameters, termed intra-memory conflict, which can affect
a model's propensity to accept contextual knowledge. To study the effect of
intra-memory conflict on an LM's ability to accept relevant context, we utilize
two knowledge conflict measures and a novel dataset containing inherently
conflicting data, DynamicQA. This dataset includes facts with a temporal
dynamic nature where facts can change over time and disputable dynamic facts,
which can change depending on the viewpoint. DynamicQA is the first to include
real-world knowledge conflicts and provide context to study the link between
the different types of knowledge conflicts. We also evaluate several measures
on their ability to reflect the presence of intra-memory conflict: semantic
entropy and a novel coherent persuasion score. With our extensive experiments,
we verify that LMs exhibit a greater degree of intra-memory conflict with
dynamic facts compared to facts that have a single truth value. Furthermore, we
reveal that facts with intra-memory conflict are harder to update with context,
suggesting that retrieval-augmented generation will struggle with the most
commonly adapted facts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, Accepted to Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Stream Analysis with Multi-Layer SAEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are a promising approach to interpreting the
internal representations of transformer language models. However, SAEs are
usually trained separately on each transformer layer, making it difficult to
use them to study how information flows across layers. To solve this problem,
we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual
stream activation vectors from every transformer layer. Given that the residual
stream is understood to preserve information across layers, we expected MLSAE
latents to `switch on' at a token position and remain active at later layers.
Interestingly, we find that individual latents are often active at a single
layer for a given token or prompt, but this layer may differ for different
tokens or prompts. We quantify these phenomena by defining a distribution over
layers and considering its variance. We find that the variance of the
distributions of latent activations over layers is about two orders of
magnitude greater when aggregating over tokens compared with a single token.
For larger underlying models, the degree to which latents are active at
multiple layers increases, which is consistent with the fact that the residual
stream activation vectors at adjacent layers become more similar. Finally, we
relax the assumption that the residual stream basis is the same at every layer
by applying pre-trained tuned-lens transformations, but our findings remain
qualitatively similar. Our results represent a new approach to understanding
how representations change as they flow through transformers. We release our
code to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think-on-Graph 2.0: Deep and Faithful <span class="highlight-title">Large Language Model</span> <span class="highlight-title">Reasoning</span>
  with Knowledge-guided Retrieval Augmented <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10805v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10805v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has enhanced large language models
(LLMs) by using knowledge retrieval to address knowledge gaps. However,
existing RAG approaches often fail to ensure the depth and completeness of the
information retrieved, which is essential for complex reasoning tasks. In this
work, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that
iteratively retrieves information from both unstructured and structured
knowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages
knowledge graphs (KGs) to connect documents via entities, facilitating deep and
knowledge-guided context retrieval. Simultaneously, it uses documents as entity
contexts to enable precise and efficient graph retrieval.
  ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate accurate
answers. We conduct a series of experiments to demonstrate the following
advantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph
retrieval, enhancing context retrieval through the KG while enabling reliable
graph retrieval based on contexts; (2) it achieves deep and faithful reasoning
in LLMs through an iterative knowledge retrieval process that integrates
contexts and the KG; and (3) ToG-2 is training-free and compatible with various
LLMs as a plug-and-play solution. Extensive experiments show that ToG-2
achieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive
datasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,
LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language in Vivo vs. in Silico: Size Matters but Larger Language Models
  Still Do Not Comprehend Language on a Par with Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vittoria Dentella, Fritz Guenther, Evelina Leivada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the limits of language is a prerequisite for Large Language
Models (LLMs) to act as theories of natural language. LLM performance in some
language tasks presents both quantitative and qualitative differences from that
of humans, however it remains to be determined whether such differences are
amenable to model size. This work investigates the critical role of model
scaling, determining whether increases in size make up for such differences
between humans and models. We test three LLMs from different families (Bard,
137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a
grammaticality judgment task featuring anaphora, center embedding,
comparatives, and negative polarity. N=1,200 judgments are collected and scored
for accuracy, stability, and improvements in accuracy upon repeated
presentation of a prompt. Results of the best performing LLM, ChatGPT-4, are
compared to results of n=80 humans on the same stimuli. We find that humans are
overall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but
that this is due to ChatGPT-4 outperforming humans only in one task condition,
namely on grammatical sentences. Additionally, ChatGPT-4 wavers more than
humans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,
respectively). Thus, while increased model size may lead to better performance,
LLMs are still not sensitive to (un)grammaticality the same way as humans are.
It seems possible but unlikely that scaling alone can fix this issue. We
interpret these results by comparing language learning in vivo and in silico,
identifying three critical differences concerning (i) the type of evidence,
(ii) the poverty of the stimulus, and (iii) the occurrence of semantic
hallucinations due to impenetrable linguistic reference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05930v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05930v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate great performance in text
generation. However, LLMs are still suffering from hallucinations. In this
work, we propose an inference-time method, Self-Highlighted Hesitation (SH2),
to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in
information theory that for an LLM, the tokens predicted with lower
probabilities are prone to be more informative than others. Our analysis shows
that the tokens assigned with lower probabilities by an LLM are more likely to
be closely related to factual information, such as nouns, proper nouns, and
adjectives. Therefore, we propose to ''highlight'' the factual information by
selecting the tokens with the lowest probabilities and concatenating them to
the original context, thus forcing the model to repeatedly read and hesitate on
these tokens before generation. During decoding, we also adopt contrastive
decoding to emphasize the difference in the output probabilities brought by the
hesitation. Experimental results demonstrate that our SH2, requiring no
additional data or models, can effectively help LLMs elicit factual knowledge
and distinguish hallucinated contexts. Significant and consistent improvements
are achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple
hallucination tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBF-<span class="highlight-title">LLM</span>: Safe Control for <span class="highlight-title">LLM</span> Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuya Miyaoka, Masaki Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the safety
filter, designed based on the CBF, to the output generation of the baseline
LLM, i.e., the sequence of the token, with the aim of intervening in the
generated text. The overall text-generation system is implemented with Llama 3
and a RoBERTa model, and the source code is available at
https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control
ability and effectiveness in reducing the number of interventions needed for
user-specified alignment tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparison of Language Modeling and Translation as Multilingual
  Pretraining Objectives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Li, Shaoxiong Ji, Timothee Mickus, Vincent Segonne, Jörg Tiedemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) display impressive performances and have
captured the attention of the NLP community. Establishing best practices in
pretraining has, therefore, become a major focus of NLP research, especially
since insights gained from monolingual English models may not necessarily apply
to more complex multilingual models. One significant caveat of the current
state of the art is that different works are rarely comparable: they often
discuss different parameter counts, training data, and evaluation methodology.
  This paper proposes a comparison of multilingual pretraining objectives in a
controlled methodological environment. We ensure that training data and model
architectures are comparable, and discuss the downstream performances across 6
languages that we observe in probing and fine-tuning scenarios. We make two key
observations: (1) the architecture dictates which pretraining objective is
optimal; (2) multilingual translation is a very effective pretraining objective
under the right conditions. We make our code, data, and model weights available
at \texttt{\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine
  Translation with a Human-centered Study <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Savoldi, Sara Papi, Matteo Negri, Ana Guerberof, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gender bias in machine translation (MT) is recognized as an issue that can
harm people and society. And yet, advancements in the field rarely involve
people, the final MT users, or inform how they might be impacted by biased
technologies. Current evaluations are often restricted to automatic methods,
which offer an opaque estimate of what the downstream impact of gender
disparities might be. We conduct an extensive human-centered study to examine
if and to what extent bias in MT brings harms with tangible costs, such as
quality of service gaps across women and men. To this aim, we collect
behavioral data from 90 participants, who post-edited MT outputs to ensure
correct gender translation. Across multiple datasets, languages, and types of
users, our study shows that feminine post-editing demands significantly more
technical and temporal effort, also corresponding to higher financial costs.
Existing bias measurements, however, fail to reflect the found disparities. Our
findings advocate for human-centered approaches that can inform the societal
impact of bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted ad EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OffsetBias: Leveraging Debiased Data for Tuning Evaluators <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, Sanghyuk Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing Large Language Models (LLMs) to assess the quality of generated
responses, such as prompting instruct-tuned models or fine-tuning judge models,
has become a widely adopted evaluation method. It is also known that such
evaluators are vulnerable to biases, such as favoring longer responses. While
it is important to overcome this problem, the specifics of these biases remain
under-explored. In this work, we qualitatively identify six types of biases
inherent in various judge models. We propose EvalBiasBench as a meta-evaluation
collection of hand-crafted test cases for each bias type. Additionally, we
present de-biasing dataset construction methods and the associated preference
dataset OffsetBias. Experimental results demonstrate that fine-tuning on our
dataset significantly enhances the robustness of judge models against biases
and improves performance across most evaluation scenarios. We release our
datasets and the fine-tuned judge model to public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">Large Language Model</span>s Understand Symbolic Graphics Programs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Against the backdrop of enthusiasm for large language models (LLMs), there is
an urgent need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer
different-grained semantic-level questions of the images or 3D geometries
without a vision encoder. To semantically understand the symbolic programs,
LLMs would need to possess the ability to "imagine" and reason how the
corresponding graphics content would look with only the symbolic description.
We use this task to evaluate LLMs by creating a large benchmark for the
semantic visual understanding of symbolic graphics programs, built procedurally
with minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v2 (46 pages, 24 figures, project page:
  https://sgp-bench.github.io/, substantial update from v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse
  Representation Adjustment in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become integral to various applications,
ensuring both their safety and utility is paramount. Jailbreak attacks, which
manipulate LLMs into generating harmful content, pose significant challenges to
this balance. Existing defenses, such as prompt engineering and safety
fine-tuning, often introduce computational overhead, increase inference
latency, and lack runtime flexibility. Moreover, overly restrictive safety
measures can degrade model utility by causing refusals of benign queries. In
this paper, we introduce Jailbreak Antidote, a method that enables real-time
adjustment of LLM safety preferences by manipulating a sparse subset of the
model's internal states during inference. By shifting the model's hidden
representations along a safety direction with varying strengths, we achieve
flexible control over the safety-utility balance without additional token
overhead or inference delays. Our analysis reveals that safety-related
information in LLMs is sparsely distributed; adjusting approximately 5% of the
internal state is as effective as modifying the entire state. Extensive
experiments on nine LLMs (ranging from 2 billion to 72 billion parameters),
evaluated against ten jailbreak attack methods and compared with six defense
strategies, validate the effectiveness and efficiency of our approach. By
directly manipulating internal states during reasoning, Jailbreak Antidote
offers a lightweight, scalable solution that enhances LLM safety while
preserving utility, opening new possibilities for real-time safety mechanisms
in widely-deployed AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Universal Method for Meaningful Signal Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Mahon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is known that human speech and certain animal vocalizations can convey
meaningful content because we can decipher the content that a given utterance
does convey. This paper explores an alternative approach to determining whether
a signal is meaningful, one that analyzes only the signal itself and is
independent of what the conveyed meaning might be. We devise a method that
takes a waveform as input and outputs a score indicating its degree of
`meaningfulness`. We cluster contiguous portions of the input to minimize the
total description length, and then take the length of the code of the assigned
cluster labels as meaningfulness score. We evaluate our method empirically,
against several baselines, and show that it is the only one to give a high
score to human speech in various languages and with various speakers, a
moderate score to animal vocalizations from birds and orcas, and a low score to
ambient noise from various sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Persuasion: Towards Conversational Recommender System with
  Credible Explanations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the aid of large language models, current conversational recommender
system (CRS) has gaining strong abilities to persuade users to accept
recommended items. While these CRSs are highly persuasive, they can mislead
users by incorporating incredible information in their explanations, ultimately
damaging the long-term trust between users and the CRS. To address this, we
propose a simple yet effective method, called PC-CRS, to enhance the
credibility of CRS's explanations during persuasion. It guides the explanation
generation through our proposed credibility-aware persuasive strategies and
then gradually refines explanations via post-hoc self-reflection. Experimental
results demonstrate the efficacy of PC-CRS in promoting persuasive and credible
explanations. Further analysis reveals the reason behind current methods
producing incredible explanations and the potential of credible explanations to
improve recommendation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024. Our code is available at
  https://github.com/mumen798/PC-CRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColPali: Efficient Document Retrieval with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are visually rich structures that convey information through text,
as well as tables, figures, page layouts, or fonts. While modern document
retrieval systems exhibit strong performance on query-to-text matching, they
struggle to exploit visual cues efficiently, hindering their performance on
practical document retrieval applications such as Retrieval Augmented
Generation. To benchmark current systems on visually rich document retrieval,
we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of
various page-level retrieving tasks spanning multiple domains, languages, and
settings. The inherent shortcomings of modern systems motivate the introduction
of a new retrieval model architecture, ColPali, which leverages the document
understanding capabilities of recent Vision Language Models to produce
high-quality contextualized embeddings solely from images of document pages.
Combined with a late interaction matching mechanism, ColPali largely
outperforms modern document retrieval pipelines while being drastically faster
and end-to-end trainable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Nishimura, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at EMNLP2024 - system demonstration track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typing to Listen at the Cocktail Party: Text-Guided Target Speaker
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07284v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07284v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hao, Jibin Wu, Jianwei Yu, Chenglin Xu, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can easily isolate a single speaker from a complex acoustic
environment, a capability referred to as the "Cocktail Party Effect." However,
replicating this ability has been a significant challenge in the field of
target speaker extraction (TSE). Traditional TSE approaches predominantly rely
on voiceprints, which raise privacy concerns and face issues related to the
quality and availability of enrollment samples, as well as intra-speaker
variability. To address these issues, this work introduces a novel text-guided
TSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language
model, LLaMA 2, processes typed text input from users to extract semantic cues.
We demonstrate that textual descriptions alone can effectively serve as cues
for extraction, thus addressing privacy concerns and reducing dependency on
voiceprints. Furthermore, our approach offers flexibility by allowing the user
to specify the extraction or suppression of a speaker and enhances robustness
against intra-speaker variability by incorporating context-dependent textual
information. Experimental results show competitive performance with text-based
cues alone and demonstrate the effectiveness of using text as a task selector.
Additionally, they achieve a new state-of-the-art when combining text-based
cues with pre-registered cues. This work represents the first integration of
LLMs with TSE, potentially establishing a new benchmark in solving the cocktail
party problem and expanding the scope of TSE applications by providing a
versatile, privacy-conscious solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, https://github.com/haoxiangsnr/llm-tse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Russian Jeopardy! Data Set for Question-Answering Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Mikhalkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) is one of the most common NLP tasks that relates to
named entity recognition, fact extraction, semantic search and some other
fields. In industry, it is much appreciated in chatbots and corporate
information systems. It is also a challenging task that attracted the attention
of a very general audience at the quiz show Jeopardy! In this article we
describe a Jeopardy!-like Russian QA data set collected from the official
Russian quiz database Chgk (che ge ka). The data set includes 379,284 quiz-like
questions with 29,375 from the Russian analogue of Jeopardy! - "Own Game". We
observe its linguistic features and the related QA-task. We conclude about
perspectives of a QA competition based on the data set collected from this
database.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of adapting a contrastively pretrained
vision-language model like CLIP (Radford et al., 2021) for few-shot
classification. The existing literature addresses this problem by learning a
linear classifier of the frozen visual features, optimizing word embeddings, or
learning external feature adapters. This paper introduces an alternative way
for CLIP adaptation without adding 'external' parameters to optimize. We find
that simply fine-tuning the last projection matrix of the vision encoder leads
to strong performance compared to the existing baselines. Furthermore, we show
that regularizing training with the distance between the fine-tuned and
pretrained matrices adds reliability for adapting CLIP through this layer.
Perhaps surprisingly, this approach, coined ProLIP, yields performances on par
or better than state of the art on 11 few-shot classification benchmarks,
few-shot domain generalization, cross-dataset transfer and test-time
adaptation. Code will be made available at
https://github.com/astra-vision/ProLIP .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint,under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Partially-Defined Events in <span class="highlight-title">Multimodal</span> Data <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Sanders, Reno Kriz, David Etter, Hannah Recknor, Alexander Martin, Cameron Carpenter, Jingyang Lin, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How are we able to learn about complex current events just from short
snippets of video? While natural language enables straightforward ways to
represent under-specified, partially observable events, visual data does not
facilitate analogous methods and, consequently, introduces unique challenges in
event understanding. With the growing prevalence of vision-capable AI agents,
these systems must be able to model events from collections of unstructured
video data. To tackle robust event modeling in multimodal settings, we
introduce a multimodal formulation for partially-defined events and cast the
extraction of these events as a three-stage span retrieval task. We propose a
corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours
of densely annotated current event videos and 1,168 text documents, containing
22.8K labeled event-centric entities. We propose a collection of LLM-driven
approaches to the task of multimodal event analysis, and evaluate them on
MultiVENT-G. Results illustrate the challenges that abstract event
understanding poses and demonstrates promise in event-centric video-language
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 9 pages; 2024 EMNLP Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Mapping with Dense Features: Grounding Cortical Semantic
  Selectivity in Natural <span class="highlight-title">Image</span>s With Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew F. Luo, Jacob Yeung, Rushikesh Zawar, Shaurya Dewan, Margaret M. Henderson, Leila Wehbe, Michael J. Tarr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in large-scale artificial neural networks have facilitated novel
insights into the functional topology of the brain. Here, we leverage this
approach to study how semantic categories are organized in the human visual
cortex. To overcome the challenge presented by the co-occurrence of multiple
categories in natural images, we introduce BrainSAIL (Semantic Attribution and
Image Localization), a method for isolating specific neurally-activating visual
concepts in images. BrainSAIL exploits semantically consistent, dense spatial
features from pre-trained vision models, building upon their demonstrated
ability to robustly predict neural activity. This method derives clean,
spatially dense embeddings without requiring any additional training, and
employs a novel denoising process that leverages the semantic consistency of
images under random augmentations. By unifying the space of whole-image
embeddings and dense visual features and then applying voxel-wise encoding
models to these features, we enable the identification of specific subregions
of each image which drive selectivity patterns in different areas of the higher
visual cortex. We validate BrainSAIL on cortical regions with known category
selectivity, demonstrating its ability to accurately localize and disentangle
selectivity to diverse visual concepts. Next, we demonstrate BrainSAIL's
ability to characterize high-level visual selectivity to scene properties and
low-level visual features such as depth, luminance, and saturation, providing
insights into the encoding of complex visual information. Finally, we use
BrainSAIL to directly compare the feature selectivity of different brain
encoding models across different regions of interest in visual cortex. Our
innovative method paves the way for significant advances in mapping and
decomposing high-level visual representations in the human brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextHawk2: A Large <span class="highlight-title">Vision-Language Model</span> Excels in Bilingual OCR and
  Grounding with 16x Fewer Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya-Qi Yu, Minghui Liao, Jiwen Zhang, Jihao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reading dense text and locating objects within images are fundamental
abilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.
Previous LVLMs, including superior proprietary models like GPT-4o, have
struggled to excel in both tasks simultaneously. Moreover, previous LVLMs with
fine-grained perception cost thousands of tokens per image, making them
resource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient
fine-grained perception and demonstrating cutting-edge performance across
general-purpose, OCR, and grounding tasks with 16 times fewer image tokens.
Critical improvements include: (1) Token Compression: Building on the efficient
architecture of its predecessor, TextHawk2 significantly reduces the number of
tokens per image by 16 times, facilitating training and deployment of the
TextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We
enhance the visual encoder through LVLM co-training, unlocking its potential
for previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:
We maintain a comparable scale of 100 million samples while diversifying the
sources of pre-training data. We assess TextHawk2 across multiple benchmarks,
where it consistently delivers superior performance and outperforms
closed-source models of similar scale, such as achieving 78.4% accuracy on
OCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%
accuracy@0.5 on RefCOCOg-test.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DART: A <span class="highlight-title">Diffusion</span>-Based Autoregressive Motion Model for Real-Time
  Text-Driven Motion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Zhao, Gen Li, Siyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditioned human motion generation, which allows for user interaction
through natural language, has become increasingly popular. Existing methods
typically generate short, isolated motions based on a single input sentence.
However, human motions are continuous and can extend over long periods,
carrying rich semantics. Creating long, complex motions that precisely respond
to streams of text descriptions, particularly in an online and real-time
setting, remains a significant challenge. Furthermore, incorporating spatial
constraints into text-conditioned motion generation presents additional
challenges, as it requires aligning the motion semantics specified by text
descriptions with geometric information, such as goal locations and 3D scene
geometry. To address these limitations, we propose DART, a Diffusion-based
Autoregressive motion primitive model for Real-time Text-driven motion control.
Our model, DART, effectively learns a compact motion primitive space jointly
conditioned on motion history and text inputs using latent diffusion models. By
autoregressively generating motion primitives based on the preceding history
and current text input, DART enables real-time, sequential motion generation
driven by natural language descriptions. Additionally, the learned motion
primitive space allows for precise spatial motion control, which we formulate
either as a latent noise optimization problem or as a Markov decision process
addressed through reinforcement learning. We present effective algorithms for
both approaches, demonstrating our model's versatility and superior performance
in various motion synthesis tasks. Experiments show our method outperforms
existing baselines in motion realism, efficiency, and controllability. Video
results are available on the project page: https://zkf1997.github.io/DART/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Cao, Masoud Hadi, Liang Pan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based 2D virtual try-on (VTON) techniques have recently
demonstrated strong performance, while the development of 3D VTON has largely
lagged behind. Despite recent advances in text-guided 3D scene editing,
integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains
challenging. The reasons are twofold. First, text prompts cannot provide
sufficient details in describing clothing. Second, 2D VTON results generated
from different viewpoints of the same 3D scene lack coherence and spatial
relationships, hence frequently leading to appearance inconsistencies and
geometric distortions. To resolve these problems, we introduce an
image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian
Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained
knowledge from 2D VTON models to 3D while improving cross-view consistency. (1)
Specifically, we propose a personalized diffusion model that utilizes low-rank
adaptation (LoRA) fine-tuning to incorporate personalized information into
pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a
reference-driven image editing approach that enables the simultaneous editing
of multi-view images while ensuring consistency. (2) Furthermore, we propose a
persona-aware 3DGS editing framework to facilitate effective editing while
maintaining consistent cross-view appearance and high-quality 3D geometry. (3)
Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which
facilitates comprehensive qualitative and quantitative 3D VTON evaluations.
Through extensive experiments and comparative analyses with existing methods,
the proposed \OM has demonstrated superior fidelity and advanced editing
capabilities, affirming its effectiveness for 3D VTON.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SePPO: Semi-Policy Preference Optimization for <span class="highlight-title">Diffusion</span> Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoan Zhang, Guangchen Lan, Dong-Jun Han, Wenlin Yao, Xiaoman Pan, Hongming Zhang, Mingxiao Li, Pengcheng Chen, Yu Dong, Christopher Brinton, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) methods are emerging as a
way to fine-tune diffusion models (DMs) for visual generation. However,
commonly used on-policy strategies are limited by the generalization capability
of the reward model, while off-policy approaches require large amounts of
difficult-to-obtain paired human-annotated data, particularly in visual
generation tasks. To address the limitations of both on- and off-policy RLHF,
we propose a preference optimization method that aligns DMs with preferences
without relying on reward models or paired human-annotated data. Specifically,
we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO
leverages previous checkpoints as reference models while using them to generate
on-policy reference samples, which replace "losing images" in preference pairs.
This approach allows us to optimize using only off-policy "winning images."
Furthermore, we design a strategy for reference model selection that expands
the exploration in the policy space. Notably, we do not simply treat reference
samples as negative examples for learning. Instead, we design an anchor-based
criterion to assess whether the reference samples are likely to be winning or
losing images, allowing the model to selectively learn from the generated
reference samples. This approach mitigates performance degradation caused by
the uncertainty in reference sample quality. We validate SePPO across both
text-to-image and text-to-video benchmarks. SePPO surpasses all previous
approaches on the text-to-image benchmarks and also demonstrates outstanding
performance on the text-to-video benchmarks. Code will be released in
https://github.com/DwanZhang-AI/SePPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoTLIP: Improving Language-<span class="highlight-title">Image</span> Pre-training for Long Text
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Kecheng Zheng, Shuailei Ma, Fan Lu, Yuxin Guo, Yifei Zhang, Wei Chen, Qingpei Guo, Yujun Shen, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding long text is of great demands in practice but beyond the reach
of most language-image pre-training (LIP) models. In this work, we empirically
confirm that the key reason causing such an issue is that the training images
are usually paired with short captions, leaving certain tokens easily
overshadowed by salient tokens. Towards this problem, our initial attempt is to
relabel the data with long captions, however, directly learning with which may
lead to performance degradation in understanding short text (e.g., in the image
classification task). Then, after incorporating corner tokens to aggregate
diverse textual information, we manage to help the model catch up to its
original level of short text understanding yet greatly enhance its capability
of long text understanding. We further look into whether the model can
continuously benefit from longer captions and notice a clear trade-off between
the performance and the efficiency. Finally, we validate the effectiveness of
our approach using a self-constructed large-scale dataset, which consists of
100M long caption oriented text-image pairs. It is noteworthy that, on the task
of long-text image retrieval, we beat the competitor using long captions with
11.1% improvement (i.e., from 72.62% to 83.72%). We will release the code, the
model, and the new dataset to facilitate the reproducibility and further
research. The project page is available at https://wuw2019.github.io/lotlip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Digital World as Humans Do: Universal Visual Grounding
  for GUI <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly take
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TuneVLSeg: Prompt Tuning <span class="highlight-title">Benchmark</span> for Vision-Language Segmentation
  Models <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabin Adhikari, Safal Thapaliya, Manish Dhakal, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have shown impressive performance in vision
tasks, but adapting them to new domains often requires expensive fine-tuning.
Prompt tuning techniques, including textual, visual, and multimodal prompting,
offer efficient alternatives by leveraging learnable prompts. However, their
application to Vision-Language Segmentation Models (VLSMs) and evaluation under
significant domain shifts remain unexplored. This work presents an open-source
benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal
prompt tuning techniques into VLSMs, making prompt tuning usable for downstream
segmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt
tuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$
different combinations. We test various prompt tuning on $8$ diverse medical
datasets, including $3$ radiology datasets (breast tumor, echocardiograph,
chest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin
cancer), and two natural domain segmentation datasets. Our study found that
textual prompt tuning struggles under significant domain shifts, from
natural-domain images to medical data. Furthermore, visual prompt tuning, with
fewer hyperparameters than multimodal prompt tuning, often achieves performance
competitive to multimodal approaches, making it a valuable first attempt. Our
work advances the understanding and applicability of different prompt-tuning
techniques for robust domain-specific segmentation. The source code is
available at https://github.com/naamiinepal/tunevlseg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACCV 2024 (oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuseReg: Denoising <span class="highlight-title">Diffusion</span> Model for Obtaining Deformation Fields
  in Unsupervised Deformable <span class="highlight-title">Image</span> Registration <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongtai Zhuo, Yiqing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable image registration aims to precisely align medical images from
different modalities or times. Traditional deep learning methods, while
effective, often lack interpretability, real-time observability and adjustment
capacity during registration inference. Denoising diffusion models present an
alternative by reformulating registration as iterative image denoising.
However, existing diffusion registration approaches do not fully harness
capabilities, neglecting the critical sampling phase that enables continuous
observability during the inference. Hence, we introduce DiffuseReg, an
innovative diffusion-based method that denoises deformation fields instead of
images for improved transparency. We also propose a novel denoising network
upon Swin Transformer, which better integrates moving and fixed images with
diffusion time step throughout the denoising process. Furthermore, we enhance
control over the denoising registration process with a novel similarity
consistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg
outperforms existing diffusion registration methods by 1.32 in Dice score. The
sampling process in DiffuseReg enables real-time output observability and
adjustment unmatched by previous deep models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024, W-AM-067, https://github.com/YutaZhuo/DiffuseReg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Bouhsine, Imad El Aaroussi, Atik Faysal, Wang Huaxia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel anchor-free contrastive learning (AFCL) method
leveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach
minimizes a semi-metric discriminative loss function that simultaneously
optimizes two key objectives: reducing the distance and orthogonality between
embeddings of similar inputs while maximizing these metrics for dissimilar
inputs, facilitating more fine-grained contrastive learning. The AFCL method,
powered by SimO loss, creates a fiber bundle topological structure in the
embedding space, forming class-specific, internally cohesive yet orthogonal
neighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,
providing visualizations that demonstrate the impact of SimO loss on the
embedding space. Our results illustrate the formation of distinct, orthogonal
class neighborhoods, showcasing the method's ability to create well-structured
embeddings that balance class separation with intra-class variability. This
work opens new avenues for understanding and leveraging the geometric
properties of learned representations in various machine learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Dawn of Video <span class="highlight-title">Generation</span>: Preliminary Explorations with SORA-like
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ailing Zeng, Yuhang Yang, Weidong Chen, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality video generation, encompassing text-to-video (T2V),
image-to-video (I2V), and video-to-video (V2V) generation, holds considerable
significance in content creation to benefit anyone express their inherent
creativity in new ways and world simulation to modeling and understanding the
world. Models like SORA have advanced generating videos with higher resolution,
more natural motion, better vision-language alignment, and increased
controllability, particularly for long video sequences. These improvements have
been driven by the evolution of model architectures, shifting from UNet to more
scalable and parameter-rich DiT models, along with large-scale data expansion
and refined training strategies. However, despite the emergence of DiT-based
closed-source and open-source models, a comprehensive investigation into their
capabilities and limitations remains lacking. Furthermore, the rapid
development has made it challenging for recent benchmarks to fully cover
SORA-like models and recognize their significant advancements. Additionally,
evaluation metrics often fail to align with human preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project: https://ailab-cvc.github.io/VideoGen-Eval/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Model <span class="highlight-title">Benchmark</span>ing with Only a Few Observations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we precisely estimate a large language model's (LLM) accuracy on
questions belonging to a specific topic within a larger question-answering
dataset? The standard direct estimator, which averages the model's accuracy on
the questions in each subgroup, may exhibit high variance for subgroups
(topics) with small sample sizes. Synthetic regression modeling, which
leverages the model's accuracy on questions about other topics, may yield
biased estimates that are too unreliable for large subgroups. We prescribe a
simple yet effective solution: an empirical Bayes (EB) estimator that balances
direct and regression estimates for each subgroup separately, improving the
precision of subgroup-level estimates of model performance. Our experiments on
multiple datasets show that this approach consistently provides more precise
estimates of the LLM performance compared to the direct and regression
approaches, achieving substantial reductions in the mean squared error.
Confidence intervals for EB estimates also have near-nominal coverage and are
narrower compared to those for the direct estimator. Additional experiments on
tabular and vision data validate the benefits of this EB approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Organizing Unstructured <span class="highlight-title">Image</span> Collections using Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Liu, Zhun Zhong, Jun Li, Gianni Franchi, Subhankar Roy, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Organizing unstructured visual data into semantic clusters is a key challenge
in computer vision. Traditional deep clustering (DC) approaches focus on a
single partition of data, while multiple clustering (MC) methods address this
limitation by uncovering distinct clustering solutions. The rise of large
language models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing
users to define clustering criteria in natural language. However, manually
specifying criteria for large datasets is impractical. In this work, we
introduce the task Semantic Multiple Clustering (SMC) that aims to
automatically discover clustering criteria from large image collections,
uncovering interpretable substructures without requiring human input. Our
framework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a
proxy to concurrently reason over large image collections, discover
partitioning criteria, expressed in natural language, and reveal semantic
substructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c
benchmarks, each containing four grouping criteria and ground-truth
annotations. We apply TeDeSC to various applications, such as discovering
biases and analyzing social media image popularity, demonstrating its utility
as a tool for automatically organizing image collections and revealing novel
insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Project webpage: https://oatmealliu.github.io/smc.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving
  Vision-Linguistic Compositionality <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, In So Kweon, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new method to enhance compositional understanding
in pre-trained vision and language models (VLMs) without sacrificing
performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches
often improve compositional reasoning at the cost of degrading multi-modal
capabilities, primarily due to the use of global hard negative (HN) loss, which
contrasts global representations of images and texts. This global HN loss
pushes HN texts that are highly similar to the original ones, damaging the
model's multi-modal representations. To overcome this limitation, we propose
Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard
negative loss and selective calibrated regularization. These innovations
provide fine-grained negative supervision while preserving the model's
representational integrity. Our extensive evaluations across diverse benchmarks
for both compositionality and multi-modal tasks show that FSC-CLIP not only
achieves compositionality on par with state-of-the-art models but also retains
strong multi-modal capabilities. Code is available at:
https://github.com/ytaek-oh/fsc-clip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Long, Main). Project page:
  https://ytaek-oh.github.io/fsc-clip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Studying and Mitigating Biases in Sign Language Understanding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine Atwell, Danielle Bragg, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that the benefits of sign language technologies are distributed
equitably among all community members is crucial. Thus, it is important to
address potential biases and inequities that may arise from the design or use
of these resources. Crowd-sourced sign language datasets, such as the ASL
Citizen dataset, are great resources for improving accessibility and preserving
linguistic diversity, but they must be used thoughtfully to avoid reinforcing
existing biases.
  In this work, we utilize the rich information about participant demographics
and lexical features present in the ASL Citizen dataset to study and document
the biases that may result from models trained on crowd-sourced sign datasets.
Further, we apply several bias mitigation techniques during model training, and
find that these techniques reduce performance disparities without decreasing
accuracy. With the publication of this work, we release the demographic
information about the participants in the ASL Citizen dataset to encourage
future bias mitigation work in this space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond FVD: Enhanced Evaluation Metrics for Video <span class="highlight-title">Generation</span> Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Ya,  Luo, Gian Favero, Zhi Hao Luo, Alexia Jolicoeur-Martineau, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Fr\'echet Video Distance (FVD) is a widely adopted metric for evaluating
video generation distribution quality. However, its effectiveness relies on
critical assumptions. Our analysis reveals three significant limitations: (1)
the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the
insensitivity of I3D features to temporal distortions; (3) the impractical
sample sizes required for reliable estimation. These findings undermine FVD's
reliability and show that FVD falls short as a standalone metric for video
generation evaluation. After extensive analysis of a wide range of metrics and
backbone architectures, we propose JEDi, the JEPA Embedding Distance, based on
features derived from a Joint Embedding Predictive Architecture, measured using
Maximum Mean Discrepancy with polynomial kernel. Our experiments on multiple
open-source datasets show clear evidence that it is a superior alternative to
the widely used FVD metric, requiring only 16% of the samples to reach its
steady value, while increasing alignment with human evaluation by 34%, on
average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARs: Multi-view Attention Regularizations for Patch-based Feature
  Recognition of Space Terrain <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Chase Jr, Karthik Dantu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual detection and tracking of surface terrain is required for
spacecraft to safely land on or navigate within close proximity to celestial
objects. Current approaches rely on template matching with pre-gathered
patch-based features, which are expensive to obtain and a limiting factor in
perceptual capability. While recent literature has focused on in-situ detection
methods to enhance navigation and operational autonomy, robust description is
still needed. In this work, we explore metric learning as the lightweight
feature description mechanism and find that current solutions fail to address
inter-class similarity and multi-view observational geometry. We attribute this
to the view-unaware attention mechanism and introduce Multi-view Attention
Regularizations (MARs) to constrain the channel and spatial attention across
multiple feature views, regularizing the what and where of attention focus. We
thoroughly analyze many modern metric learning losses with and without MARs and
demonstrate improved terrain-feature recognition performance by upwards of 85%.
We additionally introduce the Luna-1 dataset, consisting of Moon crater
landmarks and reference navigation frames from NASA mission data to support
future research in this difficult task. Luna-1 and source code are publicly
available at https://droneslab.github.io/mars/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Project page available at
  https://droneslab.github.io/mars/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM2Vec: Training <span class="highlight-title">Vision-Language Model</span>s for Massive <span class="highlight-title">Multimodal</span>
  Embedding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models have been crucial in enabling various downstream tasks such
as semantic similarity, information retrieval, and clustering. Recently, there
has been a surge of interest in developing universal text embedding models that
can generalize across tasks (e.g., MTEB). However, progress in learning
universal multimodal embedding models has been relatively slow despite their
importance. In this work, we aim to explore the potential for building
universal embeddings capable of handling a wide range of downstream tasks. Our
contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),
which covers 4 meta-tasks (i.e. classification, visual question answering,
multimodal retrieval, and visual grounding) and 36 datasets, including 20
training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->
Vector), a contrastive training framework that converts any state-of-the-art
vision-language model into an embedding model via training on MMEB. Unlike
previous models such as CLIP and BLIP, VLM2Vec can process any combination of
images and text to generate a fixed-dimensional vector based on task
instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate
them on MMEB's evaluation split. Our results show that \model achieves an
absolute average improvement of 10% to 20% over existing multimodal embedding
models on both in-distribution and out-of-distribution datasets in MMEB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIBench: A Comprehensive <span class="highlight-title">Benchmark</span> for Model Inversion Attack and
  Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiang Qiu, Hongyao Yu, Hao Fang, Wenbo Yu, Bin Chen, Xuan Wang, Shu-Tao Xia, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Inversion (MI) attacks aim at leveraging the output information of
target models to reconstruct privacy-sensitive training data, raising
widespread concerns on privacy threats of Deep Neural Networks (DNNs).
Unfortunately, in tandem with the rapid evolution of MI attacks, the lack of a
comprehensive, aligned, and reliable benchmark has emerged as a formidable
challenge. This deficiency leads to inadequate comparisons between different
attack methods and inconsistent experimental setups. In this paper, we
introduce the first practical benchmark for model inversion attacks and
defenses to address this critical gap, which is named \textit{MIBench}. This
benchmark serves as an extensible and reproducible modular-based toolbox and
currently integrates a total of 16 state-of-the-art attack and defense methods.
Moreover, we furnish a suite of assessment tools encompassing 9 commonly used
evaluation protocols to facilitate standardized and fair evaluation and
analysis. Capitalizing on this foundation, we conduct extensive experiments
from multiple perspectives to holistically compare and analyze the performance
of various methods across different scenarios, which overcomes the misalignment
issues and discrepancy prevalent in previous works. Based on the collected
attack methods and defense strategies, we analyze the impact of target
resolution, defense robustness, model predictive power, model architectures,
transferability and loss function. Our hope is that this \textit{MIBench} could
provide a unified, practical and extensible toolbox and is widely utilized by
researchers in the field to rigorously test and compare their novel methods,
ensuring equitable evaluations and thereby propelling further advancements in
the future development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging <span class="highlight-title">Multimodal</span> <span class="highlight-title">Diffusion</span> Models to Accelerate Imaging with Side
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timofey Efimov, Harry Dong, Megna Shah, Jeff Simmons, Sean Donegan, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have found phenomenal success as expressive priors for
solving inverse problems, but their extension beyond natural images to more
structured scientific domains remains limited. Motivated by applications in
materials science, we aim to reduce the number of measurements required from an
expensive imaging modality of interest, by leveraging side information from an
auxiliary modality that is much cheaper to obtain. To deal with the
non-differentiable and black-box nature of the forward model, we propose a
framework to train a multimodal diffusion model over the joint modalities,
turning inverse problems with black-box forward models into simple linear
inpainting problems. Numerically, we demonstrate the feasibility of training
diffusion models over materials imagery data, and show that our approach
achieves superior image reconstruction by leveraging the available side
information, requiring significantly less amount of data from the expensive
microscopy modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Feedback Efficient Reinforcement Learning for Online <span class="highlight-title">Diffusion</span>
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable generation through Stable Diffusion (SD) fine-tuning aims to
improve fidelity, safety, and alignment with human guidance. Existing
reinforcement learning from human feedback methods usually rely on predefined
heuristic reward functions or pretrained reward models built on large-scale
datasets, limiting their applicability to scenarios where collecting such data
is costly or difficult. To effectively and efficiently utilize human feedback,
we develop a framework, HERO, which leverages online human feedback collected
on the fly during model learning. Specifically, HERO features two key
mechanisms: (1) Feedback-Aligned Representation Learning, an online training
method that captures human feedback and provides informative learning signals
for fine-tuning, and (2) Feedback-Guided Image Generation, which involves
generating images from SD's refined initialization samples, enabling faster
convergence towards the evaluator's intent. We demonstrate that HERO is 4x more
efficient in online feedback for body part anomaly correction compared to the
best existing method. Additionally, experiments show that HERO can effectively
handle tasks like reasoning, counting, personalization, and reducing NSFW
content with only 0.5K online feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic <span class="highlight-title">Generation</span> of Dermatoscopic <span class="highlight-title">Image</span>s with GAN and Closed-Form
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Reddy Mekala, Frederik Pahde, Simon Baur, Sneha Chandrashekar, Madeline Diep, Markus Wenzel, Eric L. Wisotzky, Galip Ümit Yolcu, Sebastian Lapuschkin, Jackie Ma, Peter Eisert, Mikael Lindvall, Adam Porter, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of dermatological diagnoses, where the analysis of dermatoscopic
and microscopic skin lesion images is pivotal for the accurate and early
detection of various medical conditions, the costs associated with creating
diverse and high-quality annotated datasets have hampered the accuracy and
generalizability of machine learning models. We propose an innovative
unsupervised augmentation solution that harnesses Generative Adversarial
Network (GAN) based models and associated techniques over their latent space to
generate controlled semiautomatically-discovered semantic variations in
dermatoscopic images. We created synthetic images to incorporate the semantic
variations and augmented the training data with these images. With this
approach, we were able to increase the performance of machine learning models
and set a new benchmark amongst non-ensemble based models in skin lesion
classification on the HAM10000 dataset; and used the observed analytics and
generated models for detailed studies on model explainability, affirming the
effectiveness of our solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has been submitted to the Workshop on Synthetic Data
  for Computer Vision (SyntheticData4CV 2024 is a side event on 18th European
  Conference on Computer Vision 2024). This preprint has not undergone peer
  review or any post-submission improvements or corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifeng Chen, Sheng Yang, Sicong Du, Tao Tang, Peng Chen, Yuchi Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR simulation plays a crucial role in closed-loop simulation for
autonomous driving. Although recent advancements, such as the use of
reconstructed mesh and Neural Radiance Fields (NeRF), have made progress in
simulating the physical properties of LiDAR, these methods have struggled to
achieve satisfactory frame rates and rendering quality. To address these
limitations, we present LiDAR-GS, the first LiDAR Gaussian Splatting method,
for real-time high-fidelity re-simulation of LiDAR sensor scans in public urban
road scenes. The vanilla Gaussian Splatting, designed for camera models, cannot
be directly applied to LiDAR re-simulation. To bridge the gap between passive
camera and active LiDAR, our LiDAR-GS designs a differentiable laser beam
splatting, grounded in the LiDAR range view model. This innovation allows for
precise surface splatting by projecting lasers onto micro cross-sections,
effectively eliminating artifacts associated with local affine approximations.
Additionally, LiDAR-GS leverages Neural Gaussian Fields, which further
integrate view-dependent clues, to represent key LiDAR properties that are
influenced by the incident angle and external factors. Combining these
practices with some essential adaptations, e.g., dynamic instances
decomposition, our approach succeeds in simultaneously re-simulating depth,
intensity, and ray-drop channels, achieving state-of-the-art results in both
rendering frame rate and quality on publically available large scene datasets.
Our source code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaDD: Boosting <span class="highlight-title">Dataset</span> Distillation with Neural Network
  Architecture-Invariant Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Zhao, Xiaoheng Deng, Xiu Su, Hongyan Xu, Xiuxing Li, Yijing Liu, Shan You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation (DD) entails creating a refined, compact distilled
dataset from a large-scale dataset to facilitate efficient training. A
significant challenge in DD is the dependency between the distilled dataset and
the neural network (NN) architecture used. Training a different NN architecture
with a distilled dataset distilled using a specific architecture often results
in diminished trainning performance for other architectures. This paper
introduces MetaDD, designed to enhance the generalizability of DD across
various NN architectures. Specifically, MetaDD partitions distilled data into
meta features (i.e., the data's common characteristics that remain consistent
across different NN architectures) and heterogeneous features (i.e., the data's
unique feature to each NN architecture). Then, MetaDD employs an
architecture-invariant loss function for multi-architecture feature alignment,
which increases meta features and reduces heterogeneous features in distilled
data. As a low-memory consumption component, MetaDD can be seamlessly
integrated into any DD methodology. Experimental results demonstrate that
MetaDD significantly improves performance across various DD methods. On the
Distilled Tiny-Imagenet with Sre2L (50 IPC), MetaDD achieves cross-architecture
NN accuracy of up to 30.1\%, surpassing the second-best method (GLaD) by 1.7\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IGroupSS-Mamba: Interval Group Spatial-Spectral Mamba for Hyperspectral
  <span class="highlight-title">Image</span> Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan He, Bing Tu, Puzhao Jiang, Bo Liu, Jun Li, Antonio Plaza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) classification has garnered substantial attention
in remote sensing fields. Recent Mamba architectures built upon the Selective
State Space Models (S6) have demonstrated enormous potential in long-range
sequence modeling. However, the high dimensionality of hyperspectral data and
information redundancy pose challenges to the application of Mamba in HSI
classification, suffering from suboptimal performance and computational
efficiency. In light of this, this paper investigates a lightweight Interval
Group Spatial-Spectral Mamba framework (IGroupSS-Mamba) for HSI classification,
which allows for multi-directional and multi-scale global spatial-spectral
information extraction in a grouping and hierarchical manner. Technically, an
Interval Group S6 Mechanism (IGSM) is developed as the core component, which
partitions high-dimensional features into multiple non-overlapping groups at
intervals, and then integrates a unidirectional S6 for each group with a
specific scanning direction to achieve non-redundant sequence modeling.
Compared to conventional applying multi-directional scanning to all bands, this
grouping strategy leverages the complementary strengths of different scanning
directions while decreasing computational costs. To adequately capture the
spatial-spectral contextual information, an Interval Group Spatial-Spectral
Block (IGSSB) is introduced, in which two IGSM-based spatial and spectral
operators are cascaded to characterize the global spatial-spectral relationship
along the spatial and spectral dimensions, respectively. IGroupSS-Mamba is
constructed as a hierarchical structure stacked by multiple IGSSB blocks,
integrating a pixel aggregation-based downsampling strategy for multiscale
spatial-spectral semantic learning from shallow to deep stages. Extensive
experiments demonstrate that IGroupSS-Mamba outperforms the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamSat: Towards a General 3D Model for Novel View Synthesis of Space
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nidhi Mathihalli, Audrey Wei, Giovanni Lavezzi, Peng Mun Siew, Victor Rodriguez-Fernandez, Hodei Urrutxua, Richard Linares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis (NVS) enables to generate new images of a scene or
convert a set of 2D images into a comprehensive 3D model. In the context of
Space Domain Awareness, since space is becoming increasingly congested, NVS can
accurately map space objects and debris, improving the safety and efficiency of
space operations. Similarly, in Rendezvous and Proximity Operations missions,
3D models can provide details about a target object's shape, size, and
orientation, allowing for better planning and prediction of the target's
behavior. In this work, we explore the generalization abilities of these
reconstruction techniques, aiming to avoid the necessity of retraining for each
new scene, by presenting a novel approach to 3D spacecraft reconstruction from
single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art
single-view reconstruction model, on a high-quality dataset of 190 high-quality
spacecraft models and integrating it into the DreamGaussian framework. We
demonstrate consistent improvements in reconstruction quality across multiple
metrics, including Contrastive Language-Image Pretraining (CLIP) score
(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity
Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)
(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method
addresses the lack of domain-specific 3D reconstruction tools in the space
industry by leveraging state-of-the-art diffusion models and 3D Gaussian
splatting techniques. This approach maintains the efficiency of the
DreamGaussian framework while enhancing the accuracy and detail of spacecraft
reconstructions. The code for this work can be accessed on GitHub
(https://github.com/ARCLab-MIT/space-nvs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 75th International Astronautical Congress, October
  2024, Milan, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-in-the-loop <span class="highlight-title">Reasoning</span> For Traffic Sign Detection: Collaborative
  Approach Yolo With Video-llava 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Azarafza, Fatima Idrees, Ali Ehteshami Bejnordi, Charles Steinmetz, Stefan Henkler, Achim Rettberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic Sign Recognition (TSR) detection is a crucial component of autonomous
vehicles. While You Only Look Once (YOLO) is a popular real-time object
detection algorithm, factors like training data quality and adverse weather
conditions (e.g., heavy rain) can lead to detection failures. These failures
can be particularly dangerous when visual similarities between objects exist,
such as mistaking a 30 km/h sign for a higher speed limit sign. This paper
proposes a method that combines video analysis and reasoning, prompting with a
human-in-the-loop guide large vision model to improve YOLOs accuracy in
detecting road speed limit signs, especially in semi-real-world conditions. It
is hypothesized that the guided prompting and reasoning abilities of
Video-LLava can enhance YOLOs traffic sign detection capabilities. This
hypothesis is supported by an evaluation based on human-annotated accuracy
metrics within a dataset of recorded videos from the CARLA car simulator. The
results demonstrate that a collaborative approach combining YOLO with
Video-LLava and reasoning can effectively address challenging situations such
as heavy rain and overcast conditions that hinder YOLOs detection capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xLSTM-FER: Enhancing Student Expression Recognition with Extended Vision
  Long Short-Term Memory Network <span class="chip">APWeb</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qionghao Huang, Jili Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Student expression recognition has become an essential tool for assessing
learning experiences and emotional states. This paper introduces xLSTM-FER, a
novel architecture derived from the Extended Long Short-Term Memory (xLSTM),
designed to enhance the accuracy and efficiency of expression recognition
through advanced sequence processing capabilities for student facial expression
recognition. xLSTM-FER processes input images by segmenting them into a series
of patches and leveraging a stack of xLSTM blocks to handle these patches.
xLSTM-FER can capture subtle changes in real-world students' facial expressions
and improve recognition accuracy by learning spatial-temporal relationships
within the sequence. Experiments on CK+, RAF-DF, and FERplus demonstrate the
potential of xLSTM-FER in expression recognition tasks, showing better
performance compared to state-of-the-art methods on standard datasets. The
linear computational and memory complexity of xLSTM-FER make it particularly
suitable for handling high-resolution images. Moreover, the design of xLSTM-FER
allows for efficient processing of non-sequential inputs such as images without
additional computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper, consisting of 10 pages and 3 figures, has been accepted by
  the AIEDM Workshop at the 8th APWeb-WAIM Joint International Conference on
  Web and Big Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-oriented Clustering of Visual Latent Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Qi, Haocheng Yin, Heng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate a study of the geometry of the visual representation space -- the
information channel from the vision encoder to the action decoder -- in an
image-based control pipeline learned from behavior cloning. Inspired by the
phenomenon of neural collapse (NC) in image classification, we investigate
whether a similar law of clustering emerges in the visual representation space.
Since image-based control is a regression task without explicitly defined
classes, the central piece of the puzzle lies in determining according to what
implicit classes the visual features cluster, if such a law exists. Focusing on
image-based planar pushing, we posit the most important role of the visual
representation in a control task is to convey a goal to the action decoder. We
then classify training samples of expert demonstrations into eight
"control-oriented" classes based on (a) the relative pose between the object
and the target in the input or (b) the relative pose of the object induced by
expert actions in the output, where one class corresponds to one relative pose
orthant (REPO). Across four different instantiations of architecture, we report
the prevalent emergence of control-oriented clustering in the visual
representation space according to the eight REPOs. Beyond empirical
observation, we show such a law of clustering can be leveraged as an
algorithmic tool to improve test-time performance when training a policy with
limited expert demonstrations. Particularly, we pretrain the vision encoder
using NC as a regularization to encourage control-oriented clustering of the
visual features. Surprisingly, such an NC-pretrained vision encoder, when
finetuned end-to-end with the action decoder, boosts the test-time performance
by 10% to 35% in the low-data regime. Real-world vision-based planar pushing
experiments confirmed the surprising advantage of control-oriented visual
representation pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Object Detection via Local-global Contrastive Learning <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danai Triantafyllidou, Sarah Parisot, Ales Leonardis, Steven McDonagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual domain gaps often impact object detection performance. Image-to-image
translation can mitigate this effect, where contrastive approaches enable
learning of the image-to-image mapping under unsupervised regimes. However,
existing methods often fail to handle content-rich scenes with multiple object
instances, which manifests in unsatisfactory detection performance. Sensitivity
to such instance-level content is typically only gained through object
annotations, which can be expensive to obtain. Towards addressing this issue,
we present a novel image-to-image translation method that specifically targets
cross-domain object detection. We formulate our approach as a contrastive
learning framework with an inductive prior that optimises the appearance of
object instances through spatial attention masks, implicitly delineating the
scene into foreground regions associated with the target object instances and
background non-object regions. Instead of relying on object annotations to
explicitly account for object instances during translation, our approach learns
to represent objects by contrasting local-global information. This affords
investigation of an under-explored challenge: obtaining performant detection,
under domain shifts, without relying on object annotations nor detector model
fine-tuning. We experiment with multiple cross-domain object detection settings
across three challenging benchmarks and report state-of-the-art performance.
Project page: https://local-global-detection.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2024 - Project page: https://local-global-detection.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SELECT: A Large-Scale <span class="highlight-title">Benchmark</span> of Data Curation Strategies for <span class="highlight-title">Image</span>
  Classification <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Feuer, Jiawei Xu, Niv Cohen, Patrick Yubeaton, Govind Mittal, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data curation is the problem of how to collect and organize samples into a
dataset that supports efficient learning. Despite the centrality of the task,
little work has been devoted towards a large-scale, systematic comparison of
various curation methods. In this work, we take steps towards a formal
evaluation of data curation strategies and introduce SELECT, the first
large-scale benchmark of curation strategies for image classification.
  In order to generate baseline methods for the SELECT benchmark, we create a
new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K
to date. Our dataset extends ImageNet with 5 new training-data shifts, each
approximately the size of ImageNet-1K itself, and each assembled using a
distinct curation strategy. We evaluate our data curation baselines in two
ways: (i) using each training-data shift to train identical image
classification models from scratch (ii) using the data itself to fit a
pretrained self-supervised representation.
  Our findings show interesting trends, particularly pertaining to recent
methods for data curation such as synthetic data generation and lookup based on
CLIP embeddings. We show that although these strategies are highly competitive
for certain tasks, the curation strategy used to assemble the original
ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark
can illuminate the path for new methods to further reduce the gap. We release
our checkpoints, code, documentation, and a link to our dataset at
https://github.com/jimmyxu123/SELECT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HE-Drive: Human-Like End-to-End Driving with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junming Wang, Xingyu Zhang, Zebin Xing, Songen Gu, Xiaoyang Guo, Yang Hu, Ziying Song, Qian Zhang, Xiaoxiao Long, Wei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose HE-Drive: the first human-like-centric end-to-end
autonomous driving system to generate trajectories that are both temporally
consistent and comfortable. Recent studies have shown that imitation
learning-based planners and learning-based trajectory scorers can effectively
generate and select accuracy trajectories that closely mimic expert
demonstrations. However, such trajectory planners and scorers face the dilemma
of generating temporally inconsistent and uncomfortable trajectories. To solve
the above problems, Our HE-Drive first extracts key 3D spatial representations
through sparse perception, which then serves as conditional inputs for a
Conditional Denoising Diffusion Probabilistic Models (DDPMs)-based motion
planner to generate temporal consistency multi-modal trajectories. A
Vision-Language Models (VLMs)-guided trajectory scorer subsequently selects the
most comfortable trajectory from these candidates to control the vehicle,
ensuring human-like end-to-end driving. Experiments show that HE-Drive not only
achieves state-of-the-art performance (i.e., reduces the average collision rate
by 71% than VAD) and efficiency (i.e., 1.9X faster than SparseDrive) on the
challenging nuScenes and OpenScene datasets but also provides the most
comfortable driving experience on real-world data.For more information, visit
the project website: https://jmwang0117.github.io/HE-Drive/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhotoReg: Photometrically Registering 3D Gaussian Splatting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwen Yuan, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building accurate representations of the environment is critical for
intelligent robots to make decisions during deployment. Advances in
photorealistic environment models have enabled robots to develop
hyper-realistic reconstructions, which can be used to generate images that are
intuitive for human inspection. In particular, the recently introduced
\ac{3DGS}, which describes the scene with up to millions of primitive
ellipsoids, can be rendered in real time. \ac{3DGS} has rapidly gained
prominence. However, a critical unsolved problem persists: how can we fuse
multiple \ac{3DGS} into a single coherent model? Solving this problem will
enable robot teams to jointly build \ac{3DGS} models of their surroundings. A
key insight of this work is to leverage the {duality} between photorealistic
reconstructions, which render realistic 2D images from 3D structure, and
\emph{3D foundation models}, which predict 3D structure from image pairs. To
this end, we develop PhotoReg, a framework to register multiple photorealistic
\ac{3DGS} models with 3D foundation models. As \ac{3DGS} models are generally
built from monocular camera images, they have \emph{arbitrary scale}. To
resolve this, PhotoReg actively enforces scale consistency among the different
\ac{3DGS} models by considering depth estimates within these models. Then, the
alignment is iteratively refined with fine-grained photometric losses to
produce high-quality fused \ac{3DGS} models. We rigorously evaluate PhotoReg on
both standard benchmark datasets and our custom-collected datasets, including
with two quadruped robots. The code is released at
\url{ziweny11.github.io/photoreg}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic Literature <span class="highlight-title">Review</span> of Vision-Based Approaches to Outdoor
  Livestock Monitoring with Lessons from Wildlife Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stacey D. Scott, Zayn J. Abbas, Feerass Ellid, Eli-Henry Dykhne, Muhammad Muhaiminul Islam, Weam Ayad, Kristina Kacmorova, Dan Tulpan, Minglun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision livestock farming (PLF) aims to improve the health and welfare of
livestock animals and farming outcomes through the use of advanced
technologies. Computer vision, combined with recent advances in machine
learning and deep learning artificial intelligence approaches, offers a
possible solution to the PLF ideal of 24/7 livestock monitoring that helps
facilitate early detection of animal health and welfare issues. However, a
significant number of livestock species are raised in large outdoor habitats
that pose technological challenges for computer vision approaches. This review
provides a comprehensive overview of computer vision methods and open
challenges in outdoor animal monitoring. We include research from both the
livestock and wildlife fields in the review because of the similarities in
appearance, behaviour, and habitat for many livestock and wildlife. We focus on
large terrestrial mammals, such as cattle, horses, deer, goats, sheep, koalas,
giraffes, and elephants. We use an image processing pipeline to frame our
discussion and highlight the current capabilities and open technical challenges
at each stage of the pipeline. The review found a clear trend towards the use
of deep learning approaches for animal detection, counting, and multi-species
classification. We discuss in detail the applicability of current vision-based
methods to PLF contexts and promising directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Variational Autoencoders for Probabilistic Pose Regression <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fereidoon Zangeneh, Leonard Bruns, Amit Dekel, Alessandro Pieropan, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots rely on visual relocalization to estimate their pose from camera
images when they lose track. One of the challenges in visual relocalization is
repetitive structures in the operation environment of the robot. This calls for
probabilistic methods that support multiple hypotheses for robot's pose. We
propose such a probabilistic method to predict the posterior distribution of
camera poses given an observed image. Our proposed training strategy results in
a generative model of camera poses given an image, which can be used to draw
samples from the pose posterior distribution. Our method is streamlined and
well-founded in theory and outperforms existing methods on localization in
presence of ambiguities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoWeeder: Unsupervised Weed Mapping through Crop-Row Detection <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasquale De Marinis, Rino Vessio, Giovanna Castellano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision agriculture relies heavily on effective weed management to ensure
robust crop yields. This study presents RoWeeder, an innovative framework for
unsupervised weed mapping that combines crop-row detection with a
noise-resilient deep learning model. By leveraging crop-row information to
create a pseudo-ground truth, our method trains a lightweight deep learning
model capable of distinguishing between crops and weeds, even in the presence
of noisy data. Evaluated on the WeedMap dataset, RoWeeder achieves an F1 score
of 75.3, outperforming several baselines. Comprehensive ablation studies
further validated the model's performance. By integrating RoWeeder with drone
technology, farmers can conduct real-time aerial surveys, enabling precise weed
management across large fields. The code is available at:
\url{https://github.com/pasqualedem/RoWeeder}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Vision for Plant Phenotyping and Agriculture (CVPPA)
  workshop at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of marker-less 2D <span class="highlight-title">image</span>-based methods for infant pose
  estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Jahn, Sarah Flügge, Dajie Zhang, Luise Poustka, Sven Bölte, Florentin Wörgötter, Peter B Marschik, Tomas Kulvicius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are increasing efforts to automate clinical methods for early diagnosis
of developmental disorders, among them the General Movement Assessment (GMA), a
video-based tool to classify infant motor functioning. Optimal pose estimation
is a crucial part of the automated GMA. In this study we compare the
performance of available generic- and infant-pose estimators, and the choice of
viewing angle for optimal recordings, i.e., conventional diagonal view used in
GMA vs. top-down view. For this study, we used 4500 annotated video-frames from
75 recordings of infant spontaneous motor functions from 4 to 26 weeks. To
determine which available pose estimation method and camera angle yield the
best pose estimation accuracy on infants in a GMA related setting, the distance
to human annotations as well as the percentage of correct key-points (PCK) were
computed and compared. The results show that the best performing generic model
trained on adults, ViTPose, also performs best on infants. We see no
improvement from using specialized infant-pose estimators over the generic pose
estimators on our own infant dataset. However, when retraining a generic model
on our data, there is a significant improvement in pose estimation accuracy.
The pose estimation accuracy obtained from the top-down view is significantly
better than that obtained from the diagonal view, especially for the detection
of the hip key-points. The results also indicate only limited generalization
capabilities of infant-pose estimators to other infant datasets, which hints
that one should be careful when choosing infant pose estimators and using them
on infant datasets which they were not trained on. While the standard GMA
method uses a diagonal view for assessment, pose estimation accuracy
significantly improves using a top-down view. This suggests that a top-down
view should be included in recording setups for automated GMA research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis has advanced significantly with the development of
neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,
achieving high quality without compromising real-time rendering remains
challenging, particularly for physically-based ray tracing with view-dependent
effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D
spatial-angular representation to better incorporate view-dependent effects,
but the Gaussian representation and control scheme are sub-optimal. In this
paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),
which enhances color and opacity representations and leverages the additional
directional information in the 6D space for optimized Gaussian control. Our
approach is fully compatible with the 3DGS framework and significantly improves
real-time radiance field rendering by better modeling view-dependent effects
and fine details. Experiments demonstrate that 6DGS significantly outperforms
3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction
of 66.5% Gaussian points compared to 3DGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo Video: https://www.youtube.com/watch?v=77wN-K6Q9aM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L-C4: Language-Based Video Colorization for Creative and Consistent
  Color 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chang, Shuchen Weng, Huan Ouyang, Yu Li, Si Li, Boxin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic video colorization is inherently an ill-posed problem because each
monochrome frame has multiple optional color candidates. Previous
exemplar-based video colorization methods restrict the user's imagination due
to the elaborate retrieval process. Alternatively, conditional image
colorization methods combined with post-processing algorithms still struggle to
maintain temporal consistency. To address these issues, we present
Language-based video Colorization for Creative and Consistent Colors (L-C4) to
guide the colorization process using user-provided language descriptions. Our
model is built upon a pre-trained cross-modality generative model, leveraging
its comprehensive language understanding and robust color representation
abilities. We introduce the cross-modality pre-fusion module to generate
instance-aware text embeddings, enabling the application of creative colors.
Additionally, we propose temporally deformable attention to prevent flickering
or color shifts, and cross-clip fusion to maintain long-term color consistency.
Extensive experimental results demonstrate that L-C4 outperforms relevant
methods, achieving semantically accurate colors, unrestricted creative
correspondence, and temporally robust consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Directions for Text-guided 3D Face <span class="highlight-title">Editing</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Yichao Yan, Sehngqi Liu, Yuhao Cheng, Weiming Zhao, Lincheng Li, Mengxiao Bi, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D face editing is a significant task in multimedia, aimed at the
manipulation of 3D face models across various control signals. The success of
3D-aware GAN provides expressive 3D models learned from 2D single-view images
only, encouraging researchers to discover semantic editing directions in its
latent space. However, previous methods face challenges in balancing quality,
efficiency, and generalization. To solve the problem, we explore the
possibility of introducing the strength of diffusion model into 3D-aware GANs.
In this paper, we present Face Clan, a fast and text-general approach for
generating and manipulating 3D faces based on arbitrary attribute descriptions.
To achieve disentangled editing, we propose to diffuse on the latent space
under a pair of opposite prompts to estimate the mask indicating the region of
interest on latent codes. Based on the mask, we then apply denoising to the
masked latent codes to reveal the editing direction. Our method offers a
precisely controllable manipulation method, allowing users to intuitively
customize regions of interest with the text description. Experiments
demonstrate the effectiveness and generalization of our Face Clan for various
pre-trained GANs. It offers an intuitive and wide application for text-guided
face editing that contributes to the landscape of multimedia content creation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Efficient Variants of Segment Anything Model: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorui Sun, Jun Liu, Heng Tao Shen, Xiaofeng Zhu, Ping Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) is a foundational model for image
segmentation tasks, known for its strong generalization across diverse
applications. However, its impressive performance comes with significant
computational and resource demands, making it challenging to deploy in
resource-limited environments such as mobile devices. To address this, a
variety of SAM variants have been proposed to enhance efficiency without
sacrificing accuracy. This survey provides the first comprehensive review of
these efficient SAM variants. We begin by exploring the motivations driving
this research. We then present core techniques used in SAM and model
acceleration. This is followed by an in-depth analysis of various acceleration
strategies, categorized by approach. Finally, we offer a unified and extensive
evaluation of these methods, assessing their efficiency and accuracy on
representative benchmarks, and providing a clear comparison of their overall
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Report in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Ship Recognition and Georeferencing for the Improvement of
  Maritime Situational Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borja Carrillo Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where maritime infrastructures are crucial, advanced situational
awareness solutions are increasingly important. The use of optical camera
systems can allow real-time usage of maritime footage. This thesis presents an
investigation into leveraging deep learning and computer vision to advance
real-time ship recognition and georeferencing for the improvement of maritime
situational awareness. A novel dataset, ShipSG, is introduced, containing 3,505
images and 11,625 ship masks with corresponding class and geographic position.
After an exploration of state-of-the-art, a custom real-time segmentation
architecture, ScatYOLOv8+CBAM, is designed for the NVIDIA Jetson AGX Xavier
embedded system. This architecture adds the 2D scattering transform and
attention mechanisms to YOLOv8, achieving an mAP of 75.46% and an 25.3 ms per
frame, outperforming state-of-the-art methods by over 5%. To improve small and
distant ship recognition in high-resolution images on embedded systems, an
enhanced slicing mechanism is introduced, improving mAP by 8% to 11%.
Additionally, a georeferencing method is proposed, achieving positioning errors
of 18 m for ships up to 400 m away and 44 m for ships between 400 m and 1200 m.
The findings are also applied in real-world scenarios, such as the detection of
abnormal ship behaviour, camera integrity assessment and 3D reconstruction. The
approach of this thesis outperforms existing methods and provides a framework
for integrating recognized and georeferenced ships into real-time systems,
enhancing operational effectiveness and decision-making for maritime
stakeholders. This thesis contributes to the maritime computer vision field by
establishing a benchmark for ship segmentation and georeferencing research,
demonstrating the viability of deep-learning-based recognition and
georeferencing methods for real-time maritime monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next state prediction gives rise to entangled, yet compositional
  representations of objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tankred Saanum, Luca M. Schulze Buschoff, Peter Dayan, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional representations are thought to enable humans to generalize
across combinatorially vast state spaces. Models with learnable object slots,
which encode information about objects in separate latent codes, have shown
promise for this type of generalization but rely on strong architectural
priors. Models with distributed representations, on the other hand, use
overlapping, potentially entangled neural codes, and their ability to support
compositional generalization remains underexplored. In this paper we examine
whether distributed models can develop linearly separable representations of
objects, like slotted models, through unsupervised training on videos of object
interactions. We show that, surprisingly, models with distributed
representations often match or outperform models with object slots in
downstream prediction tasks. Furthermore, we find that linearly separable
object representations can emerge without object-centric priors, with auxiliary
objectives like next-state prediction playing a key role. Finally, we observe
that distributed models' object representations are never fully disentangled,
even if they are linearly separable: Multiple objects can be encoded through
partially overlapping neural populations while still being highly separable
with a linear classifier. We hypothesize that maintaining partially shared
codes enables distributed models to better compress object dynamics,
potentially enhancing generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with
  <span class="highlight-title">Image</span> and Point Cloud Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijie Wang, Qiyu Kang, Rui She, Kai Zhao, Yang Song, Wee Peng Tay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition plays a crucial role in the fields of robotics and computer
vision, finding applications in areas such as autonomous driving, mapping, and
localization. Place recognition identifies a place using query sensor data and
a known database. One of the main challenges is to develop a model that can
deliver accurate results while being robust to environmental variations. We
propose two multi-modal place recognition models, namely PRFusion and
PRFusion++. PRFusion utilizes global fusion with manifold metric attention,
enabling effective interaction between features without requiring camera-LiDAR
extrinsic calibrations. In contrast, PRFusion++ assumes the availability of
extrinsic calibrations and leverages pixel-point correspondences to enhance
feature learning on local windows. Additionally, both models incorporate neural
diffusion layers, which enable reliable operation even in challenging
environments. We verify the state-of-the-art performance of both models on
three large-scale benchmarks. Notably, they outperform existing models by a
substantial margin of +3.0 AR@1 on the demanding Boreas dataset. Furthermore,
we conduct ablation studies to validate the effectiveness of our proposed
methods. The codes are available at: https://github.com/sijieaaa/PRFusion
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE TITS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniBooth: Learning Latent Control for <span class="highlight-title">Image</span> Synthesis with Multi-modal
  Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leheng Li, Weichao Qiu, Xu Yan, Jing He, Kaiqiang Zhou, Yingjie Cai, Qing Lian, Bingbing Liu, Ying-Cong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OmniBooth, an image generation framework that enables spatial
control with instance-level multi-modal customization. For all instances, the
multimodal instruction can be described through text prompts or image
references. Given a set of user-defined masks and associated text or image
guidance, our objective is to generate an image, where multiple objects are
positioned at specified coordinates and their attributes are precisely aligned
with the corresponding guidance. This approach significantly expands the scope
of text-to-image generation, and elevates it to a more versatile and practical
dimension in controllability. In this paper, our core contribution lies in the
proposed latent control signals, a high-dimensional spatial feature that
provides a unified representation to integrate the spatial, textual, and image
conditions seamlessly. The text condition extends ControlNet to provide
instance-level open-vocabulary generation. The image condition further enables
fine-grained control with personalized identity. In practice, our method
empowers users with more flexibility in controllable generation, as users can
choose multi-modal conditions from text or images as needed. Furthermore,
thorough experiments demonstrate our enhanced performance in image synthesis
fidelity and alignment across different tasks and datasets. Project page:
https://len-li.github.io/omnibooth-web/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Art2Mus: Bridging Visual Arts and Music through Cross-Modal <span class="highlight-title">Generation</span> <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Rinaldi, Nicola Fanelli, Giovanna Castellano, Gennaro Vessio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence and generative models have revolutionized music
creation, with many models leveraging textual or visual prompts for guidance.
However, existing image-to-music models are limited to simple images, lacking
the capability to generate music from complex digitized artworks. To address
this gap, we introduce $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$, a novel
model designed to create music from digitized artworks or text inputs.
$\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$ extends the AudioLDM~2
architecture, a text-to-audio model, and employs our newly curated datasets,
created via ImageBind, which pair digitized artworks with music. Experimental
results demonstrate that $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$ can
generate music that resonates with the input stimuli. These findings suggest
promising applications in multimedia art, interactive installations, and
AI-driven creative tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D-PoSE: Depth as an Intermediate Representation for 3D Human Pose and
  Shape Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Vasilikopoulos, Drosakis Drosakis, Antonis Argyros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present D-PoSE (Depth as an Intermediate Representation for 3D Human Pose
and Shape Estimation), a one-stage method that estimates human pose and SMPL-X
shape parameters from a single RGB image. Recent works use larger models with
transformer backbones and decoders to improve the accuracy in human pose and
shape (HPS) benchmarks. D-PoSE proposes a vision based approach that uses the
estimated human depth-maps as an intermediate representation for HPS and
leverages training with synthetic data and the ground-truth depth-maps provided
with them for depth supervision during training. Although trained on synthetic
datasets, D-PoSE achieves state-of-the-art performance on the real-world
benchmark datasets, EMDB and 3DPW. Despite its simple lightweight design and
the CNN backbone, it outperforms ViT-based models that have a number of
parameters that is larger by almost an order of magnitude. D-PoSE code is
available at: https://github.com/nvasilik/D-PoSE
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch is Enough: Naturalistic Adversarial Patch against Vision-Language
  Pre-training Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehong Kong, Siyuan Liang, Xiaopeng Zhu, Yuansheng Zhong, Wenqi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language pre-training (VLP) models have demonstrated significant
success across various domains, yet they remain vulnerable to adversarial
attacks. Addressing these adversarial vulnerabilities is crucial for enhancing
security in multimodal learning. Traditionally, adversarial methods targeting
VLP models involve simultaneously perturbing images and text. However, this
approach faces notable challenges: first, adversarial perturbations often fail
to translate effectively into real-world scenarios; second, direct
modifications to the text are conspicuously visible. To overcome these
limitations, we propose a novel strategy that exclusively employs image patches
for attacks, thus preserving the integrity of the original text. Our method
leverages prior knowledge from diffusion models to enhance the authenticity and
naturalness of the perturbations. Moreover, to optimize patch placement and
improve the efficacy of our attacks, we utilize the cross-attention mechanism,
which encapsulates intermodal interactions by generating attention maps to
guide strategic patch placements. Comprehensive experiments conducted in a
white-box setting for image-to-text scenarios reveal that our proposed method
significantly outperforms existing techniques, achieving a 100% attack success
rate. Additionally, it demonstrates commendable performance in transfer tasks
involving text-to-image configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by Visual Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved detection of discarded fish species through BoxAL active
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Sokolova, Pieter M. Blok, Angelo Mencarelli, Arjan Vroegop, Aloysius van Helmond, Gert Kootstra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, powerful data-driven deep-learning techniques have been
developed and applied for automated catch registration. However, these methods
are dependent on the labelled data, which is time-consuming, labour-intensive,
expensive to collect and need expert knowledge. In this study, we present an
active learning technique, named BoxAL, which includes estimation of epistemic
certainty of the Faster R-CNN object-detection model. The method allows
selecting the most uncertain training images from an unlabeled pool, which are
then used to train the object-detection model. To evaluate the method, we used
an open-source image dataset obtained with a dedicated image-acquisition system
developed for commercial trawlers targeting demersal species. We demonstrated,
that our approach allows reaching the same object-detection performance as with
the random sampling using 400 fewer labelled images. Besides, mean AP score was
significantly higher at the last training iteration with 1100 training images,
specifically, 39.0&plusmn;1.6 and 34.8&plusmn;1.8 for certainty-based sampling
and random sampling, respectively. Additionally, we showed that epistemic
certainty is a suitable method to sample images that the current iteration of
the model cannot deal with yet. Our study additionally showed that the sampled
new data is more valuable for training than the remaining unlabeled data. Our
software is available on https://github.com/pieterblok/boxal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeX-NeRF: Neural Radiance Fields from Pseudo-TeX Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chonghao Zhong, Chao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) has gained significant attention for its
exceptional visual effects. However, most existing NeRF methods reconstruct 3D
scenes from RGB images captured by visible light cameras. In practical
scenarios like darkness, low light, or bad weather, visible light cameras
become ineffective. Therefore, we propose TeX-NeRF, a 3D reconstruction method
using only infrared images, which introduces the object material emissivity as
a priori, preprocesses the infrared images using Pseudo-TeX vision, and maps
the temperatures (T), emissivities (e), and textures (X) of the scene into the
saturation (S), hue (H), and value (V) channels of the HSV color space,
respectively. Novel view synthesis using the processed images has yielded
excellent results. Additionally, we introduce 3D-TeX Datasets, the first
dataset comprising infrared images and their corresponding Pseudo-TeX vision
images. Experiments demonstrate that our method not only matches the quality of
scene reconstruction achieved with high-quality RGB images but also provides
accurate temperature estimations for objects in the scene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Art Forgery Detection using Kolmogorov Arnold and Convolutional Neural
  Networks <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandro Boccuzzo, Deborah Desirée Meyer, Ludovica Schaerf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Art authentication has historically established itself as a task requiring
profound connoisseurship of one particular artist. Nevertheless, famous art
forgers such as Wolfgang Beltracchi were able to deceive dozens of art experts.
In recent years Artificial Intelligence algorithms have been successfully
applied to various image processing tasks. In this work, we leverage the
growing improvements in AI to present an art authentication framework for the
identification of the forger Wolfgang Beltracchi. Differently from existing
literature on AI-aided art authentication, we focus on a specialized model of a
forger, rather than an artist, flipping the approach of traditional AI methods.
We use a carefully compiled dataset of known artists forged by Beltracchi and a
set of known works by the forger to train a multiclass image classification
model based on EfficientNet. We compare the results with Kolmogorov Arnold
Networks (KAN) which, to the best of our knowledge, have never been tested in
the art domain. The results show a general agreement between the different
models' predictions on artworks flagged as forgeries, which are then closely
studied using visual analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024 workshop AI4VA, oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Context Adjustment Loss for Learned <span class="highlight-title">Image</span> Compression <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Han, Shiyin Jiang, Shengxi Li, Xin Deng, Mai Xu, Ce Zhu, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, learned image compression (LIC) technologies have surpassed
conventional methods notably in terms of rate-distortion (RD) performance. Most
present learned techniques are VAE-based with an autoregressive entropy model,
which obviously promotes the RD performance by utilizing the decoded causal
context. However, extant methods are highly dependent on the fixed hand-crafted
causal context. The question of how to guide the auto-encoder to generate a
more effective causal context benefit for the autoregressive entropy models is
worth exploring. In this paper, we make the first attempt in investigating the
way to explicitly adjust the causal context with our proposed Causal Context
Adjustment loss (CCA-loss). By imposing the CCA-loss, we enable the neural
network to spontaneously adjust important information into the early stage of
the autoregressive entropy model. Furthermore, as transformer technology
develops remarkably, variants of which have been adopted by many
state-of-the-art (SOTA) LIC techniques. The existing computing devices have not
adapted the calculation of the attention mechanism well, which leads to a
burden on computation quantity and inference latency. To overcome it, we
establish a convolutional neural network (CNN) image compression model and
adopt the unevenly channel-wise grouped strategy for high efficiency.
Ultimately, the proposed CNN-based LIC network trained with our Causal Context
Adjustment loss attains a great trade-off between inference latency and
rate-distortion performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PostEdit: Posterior Sampling for Efficient Zero-Shot <span class="highlight-title">Image</span> <span class="highlight-title">Editing</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Tian, Yixuan Li, Yichao Yan, Shanyan Guan, Yanhao Ge, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of image editing, three core challenges persist:
controllability, background preservation, and efficiency. Inversion-based
methods rely on time-consuming optimization to preserve the features of the
initial images, which results in low efficiency due to the requirement for
extensive network inference. Conversely, inversion-free methods lack
theoretical support for background similarity, as they circumvent the issue of
maintaining initial features to achieve efficiency. As a consequence, none of
these methods can achieve both high efficiency and background consistency. To
tackle the challenges and the aforementioned disadvantages, we introduce
PostEdit, a method that incorporates a posterior scheme to govern the diffusion
sampling process. Specifically, a corresponding measurement term related to
both the initial features and Langevin dynamics is introduced to optimize the
estimated image generated by the given target prompt. Extensive experimental
results indicate that the proposed PostEdit achieves state-of-the-art editing
performance while accurately preserving unedited regions. Furthermore, the
method is both inversion- and training-free, necessitating approximately 1.5
seconds and 18 GB of GPU memory to generate high-quality results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple <span class="highlight-title">Image</span> Segmentation Framework via In-Context Examples <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Chenchen Jing, Hengtao Li, Muzhi Zhu, Hao Chen, Xinlong Wang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there have been explorations of generalist segmentation models that
can effectively tackle a variety of image segmentation tasks within a unified
in-context learning framework. However, these methods still struggle with task
ambiguity in in-context segmentation, as not all in-context examples can
accurately convey the task information. In order to address this issue, we
present SINE, a simple image Segmentation framework utilizing in-context
examples. Our approach leverages a Transformer encoder-decoder structure, where
the encoder provides high-quality image representations, and the decoder is
designed to yield multiple task-specific output masks to effectively eliminate
task ambiguity. Specifically, we introduce an In-context Interaction module to
complement in-context information and produce correlations between the target
image and the in-context example and a Matching Transformer that uses fixed
matching and a Hungarian algorithm to eliminate differences between different
tasks. In addition, we have further perfected the current evaluation system for
in-context image segmentation, aiming to facilitate a holistic appraisal of
these models. Experiments on various segmentation tasks show the effectiveness
of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Proc. Conference on Neural Information Processing Systems
  (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/SINE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multimodal</span> Fusion Strategies for Mapping Biophysical Landscape Features <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Gordon, Nico Lang, Catherine Ressijac, Andrew Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal aerial data are used to monitor natural systems, and machine
learning can significantly accelerate the classification of landscape features
within such imagery to benefit ecology and conservation. It remains
under-explored, however, how these multiple modalities ought to be fused in a
deep learning model. As a step towards filling this gap, we study three
strategies (Early fusion, Late fusion, and Mixture of Experts) for fusing
thermal, RGB, and LiDAR imagery using a dataset of spatially-aligned
orthomosaics in these three modalities. In particular, we aim to map three
ecologically-relevant biophysical landscape features in African savanna
ecosystems: rhino middens, termite mounds, and water. The three fusion
strategies differ in whether the modalities are fused early or late, and if
late, whether the model learns fixed weights per modality for each class or
generates weights for each class adaptively, based on the input. Overall, the
three methods have similar macro-averaged performance with Late fusion
achieving an AUC of 0.698, but their per-class performance varies strongly,
with Early fusion achieving the best recall for middens and water and Mixture
of Experts achieving the best recall for mounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, ECCV 2024 Workshop in CV for Ecology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAT: Concept-level backdoor ATtacks for Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songning Lai, Jiayu Yang, Yu Huang, Lijie Hu, Tianlang Xue, Zhangyi Hu, Jiaxu Li, Haicheng Liao, Yutao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the transformative impact of deep learning across multiple domains,
the inherent opacity of these models has driven the development of Explainable
Artificial Intelligence (XAI). Among these efforts, Concept Bottleneck Models
(CBMs) have emerged as a key approach to improve interpretability by leveraging
high-level semantic information. However, CBMs, like other machine learning
models, are susceptible to security threats, particularly backdoor attacks,
which can covertly manipulate model behaviors. Understanding that the community
has not yet studied the concept level backdoor attack of CBM, because of
"Better the devil you know than the devil you don't know.", we introduce CAT
(Concept-level Backdoor ATtacks), a methodology that leverages the conceptual
representations within CBMs to embed triggers during training, enabling
controlled manipulation of model predictions at inference time. An enhanced
attack pattern, CAT+, incorporates a correlation function to systematically
select the most effective and stealthy concept triggers, thereby optimizing the
attack's impact. Our comprehensive evaluation framework assesses both the
attack success rate and stealthiness, demonstrating that CAT and CAT+ maintain
high performance on clean data while achieving significant targeted effects on
backdoored datasets. This work underscores the potential security risks
associated with CBMs and provides a robust testing methodology for future
security assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-Efficient Multiview Perception: Integrating Semantic Masking
  with Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosta Dakic, Kanchana Thilakarathna, Rodrigo N. Calheiros, Teng Joon Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiview systems have become a key technology in modern computer vision,
offering advanced capabilities in scene understanding and analysis. However,
these systems face critical challenges in bandwidth limitations and
computational constraints, particularly for resource-limited camera nodes like
drones. This paper presents a novel approach for communication-efficient
distributed multiview detection and tracking using masked autoencoders (MAEs).
We introduce a semantic-guided masking strategy that leverages pre-trained
segmentation models and a tunable power function to prioritize informative
image regions. This approach, combined with an MAE, reduces communication
overhead while preserving essential visual information. We evaluate our method
on both virtual and real-world multiview datasets, demonstrating comparable
performance in terms of detection and tracking performance metrics compared to
state-of-the-art techniques, even at high masking ratios. Our selective masking
algorithm outperforms random masking, maintaining higher accuracy and precision
as the masking ratio increases. Furthermore, our approach achieves a
significant reduction in transmission data volume compared to baseline methods,
thereby balancing multiview tracking performance with communication efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Efficient and Effective Trajectories for Differential
  Equation-based <span class="highlight-title">Image</span> Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Zhu, Jinhui Hou, Hui Liu, Huanqiang Zeng, Junhui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The differential equation-based image restoration approach aims to establish
learnable trajectories connecting high-quality images to a tractable
distribution, e.g., low-quality images or a Gaussian distribution. In this
paper, we reformulate the trajectory optimization of this kind of method,
focusing on enhancing both reconstruction quality and efficiency. Initially, we
navigate effective restoration paths through a reinforcement learning process,
gradually steering potential trajectories toward the most precise options.
Additionally, to mitigate the considerable computational burden associated with
iterative sampling, we propose cost-aware trajectory distillation to streamline
complex paths into several manageable steps with adaptable sizes. Moreover, we
fine-tune a foundational diffusion model (FLUX) with 12B parameters by using
our algorithms, producing a unified framework for handling 7 kinds of image
restoration tasks. Extensive experiments showcase the significant superiority
of the proposed method, achieving a maximum PSNR improvement of 2.1 dB over
state-of-the-art methods, while also greatly enhancing visual perceptual
quality. Project page: \url{https://zhu-zhiyu.github.io/FLUX-IR/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mDPO: Conditional Preference Optimization for <span class="highlight-title">Multimodal</span> Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/mDPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00700v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00700v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Savadikar, Xi Song, Tianfu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Generative Parameter-Efficient Fine-Tuning (GIFT) for adapting
pretrained Transformer backbones on downstream tasks. GIFT learns to generate
the fine-tuned weights for a layer directly from its pretrained weights. The
GIFT network is parameterized in a minimally-simple way by two linear layers
(without bias terms), and is shared by different pretrained layers selected for
fine-tuning (e.g., the Query layers), which result in significantly fewer
trainable parameters compared to the layer-specific methods like Low-Rank
Adapter (LoRA). We also show this formulation bridges parameter-efficient
fine-tuning and representation fine-tuning. We perform comprehensive
experiments on natural language tasks (commonsense and arithmetic reasoning,
instruction tuning, and sequence classification) and computer vision tasks
(fine-grained classification). We obtain the best performance and parameter
efficiency among baselines on commonsense and arithmetic reasoning, and
instruction following using the Llama family of models and on visual
recognition benchmarks using Vision Transformers. Notably, compared to LoRA, we
obtain 5.7% absolute increase in average accuracy with 14 times reduction of
parameters on Commonsense170k using Llama-3 (8B), and 5.4% absolute increase in
the win rate with 4 times reduction of parameters using Llama-2 (7B) during
instruction tuning. Our GIFT also obtains a slightly higher win rate on
instruction tuning than GPT 3.5 (Turbo 1106).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: https://savadikarc.github.io/gift</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-free meets 3D priors: Novel View Synthesis from a Single <span class="highlight-title">Image</span> with
  Pretrained <span class="highlight-title">Diffusion</span> Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taewon Kang, Divya Kothandaraman, Dinesh Manocha, Ming C. Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent 3D novel view synthesis (NVS) methods are limited to
single-object-centric scenes and struggle with complex environments. They often
require extensive 3D data for training, lacking generalization beyond the
training distribution. Conversely, 3D-free methods can generate text-controlled
views of complex, in-the-wild scenes using a pretrained stable diffusion model
without the need for a large amount of 3D-based training data, but lack camera
control. In this paper, we introduce a method capable of generating
camera-controlled viewpoints from a single input image, by combining the
benefits of 3D-free and 3D-based approaches. Our method excels in handling
complex and diverse scenes without extensive training or additional 3D and
multiview data. It leverages widely available pretrained NVS models for weak
guidance, integrating this knowledge into a 3D-free view synthesis approach to
achieve the desired results. Experimental results demonstrate that our method
outperforms existing models in both qualitative and quantitative evaluations,
providing high-fidelity and consistent novel view synthesis at desired camera
angles across a wide variety of scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures, v2: analysis studies and more results added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding Visual Task Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Hojel, Yutong Bai, Trevor Darrell, Amir Globerson, Amir Bar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompting is a technique for teaching models to perform a visual task
via in-context examples, without any additional training. In this work, we
analyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find
task vectors, activations that encode task-specific information. Equipped with
this insight, we demonstrate that it is possible to identify the task vectors
and use them to guide the network towards performing different tasks without
providing any input-output examples. To find task vectors, we compute the
average intermediate activations per task and use the REINFORCE algorithm to
search for the subset of task vectors. The resulting task vectors guide the
model towards performing a task better than the original model without the need
for input-output examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/alhojel/visual_task_vectors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Narrative <span class="highlight-title">Review</span> of <span class="highlight-title">Image</span> Processing Techniques Related to Prostate
  Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiqiao Wang, Hong Wu, Zhuoyuan Wang, Peiyan Yue, Dong Ni, Pheng-Ann Heng, Yi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer (PCa) poses a significant threat to men's health, with early
diagnosis being crucial for improving prognosis and reducing mortality rates.
Transrectal ultrasound (TRUS) plays a vital role in the diagnosis and
image-guided intervention of PCa.To facilitate physicians with more accurate
and efficient computer-assisted diagnosis and interventions, many image
processing algorithms in TRUS have been proposed and achieved state-of-the-art
performance in several tasks, including prostate gland segmentation, prostate
image registration, PCa classification and detection, and interventional needle
detection. The rapid development of these algorithms over the past two decades
necessitates a comprehensive summary. In consequence, this survey provides a
\textcolor{blue}{narrative } analysis of this field, outlining the evolution of
image processing methods in the context of TRUS image analysis and meanwhile
highlighting their relevant contributions. Furthermore, this survey discusses
current challenges and suggests future research directions to possibly advance
this field further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Ultrasound in Medicine & Biology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For <span class="highlight-title">Generation</span> Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship
  Modeling in Aerial Videos <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Thuan Nguyen, Pha Nguyen, Xin Li, Jackson Cothren, Alper Yilmaz, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video scene graph generation (VidSGG) has emerged as a transformative
approach to capturing and interpreting the intricate relationships among
objects and their temporal dynamics in video sequences. In this paper, we
introduce the new AeroEye dataset that focuses on multi-object relationship
modeling in aerial videos. Our AeroEye dataset features various drone scenes
and includes a visually comprehensive and precise collection of predicates that
capture the intricate relationships and spatial arrangements among objects. To
this end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that
allows the model to capture both direct and long-range temporal dependencies by
continuously updating the history of interactions in a circular manner. The
proposed approach also allows one to handle sequences with inherent cyclical
patterns and process object relationships in the correct sequential order.
Therefore, it can effectively capture periodic and overlapping relationships
while minimizing information loss. The extensive experiments on the AeroEye
dataset demonstrate the effectiveness of the proposed CYCLO model,
demonstrating its potential to perform scene understanding on drone videos.
Finally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results
on two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GMAI-MMBench: A Comprehensive <span class="highlight-title">Multimodal</span> Evaluation <span class="highlight-title">Benchmark</span> Towards
  General Medical AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03361v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03361v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, Benyou Wang, Shaoting Zhang, Bin Fu, Jianfei Cai, Bohan Zhuang, Eric J Seibel, Junjun He, <span class="highlight-author">Yu Qiao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are capable of handling diverse data
types such as imaging, text, and physiological signals, and can be applied in
various fields. In the medical field, LVLMs have a high potential to offer
substantial assistance for diagnosis and treatment. Before that, it is crucial
to develop benchmarks to evaluate LVLMs' effectiveness in various medical
applications. Current benchmarks are often built upon specific academic
literature, mainly focusing on a single domain, and lacking varying perceptual
granularities. Thus, they face specific challenges, including limited clinical
relevance, incomplete evaluations, and insufficient guidance for interactive
LVLMs. To address these limitations, we developed the GMAI-MMBench, the most
comprehensive general medical AI benchmark with well-categorized data structure
and multi-perceptual granularity to date. It is constructed from 284 datasets
across 38 medical image modalities, 18 clinical-related tasks, 18 departments,
and 4 perceptual granularities in a Visual Question Answering (VQA) format.
Additionally, we implemented a lexical tree structure that allows users to
customize evaluation tasks, accommodating various assessment needs and
substantially supporting medical AI research and applications. We evaluated 50
LVLMs, and the results show that even the advanced GPT-4o only achieves an
accuracy of 53.96%, indicating significant room for improvement. Moreover, we
identified five key insufficiencies in current cutting-edge LVLMs that need to
be addressed to advance the development of better medical applications. We
believe that GMAI-MMBench will stimulate the community to build the next
generation of LVLMs toward GMAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub: https://github.com/uni-medical/GMAI-MMBench; Hugging face:
  https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust <span class="highlight-title">Multimodal</span> Learning with Missing Modalities via
  Parameter-Efficient Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03986v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03986v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning seeks to utilize data from multiple sources to improve
the overall performance of downstream tasks. It is desirable for redundancies
in the data to make multimodal systems robust to missing or corrupted
observations in some correlated modalities. However, we observe that the
performance of several existing multimodal networks significantly deteriorates
if one or multiple modalities are absent at test time. To enable robustness to
missing modalities, we propose a simple and parameter-efficient adaptation
procedure for pretrained multimodal networks. In particular, we exploit
modulation of intermediate features to compensate for the missing modalities.
We demonstrate that such adaptation can partially bridge performance drop due
to missing modalities and outperform independent, dedicated networks trained
for the available modality combinations in some cases. The proposed adaptation
requires extremely small number of parameters (e.g., fewer than 1% of the total
parameters) and applicable to a wide range of modality combinations and tasks.
We conduct a series of experiments to highlight the missing modality robustness
of our proposed method on five different multimodal tasks across seven
datasets. Our proposed method demonstrates versatility across various tasks and
datasets, and outperforms existing methods for robust multimodal learning with
missing modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). 28 pages, 6 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit
  sensitivity maps <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Frederik Zimmermann, Andreas Kofler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel learned image reconstruction method for accelerated
cardiac MRI with multiple receiver coils based on deep convolutional neural
networks (CNNs) and algorithm unrolling. In contrast to many existing learned
MR image reconstruction techniques that necessitate coil-sensitivity map (CSM)
estimation as a distinct network component, our proposed approach avoids
explicit CSM estimation. Instead, it implicitly captures and learns to exploit
the inter-coil relationships of the images. Our method consists of a series of
novel learned image and k-space blocks with shared latent information and
adaptation to the acquisition parameters by feature-wise modulation (FiLM), as
well as coil-wise data-consistency (DC) blocks.
  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920
and 0.942 in the cine track and mapping track validation leaderboard of the
MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different
teams at the time of writing.
  Code will be made available at https://github.com/fzimmermann89/CMRxRecon
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI STACOM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating the Maze of Explainable AI: A Systematic Approach to
  Evaluating Methods and Metrics <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Klein, Carsten T. Lüth, Udo Schlegel, Till J. Bungert, Mennatallah El-Assady, Paul F. Jäger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed
methods as well as metrics aiming to evaluate their efficacy. However, current
studies are often of limited scope, examining only a handful of XAI methods and
ignoring underlying design parameters for performance, such as the model
architecture or the nature of input data. Moreover, they often rely on one or a
few metrics and neglect thorough validation, increasing the risk of selection
bias and ignoring discrepancies among metrics. These shortcomings leave
practitioners confused about which method to choose for their problem. In
response, we introduce LATEC, a large-scale benchmark that critically evaluates
17 prominent XAI methods using 20 distinct metrics. We systematically
incorporate vital design parameters like varied architectures and diverse input
modalities, resulting in 7,560 examined combinations. Through LATEC, we
showcase the high risk of conflicting metrics leading to unreliable rankings
and consequently propose a more robust evaluation scheme. Further, we
comprehensively evaluate various XAI methods to assist practitioners in
selecting appropriate methods aligning with their needs. Curiously, the
emerging top-performing method, Expected Gradients, is not examined in any
relevant related study. LATEC reinforces its role in future XAI research by
publicly releasing all 326k saliency maps and 378k metric scores as a
(meta-)evaluation dataset. The benchmark is hosted at:
https://github.com/IML-DKFZ/latec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Persistent Test-time Adaptation in Recurring Testing Scenarios <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18193v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18193v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung-Hieu Hoang, Duc Minh Vo, Minh N. Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current test-time adaptation (TTA) approaches aim to adapt to environments
that change continuously. Yet, it is unclear whether TTA methods can maintain
their adaptability over prolonged periods. To answer this question, we
introduce a diagnostic setting - **recurring TTA** where environments not only
change but also recur over time, creating an extensive data stream. This
setting allows us to examine the error accumulation of TTA models, in the most
basic scenario, when they are regularly exposed to previous testing
environments. Furthermore, we simulate a TTA process on a simple yet
representative $\epsilon$-**perturbed Gaussian Mixture Model Classifier**,
deriving theoretical insights into the dataset- and algorithm-dependent factors
contributing to gradual performance degradation. Our investigation leads us to
propose **persistent TTA (PeTTA)**, which senses when the model is diverging
towards collapse and adjusts the adaptation strategy, striking a balance
between the dual objectives of adaptation and model collapse prevention. The
supreme stability of PeTTA over existing approaches, in the face of lifelong
TTA scenarios, has been demonstrated over comprehensive experiments on various
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoregressive <span class="highlight-title">Image</span> <span class="highlight-title">Diffusion</span>: <span class="highlight-title">Generation</span> of <span class="highlight-title">Image</span> Sequence and
  Application in MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14327v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14327v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanxiong Luo, Shoujin Huang, Martin Uecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance imaging (MRI) is a widely used non-invasive imaging
modality. However, a persistent challenge lies in balancing image quality with
imaging speed. This trade-off is primarily constrained by k-space measurements,
which traverse specific trajectories in the spatial Fourier domain (k-space).
These measurements are often undersampled to shorten acquisition times,
resulting in image artifacts and compromised quality. Generative models learn
image distributions and can be used to reconstruct high-quality images from
undersampled k-space data. In this work, we present the autoregressive image
diffusion (AID) model for image sequences and use it to sample the posterior
for accelerated MRI reconstruction. The algorithm incorporates both
undersampled k-space and pre-existing information. Models trained with fastMRI
dataset are evaluated comprehensively. The results show that the AID model can
robustly generate sequentially coherent image sequences. In MRI applications,
the AID can outperform the standard diffusion model and reduce hallucinations,
due to the learned inter-image dependencies. The project code is available at
https://github.com/mrirecon/aid.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model <span class="highlight-title">Editing</span> of
  <span class="highlight-title">Large Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective Transformer for Hyperspectral <span class="highlight-title">Image</span> Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichu Xu, Di Wang, Lefei Zhang, Liangpei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has achieved satisfactory results in the field of hyperspectral
image (HSI) classification. However, existing Transformer models face two key
challenges when dealing with HSI scenes characterized by diverse land cover
types and rich spectral information: (1) fixed receptive field representation
overlooks effective contextual information; (2) redundant self-attention
feature representation. To address these limitations, we propose a novel
Selective Transformer (SFormer) for HSI classification. The SFormer is designed
to dynamically select receptive fields for capturing both spatial and spectral
contextual information, while mitigating the impact of redundant data by
prioritizing the most relevant features. This enables a highly accurate
classification of the land covers of the HSI. Specifically, a Kernel Selective
Transformer Block (KSTB) is first utilized to dynamically select an appropriate
receptive field range to effectively extract spatial-spectral features.
Furthermore, to capture the most crucial tokens, a Token Selective Transformer
Block (TSTB) is introduced, which selects the most relevant tokens based on the
ranking of attention scores for each query. Extensive experiments on four
benchmark HSI datasets demonstrate that the proposed SFormer outperforms the
state-of-the-art HSI classification models. The codes will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amandine Brunetto, Sascha Hornauer, Fabien Moutarde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound plays a major role in human perception. Along with vision, it provides
essential information for understanding our surroundings. Despite advances in
neural implicit representations, learning acoustics that align with visual
scenes remains a challenge. We propose NeRAF, a method that jointly learns
acoustic and radiance fields. NeRAF synthesizes both novel views and
spatialized room impulse responses (RIR) at new positions by conditioning the
acoustic field on 3D scene geometric and appearance priors from the radiance
field. The generated RIR can be applied to auralize any audio signal. Each
modality can be rendered independently and at spatially distinct positions,
offering greater versatility. We demonstrate that NeRAF generates high-quality
audio on SoundSpaces and RAF datasets, achieving significant performance
improvements over prior methods while being more data-efficient. Additionally,
NeRAF enhances novel view synthesis of complex scenes trained with sparse data
through cross-modal learning. NeRAF is designed as a Nerfstudio module,
providing convenient access to realistic audio-visual generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://amandinebtto.github.io/NeRAF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized
  SAR-ATR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oh-Tae Jang, Hae-Kang Song, Min-Jun Kim, Kyung-Hwan Lee, Geon Lee, Sung-Ho Kim, Hee-Sub Shin, Jae-Woo Ok, Min-Young Back, Jae-Hyuk Yoon, Kyung-Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, computer-aided design models and electromagnetic simulations have
been used to augment synthetic aperture radar (SAR) data for deep learning.
However, an automatic target recognition (ATR) model struggles with domain
shift when using synthetic data because the model learns specific clutter
patterns present in such data, which disturbs performance when applied to
measured data with different clutter distributions. This study proposes a
framework particularly designed for domain-generalized SAR-ATR called IRASNet,
enabling effective feature-level clutter reduction and domain-invariant feature
learning. First, we propose a clutter reduction module (CRM) that maximizes the
signal-to-clutter ratio on feature maps. The module reduces the impact of
clutter at the feature level while preserving target and shadow information,
thereby improving ATR performance. Second, we integrate adversarial learning
with CRM to extract clutter-reduced domain-invariant features. The integration
bridges the gap between synthetic and measured datasets without requiring
measured data during training. Third, we improve feature extraction from target
and shadow regions by implementing a positional supervision task using mask
ground truth encoding. The improvement enhances the ability of the model to
discriminate between classes. Our proposed IRASNet presents new
state-of-the-art public SAR datasets utilizing target and shadow information to
achieve superior performance across various test conditions. IRASNet not only
enhances generalization performance but also significantly improves
feature-level clutter reduction, making it a valuable advancement in the field
of radar image pattern recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High
  Quality and Efficient Rendering <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10389v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10389v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Sario, Riccardo Renzulli, Enzo Tartaglione, Marco Grangetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the introduction of NeRFs, considerable attention has been focused on
improving their training and inference times, leading to the development of
Fast-NeRFs models. Despite demonstrating impressive rendering speed and
quality, the rapid convergence of such models poses challenges for further
improving reconstruction quality. Common strategies to improve rendering
quality involves augmenting model parameters or increasing the number of
sampled points. However, these computationally intensive approaches encounter
limitations in achieving significant quality enhancements. This study
introduces a model-agnostic framework inspired by Sparsely-Gated Mixture of
Experts to enhance rendering quality without escalating computational
complexity. Our approach enables specialization in rendering different scene
components by employing a mixture of experts with varying resolutions. We
present a novel gate formulation designed to maximize expert capabilities and
propose a resolution-based routing technique to effectively induce sparsity and
decompose scenes. Our work significantly improves reconstruction quality while
maintaining competitive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted to the ECCV 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised
  Domain Generalization for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19351v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19351v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryosuke Furuta, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detectors do not work well when domains largely differ between
training and testing data. To overcome this domain gap in object detection
without requiring expensive annotations, we consider two problem settings:
semi-supervised domain generalizable object detection (SS-DGOD) and
weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain
generalization for object detection that requires labeled data from multiple
domains, SS-DGOD and WS-DGOD require labeled data only from one domain and
unlabeled or weakly-labeled data from multiple domains for training. In this
paper, we show that object detectors can be effectively trained on the two
settings with the same Mean Teacher learning framework, where a student network
is trained with pseudo-labels output from a teacher on the unlabeled or
weakly-labeled data. We provide novel interpretations of why the Mean Teacher
learning framework works well on the two settings in terms of the relationships
between the generalization gap and flat minima in parameter space. On the basis
of the interpretations, we also show that incorporating a simple regularization
method into the Mean Teacher learning framework leads to flatter minima. The
experimental results demonstrate that the regularization leads to flatter
minima and boosts the performance of the detectors trained with the Mean
Teacher learning framework on the two settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking the Frame: Visual Place Recognition by Overlap Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wei, Philipp Lindenberger, Jiri Matas, Daniel Barath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition methods struggle with occlusions and partial visual
overlaps. We propose a novel visual place recognition approach based on overlap
prediction, called VOP, shifting from traditional reliance on global image
similarities and local features to image overlap prediction. VOP proceeds
co-visible image sections by obtaining patch-level embeddings using a Vision
Transformer backbone and establishing patch-to-patch correspondences without
requiring expensive feature detection and matching. Our approach uses a voting
mechanism to assess overlap scores for potential database images. It provides a
nuanced image retrieval metric in challenging scenarios. Experimental results
show that VOP leads to more accurate relative pose estimation and localization
results on the retrieved image pairs than state-of-the-art baselines on a
number of large-scale, real-world indoor and outdoor benchmarks. The code is
available at https://github.com/weitong8591/vop.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectrum Extraction and Clipping for Implicitly Linear Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ebrahimpour Boroojeny, Matus Telgarsky, Hari Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show the effectiveness of automatic differentiation in efficiently and
correctly computing and controlling the spectrum of implicitly linear
operators, a rich family of layer types including all standard convolutional
and dense layers. We provide the first clipping method which is correct for
general convolution layers, and illuminate the representational limitation that
caused correctness issues in prior work. We study the effect of the batch
normalization layers when concatenated with convolutional layers and show how
our clipping method can be applied to their composition. By comparing the
accuracy and performance of our algorithms to the state-of-the-art methods,
using various experiments, we show they are more precise and efficient and lead
to better generalization and adversarial robustness. We provide the code for
using our methods at https://github.com/Ali-E/FastClip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-Hider: Hiding Messages into 3D Gaussian Splatting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has already become the emerging research focus
in the fields of 3D scene reconstruction and novel view synthesis. Given that
training a 3DGS requires a significant amount of time and computational cost,
it is crucial to protect the copyright, integrity, and privacy of such 3D
assets. Steganography, as a crucial technique for encrypted transmission and
copyright protection, has been extensively studied. However, it still lacks
profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS
possesses two distinct features: 1) explicit 3D representation; and 2)
real-time rendering speeds. These characteristics result in the 3DGS point
cloud files being public and transparent, with each Gaussian point having a
clear physical significance. Therefore, ensuring the security and fidelity of
the original 3D scene while embedding information into the 3DGS point cloud
files is an extremely challenging task. To solve the above-mentioned issue, we
first propose a steganography framework for 3DGS, dubbed GS-Hider, which can
embed 3D scenes and images into original GS point clouds in an invisible manner
and accurately extract the hidden messages. Specifically, we design a coupled
secured feature attribute to replace the original 3DGS's spherical harmonics
coefficients and then use a scene decoder and a message decoder to disentangle
the original RGB scene and the hidden message. Extensive experiments
demonstrated that the proposed GS-Hider can effectively conceal multimodal
messages without compromising rendering quality and possesses exceptional
security, robustness, capacity, and flexibility. Our project is available at:
https://xuanyuzhang21.github.io/project/gshider.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024, 3DGS steganography</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language
  Models for Robotic Garment Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Siyuan Huang, Qiaojun Yu, Zhengkai Jiang, Ce Hao, Yimeng Zhu, Hongsheng Li, Peng Gao, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating garment manipulation poses a significant challenge for assistive
robotics due to the diverse and deformable nature of garments. Traditional
approaches typically require separate models for each garment type, which
limits scalability and adaptability. In contrast, this paper presents a unified
approach using vision-language models (VLMs) to improve keypoint prediction
across various garment categories. By interpreting both visual and semantic
information, our model enables robots to manage different garment states with a
single model. We created a large-scale synthetic dataset using advanced
simulation techniques, allowing scalable training without extensive real-world
data. Experimental results indicate that the VLM-based method significantly
enhances keypoint detection accuracy and task success rates, providing a more
flexible and general solution for robotic garment manipulation. In addition,
this research also underscores the potential of VLMs to unify various garment
manipulation tasks within a single framework, paving the way for broader
applications in home automation and assistive robotics for future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Question Decomposition on <span class="highlight-title">Multimodal</span> <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructuReiser: A Structure-preserving Video Stylization Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radim Spetlik, David Futschik, Daniel Sykora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce StructuReiser, a novel video-to-video translation method that
transforms input videos into stylized sequences using a set of user-provided
keyframes. Unlike existing approaches, StructuReiser maintains strict adherence
to the structural elements of the target video, preserving the original
identity while seamlessly applying the desired stylistic transformations. This
enables a level of control and consistency that was previously unattainable
with traditional text-driven or keyframe-based methods. Furthermore,
StructuReiser supports real-time inference and custom keyframe editing, making
it ideal for interactive applications and expanding the possibilities for
creative expression and video manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.07243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.07243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wisth, Marco Camurri, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present visual inertial lidar legged navigation system (VILENS), an
odometry system for legged robots based on factor graphs. The key novelty is
the tight fusion of four different sensor modalities to achieve reliable
operation when the individual sensors would otherwise produce degenerate
estimation. To minimize leg odometry drift, we extend the robot's state with a
linear velocity bias term, which is estimated online. This bias is observable
because of the tight fusion of this preintegrated velocity factor with vision,
lidar, and inertial measurement unit (IMU) factors. Extensive experimental
validation on different ANYmal quadruped robots is presented, for a total
duration of 2 h and 1.8 km traveled. The experiments involved dynamic
locomotion over loose rocks, slopes, and mud, which caused challenges such as
slippage and terrain deformation. Perceptual challenges included dark and dusty
underground caverns, and open and feature-deprived areas. We show an average
improvement of 62% translational and 51% rotational errors compared to a
state-of-the-art loosely coupled approach. To demonstrate its robustness,
VILENS was also integrated with a perceptive controller and a local path
planner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video: https://youtu.be/NG4pkjJKhus</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Consistency Trajectory Models for <span class="highlight-title">Image</span> Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beomsu Kim, Jaemin Kim, Jeongsol Kim, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) excel in unconditional generation, as well as on
applications such as image editing and restoration. The success of DMs lies in
the iterative nature of diffusion: diffusion breaks down the complex process of
mapping noise to data into a sequence of simple denoising tasks. Moreover, we
are able to exert fine-grained control over the generation process by injecting
guidance terms into each denoising step. However, the iterative process is also
computationally intensive, often taking from tens up to thousands of function
evaluations. Although consistency trajectory models (CTMs) enable traversal
between any time points along the probability flow ODE (PFODE) and score
inference with a single function evaluation, CTMs only allow translation from
Gaussian noise to data. This work aims to unlock the full potential of CTMs by
proposing generalized CTMs (GCTMs), which translate between arbitrary
distributions via ODEs. We discuss the design space of GCTMs and demonstrate
their efficacy in various image manipulation tasks such as image-to-image
translation, restoration, and editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Contrastive Feature Representations for Facial Action Unit
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06165v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06165v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Shang, Bin Liu, Fengmao Lv, Fei Teng, Tianrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial action unit (AU) detection has long encountered the challenge of
detecting subtle feature differences when AUs activate. Existing methods often
rely on encoding pixel-level information of AUs, which not only encodes
additional redundant information but also leads to increased model complexity
and limited generalizability. Additionally, the accuracy of AU detection is
negatively impacted by the class imbalance issue of each AU type, and the
presence of noisy and false AU labels. In this paper, we introduce a novel
contrastive learning framework aimed for AU detection that incorporates both
self-supervised and supervised signals, thereby enhancing the learning of
discriminative features for accurate AU detection. To tackle the class
imbalance issue, we employ a negative sample re-weighting strategy that adjusts
the step size of updating parameters for minority and majority class samples.
Moreover, to address the challenges posed by noisy and false AU labels, we
employ a sampling technique that encompasses three distinct types of positive
sample pairs. This enables us to inject self-supervised signals into the
supervised signal, effectively mitigating the adverse effects of noisy labels.
Our experimental assessments, conducted on four widely-utilized benchmark
datasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance
of our approach compared to state-of-the-art methods of AU detection. Our code
is available at \url{https://github.com/Ziqiao-Shang/AUNCE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 17 figures, submitted to IEEE Transactions on Circuits and
  Systems for Video Technology (TCSVT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of All Blood Cell <span class="highlight-title">Image</span>s using ML and DL Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabia Asghar, Sanjay Kumar, Paul Hynds, Abeera Mahfooz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human blood primarily comprises plasma, red blood cells, white blood cells,
and platelets. It plays a vital role in transporting nutrients to different
organs, where it stores essential health-related data about the human body.
Blood cells are utilized to defend the body against diverse infections,
including fungi, viruses, and bacteria. Hence, blood analysis can help
physicians assess an individual's physiological condition. Blood cells have
been sub-classified into eight groups: Neutrophils, eosinophils, basophils,
lymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and
metamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of
their nucleus, shape, and cytoplasm. Traditionally, pathologists and
hematologists in laboratories have examined these blood cells using a
microscope before manually classifying them. The manual approach is slower and
more prone to human error. Therefore, it is essential to automate this process.
In our paper, transfer learning with CNN pre-trained models. VGG16, VGG19,
ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20
applied to the PBC dataset's normal DIB. The overall accuracy achieved with
these models lies between 91.375 and 94.72%. Hence, inspired by these
pre-trained architectures, a model has been proposed to automatically classify
the ten types of blood cells with increased accuracy. A novel CNN-based
framework has been presented to improve accuracy. The proposed CNN model has
been tested on the PBC dataset normal DIB. The outcomes of the experiments
demonstrate that our CNN-based framework designed for blood cell classification
attains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional
neural network model performs competitively when compared to earlier results
reported in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Pupil Tracking with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khadija Iddrisu, Waseem Shariff, Suzanne Little
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Saccades are extremely rapid movements of both eyes that occur
simultaneously, typically observed when an individual shifts their focus from
one object to another. These movements are among the swiftest produced by
humans and possess the potential to achieve velocities greater than that of
blinks. The peak angular speed of the eye during a saccade can reach as high as
700{\deg}/s in humans, especially during larger saccades that cover a visual
angle of 25{\deg}. Previous research has demonstrated encouraging outcomes in
comprehending neurological conditions through the study of saccades. A
necessary step in saccade detection involves accurately identifying the precise
location of the pupil within the eye, from which additional information such as
gaze angles can be inferred. Conventional frame-based cameras often struggle
with the high temporal precision necessary for tracking very fast movements,
resulting in motion blur and latency issues. Event cameras, on the other hand,
offer a promising alternative by recording changes in the visual scene
asynchronously and providing high temporal resolution and low latency. By
bridging the gap between traditional computer vision and event-based vision, we
present events as frames that can be readily utilized by standard deep learning
algorithms. This approach harnesses YOLOv8, a state-of-the-art object detection
technology, to process these frames for pupil tracking using the publicly
accessible Ev-Eye dataset. Experimental results demonstrate the framework's
effectiveness, highlighting its potential applications in neuroscience,
ophthalmology, and human-computer interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a preprint of a paper submitted to the 26th Irish
  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the
  copy of record will be available at IET Digital Library</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Medical <span class="highlight-title">Image</span> Representation Learning with Compositional
  Decoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaini Wang, Ling Yang, Siping Zhou, Guangquan Zhou, Wentao Zhang, Bin Cui, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-language models have advanced the development of universal models, yet
their application in medical imaging remains constrained by specific functional
requirements and the limited data. Current general-purpose models are typically
designed with task-specific branches and heads, which restricts the shared
feature space and the flexibility of model. To address these challenges, we
have developed a decomposed-composed universal medical imaging paradigm
(UniMed) that supports tasks at all levels. To this end, we first propose a
decomposed decoder that can predict two types of outputs -- pixel and semantic,
based on a defined input queue. Additionally, we introduce a composed decoder
that unifies the input and output spaces and standardizes task annotations
across different levels into a discrete token format. The coupled design of
these two components enables the model to flexibly combine tasks and mutual
benefits. Moreover, our joint representation learning strategy skilfully
leverages large amounts of unlabeled data and unsupervised loss, achieving
efficient one-stage pretraining for more robust performance. Experimental
results show that UniMed achieves state-of-the-art performance on eight
datasets across all three tasks and exhibits strong zero-shot and 100-shot
transferability. We will release the code and trained models upon the paper's
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColorwAI: Generative Colorways of Textiles through GAN and <span class="highlight-title">Diffusion</span>
  Disentanglement <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovica Schaerf, Andrea Alfarano, Eric Postma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorway creation is the task of generating textile samples in alternate
color variations maintaining an underlying pattern. The individuation of a
suitable color palette for a colorway is a complex creative task, responding to
client and market needs, stylistic and cultural specifications, and mood. We
introduce a modification of this task, the "generative colorway" creation, that
includes minimal shape modifications, and propose a framework, "ColorwAI", to
tackle this task using color disentanglement on StyleGAN and Diffusion. We
introduce a variation of the InterfaceGAN method for supervised
disentanglement, ShapleyVec. We use Shapley values to subselect a few
dimensions of the detected latent direction. Moreover, we introduce a general
framework to adopt common disentanglement methods on any architecture with a
semantic latent space and test it on Diffusion and GANs. We interpret the color
representations within the models' latent space. We find StyleGAN's W space to
be the most aligned with human notions of color. Finally, we suggest that
disentanglement can solicit a creative system for colorway creation, and
evaluate it through expert questionnaires and creativity theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024 VISART workshop, oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning an Actionable Discrete <span class="highlight-title">Diffusion</span> Policy via Large-Scale
  Actionless Video Pre-Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14407v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14407v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. However, it remains a challenge due to the domain gap between
humans and robots. Moreover, it is difficult to extract useful information
representing the dynamic world from human videos, because of its noisy and
multimodal data structure. In this paper, we introduce a novel framework to
tackle these challenges, which leverages a unified discrete diffusion to
combine generative pre-training on human videos and policy fine-tuning on a
small number of action-labeled robot videos. We start by compressing both human
and robot videos into unified video tokens. In the pre-training stage, we
employ a discrete diffusion model with a mask-and-replace diffusion strategy to
predict future video tokens in the latent space. In the fine-tuning stage, we
harness the imagined future videos to guide low-level action learning with a
limited set of robot data. Experiments demonstrate that our method generates
high-fidelity future videos for planning and enhances the fine-tuned policies
compared to previous state-of-the-art approaches with superior performance. Our
project website is available at https://video-diff.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. 24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">Large Language Model</span>s Understand Symbolic Graphics Programs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Against the backdrop of enthusiasm for large language models (LLMs), there is
an urgent need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer
different-grained semantic-level questions of the images or 3D geometries
without a vision encoder. To semantically understand the symbolic programs,
LLMs would need to possess the ability to "imagine" and reason how the
corresponding graphics content would look with only the symbolic description.
We use this task to evaluate LLMs by creating a large benchmark for the
semantic visual understanding of symbolic graphics programs, built procedurally
with minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v2 (46 pages, 24 figures, project page:
  https://sgp-bench.github.io/, substantial update from v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural
  Radiance Field Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reliance on accurate camera poses is a significant barrier to the
widespread deployment of Neural Radiance Fields (NeRF) models for 3D
reconstruction and SLAM tasks. The existing method introduces monocular depth
priors to jointly optimize the camera poses and NeRF, which fails to fully
exploit the depth priors and neglects the impact of their inherent noise. In
this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that
enables training NeRF from unknown camera poses - by jointly optimizing
learnable parameters of the radiance field and camera poses. Our approach
explicitly utilizes monocular depth priors through three key advancements: 1)
we propose a novel depth-based ray sampling strategy based on the truncated
normal distribution, which improves the convergence speed and accuracy of pose
estimation; 2) to circumvent local minima and refine depth geometry, we
introduce a coarse-to-fine training strategy that progressively improves the
depth precision; 3) we propose a more robust inter-frame point constraint that
enhances robustness against depth noise during training. The experimental
results on three datasets demonstrate that TD-NeRF achieves superior
performance in the joint optimization of camera pose and NeRF, surpassing prior
works, and generates more accurate depth geometry. The implementation of our
method has been released at https://github.com/nubot-nudt/TD-NeRF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Distortion Prior with Latent <span class="highlight-title">Diffusion</span> Models for Remote
  Sensing <span class="highlight-title">Image</span> Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhui Li, Jutao Li, Xingsong Hou, Huake Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based image compression algorithms typically focus on designing
encoding and decoding networks and improving the accuracy of entropy model
estimation to enhance the rate-distortion (RD) performance. However, few
algorithms leverage the compression distortion prior from existing compression
algorithms to improve RD performance. In this paper, we propose a latent
diffusion model-based remote sensing image compression (LDM-RSIC) method, which
aims to enhance the final decoding quality of RS images by utilizing the
generated distortion prior from a LDM. Our approach consists of two stages. In
the first stage, a self-encoder learns prior from the high-quality input image.
In the second stage, the prior is generated through an LDM, conditioned on the
decoded image of an existing learning-based image compression algorithm, to be
used as auxiliary information for generating the texture-rich enhanced image.
To better utilize the prior, a channel attention and gate-based dynamic feature
attention module (DFAM) is embedded into a Transformer-based multi-scale
enhancement network (MEN) for image enhancement. Extensive experiments
demonstrate the proposed LDM-RSIC significantly outperforms existing
state-of-the-art traditional and learning-based image compression algorithms in
terms of both subjective perception and objective metrics. Additionally, we use
the LDM-based scheme to improve the traditional image compression algorithm
JPEG2000 and obtain 32.00% bit savings on the DOTA testing set. The code will
be available at https://github.com/mlkk518/LDM-RSIC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Greit-HRNet: Grouped Lightweight High-Resolution Network for Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjia Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As multi-scale features are necessary for human pose estimation tasks,
high-resolution networks are widely applied.
  To improve efficiency, lightweight modules are proposed to replace costly
point-wise convolutions in high-resolution networks, including channel
weighting and spatial weighting methods.
  However, they fail to maintain the consistency of weights and capture global
spatial information.
  To address these problems, we present a Grouped lightweight High-Resolution
Network (Greit-HRNet), in which we propose a Greit block including a group
method Grouped Channel Weighting (GCW) and a spatial weighting method Global
Spatial Weighting (GSW).
  GCW modules group conditional channel weighting to make weights stable and
maintain the high-resolution features with the deepening of the network, while
GSW modules effectively extract global spatial information and exchange
information across channels.
  In addition, we apply the Large Kernel Attention (LKA) method to improve the
whole efficiency of our Greit-HRNet.
  Our experiments on both MS-COCO and MPII human pose estimation datasets
demonstrate the superior performance of our Greit-HRNet, outperforming other
state-of-the-art lightweight networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedThink: Explaining Medical Visual Question Answering via <span class="highlight-title">Multimodal</span>
  Decision-Making Rationale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotang Gai, Chenyi Zhou, Jiaxiang Liu, Yang Feng, Jian Wu, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Visual Question Answering (MedVQA), which offers language responses
to image-based medical inquiries, represents a challenging task and significant
advancement in healthcare. It assists medical experts to swiftly interpret
medical images, thereby enabling faster and more accurate diagnoses. However,
the model interpretability and transparency of existing MedVQA solutions are
often limited, posing challenges in understanding their decision-making
processes. To address this issue, we devise a semi-automated annotation process
to streamline data preparation and build new benchmark MedVQA datasets R-RAD,
R-SLAKE and R-Path. These datasets provide intermediate medical decision-making
rationales generated by multimodal large language models and human annotations
for question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD, SLAKE
and PathVQA. Moreover, we design a novel framework, MedThink, which finetunes
lightweight pretrained generative models by incorporating medical
decision-making rationales. MedThink includes three distinct strategies to
generate decision outcomes and corresponding rationales, thereby clearly
showcasing the medical decision-making process during reasoning. Our
comprehensive experiments show that our method achieves an accuracy of 83.5% on
R-RAD, 86.3% on R-SLAKE and 87.2% on R-Path. These results significantly exceed
those of existing state-of-the-art models with comparable parameters. Datasets
and code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 1st Place Solution to the 8th HANDS Workshop Challenge -- ARCTIC Track:
  3DGS-based Bimanual Category-agnostic Interaction Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongwan On, Kyeonghwan Gwak, Gunyoung Kang, Hyein Hwang, Soohyun Hwang, Junuk Cha, Jaewook Han, Seungryul Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report describes our 1st place solution to the 8th HANDS workshop
challenge (ARCTIC track) in conjunction with ECCV 2024. In this challenge, we
address the task of bimanual category-agnostic hand-object interaction
reconstruction, which aims to generate 3D reconstructions of both hands and the
object from a monocular video, without relying on predefined templates. This
task is particularly challenging due to the significant occlusion and dynamic
contact between the hands and the object during bimanual manipulation. We
worked to resolve these issues by introducing a mask loss and a 3D contact
loss, respectively. Moreover, we applied 3D Gaussian Splatting (3DGS) to this
task. As a result, our method achieved a value of 38.69 in the main metric,
CD$_h$, on the ARCTIC test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColPali: Efficient Document Retrieval with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are visually rich structures that convey information through text,
as well as tables, figures, page layouts, or fonts. While modern document
retrieval systems exhibit strong performance on query-to-text matching, they
struggle to exploit visual cues efficiently, hindering their performance on
practical document retrieval applications such as Retrieval Augmented
Generation. To benchmark current systems on visually rich document retrieval,
we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of
various page-level retrieving tasks spanning multiple domains, languages, and
settings. The inherent shortcomings of modern systems motivate the introduction
of a new retrieval model architecture, ColPali, which leverages the document
understanding capabilities of recent Vision Language Models to produce
high-quality contextualized embeddings solely from images of document pages.
Combined with a late interaction matching mechanism, ColPali largely
outperforms modern document retrieval pipelines while being drastically faster
and end-to-end trainable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">101</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Advisor: Dynamic Data Curation for Safety Alignment of Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Ninareh Mehrabi, Palash Goyal, Rahul Gupta, Kai-Wei Chang, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is a crucial element in large language model (LLM) alignment. Recent
studies have explored using LLMs for efficient data collection. However,
LLM-generated data often suffers from quality issues, with underrepresented or
absent aspects and low-quality datapoints. To address these problems, we
propose Data Advisor, an enhanced LLM-based method for generating data that
takes into account the characteristics of the desired dataset. Starting from a
set of pre-defined principles in hand, Data Advisor monitors the status of the
generated data, identifies weaknesses in the current dataset, and advises the
next iteration of data generation accordingly. Data Advisor can be easily
integrated into existing data generation methods to enhance data quality and
coverage. Experiments on safety alignment of three representative LLMs (i.e.,
Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in
enhancing model safety against various fine-grained safety issues without
sacrificing model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/DataAdvisor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers
  in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is essential for deploying Large Language Models (LLMs) by
enhancing memory efficiency and inference speed. Existing methods for
activation quantization mainly address channel-wise outliers, often neglecting
token-wise outliers, leading to reliance on costly per-token dynamic
quantization. To address this, we introduce PrefixQuant, a novel technique that
isolates outlier tokens offline without re-training. Specifically, PrefixQuant
identifies high-frequency outlier tokens and prefixes them in the KV cache,
preventing the generation of outlier tokens during inference and simplifying
quantization. To our knowledge, PrefixQuant is the first to enable efficient
per-tensor static quantization to outperform expensive per-token dynamic
quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and
4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization
achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5
common-sense reasoning tasks, outperforming previous per-token dynamic
quantization methods like QuaRot with 0.98 perplexity improvement and +5.98
points accuracy. Additionally, the inference speed of W4A4 quantized models
using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot
models by 1.2x to 1.3x. Our code is available at
\url{https://github.com/ChenMnZ/PrefixQuant}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A PTQ method to significantly boost the performance of static
  activation quantization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regression Conformal Prediction under Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Y. Cheung, Tucker J. Netherton, Laurence E. Court, Ashok Veeraraghavan, Guha Balakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification is crucial to account for the imperfect
predictions of machine learning algorithms for high-impact applications.
Conformal prediction (CP) is a powerful framework for uncertainty
quantification that generates calibrated prediction intervals with valid
coverage. In this work, we study how CP intervals are affected by bias - the
systematic deviation of a prediction from ground truth values - a phenomenon
prevalent in many real-world applications. We investigate the influence of bias
on interval lengths of two different types of adjustments -- symmetric
adjustments, the conventional method where both sides of the interval are
adjusted equally, and asymmetric adjustments, a more flexible method where the
interval can be adjusted unequally in positive or negative directions. We
present theoretical and empirical analyses characterizing how symmetric and
asymmetric adjustments impact the "tightness" of CP intervals for regression
tasks. Specifically for absolute residual and quantile-based non-conformity
scores, we prove: 1) the upper bound of symmetrically adjusted interval lengths
increases by $2|b|$ where $b$ is a globally applied scalar value representing
bias, 2) asymmetrically adjusted interval lengths are not affected by bias, and
3) conditions when asymmetrically adjusted interval lengths are guaranteed to
be smaller than symmetric ones. Our analyses suggest that even if predictions
exhibit significant drift from ground truth values, asymmetrically adjusted
intervals are still able to maintain the same tightness and validity of
intervals as if the drift had never happened, while symmetric ones
significantly inflate the lengths. We demonstrate our theoretical results with
two real-world prediction tasks: sparse-view computed tomography (CT)
reconstruction and time-series weather forecasting. Our work paves the way for
more bias-robust machine learning systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, code available at:
  https://github.com/matthewyccheung/conformal-metric</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer tends to overallocate attention to irrelevant context. In this
work, we introduce Diff Transformer, which amplifies attention to the relevant
context while canceling noise. Specifically, the differential attention
mechanism calculates attention scores as the difference between two separate
softmax attention maps. The subtraction cancels noise, promoting the emergence
of sparse attention patterns. Experimental results on language modeling show
that Diff Transformer outperforms Transformer in various settings of scaling up
model size and training tokens. More intriguingly, it offers notable advantages
in practical applications, such as long-context modeling, key information
retrieval, hallucination mitigation, in-context learning, and reduction of
activation outliers. By being less distracted by irrelevant context, Diff
Transformer can mitigate hallucination in question answering and text
summarization. For in-context learning, Diff Transformer not only enhances
accuracy but is also more robust to order permutation, which was considered as
a chronic robustness issue. The results position Diff Transformer as a highly
effective and promising architecture to advance large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SePPO: Semi-Policy Preference Optimization for <span class="highlight-title">Diffusion</span> Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoan Zhang, Guangchen Lan, Dong-Jun Han, Wenlin Yao, Xiaoman Pan, Hongming Zhang, Mingxiao Li, Pengcheng Chen, Yu Dong, Christopher Brinton, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) methods are emerging as a
way to fine-tune diffusion models (DMs) for visual generation. However,
commonly used on-policy strategies are limited by the generalization capability
of the reward model, while off-policy approaches require large amounts of
difficult-to-obtain paired human-annotated data, particularly in visual
generation tasks. To address the limitations of both on- and off-policy RLHF,
we propose a preference optimization method that aligns DMs with preferences
without relying on reward models or paired human-annotated data. Specifically,
we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO
leverages previous checkpoints as reference models while using them to generate
on-policy reference samples, which replace "losing images" in preference pairs.
This approach allows us to optimize using only off-policy "winning images."
Furthermore, we design a strategy for reference model selection that expands
the exploration in the policy space. Notably, we do not simply treat reference
samples as negative examples for learning. Instead, we design an anchor-based
criterion to assess whether the reference samples are likely to be winning or
losing images, allowing the model to selectively learn from the generated
reference samples. This approach mitigates performance degradation caused by
the uncertainty in reference sample quality. We validate SePPO across both
text-to-image and text-to-video benchmarks. SePPO surpasses all previous
approaches on the text-to-image benchmarks and also demonstrates outstanding
performance on the text-to-video benchmarks. Code will be released in
https://github.com/DwanZhang-AI/SePPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLEE: A Unified Framework and <span class="highlight-title">Benchmark</span> for Language-based Economic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show significant potential in economic and
strategic interactions, where communication via natural language is often
prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic
human behavior? Do they tend to reach an efficient and fair outcome? What is
the role of natural language in the strategic interaction? How do
characteristics of the economic environment influence these dynamics? These
questions become crucial concerning the economic and societal implications of
integrating LLM-based agents into real-world data-driven systems, such as
online retail platforms and recommender systems. While the ML community has
been exploring the potential of LLMs in such multi-agent setups, varying
assumptions, design choices and evaluation criteria across studies make it
difficult to draw robust and meaningful conclusions. To address this, we
introduce a benchmark for standardizing research on two-player, sequential,
language-based games. Inspired by the economic literature, we define three base
families of games with consistent parameterization, degrees of freedom and
economic measures to evaluate agents' performance (self-gain), as well as the
game outcome (efficiency and fairness). We develop an open-source framework for
interaction simulation and analysis, and utilize it to collect a dataset of LLM
vs. LLM interactions across numerous game configurations and an additional
dataset of human vs. LLM interactions. Through extensive experimentation, we
demonstrate how our framework and dataset can be used to: (i) compare the
behavior of LLM-based agents to human players in various economic contexts;
(ii) evaluate agents in both individual and collective performance measures;
and (iii) quantify the effect of the economic characteristics of the
environments on the behavior of agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Micro-Narratives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mourad Heddaya, Qingcheng Zeng, Chenhao Tan, Rob Voigt, Alexander Zentefis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Workshop on Narrative Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To induce desired behaviors in large language models (LLMs) for
interaction-driven tasks, the instruction-tuning stage typically trains LLMs on
instruction-response pairs using the next-token prediction (NTP) loss. Previous
work aiming to improve instruction-tuning performance often emphasizes the need
for higher-quality supervised fine-tuning (SFT) datasets, which typically
involves expensive data filtering with proprietary LLMs or labor-intensive data
generation by human annotators. However, these approaches do not fully leverage
the datasets' intrinsic properties, resulting in high computational and labor
costs, thereby limiting scalability and performance gains. In this paper, we
propose SFTMix, a novel recipe that elevates instruction-tuning performance
beyond the conventional NTP paradigm, without the need for well-curated
datasets. Observing that LLMs exhibit uneven confidence across the semantic
representation space, we argue that examples with different confidence levels
should play distinct roles during the instruction-tuning process. Based on this
insight, SFTMix leverages training dynamics to identify examples with varying
confidence levels, then applies a Mixup-based regularization to mitigate
overfitting on confident examples while propagating supervision signals to
improve learning on relatively unconfident ones. This approach enables SFTMix
to significantly outperform NTP across a wide range of instruction-following
and healthcare domain-specific SFT tasks, demonstrating its adaptability to
diverse LLM families and scalability to datasets of any size. Comprehensive
ablation studies further verify the robustness of SFTMix's design choices,
underscoring its versatility in consistently enhancing performance across
different LLMs and datasets in broader natural language processing
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Bouhsine, Imad El Aaroussi, Atik Faysal, Wang Huaxia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel anchor-free contrastive learning (AFCL) method
leveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach
minimizes a semi-metric discriminative loss function that simultaneously
optimizes two key objectives: reducing the distance and orthogonality between
embeddings of similar inputs while maximizing these metrics for dissimilar
inputs, facilitating more fine-grained contrastive learning. The AFCL method,
powered by SimO loss, creates a fiber bundle topological structure in the
embedding space, forming class-specific, internally cohesive yet orthogonal
neighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,
providing visualizations that demonstrate the impact of SimO loss on the
embedding space. Our results illustrate the formation of distinct, orthogonal
class neighborhoods, showcasing the method's ability to create well-structured
embeddings that balance class separation with intra-class variability. This
work opens new avenues for understanding and leveraging the geometric
properties of learned representations in various machine learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SymmetryLens: A new candidate paradigm for unsupervised symmetry
  learning via locality and equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onur Efe, Arkadas Ozakin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a new, unsupervised symmetry learning method that starts with raw
data, and gives the minimal (discrete) generator of an underlying Lie group of
symmetries, together with a symmetry equivariant representation of the data.
The method is able to learn the pixel translation operator from a dataset with
only an approximate translation symmetry, and can learn quite different types
of symmetries which are not apparent to the naked eye, equally well. The method
is based on the formulation of an information-theoretic loss function that
measures both the degree to which the dataset is symmetric under a given
candidate symmetry, and also, the degree of locality of the samples in the
dataset with respect to this symmetry. We demonstrate that this coupling
between symmetry and locality, together with a special optimization technique
developed for entropy estimation, results in a highly stable system that gives
reproducible results. The symmetry actions we consider are group
representations, however, we believe the approach has the potential to be
generalized to more general, nonlinear actions of non-commutative Lie groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSM-Symbolic: Understanding the Limitations of Mathematical <span class="highlight-title">Reasoning</span> in
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have sparked interest in
their formal reasoning capabilities, particularly in mathematics. The GSM8K
benchmark is widely used to assess the mathematical reasoning of models on
grade-school-level questions. While the performance of LLMs on GSM8K has
significantly improved in recent years, it remains unclear whether their
mathematical reasoning capabilities have genuinely advanced, raising questions
about the reliability of the reported metrics. To address these concerns, we
conduct a large-scale study on several SOTA open and closed models. To overcome
the limitations of existing evaluations, we introduce GSM-Symbolic, an improved
benchmark created from symbolic templates that allow for the generation of a
diverse set of questions. GSM-Symbolic enables more controllable evaluations,
providing key insights and more reliable metrics for measuring the reasoning
capabilities of models.Our findings reveal that LLMs exhibit noticeable
variance when responding to different instantiations of the same question.
Specifically, the performance of all models declines when only the numerical
values in the question are altered in the GSM-Symbolic benchmark. Furthermore,
we investigate the fragility of mathematical reasoning in these models and show
that their performance significantly deteriorates as the number of clauses in a
question increases. We hypothesize that this decline is because current LLMs
cannot perform genuine logical reasoning; they replicate reasoning steps from
their training data. Adding a single clause that seems relevant to the question
causes significant performance drops (up to 65%) across all state-of-the-art
models, even though the clause doesn't contribute to the reasoning chain needed
for the final answer. Overall, our work offers a more nuanced understanding of
LLMs' capabilities and limitations in mathematical reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse
  Reward Continuous Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Futuhi, Shayan Karimi, Chao Gao, Martin Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider deep deterministic policy gradient (DDPG) in the context of
reinforcement learning with sparse rewards. To enhance exploration, we
introduce a search procedure, \emph{${\epsilon}{t}$-greedy}, which generates
exploratory options for exploring less-visited states. We prove that search
using $\epsilon t$-greedy has polynomial sample complexity under mild MDP
assumptions. To more efficiently use the information provided by rewarded
transitions, we develop a new dual experience replay buffer framework,
\emph{GDRB}, and implement \emph{longest n-step returns}. The resulting
algorithm, \emph{ETGL-DDPG}, integrates all three techniques: \bm{$\epsilon
t$}-greedy, \textbf{G}DRB, and \textbf{L}ongest $n$-step, into DDPG. We
evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms
DDPG, as well as other state-of-the-art methods, across all tested
sparse-reward continuous environments. Ablation studies further highlight how
each strategy individually enhances the performance of DDPG in this setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cookbook: A framework for improving <span class="highlight-title">LLM</span> generative abilities via
  programmatic data generating templates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avanika Narayan, Mayee F. Chen, Kush Bhatia, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) on instruction datasets is a common
way to improve their generative capabilities. However, instruction datasets can
be expensive and time-consuming to manually curate, and while LLM-generated
data is less labor-intensive, it may violate user privacy agreements or terms
of service of LLM providers. Therefore, we seek a way of constructing
instruction datasets with samples that are not generated by humans or LLMs but
still improve LLM generative capabilities. In this work, we introduce Cookbook,
a framework that programmatically generates training data consisting of simple
patterns over random tokens, resulting in a scalable, cost-effective approach
that avoids legal and privacy issues. First, Cookbook uses a template -- a data
generating Python function -- to produce training data that encourages the
model to learn an explicit pattern-based rule that corresponds to a desired
task. We find that fine-tuning on Cookbook-generated data is able to improve
performance on its corresponding task by up to 52.7 accuracy points. Second,
since instruction datasets improve performance on multiple downstream tasks
simultaneously, Cookbook algorithmically learns how to mix data from various
templates to optimize performance on multiple tasks. On the standard multi-task
GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated
dataset attains the best accuracy on average compared to other 7B parameter
instruction-tuned models and is the best performing model on 3 out of 8 tasks.
Finally, we analyze when and why Cookbook improves performance and present a
metric that allows us to verify that the improvement is largely explained by
the model's generations adhering better to template rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Model <span class="highlight-title">Benchmark</span>ing with Only a Few Observations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we precisely estimate a large language model's (LLM) accuracy on
questions belonging to a specific topic within a larger question-answering
dataset? The standard direct estimator, which averages the model's accuracy on
the questions in each subgroup, may exhibit high variance for subgroups
(topics) with small sample sizes. Synthetic regression modeling, which
leverages the model's accuracy on questions about other topics, may yield
biased estimates that are too unreliable for large subgroups. We prescribe a
simple yet effective solution: an empirical Bayes (EB) estimator that balances
direct and regression estimates for each subgroup separately, improving the
precision of subgroup-level estimates of model performance. Our experiments on
multiple datasets show that this approach consistently provides more precise
estimates of the LLM performance compared to the direct and regression
approaches, achieving substantial reductions in the mean squared error.
Confidence intervals for EB estimates also have near-nominal coverage and are
narrower compared to those for the direct estimator. Additional experiments on
tabular and vision data validate the benefits of this EB approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density estimation with <span class="highlight-title">LLM</span>s: a geometric investigation of in-context
  learning trajectories <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable emergent abilities to
perform in-context learning across various tasks, including time series
forecasting. This work investigates LLMs' ability to estimate probability
density functions (PDFs) from data observed in-context; such density estimation
(DE) is a fundamental task underlying many probabilistic modeling problems. We
leverage the Intensive Principal Component Analysis (InPCA) to visualize and
analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is
that these LLMs all follow similar learning trajectories in a low-dimensional
InPCA space, which are distinct from those of traditional density estimation
methods like histograms and Gaussian kernel density estimation (KDE). We
interpret the LLaMA in-context DE process as a KDE with an adaptive kernel
width and shape. This custom kernel model captures a significant portion of
LLaMA's behavior despite having only two parameters. We further speculate on
why LLaMA's kernel width and shape differs from classical algorithms, providing
insights into the mechanism of in-context probabilistic reasoning in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond FVD: Enhanced Evaluation Metrics for Video <span class="highlight-title">Generation</span> Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Ya,  Luo, Gian Favero, Zhi Hao Luo, Alexia Jolicoeur-Martineau, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Fr\'echet Video Distance (FVD) is a widely adopted metric for evaluating
video generation distribution quality. However, its effectiveness relies on
critical assumptions. Our analysis reveals three significant limitations: (1)
the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the
insensitivity of I3D features to temporal distortions; (3) the impractical
sample sizes required for reliable estimation. These findings undermine FVD's
reliability and show that FVD falls short as a standalone metric for video
generation evaluation. After extensive analysis of a wide range of metrics and
backbone architectures, we propose JEDi, the JEPA Embedding Distance, based on
features derived from a Joint Embedding Predictive Architecture, measured using
Maximum Mean Discrepancy with polynomial kernel. Our experiments on multiple
open-source datasets show clear evidence that it is a superior alternative to
the widely used FVD metric, requiring only 16% of the samples to reach its
steady value, while increasing alignment with human evaluation by 34%, on
average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss
  Landscape Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training language models currently requires pre-determining a fixed compute
budget because the typical cosine learning rate schedule depends on the total
number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a
constant learning rate to produce a main branch of iterates that can in
principle continue indefinitely without a pre-specified compute budget. Then,
given any compute budget, one can branch out from the main branch at a proper
at any time with a rapidly decaying learning rate to produce a strong model.
Empirically, WSD generates a non-traditional loss curve: the loss remains
elevated during the stable phase but sharply declines during the decay phase.
Towards explaining this phenomenon, we conjecture that pretraining loss
exhibits a river valley landscape, which resembles a deep valley with a river
at its bottom. Under this assumption, we show that during the stable phase, the
iterate undergoes large oscillations due to the high learning rate, yet it
progresses swiftly along the river. During the decay phase, the rapidly
dropping learning rate minimizes the iterate's oscillations, moving it closer
to the river and revealing true optimization progress. Therefore, the sustained
high learning rate phase and fast decaying phase are responsible for progress
in the river and the mountain directions respectively, and are both critical.
Our analysis predicts phenomenons consistent with empirical observations and
shows that this landscape can emerge from pretraining on a simple bi-gram
dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that
reuses previous checkpoints' decay phases and keeps only one main branch, where
we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and
Cyclic-Cosine in obtaining multiple language model checkpoints across various
compute budgets in a single run for parameters scaling from 0.1B to 1.2B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages,13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matrix-weighted networks for modeling multidimensional dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tian, Sadamori Kojaku, Hiroki Sayama, Renaud Lambiotte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Networks are powerful tools for modeling interactions in complex systems.
While traditional networks use scalar edge weights, many real-world systems
involve multidimensional interactions. For example, in social networks,
individuals often have multiple interconnected opinions that can affect
different opinions of other individuals, which can be better characterized by
matrices. We propose a novel, general framework for modeling such
multidimensional interacting dynamics: matrix-weighted networks (MWNs). We
present the mathematical foundations of MWNs and examine consensus dynamics and
random walks within this context. Our results reveal that the coherence of MWNs
gives rise to non-trivial steady states that generalize the notions of
communities and structural balance in traditional networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARs: Multi-view Attention Regularizations for Patch-based Feature
  Recognition of Space Terrain <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Chase Jr, Karthik Dantu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual detection and tracking of surface terrain is required for
spacecraft to safely land on or navigate within close proximity to celestial
objects. Current approaches rely on template matching with pre-gathered
patch-based features, which are expensive to obtain and a limiting factor in
perceptual capability. While recent literature has focused on in-situ detection
methods to enhance navigation and operational autonomy, robust description is
still needed. In this work, we explore metric learning as the lightweight
feature description mechanism and find that current solutions fail to address
inter-class similarity and multi-view observational geometry. We attribute this
to the view-unaware attention mechanism and introduce Multi-view Attention
Regularizations (MARs) to constrain the channel and spatial attention across
multiple feature views, regularizing the what and where of attention focus. We
thoroughly analyze many modern metric learning losses with and without MARs and
demonstrate improved terrain-feature recognition performance by upwards of 85%.
We additionally introduce the Luna-1 dataset, consisting of Moon crater
landmarks and reference navigation frames from NASA mission data to support
future research in this difficult task. Luna-1 and source code are publicly
available at https://droneslab.github.io/mars/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Project page available at
  https://droneslab.github.io/mars/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are causal effect estimations enough for optimal recommendations under
  multitreatment scenarios? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherly Alfonso-Sánchez, Kristina P. Sendova, Cristián Bravo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When making treatment selection decisions, it is essential to include a
causal effect estimation analysis to compare potential outcomes under different
treatments or controls, assisting in optimal selection. However, merely
estimating individual treatment effects may not suffice for truly optimal
decisions. Our study addressed this issue by incorporating additional criteria,
such as the estimations' uncertainty, measured by the conditional
value-at-risk, commonly used in portfolio and insurance management. For
continuous outcomes observable before and after treatment, we incorporated a
specific prediction condition. We prioritized treatments that could yield
optimal treatment effect results and lead to post-treatment outcomes more
desirable than pretreatment levels, with the latter condition being called the
prediction criterion. With these considerations, we propose a comprehensive
methodology for multitreatment selection. Our approach ensures satisfaction of
the overlap assumption, crucial for comparing outcomes for treated and control
groups, by training propensity score models as a preliminary step before
employing traditional causal models. To illustrate a practical application of
our methodology, we applied it to the credit card limit adjustment problem.
Analyzing a fintech company's historical data, we found that relying solely on
counterfactual predictions was inadequate for appropriate credit line
modifications. Incorporating our proposed additional criteria significantly
enhanced policy performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Presto! Distilling Steps and Layers for Accelerating Music <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Novack, Ge Zhu, Jonah Casebeer, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas J. Bryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in diffusion-based text-to-music (TTM) methods, efficient,
high-quality generation remains a challenge. We introduce Presto!, an approach
to inference acceleration for score-based diffusion transformers via reducing
both sampling steps and cost per step. To reduce steps, we develop a new
score-based distribution matching distillation (DMD) method for the EDM-family
of diffusion models, the first GAN-based distillation method for TTM. To reduce
the cost per step, we develop a simple, but powerful improvement to a recent
layer distillation method that improves learning via better preserving hidden
state variance. Finally, we combine our step and layer distillation methods
together for a dual-faceted approach. We evaluate our step and layer
distillation methods independently and show each yield best-in-class
performance. Our combined distillation method can generate high-quality outputs
with improved diversity, accelerating our base model by 10-18x (230/435ms
latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --
the fastest high-quality TTM to our knowledge. Sound examples can be found at
https://presto-music.github.io/web/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simulation-Free Deep Learning Approach to Stochastic Optimal Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjian Hua, Matthieu Laurière, Eric Vanden-Eijnden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simulation-free algorithm for the solution of generic problems
in stochastic optimal control (SOC). Unlike existing methods, our approach does
not require the solution of an adjoint problem, but rather leverages Girsanov
theorem to directly calculate the gradient of the SOC objective on-policy. This
allows us to speed up the optimization of control policies parameterized by
neural networks since it completely avoids the expensive back-propagation step
through stochastic differential equations (SDEs) used in the Neural SDE
framework. In particular, it enables us to solve SOC problems in high dimension
and on long time horizons. We demonstrate the efficiency of our approach in
various domains of applications, including standard stochastic optimal control
problems, sampling from unnormalized distributions via construction of a
Schr\"odinger-F\"ollmer process, and fine-tuning of pre-trained diffusion
models. In all cases our method is shown to outperform the existing methods in
both the computing time and memory efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAMLR: A Passive-Active Multi-Armed Bandit-Based Solution for LoRa
  Channel Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Yun, Chengzhang Li, Anish Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving low duty cycle operation in low-power wireless networks in urban
environments is complicated by the complex and variable dynamics of external
interference and fading. We explore the use of reinforcement learning for
achieving low power consumption for the task of optimal selection of channels.
The learning relies on a hybrid of passive channel sampling for dealing with
external interference and active channel sampling for dealing with fading. Our
solution, Passive-Active Multi-armed bandit for LoRa (PAMLR, pronounced
"Pamela"), balances the two types of samples to achieve energy-efficient
channel selection: active channel measurements are tuned to an appropriately
low level to update noise thresholds, and to compensate passive channel
measurements are tuned to an appropriately high level for selecting the
top-most channels from channel exploration using the noise thresholds. The
rates of both types of samples are adapted in response to channel dynamics.
Based on extensive testing in multiple environments in different cities, we
validate that PAMLR can maintain excellent communication quality, as
demonstrated by a low SNR regret compared to the optimal channel allocation
policy, while substantially minimizing the energy cost associated with channel
measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tuning-Free Bilevel Optimization: New Algorithms and Convergence
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Hao Ban, Minhui Huang, Shiqian Ma, Kaiyi Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization has recently attracted considerable attention due to its
abundant applications in machine learning problems. However, existing methods
rely on prior knowledge of problem parameters to determine stepsizes, resulting
in significant effort in tuning stepsizes when these parameters are unknown. In
this paper, we propose two novel tuning-free algorithms, D-TFBO and S-TFBO.
D-TFBO employs a double-loop structure with stepsizes adaptively adjusted by
the "inverse of cumulative gradient norms" strategy. S-TFBO features a simpler
fully single-loop structure that updates three variables simultaneously with a
theory-motivated joint design of adaptive stepsizes for all variables. We
provide a comprehensive convergence analysis for both algorithms and show that
D-TFBO and S-TFBO respectively require $O(\frac{1}{\epsilon})$ and
$O(\frac{1}{\epsilon}\log^4(\frac{1}{\epsilon}))$ iterations to find an
$\epsilon$-accurate stationary point, (nearly) matching their well-tuned
counterparts using the information of problem parameters. Experiments on
various problems show that our methods achieve performance comparable to
existing well-tuned approaches, while being more robust to the selection of
initial stepsizes. To the best of our knowledge, our methods are the first to
completely eliminate the need for stepsize tuning, while achieving theoretical
guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOTOS: Layer-wise Orthogonalization for Training Robust Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ebrahimpour-Boroojeny, Hari Sundaram, Varun Chandrasekaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transferability of adversarial examples is a well-known property that
endangers all classification models, even those that are only accessible
through black-box queries. Prior work has shown that an ensemble of models is
more resilient to transferability: the probability that an adversarial example
is effective against most models of the ensemble is low. Thus, most ongoing
research focuses on improving ensemble diversity. Another line of prior work
has shown that Lipschitz continuity of the models can make models more robust
since it limits how a model's output changes with small input perturbations. In
this paper, we study the effect of Lipschitz continuity on transferability
rates. We show that although a lower Lipschitz constant increases the
robustness of a single model, it is not as beneficial in training robust
ensembles as it increases the transferability rate of adversarial examples
across models in the ensemble. Therefore, we introduce LOTOS, a new training
paradigm for ensembles, which counteracts this adverse effect. It does so by
promoting orthogonality among the top-$k$ sub-spaces of the transformations of
the corresponding affine layers of any pair of models in the ensemble. We
theoretically show that $k$ does not need to be large for convolutional layers,
which makes the computational overhead negligible. Through various experiments,
we show LOTOS increases the robust accuracy of ensembles of ResNet-18 models by
$6$ percentage points (p.p) against black-box attacks on CIFAR-10. It is also
capable of combining with the robustness of prior state-of-the-art methods for
training robust ensembles to enhance their robust accuracy by $10.7$ p.p.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Digital Twin Framework for Liquid-cooled Supercomputers as
  Demonstrated at Exascale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wesley Brewer, Matthias Maiterth, Vineet Kumar, Rafal Wojda, Sedrick Bouknight, Jesse Hines, Woong Shin, Scott Greenwood, David Grant, Wesley Williams, Feiyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ExaDigiT, an open-source framework for developing comprehensive
digital twins of liquid-cooled supercomputers. It integrates three main
modules: (1) a resource allocator and power simulator, (2) a transient
thermo-fluidic cooling model, and (3) an augmented reality model of the
supercomputer and central energy plant. The framework enables the study of
"what-if" scenarios, system optimizations, and virtual prototyping of future
systems. Using Frontier as a case study, we demonstrate the framework's
capabilities by replaying six months of system telemetry for systematic
verification and validation. Such a comprehensive analysis of a liquid-cooled
exascale supercomputer is the first of its kind. ExaDigiT elucidates complex
transient cooling system dynamics, runs synthetic or real workloads, and
predicts energy losses due to rectification and voltage conversion. Throughout
our paper, we present lessons learned to benefit HPC practitioners developing
similar digital twins. We envision the digital twin will be a key enabler for
sustainable, energy-efficient supercomputing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, To be published in the Proceedings of the
  International Conference for High Performance Computing, Networking, Storage
  and Analysis. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agnostic Smoothed Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moïse Blanchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical results in statistical learning typically consider two extreme
data-generating models: i.i.d. instances from an unknown distribution, or fully
adversarial instances, often much more challenging statistically. To bridge the
gap between these models, recent work introduced the smoothed framework, in
which at each iteration an adversary generates instances from a distribution
constrained to have density bounded by $\sigma^{-1}$ compared to some fixed
base measure $\mu$. This framework interpolates between the i.i.d. and
adversarial cases, depending on the value of $\sigma$. For the classical online
prediction problem, most prior results in smoothed online learning rely on the
arguably strong assumption that the base measure $\mu$ is known to the learner,
contrasting with standard settings in the PAC learning or consistency
literature. We consider the general agnostic problem in which the base measure
is unknown and values are arbitrary. Along this direction, Block et al. showed
that empirical risk minimization has sublinear regret under the well-specified
assumption. We propose an algorithm R-Cover based on recursive coverings which
is the first to guarantee sublinear regret for agnostic smoothed online
learning without prior knowledge of $\mu$. For classification, we prove that
R-Cover has adaptive regret $\tilde O(\sqrt{dT/\sigma})$ for function classes
with VC dimension $d$, which is optimal up to logarithmic factors. For
regression, we establish that R-Cover has sublinear oblivious regret for
function classes with polynomial fat-shattering dimension growth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound
  Framework and Characterization for Bandit Learnability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Chen, Dylan J. Foster, Yanjun Han, Jian Qian, Alexander Rakhlin, Yunbei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a unified framework for lower bound methods in
statistical estimation and interactive decision making. Classical lower bound
techniques -- such as Fano's inequality, Le Cam's method, and Assouad's lemma
-- have been central to the study of minimax risk in statistical estimation,
yet they are insufficient for the analysis of methods that collect data in an
interactive manner. The recent minimax lower bounds for interactive decision
making via the Decision-Estimation Coefficient (DEC) appear to be genuinely
different from the classical methods. We propose a unified view of these
distinct methodologies through a general algorithmic lower bound method. We
further introduce a novel complexity measure, decision dimension, which
facilitates the derivation of new lower bounds for interactive decision making.
In particular, decision dimension provides a characterization of bandit
learnability for any structured bandit model class. Further, we characterize
the sample complexity of learning convex model class up to a polynomial gap
with the decision dimension, addressing the remaining gap between upper and
lower bounds in Foster et al. (2021, 2023).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Feedback Efficient Reinforcement Learning for Online <span class="highlight-title">Diffusion</span>
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable generation through Stable Diffusion (SD) fine-tuning aims to
improve fidelity, safety, and alignment with human guidance. Existing
reinforcement learning from human feedback methods usually rely on predefined
heuristic reward functions or pretrained reward models built on large-scale
datasets, limiting their applicability to scenarios where collecting such data
is costly or difficult. To effectively and efficiently utilize human feedback,
we develop a framework, HERO, which leverages online human feedback collected
on the fly during model learning. Specifically, HERO features two key
mechanisms: (1) Feedback-Aligned Representation Learning, an online training
method that captures human feedback and provides informative learning signals
for fine-tuning, and (2) Feedback-Guided Image Generation, which involves
generating images from SD's refined initialization samples, enabling faster
convergence towards the evaluator's intent. We demonstrate that HERO is 4x more
efficient in online feedback for body part anomaly correction compared to the
best existing method. Additionally, experiments show that HERO can effectively
handle tasks like reasoning, counting, personalization, and reducing NSFW
content with only 0.5K online feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper-Representations: Learning from Populations of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Schürholt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis addresses the challenge of understanding Neural Networks through
the lens of their most fundamental component: the weights, which encapsulate
the learned information and determine the model behavior. At the core of this
thesis is a fundamental question: Can we learn general, task-agnostic
representations from populations of Neural Network models? The key contribution
of this thesis to answer that question are hyper-representations, a
self-supervised method to learn representations of NN weights. Work in this
thesis finds that trained NN models indeed occupy meaningful structures in the
weight space, that can be learned and used. Through extensive experiments, this
thesis demonstrates that hyper-representations uncover model properties, such
as their performance, state of training, or hyperparameters. Moreover, the
identification of regions with specific properties in hyper-representation
space allows to sample and generate model weights with targeted properties.
This thesis demonstrates applications for fine-tuning, and transfer learning to
great success. Lastly, it presents methods that allow hyper-representations to
generalize beyond model sizes, architectures, and tasks. The practical
implications of that are profound, as it opens the door to foundation models of
Neural Networks, which aggregate and instantiate their knowledge across models
and architectures. Ultimately, this thesis contributes to the deeper
understanding of Neural Networks by investigating structures in their weights
which leads to more interpretable, efficient, and adaptable models. By laying
the groundwork for representation learning of NN weights, this research
demonstrates the potential to change the way Neural Networks are developed,
analyzed, and used.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Dissertation accepted at University of St. Gallen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonasymptotic Analysis of Stochastic Gradient Descent with the
  Richardson-Romberg Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Sheshukova, Denis Belomestny, Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of solving strongly convex and smooth minimization
problems using stochastic gradient descent (SGD) algorithm with a constant step
size. Previous works suggested to combine the Polyak-Ruppert averaging
procedure with the Richardson-Romberg extrapolation technique to reduce the
asymptotic bias of SGD at the expense of a mild increase of the variance. We
significantly extend previous results by providing an expansion of the
mean-squared error of the resulting estimator with respect to the number of
iterations $n$. More precisely, we show that the mean-squared error can be
decomposed into the sum of two terms: a leading one of order
$\mathcal{O}(n^{-1/2})$ with explicit dependence on a minimax-optimal
asymptotic covariance matrix, and a second-order term of order
$\mathcal{O}(n^{-3/4})$ where the power $3/4$ can not be improved in general.
We also extend this result to the $p$-th moment bound keeping optimal scaling
of the remainders with respect to $n$. Our analysis relies on the properties of
the SGD iterates viewed as a time-homogeneous Markov chain. In particular, we
establish that this chain is geometrically ergodic with respect to a suitably
defined weighted Wasserstein semimetric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparsePO: Controlling Preference Alignment of <span class="highlight-title">LLM</span>s via Sparse Token
  Masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) has proven an effective step for aligning
language models to human-desired behaviors. Current variants, following the
offline Direct Preference Optimization objective, have focused on a strict
setting where all tokens are contributing signals of KL divergence and rewards
to the loss function. However, human preference is not affected by each word in
a sequence equally but is often dependent on specific words or phrases, e.g.
existence of toxic terms leads to non-preferred responses. Based on this
observation, we argue that not all tokens should be weighted equally during PO
and propose a flexible objective termed SparsePO, that aims to automatically
learn to weight the KL divergence and reward corresponding to each token during
PO training. We propose two different variants of weight-masks that can either
be derived from the reference model itself or learned on the fly. Notably, our
method induces sparsity in the learned masks, allowing the model to learn how
to best weight reward and KL divergence contributions at the token level,
learning an optimal level of mask sparsity. Extensive experiments on multiple
domains, including sentiment control, dialogue, text summarization and
text-to-code generation, illustrate that our approach assigns meaningful
weights to tokens according to the target task, generates more responses with
the desired preference and improves reasoning tasks by up to 2 percentage
points compared to other token- and response-level PO methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 papges, 9 figures, 5 tables. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CR-CTC: Consistency regularization on CTC for improved speech
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, Daniel Povey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connectionist Temporal Classification (CTC) is a widely used method for
automatic speech recognition (ASR), renowned for its simplicity and
computational efficiency. However, it often falls short in recognition
performance compared to transducer or systems combining CTC and attention-based
encoder-decoder (CTC/AED). In this work, we propose the Consistency-Regularized
CTC (CR-CTC), which enforces consistency between two CTC distributions obtained
from different augmented views of the input speech mel-spectrogram. We provide
in-depth insights into its essential behaviors from three perspectives: 1) it
conducts self-distillation between random pairs of sub-models that process
different augmented views; 2) it learns contextual representation through
masked prediction for positions within time-masked regions, especially when we
increase the amount of time masking; 3) it suppresses the extremely peaky CTC
distributions, thereby reducing overfitting and improving the generalization
ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech
datasets demonstrate the effectiveness of our CR-CTC, which achieves
performance comparable to, or even slightly better than, that of transducer and
CTC/AED.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamSat: Towards a General 3D Model for Novel View Synthesis of Space
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nidhi Mathihalli, Audrey Wei, Giovanni Lavezzi, Peng Mun Siew, Victor Rodriguez-Fernandez, Hodei Urrutxua, Richard Linares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis (NVS) enables to generate new images of a scene or
convert a set of 2D images into a comprehensive 3D model. In the context of
Space Domain Awareness, since space is becoming increasingly congested, NVS can
accurately map space objects and debris, improving the safety and efficiency of
space operations. Similarly, in Rendezvous and Proximity Operations missions,
3D models can provide details about a target object's shape, size, and
orientation, allowing for better planning and prediction of the target's
behavior. In this work, we explore the generalization abilities of these
reconstruction techniques, aiming to avoid the necessity of retraining for each
new scene, by presenting a novel approach to 3D spacecraft reconstruction from
single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art
single-view reconstruction model, on a high-quality dataset of 190 high-quality
spacecraft models and integrating it into the DreamGaussian framework. We
demonstrate consistent improvements in reconstruction quality across multiple
metrics, including Contrastive Language-Image Pretraining (CLIP) score
(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity
Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)
(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method
addresses the lack of domain-specific 3D reconstruction tools in the space
industry by leveraging state-of-the-art diffusion models and 3D Gaussian
splatting techniques. This approach maintains the efficiency of the
DreamGaussian framework while enhancing the accuracy and detail of spacecraft
reconstructions. The code for this work can be accessed on GitHub
(https://github.com/ARCLab-MIT/space-nvs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 75th International Astronautical Congress, October
  2024, Milan, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperINF: Unleashing the HyperPower of the Schulz's Method for Data
  Influence Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhou, Simin Fan, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence functions provide a principled method to assess the contribution of
individual training samples to a specific target. Yet, their high computational
costs limit their applications on large-scale models and datasets. Existing
methods proposed for influence function approximation have significantly
reduced the computational overheads. However, they mostly suffer from
inaccurate estimation due to the lack of strong convergence guarantees from the
algorithm. The family of hyperpower methods are well-known for their rigorous
convergence guarantees on matrix inverse approximation, while the matrix
multiplication operation can involve intractable memory and computation costs
on large-scale models. We propose HyperINF, an efficient and accurate influence
function approximation method which leverages the hyperpower method,
specifically Schulz's iterative algorithm.
  To deal with the computation-intensive matrix multiplication, we incorporate
the generalized fisher information (GFIM) as a low-rank approximation of the
Hessian matrix, which reduces the memory and computation overheads to constant
costs independent of ranks on LoRA-tuned models.
  We first demonstrate the superior accuracy and stability of \method compared
to other baselines through a synthetic convergence simulation for matrix
inversion. We further validate the efficacy of \method through extensive
real-world data attribution tasks, including mislabeled data detection and data
selection for LLM and VLM fine-tuning.
  On LoRA-tuned models, HyperINF achieves superior downstream performance with
minimal memory and computational overhead, while other baselines suffer from
significant degradation. Our codebase is available at
https://github.com/Blackzxy/HyperINF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Science<span class="highlight-title">Agent</span>Bench: Toward Rigorous Assessment of Language <span class="highlight-title">Agent</span>s for
  Data-Driven Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements of language language models (LLMs) have piqued growing
interest in developing LLM-based language agents to automate scientific
discovery end-to-end, which has sparked both excitement and skepticism about
the true capabilities of such agents. In this work, we argue that for an agent
to fully automate scientific discovery, it must be able to complete all
essential tasks in the workflow. Thus, we call for rigorous assessment of
agents on individual tasks in a scientific workflow before making bold claims
on end-to-end automation. To this end, we present ScienceAgentBench, a new
benchmark for evaluating language agents for data-driven scientific discovery.
To ensure the scientific authenticity and real-world relevance of our
benchmark, we extract 102 tasks from 44 peer-reviewed publications in four
disciplines and engage nine subject matter experts to validate them. We unify
the target output for every task to a self-contained Python program file and
employ an array of evaluation metrics to examine the generated programs,
execution results, and costs. Each task goes through multiple rounds of manual
validation by annotators and subject matter experts to ensure its annotation
quality and scientific plausibility. We also propose two effective strategies
to mitigate data contamination concerns. Using our benchmark, we evaluate five
open-weight and proprietary LLMs, each with three frameworks: direct prompting,
OpenHands, and self-debug. Given three attempts for each task, the
best-performing agent can only solve 32.4% of the tasks independently and 34.3%
with expert-provided knowledge. These results underscore the limited capacities
of current language agents in generating code for data-driven discovery, let
alone end-to-end automation for scientific research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compression via Pre-trained Transformers: A Study on Byte-Level
  <span class="highlight-title">Multimodal</span> Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Heurtel-Depeiges, Anian Ruoss, Joel Veness, Tim Genewein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have recently been shown to be strong data compressors.
However, when accounting for their excessive parameter count, their compression
ratios are actually inferior to standard compression algorithms. Moreover,
naively reducing the number of parameters may not necessarily help as it leads
to worse predictions and thus weaker compression. In this paper, we conduct a
large-scale empirical study to investigate whether there is a sweet spot where
competitive compression ratios with pre-trained vanilla transformers are
possible. To this end, we train families of models on 165GB of raw byte
sequences of either text, image, or audio data (and all possible combinations
of the three) and then compress 1GB of out-of-distribution (OOD) data from each
modality. We find that relatively small models (i.e., millions of parameters)
can outperform standard general-purpose compression algorithms (gzip, LZMA2)
and even domain-specific compressors (PNG, JPEG 2000, FLAC) - even when
factoring in parameter count. We achieve, e.g., the lowest compression ratio of
0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and
dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we
investigate the effect of unimodal versus multimodal training. We find that
even small models can be trained to perform well on multiple modalities, but,
in contrast to previously reported results with large-scale foundation models,
transfer to unseen modalities is generally weak.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TidalDecode: Fast and Accurate <span class="highlight-title">LLM</span> Decoding with Position Persistent
  Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have driven significant advancements across
diverse NLP tasks, with long-context models gaining prominence for handling
extended inputs. However, the expanding key-value (KV) cache size required by
Transformer architectures intensifies the memory constraints, particularly
during the decoding phase, creating a significant bottleneck. Existing sparse
attention mechanisms designed to address this bottleneck have two limitations:
(1) they often fail to reliably identify the most relevant tokens for
attention, and (2) they overlook the spatial coherence of token selection
across consecutive Transformer layers, which can lead to performance
degradation and substantial overhead in token selection. This paper introduces
TidalDecode, a simple yet effective algorithm and system for fast and accurate
LLM decoding through position persistent sparse attention. TidalDecode
leverages the spatial coherence of tokens selected by existing sparse attention
methods and introduces a few token selection layers that perform full attention
to identify the tokens with the highest attention scores, while all other
layers perform sparse attention with the pre-selected tokens. This design
enables TidalDecode to substantially reduce the overhead of token selection for
sparse attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely
matches the generative performance of full attention methods while reducing the
LLM decoding latency by up to 2.1x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Function Gradient Approximation with Random Shallow ReLU Networks with
  Control Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lamperski, Siddharth Salapaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are widely used to approximate unknown functions in control.
A common neural network architecture uses a single hidden layer (i.e. a shallow
network), in which the input parameters are fixed in advance and only the
output parameters are trained. The typical formal analysis asserts that if
output parameters exist to approximate the unknown function with sufficient
accuracy, then desired control performance can be achieved. A long-standing
theoretical gap was that no conditions existed to guarantee that, for the fixed
input parameters, required accuracy could be obtained by training the output
parameters. Our recent work has partially closed this gap by demonstrating that
if input parameters are chosen randomly, then for any sufficiently smooth
function, with high-probability there are output parameters resulting in
$O((1/m)^{1/2})$ approximation errors, where $m$ is the number of neurons.
However, some applications, notably continuous-time value function
approximation, require that the network approximates the both the unknown
function and its gradient with sufficient accuracy. In this paper, we show that
randomly generated input parameters and trained output parameters result in
gradient errors of $O((\log(m)/m)^{1/2})$, and additionally, improve the
constants from our prior work. We show how to apply the result to policy
evaluation problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review for American Control Conference, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-oriented Clustering of Visual Latent Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Qi, Haocheng Yin, Heng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate a study of the geometry of the visual representation space -- the
information channel from the vision encoder to the action decoder -- in an
image-based control pipeline learned from behavior cloning. Inspired by the
phenomenon of neural collapse (NC) in image classification, we investigate
whether a similar law of clustering emerges in the visual representation space.
Since image-based control is a regression task without explicitly defined
classes, the central piece of the puzzle lies in determining according to what
implicit classes the visual features cluster, if such a law exists. Focusing on
image-based planar pushing, we posit the most important role of the visual
representation in a control task is to convey a goal to the action decoder. We
then classify training samples of expert demonstrations into eight
"control-oriented" classes based on (a) the relative pose between the object
and the target in the input or (b) the relative pose of the object induced by
expert actions in the output, where one class corresponds to one relative pose
orthant (REPO). Across four different instantiations of architecture, we report
the prevalent emergence of control-oriented clustering in the visual
representation space according to the eight REPOs. Beyond empirical
observation, we show such a law of clustering can be leveraged as an
algorithmic tool to improve test-time performance when training a policy with
limited expert demonstrations. Particularly, we pretrain the vision encoder
using NC as a regularization to encourage control-oriented clustering of the
visual features. Surprisingly, such an NC-pretrained vision encoder, when
finetuned end-to-end with the action decoder, boosts the test-time performance
by 10% to 35% in the low-data regime. Real-world vision-based planar pushing
experiments confirmed the surprising advantage of control-oriented visual
representation pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SELECT: A Large-Scale <span class="highlight-title">Benchmark</span> of Data Curation Strategies for <span class="highlight-title">Image</span>
  Classification <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Feuer, Jiawei Xu, Niv Cohen, Patrick Yubeaton, Govind Mittal, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data curation is the problem of how to collect and organize samples into a
dataset that supports efficient learning. Despite the centrality of the task,
little work has been devoted towards a large-scale, systematic comparison of
various curation methods. In this work, we take steps towards a formal
evaluation of data curation strategies and introduce SELECT, the first
large-scale benchmark of curation strategies for image classification.
  In order to generate baseline methods for the SELECT benchmark, we create a
new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K
to date. Our dataset extends ImageNet with 5 new training-data shifts, each
approximately the size of ImageNet-1K itself, and each assembled using a
distinct curation strategy. We evaluate our data curation baselines in two
ways: (i) using each training-data shift to train identical image
classification models from scratch (ii) using the data itself to fit a
pretrained self-supervised representation.
  Our findings show interesting trends, particularly pertaining to recent
methods for data curation such as synthetic data generation and lookup based on
CLIP embeddings. We show that although these strategies are highly competitive
for certain tasks, the curation strategy used to assemble the original
ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark
can illuminate the path for new methods to further reduce the gap. We release
our checkpoints, code, documentation, and a link to our dataset at
https://github.com/jimmyxu123/SELECT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreSh: Frequency Shifting for Accelerated Neural Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Kania, Marko Mihajlovic, Sergey Prokudin, Jacek Tabor, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) have recently gained attention as a
powerful approach for continuously representing signals such as images, videos,
and 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to
exhibit a low-frequency bias, limiting their ability to capture high-frequency
details accurately. This limitation is typically addressed by incorporating
high-frequency input embeddings or specialized activation layers. In this work,
we demonstrate that these embeddings and activations are often configured with
hyperparameters that perform well on average but are suboptimal for specific
input signals under consideration, necessitating a costly grid search to
identify optimal settings. Our key observation is that the initial frequency
spectrum of an untrained model's output correlates strongly with the model's
eventual performance on a given target signal. Leveraging this insight, we
propose frequency shifting (or FreSh), a method that selects embedding
hyperparameters to align the frequency spectrum of the model's initial output
with that of the target signal. We show that this simple initialization
technique improves performance across various neural representation methods and
tasks, achieving results comparable to extensive hyperparameter sweeps but with
only marginal computational overhead compared to training a single model with
default hyperparameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhotoReg: Photometrically Registering 3D Gaussian Splatting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwen Yuan, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building accurate representations of the environment is critical for
intelligent robots to make decisions during deployment. Advances in
photorealistic environment models have enabled robots to develop
hyper-realistic reconstructions, which can be used to generate images that are
intuitive for human inspection. In particular, the recently introduced
\ac{3DGS}, which describes the scene with up to millions of primitive
ellipsoids, can be rendered in real time. \ac{3DGS} has rapidly gained
prominence. However, a critical unsolved problem persists: how can we fuse
multiple \ac{3DGS} into a single coherent model? Solving this problem will
enable robot teams to jointly build \ac{3DGS} models of their surroundings. A
key insight of this work is to leverage the {duality} between photorealistic
reconstructions, which render realistic 2D images from 3D structure, and
\emph{3D foundation models}, which predict 3D structure from image pairs. To
this end, we develop PhotoReg, a framework to register multiple photorealistic
\ac{3DGS} models with 3D foundation models. As \ac{3DGS} models are generally
built from monocular camera images, they have \emph{arbitrary scale}. To
resolve this, PhotoReg actively enforces scale consistency among the different
\ac{3DGS} models by considering depth estimates within these models. Then, the
alignment is iteratively refined with fine-grained photometric losses to
produce high-quality fused \ac{3DGS} models. We rigorously evaluate PhotoReg on
both standard benchmark datasets and our custom-collected datasets, including
with two quadruped robots. The code is released at
\url{ziweny11.github.io/photoreg}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic Literature <span class="highlight-title">Review</span> of Vision-Based Approaches to Outdoor
  Livestock Monitoring with Lessons from Wildlife Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stacey D. Scott, Zayn J. Abbas, Feerass Ellid, Eli-Henry Dykhne, Muhammad Muhaiminul Islam, Weam Ayad, Kristina Kacmorova, Dan Tulpan, Minglun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision livestock farming (PLF) aims to improve the health and welfare of
livestock animals and farming outcomes through the use of advanced
technologies. Computer vision, combined with recent advances in machine
learning and deep learning artificial intelligence approaches, offers a
possible solution to the PLF ideal of 24/7 livestock monitoring that helps
facilitate early detection of animal health and welfare issues. However, a
significant number of livestock species are raised in large outdoor habitats
that pose technological challenges for computer vision approaches. This review
provides a comprehensive overview of computer vision methods and open
challenges in outdoor animal monitoring. We include research from both the
livestock and wildlife fields in the review because of the similarities in
appearance, behaviour, and habitat for many livestock and wildlife. We focus on
large terrestrial mammals, such as cattle, horses, deer, goats, sheep, koalas,
giraffes, and elephants. We use an image processing pipeline to frame our
discussion and highlight the current capabilities and open technical challenges
at each stage of the pipeline. The review found a clear trend towards the use
of deep learning approaches for animal detection, counting, and multi-species
classification. We discuss in detail the applicability of current vision-based
methods to PLF contexts and promising directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Fine-Tuning of Generalist Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Bagatella, Jonas Hübotter, Georg Martius, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained generalist policies are rapidly gaining relevance in robot
learning due to their promise of fast adaptation to novel, in-domain tasks.
This adaptation often relies on collecting new demonstrations for a specific
task of interest and applying imitation learning algorithms, such as behavioral
cloning. However, as soon as several tasks need to be learned, we must decide
which tasks should be demonstrated and how often? We study this multi-task
problem and explore an interactive framework in which the agent adaptively
selects the tasks to be demonstrated. We propose AMF (Active Multi-task
Fine-tuning), an algorithm to maximize multi-task policy performance under a
limited demonstration budget by collecting demonstrations yielding the largest
information gain on the expert policy. We derive performance guarantees for AMF
under regularity assumptions and demonstrate its empirical effectiveness to
efficiently fine-tune neural policies in complex and high-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEPT: Decoupled Embeddings for Pre-training Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Model pre-training benefits from a broader data mixture to enhance
performance across domains and languages. However, training on such
heterogeneous text corpora is complex, requiring extensive and cost-intensive
efforts. Since these data sources vary in lexical, syntactic, and semantic
aspects, they cause negative interference or the "curse of multilinguality". We
propose a novel pre-training framework to alleviate this curse. Our method,
DEPT, decouples the embedding layers from the transformer body while
simultaneously training the latter in multiple contexts. DEPT enables the model
to train without being bound to a shared global vocabulary. DEPT: (1) can train
robustly and effectively under significant data heterogeneity, (2) reduces the
parameter count of the token embeddings by up to 80% and the communication
costs by 675x for billion-scale models (3) enhances model generalization and
plasticity in adapting to new languages and domains, and (4) allows training
with custom optimized vocabulary per data source. We prove DEPT's potential by
performing the first vocabulary-agnostic federated multilingual pre-training of
a 1.3 billion-parameter model across high and low-resource languages, reducing
its parameter count by 409 million.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRIDA: Free-Rider Detection using Privacy Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pol G. Recasens, Ádám Horváth, Alberto Gutierrez-Torre, Jordi Torres, Josep Ll. Berral, Balázs Pejó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is increasingly popular as it enables multiple parties
with limited datasets and resources to train a high-performing machine learning
model collaboratively. However, similarly to other collaborative systems,
federated learning is vulnerable to free-riders -- participants who do not
contribute to the training but still benefit from the shared model. Free-riders
not only compromise the integrity of the learning process but also slow down
the convergence of the global model, resulting in increased costs for the
honest participants.
  To address this challenge, we propose FRIDA: free-rider detection using
privacy attacks, a framework that leverages inference attacks to detect
free-riders. Unlike traditional methods that only capture the implicit effects
of free-riding, FRIDA directly infers details of the underlying training
datasets, revealing characteristics that indicate free-rider behaviour. Through
extensive experiments, we demonstrate that membership and property inference
attacks are effective for this purpose. Our evaluation shows that FRIDA
outperforms state-of-the-art methods, especially in non-IID settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RelUNet: Relative Channel Fusion U-Net for Multichannel Speech
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Aldarmaki, Thamar Solorio, Bhiksha Raj, Hanan Aldarmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural multi-channel speech enhancement models, in particular those based on
the U-Net architecture, demonstrate promising performance and generalization
potential. These models typically encode input channels independently, and
integrate the channels during later stages of the network. In this paper, we
propose a novel modification of these models by incorporating relative
information from the outset, where each channel is processed in conjunction
with a reference channel through stacking. This input strategy exploits
comparative differences to adaptively fuse information between channels,
thereby capturing crucial spatial information and enhancing the overall
performance. The experiments conducted on the CHiME-3 dataset demonstrate
improvements in speech enhancement metrics across various architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Thimonier, José Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervision is often used for pre-training to foster performance on a
downstream task by constructing meaningful representations of samples.
Self-supervised learning (SSL) generally involves generating different views of
the same sample and thus requires data augmentations that are challenging to
construct for tabular data. This constitutes one of the main challenges of
self-supervision for structured data. In the present work, we propose a novel
augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on
a Joint Embedding Predictive Architecture (JEPA) and is akin to mask
reconstruction in the latent space. It involves predicting the latent
representation of one subset of features from the latent representation of a
different subset within the same sample, thereby learning rich representations
without augmentations. We use our method as a pre-training technique and train
several deep classifiers on the obtained representation. Our experimental
results demonstrate a substantial improvement in both classification and
regression tasks, outperforming models trained directly on samples in their
original data space. Moreover, T-JEPA enables some methods to consistently
outperform or match the performance of traditional methods likes Gradient
Boosted Decision Trees. To understand why, we extensively characterize the
obtained representations and show that T-JEPA effectively identifies relevant
features for downstream tasks without access to the labels. Additionally, we
introduce regularization tokens, a novel regularization method critical for
training of JEPA-based models on structured data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mDPO: Conditional Preference Optimization for <span class="highlight-title">Multimodal</span> Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/mDPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Membership Inference Attacks on <span class="highlight-title">LLM</span>s are Rushing Nowhere (and How
  to Fix It) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whether LLMs memorize their training data and what this means, from privacy
leakage to detecting copyright violations -- has become a rapidly growing area
of research over the last two years. In recent months, more than 10 new methods
have been proposed to perform Membership Inference Attacks (MIAs) against LLMs.
Contrary to traditional MIAs which rely on fixed -- but randomized -- records
or models, these methods are mostly evaluated on datasets collected post-hoc.
Sets of members and non-members, used to evaluate the MIA, are constructed
using informed guesses after the release of a model. This lack of randomization
raises concerns of a distribution shift between members and non-members. In the
first part, we review the literature on MIAs against LLMs. While most work
focuses on sequence-level MIAs evaluated in post-hoc setups, we show that a
range of target models, motivations and units of interest have been considered
in the literature. We then quantify distribution shifts present in the 6
datasets used in the literature, ranging from books to papers, using a bag of
word classifier. Our analysis reveals that all of them suffer from severe
distribution shifts. This challenges the validity of using such setups to
measure LLM memorization and may undermine the benchmarking of recently
proposed methods. Yet, all hope might not be lost. In the second part, we
introduce important considerations to properly evaluate MIAs against LLMs and
discuss potential ways forward: randomized test splits, injections of
randomized (unique) sequences, randomized finetuning, and post-hoc control
methods. While each option comes with its advantages and limitations, we
believe they collectively provide solid grounds to guide the development of MIA
methods and study LLM memorization. We conclude by proposing comprehensive,
easy-to-use benchmarks for sequence- and document-level MIAs against LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00700v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00700v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Savadikar, Xi Song, Tianfu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Generative Parameter-Efficient Fine-Tuning (GIFT) for adapting
pretrained Transformer backbones on downstream tasks. GIFT learns to generate
the fine-tuned weights for a layer directly from its pretrained weights. The
GIFT network is parameterized in a minimally-simple way by two linear layers
(without bias terms), and is shared by different pretrained layers selected for
fine-tuning (e.g., the Query layers), which result in significantly fewer
trainable parameters compared to the layer-specific methods like Low-Rank
Adapter (LoRA). We also show this formulation bridges parameter-efficient
fine-tuning and representation fine-tuning. We perform comprehensive
experiments on natural language tasks (commonsense and arithmetic reasoning,
instruction tuning, and sequence classification) and computer vision tasks
(fine-grained classification). We obtain the best performance and parameter
efficiency among baselines on commonsense and arithmetic reasoning, and
instruction following using the Llama family of models and on visual
recognition benchmarks using Vision Transformers. Notably, compared to LoRA, we
obtain 5.7% absolute increase in average accuracy with 14 times reduction of
parameters on Commonsense170k using Llama-3 (8B), and 5.4% absolute increase in
the win rate with 4 times reduction of parameters using Llama-2 (7B) during
instruction tuning. Our GIFT also obtains a slightly higher win rate on
instruction tuning than GPT 3.5 (Turbo 1106).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: https://savadikarc.github.io/gift</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jogging the Memory of Unlearned <span class="highlight-title">LLM</span>s Through Targeted Relearning Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, Virginia Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning is a promising approach to mitigate undesirable
memorization of training data in LLMs. However, in this work we show that
existing approaches for unlearning in LLMs are surprisingly susceptible to a
simple set of targeted relearning attacks. With access to only a small and
potentially loosely related set of data, we find that we can "jog" the memory
of unlearned models to reverse the effects of unlearning. For example, we show
that relearning on public medical articles can lead an unlearned LLM to output
harmful knowledge about bioweapons, and relearning general wiki information
about the book series Harry Potter can force the model to output verbatim
memorized text. We formalize this unlearning-relearning pipeline, explore the
attack across three popular unlearning benchmarks, and discuss future
directions and guidelines that result from our study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Successor Features with Distributed Hebbian Temporal Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13391v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13391v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenii Dzhivelikian, Petr Kuderov, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to address the challenge of online
temporal memory learning for decision-making under uncertainty in
non-stationary, partially observable environments. The proposed algorithm,
Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism
and a multicomponent neuron model. DHTM aims to capture sequential data
relationships and make cumulative predictions about future observations,
forming Successor Features (SF). Inspired by neurophysiological models of the
neocortex, the algorithm utilizes distributed representations, sparse
transition matrices, and local Hebbian-like learning rules to overcome the
instability and slow learning process of traditional temporal memory algorithms
like RNN and HMM. Experimental results demonstrate that DHTM outperforms LSTM
and a biologically inspired HMM-like algorithm, CSCG, in the case of
non-stationary datasets. Our findings suggest that DHTM is a promising approach
for addressing the challenges of online sequence learning and planning in
dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full Line Code Completion: Bringing AI to Desktop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Semenkin, Vitaliy Bibaev, Yaroslav Sokolov, Kirill Krylov, Alexey Kalina, Anna Khannanova, Danila Savenkov, Darya Rovdo, Igor Davidenko, Kirill Karnaukhov, Maxim Vakhrushev, Mikhail Kostyukov, Mikhail Podvitskii, Petr Surkov, Yaroslav Golubev, Nikita Povarov, Timofey Bryksin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several industrial solutions for the problem of multi-token
code completion appeared, each making a great advance in the area but mostly
focusing on cloud-based runtime and avoiding working on the end user's device.
  In this work, we describe our approach for building a multi-token code
completion feature for the JetBrains' IntelliJ Platform, which we call Full
Line Code Completion. The feature suggests only syntactically correct code and
works fully locally, i.e., data querying and the generation of suggestions
happens on the end user's machine. We share important time and
memory-consumption restrictions, as well as design principles that a code
completion engine should satisfy. Working entirely on the end user's device,
our code completion engine enriches user experience while being not only fast
and compact but also secure. We share a number of useful techniques to meet the
stated development constraints and also describe offline and online evaluation
pipelines that allowed us to make better decisions.
  Our online evaluation shows that the usage of the tool leads to 1.3 times
more Python code in the IDE being produced by code completion. The described
solution was initially started with a help of researchers and was then bundled
into all JetBrains IDEs where it is now used by millions of users. Thus, we
believe that this work is useful for bridging academia and industry, providing
researchers with the knowledge of what happens when complex research-based
solutions are integrated into real products.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stateful <span class="highlight-title">Large Language Model</span> Serving with Pensieve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfan Yu, Jinkun Lin, Jinyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are wildly popular today and it is important to
serve them efficiently. Existing LLM serving systems are stateless across
requests. Consequently, when LLMs are used in the common setting of multi-turn
conversations, a growing log of the conversation history must be processed
alongside any request by the serving system at each turn, resulting in repeated
processing.
  In this paper, we design $Pensieve$, a system optimized for multi-turn
conversation LLM serving. $Pensieve$ maintains the conversation state across
requests by caching previously processed history to avoid duplicate processing.
$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to
efficiently store and retrieve cached data. $Pensieve$ also generalizes the
recent PagedAttention kernel to support attention between multiple input tokens
with a GPU cache spread over non-contiguous memory. Our evaluation shows that
$Pensieve$ can achieve $1.14$-$3.0\times$ the throughput of vLLM and
TensorRT-LLM and significantly reduce latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Convex Optimization with a Separation Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakaria Mhammedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new projection-free algorithm for Online Convex
Optimization (OCO) with a state-of-the-art regret guarantee among
separation-based algorithms. Existing projection-free methods based on the
classical Frank-Wolfe algorithm achieve a suboptimal regret bound of
$O(T^{3/4})$, while more recent separation-based approaches guarantee a regret
bound of $O(\kappa \sqrt{T})$, where $\kappa$ denotes the asphericity of the
feasible set, defined as the ratio of the radii of the containing and contained
balls. However, for ill-conditioned sets, $\kappa$ can be arbitrarily large,
potentially leading to poor performance. Our algorithm achieves a regret bound
of $\widetilde{O}(\sqrt{dT} + \kappa d)$, while requiring only
$\widetilde{O}(1)$ calls to a separation oracle per round. Crucially, the main
term in the bound, $\widetilde{O}(\sqrt{d T})$, is independent of $\kappa$,
addressing the limitations of previous methods. Additionally, as a by-product
of our analysis, we recover the $O(\kappa \sqrt{T})$ regret bound of existing
OCO algorithms with a more straightforward analysis and improve the regret
bound for projection-free online exp-concave optimization. Finally, for
constrained stochastic convex optimization, we achieve a state-of-the-art
convergence rate of $\widetilde{O}(\sigma/\sqrt{T} + \kappa d/T)$, where
$\sigma$ represents the noise in the stochastic gradients, while requiring only
$\widetilde{O}(1)$ calls to a separation oracle per iteration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAnDOIT: Causal Discovery with Observational and Interventional Data
  from Time-Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Castri, Sariah Mghames, Marc Hanheide, Nicola Bellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of cause-and-effect is of the utmost importance in many branches of
science, but also for many practical applications of intelligent systems. In
particular, identifying causal relationships in situations that include hidden
factors is a major challenge for methods that rely solely on observational data
for building causal models. This paper proposes CAnDOIT, a causal discovery
method to reconstruct causal models using both observational and interventional
time-series data. The use of interventional data in the causal analysis is
crucial for real-world applications, such as robotics, where the scenario is
highly complex and observational data alone are often insufficient to uncover
the correct causal structure. Validation of the method is performed initially
on randomly generated synthetic models and subsequently on a well-known
benchmark for causal structure learning in a robotic manipulation environment.
The experiments demonstrate that the approach can effectively handle data from
interventions and exploit them to enhance the accuracy of the causal analysis.
A Python implementation of CAnDOIT has also been developed and is publicly
available on GitHub: https://github.com/lcastri/causalflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Advanced Intelligent Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Aggregation of Prediction Intervals under Unsupervised Domain
  Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Ge, Debarghya Mukherjee, Jianqing Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning models are increasingly deployed in dynamic environments,
it becomes paramount to assess and quantify uncertainties associated with
distribution shifts. A distribution shift occurs when the underlying
data-generating process changes, leading to a deviation in the model's
performance. The prediction interval, which captures the range of likely
outcomes for a given prediction, serves as a crucial tool for characterizing
uncertainties induced by their underlying distribution. In this paper, we
propose methodologies for aggregating prediction intervals to obtain one with
minimal width and adequate coverage on the target domain under unsupervised
domain shift, under which we have labeled samples from a related source domain
and unlabeled covariates from the target domain. Our analysis encompasses
scenarios where the source and the target domain are related via i) a bounded
density ratio, and ii) a measure-preserving transformation. Our proposed
methodologies are computationally efficient and easy to implement. Beyond
illustrating the performance of our method through real-world datasets, we also
delve into the theoretical details. This includes establishing rigorous
theoretical guarantees, coupled with finite sample bounds, regarding the
coverage and width of our prediction intervals. Our approach excels in
practical applications and is underpinned by a solid theoretical framework,
ensuring its reliability and effectiveness across diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principal-<span class="highlight-title">Agent</span> Reinforcement Learning: Orchestrating AI <span class="highlight-title">Agent</span>s with
  Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dima Ivanov, Paul Dütting, Inbal Talgam-Cohen, Tonghan Wang, David C. Parkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing deployment of AI is shaping the future landscape of the
internet, which is set to become an integrated ecosystem of AI agents.
Orchestrating the interaction among AI agents necessitates decentralized,
self-sustaining mechanisms that harmonize the tension between individual
interests and social welfare. In this paper we tackle this challenge by
synergizing reinforcement learning with principal-agent theory from economics.
Taken separately, the former allows unrealistic freedom of intervention, while
the latter struggles to scale in sequential settings. Combining them achieves
the best of both worlds. We propose a framework where a principal guides an
agent in a Markov Decision Process (MDP) using a series of contracts, which
specify payments by the principal based on observable outcomes of the agent's
actions. We present and analyze a meta-algorithm that iteratively optimizes the
policies of the principal and agent, showing its equivalence to a contraction
operator on the principal's Q-function, and its convergence to subgame-perfect
equilibrium. We then scale our algorithm with deep Q-learning and analyze its
convergence in the presence of approximation error, both theoretically and
through experiments with randomly generated binary game-trees. Extending our
framework to multiple agents, we apply our methodology to the combinatorial
Coin Game. Addressing this multi-agent sequential social dilemma is a promising
first step toward scaling our approach to more complex, real-world instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creative Beam Search: <span class="highlight-title">LLM</span>-as-a-Judge For Improving Response <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are revolutionizing several areas, including artificial
creativity. However, the process of generation in machines profoundly diverges
from that observed in humans. In particular, machine generation is
characterized by a lack of intentionality and an underlying creative process.
We propose a method called Creative Beam Search that uses Diverse Beam Search
and LLM-as-a-Judge to perform response generation and response validation. The
results of a qualitative experiment show how our approach can provide better
output than standard sampling techniques. We also show that the response
validation step is a necessary complement to the response generation step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a short paper at the 15th International Conference on
  Computational Creativity (ICCC'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forest Proximities for Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Shaw, Jake Rhodes, Soukaina Filali Boubrahimi, Kevin R. Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RF-GAP has recently been introduced as an improved random forest proximity
measure. In this paper, we present PF-GAP, an extension of RF-GAP proximities
to proximity forests, an accurate and efficient time series classification
model. We use the forest proximities in connection with Multi-Dimensional
Scaling to obtain vector embeddings of univariate time series, comparing the
embeddings to those obtained using various time series distance measures. We
also use the forest proximities alongside Local Outlier Factors to investigate
the connection between misclassified points and outliers, comparing with
nearest neighbor classifiers which use time series distance measures. We show
that the forest proximities may exhibit a stronger connection between
misclassified points and outliers than nearest neighbor classifiers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Foundation Models as Data Compression: On Information, Model
  Weights and Copyright Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Claudia Cevenini, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training process of foundation models as for other classes of deep
learning systems is based on minimizing the reconstruction error over a
training set. For this reason, they are susceptible to the memorization and
subsequent reproduction of training samples. In this paper, we introduce a
training-as-compressing perspective, wherein the model's weights embody a
compressed representation of the training data. From a copyright standpoint,
this point of view implies that the weights could be considered a reproduction
or a derivative work of a potentially protected set of works. We investigate
the technical and legal challenges that emerge from this framing of the
copyright of outputs generated by foundation models, including their
implications for practitioners and researchers. We demonstrate that adopting an
information-centric approach to the problem presents a promising pathway for
tackling these emerging complex legal issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight presentation at GenLaw'24, see
  https://www.genlaw.org/2024-icml-papers#training-foundation-models-as-data-compression-on-information-model-weights-and-copyright-law</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For <span class="highlight-title">Generation</span> Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Based Optimal Design of Fibrillar Adhesives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05928v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05928v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shojaeifard, Matteo Ferraresso, Alessandro Lucantonio, Mattia Bacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fibrillar adhesion, observed in animals like beetles, spiders, and geckos,
relies on nanoscopic or microscopic fibrils to enhance surface adhesion via
'contact splitting.' This concept has inspired engineering applications across
robotics, transportation, and medicine. Recent studies suggest that functional
grading of fibril properties can improve adhesion, but this is a complex design
challenge that has only been explored in simplified geometries. While machine
learning (ML) has gained traction in adhesive design, no previous attempts have
targeted fibril-array scale optimization. In this study, we propose an ML-based
tool that optimizes the distribution of fibril compliance to maximize adhesive
strength. Our tool, featuring two deep neural networks (DNNs), recovers
previous design results for simple geometries and introduces novel solutions
for complex configurations. The Predictor DNN estimates adhesive strength based
on random compliance distributions, while the Designer DNN optimizes compliance
for maximum strength using gradient-based optimization. Our method
significantly reduces test error and accelerates the optimization process,
offering a high-performance solution for designing fibrillar adhesives and
micro-architected materials aimed at fracture resistance by achieving equal
load sharing (ELS).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking Leading Safety-Aligned <span class="highlight-title">LLM</span>s with Simple Adaptive Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02151v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02151v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that even the most recent safety-aligned LLMs are not robust to
simple adaptive jailbreaking attacks. First, we demonstrate how to successfully
leverage access to logprobs for jailbreaking: we initially design an
adversarial prompt template (sometimes adapted to the target LLM), and then we
apply random search on a suffix to maximize a target logprob (e.g., of the
token "Sure"), potentially with multiple restarts. In this way, we achieve 100%
attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,
Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,
Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that
was adversarially trained against the GCG attack. We also show how to jailbreak
all Claude models -- that do not expose logprobs -- via either a transfer or
prefilling attack with a 100% success rate. In addition, we show how to use
random search on a restricted set of tokens for finding trojan strings in
poisoned models -- a task that shares many similarities with jailbreaking --
which is the algorithm that brought us the first place in the SaTML'24 Trojan
Detection Competition. The common theme behind these attacks is that adaptivity
is crucial: different models are vulnerable to different prompting templates
(e.g., R2D2 is very sensitive to in-context learning prompts), some models have
unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and
in some settings, it is crucial to restrict the token search space based on
prior knowledge (e.g., for trojan detection). For reproducibility purposes, we
provide the code, logs, and jailbreak artifacts in the JailbreakBench format at
https://github.com/tml-epfl/llm-adaptive-attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved
  writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B),
  jailbreak artifacts for all attacks are available, evaluation with different
  judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots
  over iterations, ablation on the suffix length for random search), examples
  of jailbroken generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Model-Agnostic Multi-Group Equivariant Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razan Baltaji, Sourya Basu, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing model-agnostic group equivariant networks, such as equitune
(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be
computationally expensive for large product groups. We address this problem by
providing efficient model-agnostic equivariant designs for two related
problems: one where the network has multiple inputs each with potentially
different groups acting on them, and another where there is a single input but
the group acting on it is a large product group. For the first design, we
initially consider a linear model and characterize the entire equivariant space
that satisfies this constraint. This characterization gives rise to a novel
fusion layer between different channels that satisfies an invariance-symmetry
(IS) constraint, which we call an IS layer. We then extend this design beyond
linear models, similar to equitune, consisting of equivariant and IS layers. We
also show that the IS layer is a universal approximator of invariant-symmetric
functions. Inspired by the first design, we use the notion of the IS property
to design a second efficient model-agnostic equivariant design for large
product groups acting on a single input. For the first design, we provide
experiments on multi-image classification where each view is transformed
independently with transformations such as rotations. We find equivariant
models are robust to such transformations and perform competitively otherwise.
For the second design, we consider three applications: language
compositionality on the SCAN dataset to product groups; fairness in natural
language generation from GPT-2 to address intersectionality; and robust
zero-shot image classification with CLIP. Overall, our methods are simple and
general, competitive with equitune and its variants, while also being
computationally more efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  Prompts Do Not Improve Performances of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses "You are a helpful assistant" as part of its
default system prompt. Despite current practices of adding personas to system
prompts, it remains unclear how different personas affect a model's performance
on objective tasks. In this study, we present a systematic evaluation of
personas in system prompts. We curate a list of 162 roles covering 6 types of
interpersonal relationships and 8 domains of expertise. Through extensive
analysis of 4 popular families of LLMs and 2,410 factual questions, we
demonstrate that adding personas in system prompts does not improve model
performance across a range of questions compared to the control setting where
no persona is added. Nevertheless, further analysis suggests that the gender,
type, and domain of the persona can all influence the resulting prediction
accuracies. We further experimented with a list of persona search strategies
and found that, while aggregating results from the best persona for each
question significantly improves prediction accuracy, automatically identifying
the best persona is challenging, with predictions often performing no better
than random selection. Overall, our findings suggest that while adding a
persona may lead to performance gains in certain settings, the effect of each
persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Steer Markovian <span class="highlight-title">Agent</span>s under Model Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Huang, Vinzenz Thoma, Zebang Shen, Heinrich H. Nax, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing incentives for an adapting population is a ubiquitous problem in a
wide array of economic applications and beyond. In this work, we study how to
design additional rewards to steer multi-agent systems towards desired policies
\emph{without} prior knowledge of the agents' underlying learning dynamics.
Motivated by the limitation of existing works, we consider a new and general
category of learning dynamics called \emph{Markovian agents}. We introduce a
model-based non-episodic Reinforcement Learning (RL) formulation for our
steering problem. Importantly, we focus on learning a \emph{history-dependent}
steering strategy to handle the inherent model uncertainty about the agents'
learning dynamics. We introduce a novel objective function to encode the
desiderata of achieving a good steering outcome with reasonable cost.
Theoretically, we identify conditions for the existence of steering strategies
to guide agents to the desired policies. Complementing our theoretical
contributions, we provide empirical algorithms to approximately solve our
objective, which effectively tackles the challenge in learning
history-dependent strategies. We demonstrate the efficacy of our algorithms
through empirical evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Fusion: Capturing Dependencies in Contrastive Learning via
  Transformer Projection Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Learning (CL) has emerged as a powerful method for training
feature extraction models using unlabeled data. Recent studies suggest that
incorporating a linear projection head post-backbone significantly enhances
model performance. In this work, we investigate the use of a transformer model
as a projection head within the CL framework, aiming to exploit the
transformer's capacity for capturing long-range dependencies across embeddings
to further improve performance. Our key contributions are fourfold: First, we
introduce a novel application of transformers in the projection head role for
contrastive learning, marking the first endeavor of its kind. Second, our
experiments reveal a compelling "Deep Fusion" phenomenon where the attention
mechanism progressively captures the correct relational dependencies among
samples from the same class in deeper layers. Third, we provide a theoretical
framework that explains and supports this "Deep Fusion" behavior. Finally, we
demonstrate through experimental results that our model achieves superior
performance compared to the existing approach of using a feed-forward layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, William Brandon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To combat the memory bandwidth-bound nature of autoregressive LLM inference,
previous research has proposed the speculative decoding frame-work. To perform
speculative decoding, a small draft model proposes candidate continuations of
the input sequence that are then verified in parallel by the base model. One
way to specify the draft model, as used in the recent Medusa decoding
framework, is as a collection of lightweight heads, called draft heads, that
operate on the base model's hidden states. To date, all existing draft heads
have been sequentially independent, meaning that they speculate tokens in the
candidate continuation independently of any preceding tokens in the candidate
continuation. In this work, we propose Hydra heads: a sequentially-dependent
drop-in replacement for standard draft heads that significantly improves the
accuracy of draft head speculation. We further explore the design space of
Hydra head training objectives and architectures, and propose a carefully tuned
Hydra head recipe, which we call Hydra++, that improves decoding throughput by
up to 1.31x and 2.70x compared to Medusa decoding and autoregressive de-coding
respectively. Overall, Hydra heads are a simple and well-motivated intervention
on standard draft heads that significantly improve the end-to-end speed of
draft head-based speculative decoding. We make our code publicly available at
https://github.com/zankner/Hydra.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The SkipSponge Attack: Sponge Weight Poisoning of Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06357v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06357v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jona te Lintelo, Stefanos Koffas, Stjepan Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sponge attacks aim to increase the energy consumption and computation time of
neural networks. In this work, we present a novel sponge attack called
SkipSponge. SkipSponge is the first sponge attack that is performed directly on
the parameters of a pre-trained model using only a few data samples. Our
experiments show that SkipSponge can successfully increase the energy
consumption of image classification models, GANs, and autoencoders requiring
fewer samples than the state-of-the-art (Sponge Poisoning). We show that
poisoning defenses are ineffective if not adjusted specifically for the defense
against SkipSponge (i.e., they decrease target layer bias values). Our work
shows that SkipSponge is more effective on the GANs and the autoencoders than
Sponge Poisoning. Additionally, SkipSponge is stealthier than Sponge Poisoning
as it does not require significant changes in the victim model's weights. Our
experiments indicate that SkipSponge can be performed even when an attacker has
access to only 1% of the entire dataset and reaches up to 13% energy increase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust <span class="highlight-title">Multimodal</span> Learning with Missing Modalities via
  Parameter-Efficient Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03986v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03986v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning seeks to utilize data from multiple sources to improve
the overall performance of downstream tasks. It is desirable for redundancies
in the data to make multimodal systems robust to missing or corrupted
observations in some correlated modalities. However, we observe that the
performance of several existing multimodal networks significantly deteriorates
if one or multiple modalities are absent at test time. To enable robustness to
missing modalities, we propose a simple and parameter-efficient adaptation
procedure for pretrained multimodal networks. In particular, we exploit
modulation of intermediate features to compensate for the missing modalities.
We demonstrate that such adaptation can partially bridge performance drop due
to missing modalities and outperform independent, dedicated networks trained
for the available modality combinations in some cases. The proposed adaptation
requires extremely small number of parameters (e.g., fewer than 1% of the total
parameters) and applicable to a wide range of modality combinations and tasks.
We conduct a series of experiments to highlight the missing modality robustness
of our proposed method on five different multimodal tasks across seven
datasets. Our proposed method demonstrates versatility across various tasks and
datasets, and outperforms existing methods for robust multimodal learning with
missing modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). 28 pages, 6 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preventing Collapse in Contrastive Learning with Orthonormal Prototypes
  (CLOP) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Manh Nguyen, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has emerged as a powerful method in deep learning,
excelling at learning effective representations through contrasting samples
from different distributions. However, neural collapse, where embeddings
converge into a lower-dimensional space, poses a significant challenge,
especially in semi-supervised and self-supervised setups. In this paper, we
first theoretically analyze the effect of large learning rates on contrastive
losses that solely rely on the cosine similarity metric, and derive a
theoretical bound to mitigate this collapse. {Building on these insights, we
propose CLOP, a novel semi-supervised loss function designed to prevent neural
collapse by promoting the formation of orthogonal linear subspaces among class
embeddings.} Unlike prior approaches that enforce a simplex ETF structure, CLOP
focuses on subspace separation, leading to more distinguishable embeddings.
Through extensive experiments on real and synthetic datasets, we demonstrate
that CLOP enhances performance, providing greater stability across different
learning rates and batch sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit
  sensitivity maps <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Frederik Zimmermann, Andreas Kofler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel learned image reconstruction method for accelerated
cardiac MRI with multiple receiver coils based on deep convolutional neural
networks (CNNs) and algorithm unrolling. In contrast to many existing learned
MR image reconstruction techniques that necessitate coil-sensitivity map (CSM)
estimation as a distinct network component, our proposed approach avoids
explicit CSM estimation. Instead, it implicitly captures and learns to exploit
the inter-coil relationships of the images. Our method consists of a series of
novel learned image and k-space blocks with shared latent information and
adaptation to the acquisition parameters by feature-wise modulation (FiLM), as
well as coil-wise data-consistency (DC) blocks.
  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920
and 0.942 in the cine track and mapping track validation leaderboard of the
MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different
teams at the time of writing.
  Code will be made available at https://github.com/fzimmermann89/CMRxRecon
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI STACOM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation noising effectively prevents harmful fine-tuning on <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Releasing open-source large language models (LLMs) presents a dual-use risk
since bad actors can easily fine-tune these models for harmful purposes. Even
without the open release of weights, weight stealing and fine-tuning APIs make
closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety
measures like preventing jailbreaks and improving safety guardrails are
important, such measures can easily be reversed through fine-tuning. In this
work, we propose Representation Noising (RepNoise), a defence mechanism that is
effective even when attackers have access to the weights. RepNoise works by
removing information about harmful representations such that it is difficult to
recover them during fine-tuning. Importantly, our defence is also able to
generalize across different subsets of harm that have not been seen during the
defence process as long as they are drawn from the same distribution of the
attack set. Our method does not degrade the general capability of LLMs and
retains the ability to train the model on harmless tasks. We provide empirical
evidence that the effectiveness of our defence lies in its "depth": the degree
to which information about harmful representations is removed across all layers
of the LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPs 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable and Learnable Wireless Simulation with Geometric
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hehn, Markus Peschl, Tribhuvanesh Orekondy, Arash Behboodi, Johann Brehmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling the propagation of electromagnetic wireless signals is critical for
designing modern communication systems. Wireless ray tracing simulators model
signal propagation based on the 3D geometry and other scene parameters, but
their accuracy is fundamentally limited by underlying modelling assumptions and
correctness of parameters. In this work, we introduce Wi-GATr, a
fully-learnable neural simulation surrogate designed to predict the channel
observations based on scene primitives (e.g., surface mesh, antenna position
and orientation). Recognizing the inherently geometric nature of these
primitives, Wi-GATr leverages an equivariant Geometric Algebra Transformer that
operates on a tokenizer specifically tailored for wireless simulation. We
evaluate our approach on a range of tasks (i.e., signal strength and delay
spread prediction, receiver localization, and geometry reconstruction) and find
that Wi-GATr is accurate, fast, sample-efficient, and robust to
symmetry-induced transformations. Remarkably, we find our results also
translate well to the real world: Wi-GATr demonstrates more than 35% lower
error than hybrid techniques, and 70% lower error than a calibrated wireless
tracer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Gradient Estimation of Variational Quantum Circuits with Lie
  Algebraic Symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Heidari, Masih Mozakka, Wojciech Szpankowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid quantum-classical optimization and learning strategies are among the
most promising approaches to harnessing quantum information or gaining a
quantum advantage over classical methods. However, efficient estimation of the
gradient of the objective function in such models remains a challenge due to
several factors including the exponential dimensionality of the Hilbert spaces,
and information loss of quantum measurements. In this work, we developed an
efficient framework that makes the Hadamard test efficiently applicable to
gradient estimation for a broad range of quantum systems, an advance that had
been wanting from the outset. Under certain mild structural assumptions, the
gradient is estimated with the measurement shots that scale logarithmically
with the number of parameters and with polynomial classical and quantum time.
This is an exponential reduction in the measurement cost and polynomial speed
up in time compared to existing works. The structural assumptions are (1) the
dimension of the dynamical Lie algebra is polynomial in the number of qubits,
and (2) the observable has a bounded Hilbert-Schmidt norm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better
  Together <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilara Soylu, Christopher Potts, Omar Khattab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dr. Jekyll and Mr. Hyde: Two Faces of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03853v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03853v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro Conti, Stjepan Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, we have witnessed a rise in the use of Large Language Models
(LLMs), especially in applications like chatbots. Safety mechanisms are
implemented to prevent improper responses from these chatbots. In this work, we
bypass these measures for ChatGPT and Gemini by making them impersonate complex
personas with personality characteristics that are not aligned with a truthful
assistant. First, we create elaborate biographies of these personas, which we
then use in a new session with the same chatbots. Our conversations then follow
a role-play style to elicit prohibited responses. Using personas, we show that
prohibited responses are provided, making it possible to obtain unauthorized,
illegal, or harmful information in both ChatGPT and Gemini. We also introduce
several ways of activating such adversarial personas, showing that both
chatbots are vulnerable to this attack. With the same principle, we introduce
two defenses that push the model to interpret trustworthy personalities and
make it more robust against such attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Neural-Evolutionary Algorithm for Autonomous Transit Network Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Holliday, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning a public transit network is a challenging optimization problem, but
essential in order to realize the benefits of autonomous buses. We propose a
novel algorithm for planning networks of routes for autonomous buses. We first
train a graph neural net model as a policy for constructing route networks, and
then use the policy as one of several mutation operators in a evolutionary
algorithm. We evaluate this algorithm on a standard set of benchmarks for
transit network design, and find that it outperforms the learned policy alone
by up to 20% and a plain evolutionary algorithm approach by up to 53% on
realistic benchmark instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works. arXiv admin note: text overlap with
  arXiv:2306.00720</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-Enhanced Neural Solvers for Efficient Adaptation in Combinatorial
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Chalumeau, Refiloe Shabe, Noah De Nicola, Arnu Pretorius, Thomas D. Barrett, Nathan Grinsztajn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial Optimization is crucial to numerous real-world applications,
yet still presents challenges due to its (NP-)hard nature. Amongst existing
approaches, heuristics often offer the best trade-off between quality and
scalability, making them suitable for industrial use. While Reinforcement
Learning (RL) offers a flexible framework for designing heuristics, its
adoption over handcrafted heuristics remains incomplete within industrial
solvers. Existing learned methods still lack the ability to adapt to specific
instances and fully leverage the available computational budget. The current
best methods either rely on a collection of pre-trained policies, or on
data-inefficient fine-tuning; hence failing to fully utilize newly available
information within the constraints of the budget. In response, we present
MEMENTO, an approach that leverages memory to improve the adaptation of neural
solvers at inference time. MEMENTO enables updating the action distribution
dynamically based on the outcome of previous decisions. We validate its
effectiveness on benchmark problems, in particular Traveling Salesman and
Capacitated Vehicle Routing, demonstrating its superiority over tree-search and
policy-gradient fine-tuning; and showing it can be zero-shot combined with
diversity-based solvers. We successfully train all RL auto-regressive solvers
on large instances, and show that MEMENTO can scale and is data-efficient.
Overall, MEMENTO enables to push the state-of-the-art on 11 out of 12 evaluated
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Guiding Information for Adaptive Collocation Point
  Sampling in PINNs <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Florido, He Wang, Amirul Khan, Peter K. Jimack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) provide a means of obtaining
approximate solutions of partial differential equations and systems through the
minimisation of an objective function which includes the evaluation of a
residual function at a set of collocation points within the domain. The quality
of a PINNs solution depends upon numerous parameters, including the number and
distribution of these collocation points. In this paper we consider a number of
strategies for selecting these points and investigate their impact on the
overall accuracy of the method. In particular, we suggest that no single
approach is likely to be "optimal" but we show how a number of important
metrics can have an impact in improving the quality of the results obtained
when using a fixed number of residual evaluations. We illustrate these
approaches through the use of two benchmark test problems: Burgers' equation
and the Allen-Cahn equation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 2 tables. Published in the conference
  proceedings of the International Conference on Computational Science (ICCS)
  2024. Replacement to correct a typo regarding the value of viscosity listed
  in the captions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerKV: Optimizing <span class="highlight-title">Large Language Model</span> Serving with Layer-wise KV
  Cache Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xiong, Hao Wu, Changxu Shao, Ziqing Wang, Rui Zhang, Yuhong Guo, Junping Zhao, Ke Zhang, Zhenxuan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expanding context windows in large language models (LLMs) have greatly
enhanced their capabilities in various applications, but they also introduce
significant challenges in maintaining low latency, particularly in Time to
First Token (TTFT). This paper identifies that the sharp rise in TTFT as
context length increases is predominantly driven by queuing delays, which are
caused by the growing demands for GPU Key-Value (KV) cache allocation clashing
with the limited availability of KV cache blocks. To address this issue, we
propose LayerKV, a simple yet effective plug-in method that effectively reduces
TTFT without requiring additional hardware or compromising output performance,
while seamlessly integrating with existing parallelism strategies and
scheduling techniques. Specifically, LayerKV introduces layer-wise KV block
allocation, management, and offloading for fine-grained control over system
memory, coupled with an SLO-aware scheduler to optimize overall Service Level
Objectives (SLOs). Comprehensive evaluations on representative models, ranging
from 7B to 70B parameters, across various GPU configurations, demonstrate that
LayerKV improves TTFT latency up to 69x and reduces SLO violation rates by
28.7%, significantly enhancing the user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Convergence of Hermitian Dynamic Mode Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Boullé, Matthew J. Colbrook
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the convergence of Hermitian Dynamic Mode Decomposition (DMD) to the
spectral properties of self-adjoint Koopman operators. Hermitian DMD is a
data-driven method that approximates the Koopman operator associated with an
unknown nonlinear dynamical system, using discrete-time snapshots. This
approach preserves the self-adjointness of the operator in its
finite-dimensional approximations. \rev{We prove that, under suitably broad
conditions, the spectral measures corresponding to the eigenvalues and
eigenfunctions computed by Hermitian DMD converge to those of the underlying
Koopman operator}. This result also applies to skew-Hermitian systems (after
multiplication by $i$), applicable to generators of continuous-time
measure-preserving systems. Along the way, we establish a general theorem on
the convergence of spectral measures for finite sections of self-adjoint
operators, including those that are unbounded, which is of independent interest
to the wider spectral community. We numerically demonstrate our results by
applying them to two-dimensional Schr\"odinger equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2312.00137</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Intelligence: A Framework for Certifying Knowledge
  Comprehension in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge comprehension capability is an important aspect of human
intelligence. As Large Language Models (LLMs) are being envisioned as
superhuman agents, it is crucial for them to be proficient at knowledge
comprehension. However, existing benchmarking studies do not provide
consistent, generalizable, and formal guarantees on the knowledge comprehension
capabilities of LLMs. In this work, we propose the first framework to certify
knowledge comprehension in LLMs with formal probabilistic guarantees. Our
certificates are quantitative -- they consist of high-confidence, tight bounds
on the probability that a target LLM gives the correct answer on any knowledge
comprehension prompt sampled from a distribution. We design and certify novel
specifications that precisely represent distributions of knowledge
comprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for
specifications over the Wikidata5m knowledge graph. We find that the knowledge
comprehension capability improves significantly with scaling the size of the
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transition Path Sampling with Improved Off-Policy Training of <span class="highlight-title">Diffusion</span>
  Path Samplers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19961v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19961v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiyoung Seong, Seonghyun Park, Seonghwan Kim, Woo Youn Kim, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding transition pathways between meta-stable states in molecular
systems is crucial to advance material design and drug discovery. However,
unbiased molecular dynamics simulations are computationally infeasible due to
the high energy barriers separating these states. Although recent machine
learning techniques offer potential solutions, they are often limited to simple
systems or rely on collective variables (CVs) derived from costly domain
expertise. In this paper, we introduce a novel approach that trains diffusion
path samplers (DPS) for transition path sampling (TPS) without the need for
CVs. We recast the problem as an amortized sampling of the target path measure,
minimizing the log-variance divergence between the path measure induced by our
DPS and the target path measure. To ensure scalability for high-dimensional
tasks, we introduce (1) a new off-policy training objective based on learning
control variates with replay buffers and (2) a scale-based equivariant
parameterization of the bias forces. We evaluate our approach, coined TPS-DPS,
on a synthetic double-well potential and three peptides: Alanine Dipeptide,
Polyproline Helix, and Chignolin. Results show that our approach produces more
realistic and diverse transition pathways compared to existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 1 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model <span class="highlight-title">Editing</span> of
  <span class="highlight-title">Large Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-MoE: Towards Compositional <span class="highlight-title">Large Language Model</span>s with
  Self-Specialized Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Self-MoE, an approach that transforms a monolithic LLM into a
compositional, modular system of self-specialized experts, named MiXSE (MiXture
of Self-specialized Experts). Our approach leverages self-specialization, which
constructs expert modules using self-generated synthetic data, each equipping a
shared base LLM with distinct domain-specific capabilities, activated via
self-optimized routing. This allows for dynamic and capability-specific
handling of various target tasks, enhancing overall capabilities, without
extensive human-labeled data and added parameters. Our empirical results reveal
that specializing LLMs may exhibit potential trade-offs in performances on
non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial
improvements (6.5%p on average) over the base LLM across diverse benchmarks
such as knowledge, reasoning, math, and coding. It also consistently
outperforms other methods, including instance merging and weight merging, while
offering better flexibility and interpretability by design with semantic
experts and routing. Our findings highlight the critical role of modularity,
the applicability of Self-MoE to multiple base LLMs, and the potential of
self-improvement in achieving efficient, scalable, and adaptable systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Elementary Predictor Obtaining $2\sqrt{T}+1$ Distance to Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshwar Ram Arunachaleswaran, Natalie Collina, Aaron Roth, Mirah Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blasiok et al. [2023] proposed distance to calibration as a natural measure
of calibration error that unlike expected calibration error (ECE) is
continuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument
establishing the existence of an online predictor that can obtain $O(\sqrt{T})$
distance to calibration in the adversarial setting, which is known to be
impossible for ECE. They leave as an open problem finding an explicit,
efficient algorithm. We resolve this problem and give an extremely simple,
efficient, deterministic algorithm that obtains distance to calibration error
at most $2\sqrt{T}+1$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Function-Space MCMC for Bayesian Wide Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14325v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14325v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Pezzetti, Stefano Favaro, Stefano Peluchetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Neural Networks represent a fascinating confluence of deep learning
and probabilistic reasoning, offering a compelling framework for understanding
uncertainty in complex predictive models. In this paper, we investigate the use
of the preconditioned Crank-Nicolson algorithm and its Langevin version to
sample from the reparametrised posterior distribution of the weights as the
widths of Bayesian Neural Networks grow larger. In addition to being robust in
the infinite-dimensional setting, we prove that the acceptance probabilities of
the proposed methods approach 1 as the width of the network increases,
independently of any stepsize tuning. Moreover, we examine and compare how the
mixing speeds of the underdamped Langevin Monte Carlo, the preconditioned
Crank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are
influenced by changes in the network width in some real-world cases. Our
findings suggest that, in wide Bayesian Neural Networks configurations, the
preconditioned Crank-Nicolson method allows for more efficient sampling of the
reparametrised posterior distribution, as evidenced by a higher effective
sample size and improved diagnostic results compared with the other analysed
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Centric Foundation Models in Computational Healthcare: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunkun Zhang, Jin Gao, Zheling Tan, Lingfeng Zhou, Kexin Ding, Mu Zhou, Shaoting Zhang, Dequan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of foundation models (FMs) as an emerging suite of AI techniques
has struck a wave of opportunities in computational healthcare. The interactive
nature of these models, guided by pre-training data and human instructions, has
ignited a data-centric AI paradigm that emphasizes better data
characterization, quality, and scale. In healthcare AI, obtaining and
processing high-quality clinical data records has been a longstanding
challenge, ranging from data quantity, annotation, patient privacy, and ethics.
In this survey, we investigate a wide range of data-centric approaches in the
FM era (from model pre-training to inference) towards improving the healthcare
workflow. We discuss key perspectives in AI security, assessment, and alignment
with human values. Finally, we offer a promising outlook of FM-based analytics
to enhance the performance of patient outcome and clinical workflow in the
evolving landscape of healthcare and medicine. We provide an up-to-date list of
healthcare-related foundation models and datasets at
https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Survey content updated to include recent research work and progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geodesic Optimization for Predictive Shift Adaptation on EEG data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apolline Mellot, Antoine Collas, Sylvain Chevallier, Alexandre Gramfort, Denis A. Engemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) data is often collected from diverse contexts
involving different populations and EEG devices. This variability can induce
distribution shifts in the data $X$ and in the biomedical variables of interest
$y$, thus limiting the application of supervised machine learning (ML)
algorithms. While domain adaptation (DA) methods have been developed to
mitigate the impact of these shifts, such methods struggle when distribution
shifts occur simultaneously in $X$ and $y$. As state-of-the-art ML models for
EEG represent the data by spatial covariance matrices, which lie on the
Riemannian manifold of Symmetric Positive Definite (SPD) matrices, it is
appealing to study DA techniques operating on the SPD manifold. This paper
proposes a novel method termed Geodesic Optimization for Predictive Shift
Adaptation (GOPSA) to address test-time multi-source DA for situations in which
source domains have distinct $y$ distributions. GOPSA exploits the geodesic
structure of the Riemannian manifold to jointly learn a domain-specific
re-centering operator representing site-specific intercepts and the regression
model. We performed empirical benchmarks on the cross-site generalization of
age-prediction models with resting-state EEG data from a large multi-national
dataset (HarMNqEEG), which included $14$ recording sites and more than $1500$
human participants. Compared to state-of-the-art methods, our results showed
that GOPSA achieved significantly higher performance on three regression
metrics ($R^2$, MAE, and Spearman's $\rho$) for several source-target site
combinations, highlighting its effectiveness in tackling multi-source DA with
predictive shifts in EEG data analysis. Our method has the potential to combine
the advantages of mixed-effects modeling with machine learning for biomedical
applications of EEG, such as multicenter clinical trials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLAME: Adaptive and Reactive Concept Drift Mitigation for Federated
  Learning Deployments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Mavromatis, Stefano De Feo, Aftab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Federated Learning with Adaptive Monitoring and
Elimination (FLAME), a novel solution capable of detecting and mitigating
concept drift in Federated Learning (FL) Internet of Things (IoT) environments.
Concept drift poses significant challenges for FL models deployed in dynamic
and real-world settings. FLAME leverages an FL architecture, considers a
real-world FL pipeline, and proves capable of maintaining model performance and
accuracy while addressing bandwidth and privacy constraints. Introducing
various features and extensions on previous works, FLAME offers a robust
solution to concept drift, significantly reducing computational load and
communication overhead. Compared to well-known lightweight mitigation methods,
FLAME demonstrates superior performance in maintaining high F1 scores and
reducing resource utilisation in large-scale IoT deployments, making it a
promising approach for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication at ACM EWSN 2024 - EMERGE Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Long Range Dependencies on Graphs via Random Walks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexiong Chen, Till Hendrik Schulz, Karsten Borgwardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing graph neural networks (GNNs) excel at capturing local
relationships but struggle with long-range dependencies in graphs. In contrast,
graph transformers (GTs) enable global information exchange but often
oversimplify the graph structure by representing graphs as sets of fixed-length
vectors. This work introduces a novel architecture that overcomes the
shortcomings of both approaches by combining the long-range information of
random walks with local message passing. By treating random walks as sequences,
our architecture leverages recent advances in sequence models to effectively
capture long-range dependencies within these walks. Based on this concept, we
propose a framework that offers (1) more expressive graph representations
through random walk sequences, (2) the ability to utilize any sequence model
for capturing long-range dependencies, and (3) the flexibility by integrating
various GNN and GT architectures. Our experimental evaluations demonstrate that
our approach achieves significant performance improvements on 19 graph and node
benchmark datasets, notably outperforming existing methods by up to 13\% on the
PascalVoc-SP and COCO-SP datasets. The code is available at
https://github.com/BorgwardtLab/NeuralWalker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Pricing in Securities Lending Market: Application in Revenue
  Optimization for an <span class="highlight-title">Agent</span> Lender Portfolio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13687v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13687v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu, Yung-Cheng Hsu, William Biscarri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Securities lending is an important part of the financial market structure,
where agent lenders help long term institutional investors to lend out their
securities to short sellers in exchange for a lending fee. Agent lenders within
the market seek to optimize revenue by lending out securities at the highest
rate possible. Typically, this rate is set by hard-coded business rules or
standard supervised machine learning models. These approaches are often
difficult to scale and are not adaptive to changing market conditions. Unlike a
traditional stock exchange with a centralized limit order book, the securities
lending market is organized similarly to an e-commerce marketplace, where agent
lenders and borrowers can transact at any agreed price in a bilateral fashion.
This similarity suggests that the use of typical methods for addressing dynamic
pricing problems in e-commerce could be effective in the securities lending
market. We show that existing contextual bandit frameworks can be successfully
utilized in the securities lending market. Using offline evaluation on real
historical data, we show that the contextual bandit approach can consistently
outperform typical approaches by at least 15% in terms of total revenue
generated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I Bet You Did Not Mean That: Testing Semantic Importance via Betting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacopo Teneggi, Jeremias Sulam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have extended notions of feature importance to semantic concepts
that are inherently interpretable to the users interacting with a black-box
predictive model. Yet, precise statistical guarantees, such as false positive
rate and false discovery rate control, are needed to communicate findings
transparently and to avoid unintended consequences in real-world scenarios. In
this paper, we formalize the global (i.e., over a population) and local (i.e.,
for a sample) statistical importance of semantic concepts for the predictions
of opaque models by means of conditional independence, which allows for
rigorous testing. We use recent ideas of sequential kernelized independence
testing (SKIT) to induce a rank of importance across concepts, and showcase the
effectiveness and flexibility of our framework on synthetic datasets as well as
on image classification tasks using several and diverse vision-language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Can Transformers Count to n? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilad Yehudai, Haim Kaplan, Asma Ghandeharioun, Mor Geva, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models based on the transformer architectures can solve highly
complex tasks. But are there simple tasks that such models cannot solve? Here
we focus on very simple counting tasks, that involve counting how many times a
token in the vocabulary have appeared in a string. We show that if the
dimension of the transformer state is linear in the context length, this task
can be solved. However, the solution we propose does not scale beyond this
limit, and we provide theoretical arguments for why it is likely impossible for
a size limited transformer to implement this task. Our empirical results
demonstrate the same phase-transition in performance, as anticipated by the
theoretical argument. Our results demonstrate the importance of understanding
how transformers can solve simple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ softmax is not enough (for sharp out-of-distribution) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petar Veličković, Christos Perivolaropoulos, Federico Barbero, Razvan Pascanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key property of reasoning systems is the ability to make sharp decisions on
their input data. For contemporary AI systems, a key carrier of sharp behaviour
is the softmax function, with its capability to perform differentiable
query-key lookups. It is a common belief that the predictive power of networks
leveraging softmax arises from "circuits" which sharply perform certain kinds
of computations consistently across many diverse inputs. However, for these
circuits to be robust, they would need to generalise well to arbitrary valid
inputs. In this paper, we dispel this myth: even for tasks as simple as finding
the maximum key, any learned circuitry must disperse as the number of items
grows at test time. We attribute this to a fundamental limitation of the
softmax function to robustly approximate sharp functions, prove this phenomenon
theoretically, and propose adaptive temperature as an ad-hoc technique for
improving the sharpness of softmax at inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments welcome. 15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Bits and Bandits: Quantifying the Regret-Information Trade-off 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16581v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16581v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itai Shufaro, Nadav Merlis, Nir Weinberger, Shie Mannor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many sequential decision problems, an agent performs a repeated task. He
then suffers regret and obtains information that he may use in the following
rounds. However, sometimes the agent may also obtain information and avoid
suffering regret by querying external sources. We study the trade-off between
the information an agent accumulates and the regret it suffers. We invoke
information-theoretic methods for obtaining regret lower bounds, that also
allow us to easily re-derive several known lower bounds. We introduce the first
Bayesian regret lower bounds that depend on the information an agent
accumulates. We also prove regret upper bounds using the amount of information
the agent accumulates. These bounds show that information measured in bits, can
be traded off for regret, measured in reward. Finally, we demonstrate the
utility of these bounds in improving the performance of a question-answering
task with large language models, allowing us to obtain valuable insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.02135v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.02135v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Levin, Denis Belomestny, Alexey Naumov, Sergey Samsonov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy evaluation is an important instrument for the comparison of different
algorithms in Reinforcement Learning (RL). Yet even a precise knowledge of the
value function $V^{\pi}$ corresponding to a policy $\pi$ does not provide
reliable information on how far is the policy $\pi$ from the optimal one. We
present a novel model-free upper value iteration procedure $({\sf UVIP})$ that
allows us to estimate the suboptimality gap $V^{\star}(x) - V^{\pi}(x)$ from
above and to construct confidence intervals for $V^\star$. Our approach relies
on upper bounds to the solution of the Bellman optimality equation via
martingale approach. We provide theoretical guarantees for ${\sf UVIP}$ under
general assumptions and illustrate its performance on a number of benchmark RL
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICOMP-2024 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Art2Mus: Bridging Visual Arts and Music through Cross-Modal <span class="highlight-title">Generation</span> <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Rinaldi, Nicola Fanelli, Giovanna Castellano, Gennaro Vessio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence and generative models have revolutionized music
creation, with many models leveraging textual or visual prompts for guidance.
However, existing image-to-music models are limited to simple images, lacking
the capability to generate music from complex digitized artworks. To address
this gap, we introduce $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$, a novel
model designed to create music from digitized artworks or text inputs.
$\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$ extends the AudioLDM~2
architecture, a text-to-audio model, and employs our newly curated datasets,
created via ImageBind, which pair digitized artworks with music. Experimental
results demonstrate that $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$ can
generate music that resonates with the input stimuli. These findings suggest
promising applications in multimedia art, interactive installations, and
AI-driven creative tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedBiP: Heterogeneous One-Shot Federated Learning with Personalized
  Latent <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Chen, Hang Li, Yao Zhang, Gengyuan Zhang, Jinhe Bi, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-Shot Federated Learning (OSFL), a special decentralized machine learning
paradigm, has recently gained significant attention. OSFL requires only a
single round of client data or model upload, which reduces communication costs
and mitigates privacy threats compared to traditional FL. Despite these
promising prospects, existing methods face challenges due to client data
heterogeneity and limited data quantity when applied to real-world OSFL
systems. Recently, Latent Diffusion Models (LDM) have shown remarkable
advancements in synthesizing high-quality images through pretraining on
large-scale datasets, thereby presenting a potential solution to overcome these
issues. However, directly applying pretrained LDM to heterogeneous OSFL results
in significant distribution shifts in synthetic data, leading to performance
degradation in classification models trained on such data. This issue is
particularly pronounced in rare domains, such as medical imaging, which are
underrepresented in LDM's pretraining data. To address this challenge, we
propose Federated Bi-Level Personalization (FedBiP), which personalizes the
pretrained LDM at both instance-level and concept-level. Hereby, FedBiP
synthesizes images following the client's local data distribution without
compromising the privacy regulations. FedBiP is also the first approach to
simultaneously address feature space heterogeneity and client data scarcity in
OSFL. Our method is validated through extensive experiments on three OSFL
benchmarks with feature space heterogeneity, as well as on challenging medical
and satellite image datasets with label heterogeneity. The results demonstrate
the effectiveness of FedBiP, which substantially outperforms other OSFL
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attentive-based Multi-level Feature Fusion for Voice Disorder Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lipeng Shen, Yifan Xiong, Dongyue Guo, Wei Mo, Lingyu Yu, Hui Yang, Yi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice disorders negatively impact the quality of daily life in various ways.
However, accurately recognizing the category of pathological features from raw
audio remains a considerable challenge due to the limited dataset. A promising
method to handle this issue is extracting multi-level pathological information
from speech in a comprehensive manner by fusing features in the latent space.
In this paper, a novel framework is designed to explore the way of high-quality
feature fusion for effective and generalized detection performance.
Specifically, the proposed model follows a two-stage training paradigm: (1)
ECAPA-TDNN and Wav2vec 2.0 which have shown remarkable effectiveness in various
domains are employed to learn the universal pathological information from raw
audio; (2) An attentive fusion module is dedicatedly designed to establish the
interaction between pathological features projected by EcapTdnn and Wav2vec 2.0
respectively and guide the multi-layer fusion, the entire model is jointly
fine-tuned from pre-trained features by the automatic voice pathology detection
task. Finally, comprehensive experiments on the FEMH and SVD datasets
demonstrate that the proposed framework outperforms the competitive baselines,
and achieves the accuracy of 90.51% and 87.68%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R-Bench: Are your Large <span class="highlight-title">Multimodal</span> Model Robust to Real-world
  Corruptions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyi Li, Jianbo Zhang, Zicheng Zhang, Haoning Wu, Yuan Tian, Wei Sun, Guo Lu, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The outstanding performance of Large Multimodal Models (LMMs) has made them
widely applied in vision-related tasks. However, various corruptions in the
real world mean that images will not be as ideal as in simulations, presenting
significant challenges for the practical application of LMMs. To address this
issue, we introduce R-Bench, a benchmark focused on the **Real-world Robustness
of LMMs**. Specifically, we: (a) model the complete link from user capture to
LMMs reception, comprising 33 corruption dimensions, including 7 steps
according to the corruption sequence, and 7 groups based on low-level
attributes; (b) collect reference/distorted image dataset before/after
corruption, including 2,970 question-answer pairs with human labeling; (c)
propose comprehensive evaluation for absolute/relative robustness and benchmark
20 mainstream LMMs. Results show that while LMMs can correctly handle the
original reference images, their performance is not stable when faced with
distorted images, and there is a significant gap in robustness compared to the
human visual system. We hope that R-Bench will inspire improving the robustness
of LMMs, **extending them from experimental simulations to the real-world
application**. Check https://q-future.github.io/R-Bench for details.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Screen Content Coding in VVC Using Soft Context Formation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Och, Shabhrish Reddy Uddehal, Tilo Strutz, André Kaup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Screen content images typically contain a mix of natural and synthetic image
parts. Synthetic sections usually are comprised of uniformly colored areas and
repeating colors and patterns. In the VVC standard, these properties are
exploited using Intra Block Copy and Palette Mode. In this paper, we show that
pixel-wise lossless coding can outperform lossy VVC coding in such areas. We
propose an enhanced VVC coding approach for screen content images using the
principle of soft context formation. First, the image is separated into two
layers in a block-wise manner using a learning-based method with four block
features. Synthetic image parts are coded losslessly using soft context
formation, the rest with VVC.We modify the available soft context formation
coder to incorporate information gained by the decoded VVC layer for improved
coding efficiency. Using this approach, we achieve Bjontegaard-Delta-rate gains
of 4.98% on the evaluated data sets compared to VVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Nishimura, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at EMNLP2024 - system demonstration track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-06T00:00:00Z">2024-10-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Watermarking Decision Tree Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Calzavara, Lorenzo Cazzaro, Donald Gera, Salvatore Orlando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protecting the intellectual property of machine learning models is a hot
topic and many watermarking schemes for deep neural networks have been proposed
in the literature. Unfortunately, prior work largely neglected the
investigation of watermarking techniques for other types of models, including
decision tree ensembles, which are a state-of-the-art model for classification
tasks on non-perceptual data. In this paper, we present the first watermarking
scheme designed for decision tree ensembles, focusing in particular on random
forest models. We discuss watermark creation and verification, presenting a
thorough security analysis with respect to possible attacks. We finally perform
an experimental evaluation of the proposed scheme, showing excellent results in
terms of accuracy and security against the most relevant threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniMuMo: Unified Text, Music and Motion <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yang, Kun Su, Yutong Zhang, Jiaben Chen, Kaizhi Qian, Gaowen Liu, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce UniMuMo, a unified multimodal model capable of taking arbitrary
text, music, and motion data as input conditions to generate outputs across all
three modalities. To address the lack of time-synchronized data, we align
unpaired music and motion data based on rhythmic patterns to leverage existing
large-scale music-only and motion-only datasets. By converting music, motion,
and text into token-based representation, our model bridges these modalities
through a unified encoder-decoder transformer architecture. To support multiple
generation tasks within a single framework, we introduce several architectural
improvements. We propose encoding motion with a music codebook, mapping motion
into the same feature space as music. We introduce a music-motion parallel
generation scheme that unifies all music and motion generation tasks into a
single transformer decoder architecture with a single training task of
music-motion joint generation. Moreover, the model is designed by fine-tuning
existing pre-trained single-modality models, significantly reducing
computational demands. Extensive experiments demonstrate that UniMuMo achieves
competitive results on all unidirectional generation benchmarks across music,
motion, and text modalities. Quantitative results are available in the
\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Guided Dynamic Modality Attention Fusion Framework for
  <span class="highlight-title">Multimodal</span> Sentiment Analysis <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Feng, Yuming Lin, Lihua He, You Li, Liang Chang, Ya Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Sentiment Analysis (MSA) utilizes multimodal data to infer the
users' sentiment. Previous methods focus on equally treating the contribution
of each modality or statically using text as the dominant modality to conduct
interaction, which neglects the situation where each modality may become
dominant. In this paper, we propose a Knowledge-Guided Dynamic Modality
Attention Fusion Framework (KuDA) for multimodal sentiment analysis. KuDA uses
sentiment knowledge to guide the model dynamically selecting the dominant
modality and adjusting the contributions of each modality. In addition, with
the obtained multimodal representation, the model can further highlight the
contribution of dominant modality through the correlation evaluation loss.
Extensive experiments on four MSA benchmark datasets indicate that KuDA
achieves state-of-the-art performance and is able to adapt to different
scenarios of dominant modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To Forget or Not? Towards Practical Knowledge Unlearning for Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) trained on extensive corpora inevitably retain
sensitive data, such as personal privacy information and copyrighted material.
Recent advancements in knowledge unlearning involve updating LLM parameters to
erase specific knowledge. However, current unlearning paradigms are mired in
vague forgetting boundaries, often erasing knowledge indiscriminately. In this
work, we introduce KnowUnDo, a benchmark containing copyrighted content and
user privacy domains to evaluate if the unlearning process inadvertently erases
essential knowledge. Our findings indicate that existing unlearning methods
often suffer from excessive unlearning. To address this, we propose a simple
yet effective method, MemFlex, which utilizes gradient information to precisely
target and unlearn sensitive parameters. Experimental results show that MemFlex
is superior to existing methods in both precise knowledge unlearning and
general knowledge retaining of LLMs. Code and dataset are released at
https://github.com/zjunlp/KnowUnDo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings; Code and dataset are released at
  https://github.com/zjunlp/KnowUnDo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimedia Framework for Continuum Robots: Systematic, Computational,
  and Control Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Yu Hsieh, June-Hao Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuum robots, which often rely on interdisciplinary and multimedia
collaborations, have been increasingly recognized for their potential to
revolutionize the field of human-computer interaction (HCI) in varied
applications due to their adaptive, responsive, and flexible characteristics.
Despite their promises, the lack of an integrated framework poses a significant
limitation for both users and developers, resulting in inefficiency and
complexity during preliminary developments. Thus, this paper introduces a
unified framework for continuum robotic systems that addresses these challenges
by integrating system architecture, dynamics computation, and control strategy
within a computer-aided design (CAD) platform. The proposed method allows for
efficient modeling and quick preview of the robot performance, and thus
facilitating iterative design and implementation, with a view to enhancing the
quality of robot developments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-05T00:00:00Z">2024-10-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIM 2024 Challenge on Video Super-Resolution Quality Assessment: Methods
  and Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Molodetskikh, Artem Borisov, Dmitriy Vatolin, Radu Timofte, Jianzhao Liu, Tianwu Zhi, Yabin Zhang, Yang Li, Jingwen Xu, Yiting Liao, Qing Luo, Ao-Xiang Zhang, Peng Zhang, Haibo Lei, Linyan Jiang, Yaqing Li, Yuqin Cao, Wei Sun, Weixia Zhang, Yinan Sun, Ziheng Jia, Yuxin Zhu, Xiongkuo Min, Guangtao Zhai, Weihua Luo, Yupeng Z., Hong Y
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Video Super-Resolution (SR) Quality Assessment (QA)
Challenge that was part of the Advances in Image Manipulation (AIM) workshop,
held in conjunction with ECCV 2024. The task of this challenge was to develop
an objective QA method for videos upscaled 2x and 4x by modern image- and
video-SR algorithms. QA methods were evaluated by comparing their output with
aggregate subjective scores collected from >150,000 pairwise votes obtained
through crowd-sourced comparisons across 52 SR methods and 1124 upscaled
videos. The goal was to advance the state-of-the-art in SR QA, which had proven
to be a challenging problem with limited applicability of traditional QA
methods. The challenge had 29 registered participants, and 5 teams had
submitted their final results, all outperforming the current state-of-the-art.
All data, including the private test subset, has been made publicly available
on the challenge homepage at
https://challenges.videoprocessing.ai/challenges/super-resolution-metrics-challenge.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Benchmark</span>ing Cross-Domain Audio-Visual Deception Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobao Guo, Zitong Yu, Nithish Muthuchamy Selvaraj, Bingquan Shen, Adams Wai-Kin Kong, Alex C. Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated deception detection is crucial for assisting humans in accurately
assessing truthfulness and identifying deceptive behavior. Conventional
contact-based techniques, like polygraph devices, rely on physiological signals
to determine the authenticity of an individual's statements. Nevertheless,
recent developments in automated deception detection have demonstrated that
multimodal features derived from both audio and video modalities may outperform
human observers on publicly available datasets. Despite these positive
findings, the generalizability of existing audio-visual deception detection
approaches across different scenarios remains largely unexplored. To close this
gap, we present the first cross-domain audio-visual deception detection
benchmark, that enables us to assess how well these methods generalize for use
in real-world scenarios. We used widely adopted audio and visual features and
different architectures for benchmarking, comparing single-to-single and
multi-to-single domain generalization performance. To further exploit the
impacts using data from multiple source domains for training, we investigate
three types of domain sampling strategies, including domain-simultaneous,
domain-alternating, and domain-by-domain for multi-to-single domain
generalization evaluation. We also propose an algorithm to enhance the
generalization performance by maximizing the gradient inner products between
modality encoders, named ``MM-IDGM". Furthermore, we proposed the
Attention-Mixer fusion method to improve performance, and we believe that this
new cross-domain benchmark will facilitate future research in audio-visual
deception detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-04T00:00:00Z">2024-10-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SONIQUE: Video Background Music <span class="highlight-title">Generation</span> Using Unpaired Audio-Visual
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liqian Zhang, Magdalena Fuentes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SONIQUE, a model for generating background music tailored to video
content. Unlike traditional video-to-music generation approaches, which rely
heavily on paired audio-visual datasets, SONIQUE leverages unpaired data,
combining royalty-free music and independent video sources. By utilizing large
language models (LLMs) for video understanding and converting visual
descriptions into musical tags, alongside a U-Net-based conditional diffusion
model, SONIQUE enables customizable music generation. Users can control
specific aspects of the music, such as instruments, genres, tempo, and
melodies, ensuring the generated output fits their creative vision. SONIQUE is
open-source, with a demo available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Jailbreak Attack for <span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> Models via <span class="highlight-title">Editing</span> Step
  by Step 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Kuiyi Gao, Zihan Jia, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based image generation models, such as Stable Diffusion and DALL-E 3,
hold significant potential in content creation and publishing workflows, making
them the focus in recent years. Despite their remarkable capability to generate
diverse and vivid images, considerable efforts are being made to prevent the
generation of harmful content, such as abusive, violent, or pornographic
material. To assess the safety of existing models, we introduce a novel
jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises
image generation models through a step-by-step editing process. Specifically,
for malicious queries that cannot bypass the safeguards with a single prompt,
we intentionally decompose the query into multiple sub-queries. The image
generation models are then prompted to generate and iteratively edit images
based on these sub-queries. To evaluate the effectiveness of our CoJ attack
method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine
safety scenarios, three types of editing operations, and three editing
elements. Experiments on four widely-used image generation services provided by
GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack
method can successfully bypass the safeguards of models for over 60% cases,
which significantly outperforms other jailbreaking methods (i.e., 14%).
Further, to enhance these models' safety against our CoJ attack method, we also
propose an effective prompting-based method, Think Twice Prompting, that can
successfully defend over 95% of CoJ attack. We release our dataset and code to
facilitate the AI safety research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does SpatioTemporal information benefit Two video summarization
  <span class="highlight-title">benchmark</span>s? <span class="chip">ECAI
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aashutosh Ganesh, Mirela Popa, Daan Odijk, Nava Tintarev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important aspect of summarizing videos is understanding the temporal
context behind each part of the video to grasp what is and is not important.
Video summarization models have in recent years modeled spatio-temporal
relationships to represent this information. These models achieved
state-of-the-art correlation scores on important benchmark datasets. However,
what has not been reviewed is whether spatio-temporal relationships are even
required to achieve state-of-the-art results. Previous work in activity
recognition has found biases, by prioritizing static cues such as scenes or
objects, over motion information. In this paper we inquire if similar spurious
relationships might influence the task of video summarization. To do so, we
analyse the role that temporal information plays on existing benchmark
datasets. We first estimate a baseline with temporally invariant models to see
how well such models rank on benchmark datasets (TVSum and SumMe). We then
disrupt the temporal order of the videos to investigate the impact it has on
existing state-of-the-art models. One of our findings is that the temporally
invariant models achieve competitive correlation scores that are close to the
human baselines on the TVSum dataset. We also demonstrate that existing models
are not affected by temporal perturbations. Furthermore, with certain
disruption strategies that shuffle fixed time segments, we can actually improve
their correlation scores. With these results, we find that spatio-temporal
relationship play a minor role and we raise the question whether these
benchmarks adequately model the task of video summarization. Code available at:
https://github.com/AashGan/TemporalPerturbSum
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at AEQUITAS workshop, Co-located with ECAI
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enriching Music Descriptions with a Finetuned-<span class="highlight-title">LLM</span> and Metadata for
  Text-to-Music Retrieval <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeungHeon Doh, Minhee Lee, Dasaem Jeong, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Music Retrieval, finding music based on a given natural language
query, plays a pivotal role in content discovery within extensive music
databases. To address this challenge, prior research has predominantly focused
on a joint embedding of music audio and text, utilizing it to retrieve music
tracks that exactly match descriptive queries related to musical attributes
(i.e. genre, instrument) and contextual elements (i.e. mood, theme). However,
users also articulate a need to explore music that shares similarities with
their favorite tracks or artists, such as \textit{I need a similar track to
Superstition by Stevie Wonder}. To address these concerns, this paper proposes
an improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes
rich text descriptions generated with a finetuned large language model and
metadata. To accomplish this, we obtained various types of seed text from
several existing music tag and caption datasets and a knowledge graph dataset
of artists and tracks. The experimental results show the effectiveness of
TTMR++ in comparison to state-of-the-art music-text joint embedding models
through a comprehensive evaluation involving various musical text queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the IEEE ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2AR: A Web-based Modeling Environment for the Augmented Reality
  Workflow Modeling Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Muff, Hans-Georg Fill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces M2AR, a new web-based, two- and three-dimensional
modeling environment that enables the modeling and execution of augmented
reality applications without requiring programming knowledge. The platform is
based on a 3D JavaScript library and the mixed reality immersive web standard
WebXR. For a first demonstration of its feasibility, the previously introduced
Augmented Reality Workflow Modeling Language (ARWFML) has been successfully
implemented using this environment. The usefulness of the new modeling
environment is demonstrated by showing use cases of the ARWFML on M2AR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedMAC: Tackling Partial-Modality Missing in Federated Learning with
  Cross-Modal Aggregation and Contrastive Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manh Duong Nguyen, Trung Thanh Nguyen, Huy Hieu Pham, Trong Nghia Hoang, Phi Le Nguyen, Thanh Trung Huynh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a method for training machine learning models
using distributed data sources. It ensures privacy by allowing clients to
collaboratively learn a shared global model while storing their data locally.
However, a significant challenge arises when dealing with missing modalities in
clients' datasets, where certain features or modalities are unavailable or
incomplete, leading to heterogeneous data distribution. While previous studies
have addressed the issue of complete-modality missing, they fail to tackle
partial-modality missing on account of severe heterogeneity among clients at an
instance level, where the pattern of missing data can vary significantly from
one sample to another. To tackle this challenge, this study proposes a novel
framework named FedMAC, designed to address multi-modality missing under
conditions of partial-modality missing in FL. Additionally, to avoid trivial
aggregation of multi-modal features, we introduce contrastive-based
regularization to impose additional constraints on the latent representation
space. The experimental results demonstrate the effectiveness of FedMAC across
various client configurations with statistical heterogeneity, outperforming
baseline methods by up to 26% in severe missing scenarios, highlighting its
potential as a solution for the challenge of partially missing modalities in
federated systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 22nd International Symposium on Network Computing and
  Applications (NCA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Episodic fine-tuning prototypical networks for optimization-based
  few-shot learning: Application to audio classification <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanyu Zhuang, Geoffroy Peeters, Gaël Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Prototypical Network (ProtoNet) has emerged as a popular choice in
Few-shot Learning (FSL) scenarios due to its remarkable performance and
straightforward implementation. Building upon such success, we first propose a
simple (yet novel) method to fine-tune a ProtoNet on the (labeled) support set
of the test episode of a C-way-K-shot test episode (without using the query set
which is only used for evaluation). We then propose an algorithmic framework
that combines ProtoNet with optimization-based FSL algorithms (MAML and
Meta-Curvature) to work with such a fine-tuning method. Since
optimization-based algorithms endow the target learner model with the ability
to fast adaption to only a few samples, we utilize ProtoNet as the target model
to enhance its fine-tuning performance with the help of a specifically designed
episodic fine-tuning strategy. The experimental results confirm that our
proposed models, MAML-Proto and MC-Proto, combined with our unique fine-tuning
method, outperform regular ProtoNet by a large margin in few-shot audio
classification tasks on the ESC-50 and Speech Commands v2 datasets. We note
that although we have only applied our model to the audio domain, it is a
general method and can be easily extended to other domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MLSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has been widely studied and
applied in numerous applications. However, the emphasis on brief summary texts
during pre-training prevents CLIP from understanding long descriptions. This
issue is particularly acute regarding videos given that videos often contain
abundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra
Length) model, which aims to unleash the long-description understanding
capability of video CLIP models. Firstly, we establish an automatic data
collection system and gather a large-scale VILD pre-training dataset with VIdeo
and Long-Description pairs. Then, we propose Text-similarity-guided Primary
Component Matching (TPCM) to better learn the distribution of feature space
while expanding the long description capability. We also introduce two new
tasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware
Description Ranking (HDR) for further understanding improvement. Finally, we
construct a Long Video Description Ranking (LVDR) benchmark for evaluating the
long-description capability more comprehensively. Extensive experimental
results on widely-used text-video retrieval benchmarks with both short and long
descriptions and our LVDR benchmark can fully demonstrate the effectiveness of
our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TGIF: Text-Guided Inpainting Forgery <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Mareen, Dimitrios Karageorgiou, Glenn Van Wallendael, Peter Lambert, Symeon Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital image manipulation has become increasingly accessible and realistic
with the advent of generative AI technologies. Recent developments allow for
text-guided inpainting, making sophisticated image edits possible with minimal
effort. This poses new challenges for digital media forensics. For example,
diffusion model-based approaches could either splice the inpainted region into
the original image, or regenerate the entire image. In the latter case,
traditional image forgery localization (IFL) methods typically fail. This paper
introduces the Text-Guided Inpainting Forgery (TGIF) dataset, a comprehensive
collection of images designed to support the training and evaluation of image
forgery localization and synthetic image detection (SID) methods. The TGIF
dataset includes approximately 75k forged images, originating from popular
open-source and commercial methods, namely SD2, SDXL, and Adobe Firefly. We
benchmark several state-of-the-art IFL and SID methods on TGIF. Whereas
traditional IFL methods can detect spliced images, they fail to detect
regenerated inpainted images. Moreover, traditional SID may detect the
regenerated inpainted images to be fake, but cannot localize the inpainted
area. Finally, both IFL and SID methods fail when exposed to stronger
compression, while they are less robust to modern compression algorithms, such
as WEBP. In conclusion, this work demonstrates the inefficiency of
state-of-the-art detectors on local manipulations performed by modern
generative approaches, and aspires to help with the development of more capable
IFL and SID methods. The dataset and code can be downloaded at
https://github.com/IDLabMedia/tgif-dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, accepted at IEEE WIFS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deepfake Detection: A Comprehensive <span class="highlight-title">Survey</span> from the Reliability
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10881v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10881v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Wang, Xin Liao, Kam Pui Chow, Xiaodong Lin, Yinglong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mushroomed Deepfake synthetic materials circulated on the internet have
raised a profound social impact on politicians, celebrities, and individuals
worldwide. In this survey, we provide a thorough review of the existing
Deepfake detection studies from the reliability perspective. We identify three
reliability-oriented research challenges in the current Deepfake detection
domain: transferability, interpretability, and robustness. Moreover, while
solutions have been frequently addressed regarding the three challenges, the
general reliability of a detection model has been barely considered, leading to
the lack of reliable evidence in real-life usages and even for prosecutions on
Deepfake-related cases in court. We, therefore, introduce a model reliability
study metric using statistical random sampling knowledge and the publicly
available benchmark datasets to review the reliability of the existing
detection models on arbitrary Deepfake candidate suspects. Case studies are
further executed to justify the real-life Deepfake cases including different
groups of victims with the help of the reliably qualified detection models as
reviewed in this survey. Reviews and experiments on the existing approaches
provide informative discussions and future research directions for Deepfake
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-03T00:00:00Z">2024-10-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">102</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vinoground: Scrutinizing LMMs over Dense Temporal <span class="highlight-title">Reasoning</span> with Short
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrui Zhang, Mu Cai, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been growing sentiment recently that modern large multimodal models
(LMMs) have addressed most of the key challenges related to short video
comprehension. As a result, both academia and industry are gradually shifting
their attention towards the more complex challenges posed by understanding
long-form videos. However, is this really the case? Our studies indicate that
LMMs still lack many fundamental reasoning capabilities even when dealing with
short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation
benchmark encompassing 1000 short and natural video-caption pairs. We
demonstrate that existing LMMs severely struggle to distinguish temporal
differences between different actions and object transformations. For example,
the best model GPT-4o only obtains ~50% on our text and video scores, showing a
large gap compared to the human baseline of ~90%. All open-source multimodal
models and CLIP-based models perform much worse, producing mostly random chance
performance. Through this work, we shed light onto the fact that temporal
reasoning in short videos is a problem yet to be fully solved. The dataset and
evaluation code are available at https://vinoground.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://vinoground.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Erasing Conceptual Knowledge from Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept erasure in language models has traditionally lacked a comprehensive
evaluation framework, leading to incomplete assessments of effectiveness of
erasure methods. We propose an evaluation paradigm centered on three critical
criteria: innocence (complete knowledge removal), seamlessness (maintaining
conditional fluent generation), and specificity (preserving unrelated task
performance). Our evaluation metrics naturally motivate the development of
Erasure of Language Memory (ELM), a new method designed to address all three
dimensions. ELM employs targeted low-rank updates to alter output distributions
for erased concepts while preserving overall model capabilities including
fluency when prompted for an erased concept. We demonstrate ELM's efficacy on
biosecurity, cybersecurity, and literary domain erasure tasks. Comparative
analysis shows that ELM achieves superior performance across our proposed
metrics, including near-random scores on erased topic assessments, generation
fluency, maintained accuracy on unrelated benchmarks, and robustness under
adversarial attacks. Our code, data, and trained models are available at
https://elm.baulab.info
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://elm.baulab.info</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Straka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on
Multilingual Coreference Resolution. In this third iteration of the shared
task, a novel objective is to also predict empty nodes needed for zero
coreference mentions (while the empty nodes were given on input in previous
years). This way, coreference resolution can be performed on raw text. We
evaluate two model variants: a~two-stage approach (where the empty nodes are
predicted first using a pretrained encoder model and then processed together
with sentence words by another pretrained model) and a single-stage approach
(where a single pretrained encoder model generates empty nodes, coreference
mentions, and coreference links jointly). In both settings, CorPipe surpasses
other participants by a large margin of 3.9 and 2.8 percent points,
respectively. The source code and the trained model are available at
https://github.com/ufal/crac2024-corpipe .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIEVE: General Purpose Data Filtering System Matching <span class="highlight-title">GPT</span>-4o Accuracy at
  1% the Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifan Zhang, Robert Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating specialized large language models requires vast amounts of clean,
special purpose data for training and fine-tuning. With only a handful of
existing large-scale, domain-specific datasets, creation of new datasets is
required in most applications. This requires the development of new
application-specific filtering of web-scale data. Filtering with a
high-performance, general-purpose LLM such as GPT-4o can be highly effective,
but this is extremely expensive at web-scale. This paper proposes SIEVE, a
lightweight alternative that matches GPT-4o accuracy at a fraction of the cost.
SIEVE can perform up to 500 filtering operations for the cost of one GPT-4o
filtering call. The key to SIEVE is a seamless integration of GPT-4o and
lightweight T5 models, using active learning to fine-tune T5 in the background
with a small number of calls to GPT-4o. Once trained, it performs as well as
GPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the
OpenWebText dataset, using five highly customized filter tasks targeting high
quality and domain-specific content. Our results demonstrate the effectiveness
and efficiency of our method in curating large, high-quality datasets for
language model training at a substantially lower cost (1%) than existing
techniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o
achieve similar accuracy, with human evaluators preferring SIEVE's filtering
results to those of GPT-4o.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Language Models on Synthetic Edit Sequences Improves Code
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ulyana Piterbarg, Lerrel Pinto, Rob Fergus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineers mainly write code by editing existing programs. In
contrast, large language models (LLMs) autoregressively synthesize programs in
a single pass. One explanation for this is the scarcity of open-sourced edit
data. While high-quality instruction data for code synthesis is already scarce,
high-quality edit data is even scarcer. To fill this gap, we develop a
synthetic data generation algorithm called LintSeq. This algorithm refactors
existing code into a sequence of code edits by using a linter to procedurally
sample across the error-free insertions that can be used to sequentially write
programs. It outputs edit sequences as text strings consisting of consecutive
program diffs. To test LintSeq, we use it to refactor a dataset of instruction
+ program pairs into instruction + program-diff-sequence tuples. Then, we
instruction finetune a series of smaller LLMs ranging from 2.6B to 14B
parameters on both the re-factored and original versions of this dataset,
comparing zero-shot performance on code synthesis benchmarks. We show that
during repeated sampling, edit sequence finetuned models produce more diverse
programs than baselines. This results in better inference-time scaling for
benchmark coverage as a function of samples, i.e. the fraction of problems
"pass@k" solved by any attempt given "k" tries. For example, on HumanEval
pass@50, small LLMs finetuned on synthetic edit sequences are competitive with
GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)
in absolute score. Finally, we also pretrain our own tiny LMs for code
understanding. We show that finetuning tiny models on synthetic code edits
results in state-of-the-art code synthesis for the on-device model class. Our
150M parameter edit sequence LM matches or outperforms code models with twice
as many parameters, both with and without repeated sampling, including Codex
and AlphaCode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt
  Optimization for Text <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned
to extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neutral residues: revisiting adapters for model extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franck Signe Talla, Herve Jegou, Edouard Grave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of extending a pretrained large language model to a
new domain that was not seen at training time, like adding a language for which
the original model has seen no or little training data. Popular solutions like
fine-tuning or low-rank adaptation are successful at domain adaptation, but
formally they do not add any extra capacity and degrade the performance in the
original domain.
  Our paper analyzes this extension problem under three angles: data,
architecture and training procedure, which are advantageously considered
jointly. In particular, we improve adapters and make it possible to learn an
entire new language while ensuring that the output of the neural network is
almost unchanged in the original domain. For this purpose, we modify the new
residual blocks in a way that leads each new residual block to output
near-zeros in the original domain.
  This solution of neutral residues, which borrows architectural components
from mixture of experts, is effective: with only 20% extra learnable weights
compared to an original model trained on English, we get results that are
significantly better than concurrent approaches (fine-tuning, low-rank or
vanilla adapters) in terms of the trade-off between learning a new language and
not forgetting English.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) has demonstrated
effectiveness in aligning large language models (LLMs) with human preferences.
However, token-level RLHF suffers from the credit assignment problem over long
sequences, where delayed rewards make it challenging for the model to discern
which actions contributed to successful outcomes. This hinders learning
efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple
yet effective RLHF framework that incorporates macro actions -- sequences of
tokens or higher-level language constructs -- into the learning process. By
operating at this higher level of abstraction, our approach reduces the
temporal distance between actions and rewards, facilitating faster and more
accurate credit assignment. This results in more stable policy gradient
estimates and enhances learning efficiency within each episode, all without
increasing computational complexity during training or inference. We validate
our approach through extensive experiments across various model sizes and
tasks, including text summarization, dialogue generation, question answering,
and program synthesis. Our method achieves substantial performance improvements
over standard RLHF, with performance gains of up to 30% in text summarization
and code generation, 18% in dialogue, and 8% in question answering tasks.
Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in
terms of training time and continues to outperform it with further training. We
will make our code and data publicly available at
https://github.com/ernie-research/MA-RLHF .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding <span class="highlight-title">Large Language Model</span>s In Embodied Environment With Imperfect
  World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolan Liu, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite a widespread success in various applications, large language models
(LLMs) often stumble when tackling basic physical reasoning or executing
robotics tasks, due to a lack of direct experience with the physical nuances of
the real world. To address these issues, we propose a Grounding Large language
model with Imperfect world MOdel (GLIMO), which utilizes proxy world models
such as simulators to collect and synthesize trining data. GLIMO incorporates
an LLM agent-based data generator to automatically create high-quality and
diverse instruction datasets. The generator includes an iterative self-refining
module for temporally consistent experience sampling, a diverse set of
question-answering instruction seeds, and a retrieval-augmented generation
module for reflecting on prior experiences. Comprehensive experiments show that
our approach improve the performance of strong open-source LLMs like LLaMA-3
with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$
across three different benchmarks, respectively. The performance is able to
compete with or surpass their larger counterparts such as GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Salient Information Prompting to Steer Content in Prompt-based
  Abstractive Summarization <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Xu, Mohammed Asad Karim, Saket Dingliwal, Aparna Elangovan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned
to extract salient keyphrases. By using SigExt, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Justice or Prejudice? Quantifying Biases in <span class="highlight-title">LLM</span>-as-a-Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh V Chawla, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge has been widely utilized as an evaluation method in various
benchmarks and served as supervised rewards in model training. However, despite
their excellence in many domains, potential issues are under-explored,
undermining their reliability and the scope of their utility. Therefore, we
identify 12 key potential biases and propose a new automated bias
quantification framework-CALM-which systematically quantifies and analyzes each
type of bias in LLM-as-a-Judge by using automated and principle-guided
modification. Our experiments cover multiple popular language models, and the
results indicate that while advanced models have achieved commendable overall
performance, significant biases persist in certain specific tasks. Empirical
results suggest that there remains room for improvement in the reliability of
LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence
of these biases and give some suggestions for the reliable application of
LLM-as-a-Judge. Our work highlights the need for stakeholders to address these
issues and remind users to exercise caution in LLM-as-a-Judge applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DivScene: <span class="highlight-title">Benchmark</span>ing LVLMs for Object Navigation with Diverse Scenes
  and Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object navigation in unknown environments is crucial for deploying embodied
agents in real-world applications. While we have witnessed huge progress due to
large-scale scene datasets, faster simulators, and stronger models, previous
studies mainly focus on limited scene types and target objects. In this paper,
we study a new task of navigating to diverse target objects in a large number
of scene types. To benchmark the problem, we present a large-scale scene
dataset, DivScene, which contains 4,614 scenes across 81 different types. With
the dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a
Large Vision Language Model (LVLM) through imitation learning. The LVLM is
trained to take previous observations from the environment and generate the
next actions. We also introduce CoT explanation traces of the action prediction
for better performance when tuning LVLMs. Our extensive experiments find that
we can build a performant LVLM-based agent through imitation learning on the
shortest paths constructed by a BFS planner without any human supervision. Our
agent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we
carry out various analyses showing the generalization ability of our agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Multi-Modal Interleaved Document Representation for Information
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Retrieval (IR) methods aim to identify relevant documents in
response to a given query, which have gained remarkable attention due to their
successful application in various natural language tasks. However, existing
approaches typically consider only the textual information within the
documents, which overlooks the fact that documents can contain multiple
modalities, including texts, images, and tables. Further, they often segment
each long document into multiple discrete passages for embedding, preventing
them from capturing the overall document context and interactions between
paragraphs. We argue that these two limitations lead to suboptimal document
representations for retrieval. In this work, to address them, we aim to produce
more comprehensive and nuanced document representations by holistically
embedding documents interleaved with different modalities. Specifically, we
achieve this by leveraging the capability of recent vision-language models that
enable the processing and integration of text, images, and tables into a
unified format and representation. Moreover, to mitigate the information loss
from segmenting documents into passages, instead of representing and retrieving
passages individually, we further merge the representations of segmented
passages into one single document representation, while we additionally
introduce a reranking strategy to decouple and identify the relevant passage
within the document if necessary. Then, through extensive experiments on
diverse information retrieval scenarios considering both the textual and
multimodal queries, we show that our approach substantially outperforms
relevant baselines, thanks to the consideration of the multimodal information
interleaved within the documents in a unified way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Inference-Time Compute: <span class="highlight-title">LLM</span>s Can Predict if They Can Do Better,
  Even Mid-<span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohin Manvi, Anikait Singh, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time computation is a powerful paradigm to enhance the performance
of large language models (LLMs), with Best-of-N sampling being a widely used
technique. However, this method is computationally expensive, requiring both
(1) an external reward model and (2) the generation of multiple samples. In
this work, we introduce a new generative self-evaluation scheme designed to
adaptively reduce the number of generated samples while maintaining or even
improving performance. We use a generative reward model formulation, allowing
the LLM to predict mid-generation the probability that restarting the
generation will yield a better response. These predictions are obtained without
an external reward model and can be used to decide whether or not to generate
more samples, prune unpromising samples early on, or to pick the best sample.
This capability is very inexpensive as it involves generating a single
predefined token. Trained using a dataset constructed with real unfiltered
LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval
increases from 21% to 34% with 16 samples and math performance on GSM8K
improves from 84% to 91%. By sampling only when the LLM determines that it is
beneficial to do so and adaptively adjusting temperature annealing, we
demonstrate that 74% of the improvement from using 16 samples can be achieved
with only 1.2 samples on average. We further demonstrate that 50-75% of samples
can be pruned early in generation with minimal degradation in performance.
Overall, our methods enable more efficient and scalable compute utilization
during inference for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>s as Markov Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boullé, Ievgen Redko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have proven to be remarkably efficient, both
across a wide range of natural language processing tasks and well beyond them.
However, a comprehensive theoretical analysis of the origins of their
impressive performance remains elusive. In this paper, we approach this
challenging task by drawing an equivalence between generic autoregressive
language models with vocabulary of size $T$ and context window of size $K$ and
Markov chains defined on a finite state space of size $\mathcal{O}(T^K)$. We
derive several surprising findings related to the existence of a stationary
distribution of Markov chains that capture the inference power of LLMs, their
speed of convergence to it, and the influence of the temperature on the latter.
We then prove pre-training and in-context generalization bounds and show how
the drawn equivalence allows us to enrich their interpretation. Finally, we
illustrate our theoretical guarantees with experiments on several recent LLMs
to highlight how they capture the behavior observed in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-Specific Retrieval-Augmented <span class="highlight-title">Generation</span> Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan C. Barron, Ves Grantcharov, Selma Wanna, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim Ø. Rasmussen, Cynthia Matuszek, Boian S. Alexandrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling
  for Retrieval-Augmented <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, Zhongwei Wan, Xiaodan Liang, Chengming Li, Zhenan Sun, Lingpeng Kong, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present UncertaintyRAG, a novel approach for long-context
Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio
(SNR)-based span uncertainty to estimate similarity between text chunks. This
span uncertainty enhances model calibration, improving robustness and
mitigating semantic inconsistencies introduced by random chunking. Leveraging
this insight, we propose an efficient unsupervised learning technique to train
the retrieval model, alongside an effective data sampling and scaling strategy.
UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving
state-of-the-art results while using only 4% of the training data compared to
other advanced open-source retrieval models under distribution shift settings.
Our method demonstrates strong calibration through span uncertainty, leading to
improved generalization and robustness in long-context RAG tasks. Additionally,
UncertaintyRAG provides a lightweight retrieval model that can be integrated
into any large language model with varying context window lengths, without the
need for fine-tuning, showcasing the flexibility of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Instruction Tuning With Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of video large multimodal models (LMMs) has been hindered by
the difficulty of curating large amounts of high-quality raw data from the web.
To address this, we propose an alternative approach by creating a high-quality
synthetic dataset specifically for video instruction-following, namely
LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,
open-ended question-answering (QA), and multiple-choice QA. By training on this
dataset, in combination with existing visual instruction tuning data, we
introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that
LLaVA-Video achieves strong performance across various video benchmarks,
highlighting the effectiveness of our dataset. We plan to release the dataset,
its generation pipeline, and the model checkpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-Critic: Learning to Evaluate <span class="highlight-title">Multimodal</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)
designed as a generalist evaluator to assess performance across a wide range of
multimodal tasks. LLaVA-Critic is trained using a high-quality critic
instruction-following dataset that incorporates diverse evaluation criteria and
scenarios. Our experiments demonstrate the model's effectiveness in two key
areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation
scores, performing on par with or surpassing GPT models on multiple evaluation
benchmarks; and (2) Preference Learning, where it generates reward signals for
preference learning, enhancing model alignment capabilities. This work
underscores the potential of open-source LMMs in self-critique and evaluation,
setting the stage for future research into scalable, superhuman alignment
feedback mechanisms for LMMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>s Know More Than They Show: On the Intrinsic Representation of <span class="highlight-title">LLM</span>
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often produce errors, including factual
inaccuracies, biases, and reasoning failures, collectively referred to as
"hallucinations". Recent studies have demonstrated that LLMs' internal states
encode information regarding the truthfulness of their outputs, and that this
information can be utilized to detect errors. In this work, we show that the
internal representations of LLMs encode much more information about
truthfulness than previously recognized. We first discover that the
truthfulness information is concentrated in specific tokens, and leveraging
this property significantly enhances error detection performance. Yet, we show
that such error detectors fail to generalize across datasets, implying that --
contrary to prior claims -- truthfulness encoding is not universal but rather
multifaceted. Next, we show that internal representations can also be used for
predicting the types of errors the model is likely to make, facilitating the
development of tailored mitigation strategies. Lastly, we reveal a discrepancy
between LLMs' internal encoding and external behavior: they may encode the
correct answer, yet consistently generate an incorrect one. Taken together,
these insights deepen our understanding of LLM errors from the model's internal
perspective, which can guide future research on enhancing error analysis and
mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Attention Improves Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaniv Leviathan, Matan Kalman, Yossi Matias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unneeded elements in the attention's context degrade performance. We
introduce Selective Attention, a simple parameter-free change to the standard
attention mechanism which reduces attention to unneeded elements. Selective
attention improves language modeling performance in a variety of model sizes
and context lengths. For example, a range of transformers trained with the
language modeling objective on C4 with selective attention perform equivalently
to standard transformers with ~2X more heads and parameters in their attention
modules. Selective attention also allows decreasing the size of the attention's
context buffer, leading to meaningful reductions in the memory and compute
requirements during inference. For example, transformers with 100M parameters
trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and
47X less memory for their attention module, respectively, when equipped with
selective attention, as those without selective attention, with the same
validation perplexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HELMET: How to Evaluate Long-Context Language Models Effectively and
  Thoroughly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izasak, Moshe Wasserblat, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been many benchmarks for evaluating long-context language models
(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack
(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate
to the diverse downstream applications of LCLMs, and the inconsistency further
complicates model comparison. We investigate the underlying reasons behind
current practices and find that existing benchmarks often provide noisy signals
due to low coverage of applications, insufficient lengths, unreliable metrics,
and incompatibility with base models. In this work, we present HELMET (How to
Evaluate Long-context Models Effectively and Thoroughly), a comprehensive
benchmark encompassing seven diverse, application-centric categories. We also
address many issues in previous benchmarks by adding controllable lengths up to
128k tokens, model-based evaluation for reliable metrics, and few-shot
prompting for robustly evaluating base models. Consequently, we demonstrate
that HELMET offers more reliable and consistent rankings of frontier LCLMs.
Through a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks
like NIAH are not good predictors of downstream performance; (2) the diverse
categories in HELMET exhibit distinct trends and low correlation with each
other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models
significantly lag behind closed ones when the task requires full-context
reasoning or following complex instructions -- the gap widens with increased
lengths. Finally, we recommend using our RAG tasks for fast model development,
as they are easy to run and more predictive of other downstream performance;
ultimately, we advocate for a holistic evaluation across diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data are available here:
  https://github.com/princeton-nlp/HELMET</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Proper Treatment of Tokenization in Psycholinguistics <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Giulianelli, Luca Malagutti, Juan Luis Gastaldi, Brian DuSell, Tim Vieira, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are widely used in computational psycholinguistics to test
theories that relate the negative log probability (the surprisal) of a region
of interest (a substring of characters) under a language model to its cognitive
cost experienced by readers, as operationalized, for example, by gaze duration
on the region. However, the application of modern language models to
psycholinguistic studies is complicated by the practice of using tokenization
as an intermediate step in training a model. Doing so results in a language
model over token strings rather than one over character strings. Vexingly,
regions of interest are generally misaligned with these token strings. The
paper argues that token-level language models should be (approximately)
marginalized into character-level language models before they are used in
psycholinguistic studies to compute the surprisal of a region of interest;
then, the marginalized character-level language model can be used to compute
the surprisal of an arbitrary character substring, which we term a focal area,
that the experimenter may wish to use as a predictor. Our proposal of
marginalizing a token-level model into a character-level one solves this
misalignment issue independently of the tokenization scheme. Empirically, we
discover various focal areas whose surprisal is a better psychometric predictor
than the surprisal of the region of interest itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main conference long paper at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiddenGuard: Fine-Grained Safe <span class="highlight-title">Generation</span> with Specialized
  Representation Router 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) grow increasingly powerful, ensuring their
safety and alignment with human values remains a critical challenge. Ideally,
LLMs should provide informative responses while avoiding the disclosure of
harmful or sensitive information. However, current alignment approaches, which
rely heavily on refusal strategies, such as training models to completely
reject harmful prompts or applying coarse filters are limited by their binary
nature. These methods either fully deny access to information or grant it
without sufficient nuance, leading to overly cautious responses or failures to
detect subtle harmful content. For example, LLMs may refuse to provide basic,
public information about medication due to misuse concerns. Moreover, these
refusal-based methods struggle to handle mixed-content scenarios and lack the
ability to adapt to context-dependent sensitivities, which can result in
over-censorship of benign content. To overcome these challenges, we introduce
HiddenGuard, a novel framework for fine-grained, safe generation in LLMs.
HiddenGuard incorporates Prism (rePresentation Router for In-Stream
Moderation), which operates alongside the LLM to enable real-time, token-level
detection and redaction of harmful content by leveraging intermediate hidden
states. This fine-grained approach allows for more nuanced, context-aware
moderation, enabling the model to generate informative responses while
selectively redacting or replacing sensitive information, rather than outright
refusal. We also contribute a comprehensive dataset with token-level
fine-grained annotations of potentially harmful information across diverse
contexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1
score for detecting and redacting harmful content while preserving the overall
utility and informativeness of the model's responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DailyDilemmas: Revealing Value Preferences of <span class="highlight-title">LLM</span>s with Quandaries of
  Daily Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we increasingly seek guidance from LLMs for decision-making in daily life,
many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of the users. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
includes two possible actions and with each action, the affected parties and
human values invoked. Based on these dilemmas, we consolidated a set of human
values across everyday topics e.g., interpersonal relationships, workplace, and
environmental issues. We evaluated LLMs on these dilemmas to determine what
action they will take and the values represented by these actions. Then, we
analyzed these values through the lens of five popular theories inspired by
sociology, psychology and philosophy. These theories are: World Value Survey,
Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and
Plutchik Wheel of Emotion. We find that LLMs are most aligned with the
self-expression over survival values in terms of World Value Survey, care over
loyalty in Moral Foundation Theory. Interestingly, we find large preferences
differences in models for some core values such as truthfulness e.g.,
Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to
select it by 9.4%. We also study the recent guidance released by OpenAI
(ModelSpec), and Anthropic (Constitutional AI) to understand how their released
principles reflect their actual value prioritization when facing nuanced moral
reasoning in daily-life settings. We find that end users cannot effectively
steer such prioritization using system prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling an End-to-End Voice Assistant Without Instruction Training
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Held, Ella Li, Michael Ryan, Weiyan Shi, Yanzhe Zhang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants, such as Siri and Google Assistant, typically model audio
and text separately, resulting in lost speech information and increased
complexity. Recent efforts to address this with end-to-end Speech Large
Language Models (LLMs) trained with supervised finetuning (SFT)
  have led to models ``forgetting" capabilities from text-only LLMs. Our work
proposes an alternative paradigm for training Speech LLMs without instruction
data, using the response of a text-only LLM to transcripts as self-supervision.
Importantly, this process can be performed without annotated responses. We show
that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question
Answering, Classification, and Translation. Furthermore, we show that DiVA
better meets user preferences, achieving a 72\% win rate compared with
state-of-the-art models like Qwen 2 Audio, despite using $>$100x less training
compute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CulturalBench: a Robust, Diverse and Challenging <span class="highlight-title">Benchmark</span> on Measuring
  the (Lack of) Cultural Knowledge of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To make large language models (LLMs) more helpful across diverse cultures, it
is essential to have effective cultural knowledge benchmarks to measure and
track our progress. Effective benchmarks need to be robust, diverse, and
challenging. We introduce CulturalBench: a set of 1,227 human-written and
human-verified questions for effectively assessing LLMs' cultural knowledge,
covering 45 global regions including the underrepresented ones like Bangladesh,
Zimbabwe, and Peru. Questions - each verified by five independent annotators -
span 17 diverse topics ranging from food preferences to greeting etiquettes. We
evaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which
share the same questions but asked differently. We find that LLMs are sensitive
to such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to
human performance (92.6% accuracy), CulturalBench-Hard is more challenging for
frontier LLMs with the best performing model (GPT-4o) at only 61.5% and the
worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with
tricky questions that have multiple correct answers (e.g., What utensils do the
Chinese usually use?), revealing a tendency to converge to a single answer. Our
results also indicate that OpenAI GPT-4o substantially outperform other
proprietary and open source models in questions related to all but one region
(Oceania). Nonetheless, all models consistently underperform on questions
related to South America and the Middle East.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAN: Fourier Analysis Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jing Su, Jun Zhang, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success achieved by neural networks, particularly
those represented by MLP and Transformer, we reveal that they exhibit potential
flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize
the periodic data rather than genuinely understanding the underlying principles
of periodicity. However, periodicity is a crucial trait in various forms of
reasoning and generalization, underpinning predictability across natural and
engineered systems through recurring patterns in observations. In this paper,
we propose FAN, a novel network architecture based on Fourier Analysis, which
empowers the ability to efficiently model and reason about periodic phenomena.
By introducing Fourier Series, the periodicity is naturally integrated into the
structure and computational processes of the neural network, thus achieving a
more accurate expression and prediction of periodic patterns. As a promising
substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in
various models with fewer parameters and FLOPs. Through extensive experiments,
we demonstrate the effectiveness of FAN in modeling and reasoning about
periodic functions, and the superiority and generalizability of FAN across a
range of real-world tasks, including symbolic formula representation, time
series forecasting, and language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining Language Modeling Assumptions Using an Annotated Literary
  Dialect Corpus <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig Messner, Tom Lippincott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dataset of 19th century American literary orthovariant tokens
with a novel layer of human-annotated dialect group tags designed to serve as
the basis for computational experiments exploring literarily meaningful
orthographic variation. We perform an initial broad set of experiments over
this dataset using both token (BERT) and character (CANINE)-level contextual
language models. We find indications that the "dialect effect" produced by
intentional orthographic variation employs multiple linguistic channels, and
that these channels are able to be surfaced to varied degrees given particular
language modelling assumptions. Specifically, we find evidence showing that
choice of tokenization scheme meaningfully impact the type of orthographic
information a model is able to surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NLP4DH@EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Train Long-Context Language Models (Effectively) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study continued training and supervised fine-tuning (SFT) of a language
model (LM) to make effective use of long-context information. We first
establish a reliable evaluation protocol to guide model development -- Instead
of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set
of long-context tasks, and we evaluate models after SFT with instruction data
as this better reveals long-context abilities. Supported by our robust
evaluations, we run thorough experiments to decide the data mix for continued
pre-training, the instruction tuning dataset, and many other design choices. We
find that (1) code repositories and books are excellent sources of long data,
but it is crucial to combine them with high-quality short data; (2) training
with a sequence length beyond the evaluation length boosts long-context
performance; (3) for SFT, using only short instruction datasets yields strong
performance on long-context tasks. Our final model, ProLong-8B, which is
initialized from Llama-3 and trained on 40B tokens, demonstrates
state-of-the-art long-context performance among similarly sized models at a
length of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of
long-context tasks despite having seen only 5% as many tokens during
long-context training. Additionally, ProLong can effectively process up to 512K
tokens, one of the longest context windows of publicly available LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code, data, and models are available at
  https://github.com/princeton-nlp/ProLong</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hate Personified: Investigating the role of <span class="highlight-title">LLM</span>s in content moderation <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Masud, Sahajpreet Singh, Viktor Hangya, Alexander Fraser, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For subjective tasks such as hate detection, where people perceive hate
differently, the Large Language Model's (LLM) ability to represent diverse
groups is unclear. By including additional context in prompts, we
comprehensively analyze LLM's sensitivity to geographical priming, persona
attributes, and numerical information to assess how well the needs of various
groups are reflected. Our findings on two LLMs, five languages, and six
datasets reveal that mimicking persona-based attributes leads to annotation
variability. Meanwhile, incorporating geographical signals leads to better
regional alignment. We also find that the LLMs are sensitive to numerical
anchors, indicating the ability to leverage community-based flagging efforts
and exposure to adversaries. Our work provides preliminary guidelines and
highlights the nuances of applying LLMs in culturally sensitive cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 Figures, 13 Tables, EMNLP'24 Mains</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring and Improving Persuasiveness of Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somesh Singh, Yaman K Singla, Harini SI, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly being used in workflows involving generating content to
be consumed by humans (e.g., marketing) and also in directly interacting with
humans (e.g., through chatbots). The development of such systems that are
capable of generating verifiably persuasive messages presents both
opportunities and challenges for society. On the one hand, such systems could
positively impact domains like advertising and social good, such as addressing
drug addiction, and on the other, they could be misused for spreading
misinformation and shaping political opinions. To channel LLMs' impact on
society, we need to develop systems to measure and benchmark their
persuasiveness. With this motivation, we introduce PersuasionBench and
PersuasionArena, the first large-scale benchmark and arena containing a battery
of tasks to measure the persuasion ability of generative models automatically.
We investigate to what extent LLMs know and leverage linguistic patterns that
can help them generate more persuasive language. Our findings indicate that the
persuasiveness of LLMs correlates positively with model size, but smaller
models can also be made to have a higher persuasiveness than much larger
models. Notably, targeted training using synthetic and natural datasets
significantly enhances smaller models' persuasive capabilities, challenging
scale-dependent assumptions. Our findings carry key implications for both model
developers and policymakers. For instance, while the EU AI Act and California's
SB-1047 aim to regulate AI models based on the number of floating point
operations, we demonstrate that simple metrics like this alone fail to capture
the full scope of AI's societal impact. We invite the community to explore and
contribute to PersuasionArena and PersuasionBench, available at
https://bit.ly/measure-persuasion, to advance our understanding of AI-driven
persuasion and its societal implications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Undesirable Memorization in <span class="highlight-title">Large Language Model</span>s: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Satvaty, Suzan Verberne, Fatih Turkmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent research increasingly showcases the remarkable capabilities of
Large Language Models (LLMs), it's vital to confront their hidden pitfalls.
Among these challenges, the issue of memorization stands out, posing
significant ethical and legal risks. In this paper, we presents a
Systematization of Knowledge (SoK) on the topic of memorization in LLMs.
Memorization is the effect that a model tends to store and reproduce phrases or
passages from the training data and has been shown to be the fundamental issue
to various privacy and security attacks against LLMs.
  We begin by providing an overview of the literature on the memorization,
exploring it across five key dimensions: intentionality, degree,
retrievability, abstraction, and transparency. Next, we discuss the metrics and
methods used to measure memorization, followed by an analysis of the factors
that contribute to memorization phenomenon. We then examine how memorization
manifests itself in specific model architectures and explore strategies for
mitigating these effects. We conclude our overview by identifying potential
research topics for the near future: to develop methods for balancing
performance and privacy in LLMs, and the analysis of memorization in specific
contexts, including conversational agents, retrieval-augmented generation,
multilingual language models, and diffusion language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Immunogenicity Prediction with Dual Attention Enables Vaccine Target
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Li, Yang Tan, Song Ke, Liang Hong, Bingxin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Immunogenicity prediction is a central topic in reverse vaccinology for
finding candidate vaccines that can trigger protective immune responses.
Existing approaches typically rely on highly compressed features and simple
model architectures, leading to limited prediction accuracy and poor
generalizability. To address these challenges, we introduce ProVaccine, a novel
deep learning solution with a dual attention mechanism that integrates
pre-trained latent vector representations of protein sequences and structures.
We also compile the most comprehensive immunogenicity dataset to date,
encompassing over 9,500 antigen sequences, structures, and immunogenicity
labels from bacteria, viruses, and tumors. Extensive experiments demonstrate
that ProVaccine outperforms existing methods across a wide range of evaluation
metrics. Furthermore, we establish a post-hoc validation protocol to assess the
practical significance of deep learning models in tackling vaccine design
challenges. Our work provides an effective tool for vaccine design and sets
valuable benchmarks for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention in <span class="highlight-title">Large Language Model</span>s Yields Efficient Zero-Shot Re-Rankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Chen, Bernal Jiménez Gutiérrez, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval (IR) systems have played a vital role in modern digital
life and have cemented their continued usefulness in this new era of generative
AI via retrieval-augmented generation. With strong language processing
capabilities and remarkable versatility, large language models (LLMs) have
become popular choices for zero-shot re-ranking in IR systems. So far,
LLM-based re-ranking methods rely on strong generative capabilities, which
restricts their use to either specialized or powerful proprietary models. Given
these restrictions, we ask: is autoregressive generation necessary and optimal
for LLMs to perform re-ranking? We hypothesize that there are abundant signals
relevant to re-ranking within LLMs that might not be used to their full
potential via generation. To more directly leverage such signals, we propose
in-context re-ranking (ICR), a novel method that leverages the change in
attention pattern caused by the search query for accurate and efficient
re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration
method using a content-free query. Due to the absence of generation, ICR only
requires two ($O(1)$) forward passes to re-rank $N$ documents, making it
substantially more efficient than generative re-ranking methods that require at
least $O(N)$ forward passes. Our novel design also enables ICR to be applied to
any LLM without specialized training while guaranteeing a well-formed ranking.
Extensive experiments with two popular open-weight LLMs on standard single-hop
and multi-hop information retrieval benchmarks show that ICR outperforms
RankGPT while cutting the latency by more than 60% in practice. Through
detailed analyses, we show that ICR's performance is specially strong on tasks
that require more complex re-ranking signals. Our findings call for further
exploration on novel ways of utilizing open-weight LLMs beyond text generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span> for Multi-Domain Translation: <span class="highlight-title">Benchmark</span>ing and
  Domain <span class="highlight-title">CoT</span> Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiang Hu, Pei Zhang, Baosong Yang, Jun Xie, Derek F. Wong, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving consistent high-quality machine translation (MT) across diverse
domains remains a significant challenge, primarily due to the limited and
imbalanced parallel training data available in various domains. While large
language models (LLMs) have demonstrated impressive general understanding and
generation abilities, their potential in multi-domain MT is under-explored. We
establish a comprehensive benchmark for multi-domain translation, featuring 25
German$\Leftrightarrow$English and 22 Chinese$\Leftrightarrow$English test sets
respectively covering 15 domains. Our evaluation of prominent LLMs reveals a
discernible performance gap against traditional MT systems, highlighting domain
overfitting and catastrophic forgetting issues after fine-tuning on
domain-limited corpora. To mitigate this, we propose a domain Chain of Thought
(CoT) fine-tuning technique that utilizes the intrinsic multi-domain
intelligence of LLMs to improve translation performance. This method inspires
the LLM to perceive domain information from the source text, which then serves
as a helpful hint to guide the translation process. Despite being trained on a
small dataset of four domains, our CoT fine-tune approach achieves notable
enhancements in translation accuracy and domain robustness than traditional
fine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20
German$\rightarrow$English distinct out-of-domain tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-Eye: Abductive NLI for <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mor Ventura, Michael Toker, Nitay Calderon, Zorik Gekhman, Yonatan Bitton, Roi Reichart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Will a Visual Language Model (VLM)-based bot warn us about slipping if it
detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet
their ability to infer outcomes and causes remains underexplored. To address
this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual
abductive reasoning skills. NL-Eye adapts the abductive Natural Language
Inference (NLI) task to the visual domain, requiring models to evaluate the
plausibility of hypothesis images based on a premise image and explain their
decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050
images) spanning diverse reasoning categories: physical, functional, logical,
emotional, cultural, and social. The data curation process involved two steps -
writing textual descriptions and generating images using text-to-image models,
both requiring substantial human involvement to ensure high-quality and
challenging scenes. Our experiments show that VLMs struggle significantly on
NL-Eye, often performing at random baseline levels, while humans excel in both
plausibility prediction and explanation quality. This demonstrates a deficiency
in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a
crucial step toward developing VLMs capable of robust multimodal reasoning for
real-world applications, including accident-prevention bots and generated video
verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndicSentEval: How Effectively do Multilingual Transformer Models encode
  Linguistic Properties for Indic Languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhilesh Aravapalli, Mounika Marreddy, Subba Reddy Oota, Radhika Mamidi, Manish Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have revolutionized the field of natural language
processing. To understand why they perform so well and to assess their
reliability, several studies have focused on questions such as: Which
linguistic properties are encoded by these models, and to what extent? How
robust are these models in encoding linguistic properties when faced with
perturbations in the input text? However, these studies have mainly focused on
BERT and the English language. In this paper, we investigate similar questions
regarding encoding capability and robustness for 8 linguistic properties across
13 different perturbations in 6 Indic languages, using 9 multilingual
Transformer models (7 universal and 2 Indic-specific). To conduct this study,
we introduce a novel multilingual benchmark dataset, IndicSentEval, containing
approximately $\sim$47K sentences. Surprisingly, our probing analysis of
surface, syntactic, and semantic properties reveals that while almost all
multilingual models demonstrate consistent encoding performance for English,
they show mixed results for Indic languages. As expected, Indic-specific
multilingual models capture linguistic properties in Indic languages better
than universal models. Intriguingly, universal models broadly exhibit better
robustness compared to Indic-specific models, particularly under perturbations
such as dropping both nouns and verbs, dropping only verbs, or keeping only
nouns. Overall, this study provides valuable insights into probing and
perturbation-specific strengths and weaknesses of popular multilingual
Transformer-based models for different Indic languages. We make our code and
dataset publicly available [https://tinyurl.com/IndicSentEval}].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in
  Under-Resourced Languages Using Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mesay Gemeda Yigezu, Melkamu Abay Mersha, Girma Yohannis Bade, Jugal Kalita, Olga Kolesnikova, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of fake news has emerged as a significant threat to the
integrity of information dissemination, particularly on social media platforms.
Misinformation can spread quickly due to the ease of creating and disseminating
content, affecting public opinion and sociopolitical events. Identifying false
information is therefore essential to reducing its negative consequences and
maintaining the reliability of online news sources. Traditional approaches to
fake news detection often rely solely on content-based features, overlooking
the crucial role of social context in shaping the perception and propagation of
news articles. In this paper, we propose a comprehensive approach that
integrates social context-based features with news content features to enhance
the accuracy of fake news detection in under-resourced languages. We perform
several experiments utilizing a variety of methodologies, including traditional
machine learning, neural networks, ensemble learning, and transfer learning.
Assessment of the outcomes of the experiments shows that the ensemble learning
approach has the highest accuracy, achieving a 0.99 F1 score. Additionally,
when compared with monolingual models, the fine-tuned model with the target
language outperformed others, achieving a 0.94 F1 score. We analyze the
functioning of the models, considering the important features that contribute
to model performance, using explainable AI techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Agent</span>s' Room: Narrative <span class="highlight-title">Generation</span> through Multi-step Collaboration <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing compelling fiction is a multifaceted process combining elements such
as crafting a plot, developing interesting characters, and using evocative
language. While large language models (LLMs) show promise for story writing,
they currently rely heavily on intricate prompting, which limits their use. We
propose Agents' Room, a generation framework inspired by narrative theory, that
decomposes narrative writing into subtasks tackled by specialized agents. To
illustrate our method, we introduce Tell Me A Story, a high-quality dataset of
complex writing prompts and human-written stories, and a novel evaluation
framework designed specifically for assessing long narratives. We show that
Agents' Room generates stories that are preferred by expert evaluators over
those produced by baseline systems by leveraging collaboration and
specialization to decompose the complex story writing task into tractable
components. We provide extensive analysis with automated and human-based
metrics of the generated output.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Implicit Bias Detection and Mitigation in Multi-<span class="highlight-title">Agent</span> <span class="highlight-title">LLM</span>
  Interactions <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angana Borah, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to evolve, they are increasingly
being employed in numerous studies to simulate societies and execute diverse
social tasks. However, LLMs are susceptible to societal biases due to their
exposure to human-generated data. Given that LLMs are being used to gain
insights into various societal aspects, it is essential to mitigate these
biases. To that end, our study investigates the presence of implicit gender
biases in multi-agent LLM interactions and proposes two strategies to mitigate
these biases. We begin by creating a dataset of scenarios where implicit gender
biases might arise, and subsequently develop a metric to assess the presence of
biases. Our empirical analysis reveals that LLMs generate outputs characterized
by strong implicit bias associations (>= 50\% of the time). Furthermore, these
biases tend to escalate following multi-agent interactions. To mitigate them,
we propose two strategies: self-reflection with in-context examples (ICE); and
supervised fine-tuning. Our research demonstrates that both methods effectively
mitigate implicit biases, with the ensemble of fine-tuning and self-reflection
proving to be the most successful.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Variational Autoencoders for Spectrogram Compression in
  Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Yakovenko, Ivan Bondarenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For many Automatic Speech Recognition (ASR) tasks audio features as
spectrograms show better results than Mel-frequency Cepstral Coefficients
(MFCC), but in practice they are hard to use due to a complex dimensionality of
a feature space. The following paper presents an alternative approach towards
generating compressed spectrogram representation, based on Convolutional
Variational Autoencoders (VAE). A Convolutional VAE model was trained on a
subsample of the LibriSpeech dataset to reconstruct short fragments of audio
spectrograms (25 ms) from a 13-dimensional embedding. The trained model for a
40-dimensional (300 ms) embedding was used to generate features for corpus of
spoken commands on the GoogleSpeechCommands dataset. Using the generated
features an ASR system was built and compared to the model with MFCC features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Theory and Practice of Natural Computing 9th International
  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Unsupervised Constituency Parsing via Maximizing Semantic
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Xiangheng He, Yusuke Miyao, Danushka Bollegala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised constituency parsers organize phrases within a sentence into a
tree-shaped syntactic constituent structure that reflects the organization of
sentence semantics. However, the traditional objective of maximizing sentence
log-likelihood (LL) does not explicitly account for the close relationship
between the constituent structure and the semantics, resulting in a weak
correlation between LL values and parsing accuracy. In this paper, we introduce
a novel objective for training unsupervised parsers: maximizing the information
between constituent structures and sentence semantics (SemInfo). We introduce a
bag-of-substrings model to represent the semantics and apply the
probability-weighted information metric to estimate the SemInfo. Additionally,
we develop a Tree Conditional Random Field (TreeCRF)-based model to apply the
SemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG)
induction, the state-of-the-art method for unsupervised constituency parsing.
Experiments demonstrate that SemInfo correlates more strongly with parsing
accuracy than LL. Our algorithm significantly enhances parsing accuracy by an
average of 7.85 points across five PCFG variants and in four languages,
achieving new state-of-the-art results in three of the four languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColaCare: Enhancing Electronic Health Record Modeling through Large
  Language Model-Driven Multi-<span class="highlight-title">Agent</span> Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Wang, Yinghao Zhu, Huiya Zhao, Xiaochen Zheng, Tianlong Wang, Wen Tang, Yasha Wang, Chengwei Pan, Ewen M. Harrison, Junyi Gao, Liantao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by clinical consultations, ColaCare employs two types of
agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.
Expert models process and generate predictions from numerical EHR data, while
LLM agents produce reasoning references and decision-making reports within the
collaborative consultation framework. We additionally incorporate the Merck
Manual of Diagnosis and Therapy (MSD) medical guideline within a
retrieval-augmented generation (RAG) module for authoritative evidence support.
Extensive experiments conducted on four distinct EHR datasets demonstrate
ColaCare's superior performance in mortality prediction tasks, underscoring its
potential to revolutionize clinical decision support systems and advance
personalized precision medicine. The code, complete prompt templates, more case
studies, etc. are publicly available at the anonymous link:
https://colacare.netlify.app.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedVisionLlama: Leveraging Pre-Trained <span class="highlight-title">Large Language Model</span> Layers to
  Enhance Medical <span class="highlight-title">Image</span> Segmentation <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), known for their versatility in textual data,
are increasingly being explored for their potential to enhance medical image
segmentation, a crucial task for accurate diagnostic imaging. This study
explores enhancing Vision Transformers (ViTs) for medical image segmentation by
integrating pre-trained LLM transformer blocks. Our approach, which
incorporates a frozen LLM transformer block into the encoder of a ViT-based
model, leads to substantial improvements in segmentation performance across
various medical imaging modalities. We propose a Hybrid Attention Mechanism
that combines global and local feature learning with a Multi-Scale Fusion Block
for aggregating features across different scales. The enhanced model shows
significant performance gains, including an average Dice score increase from
0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.
These results demonstrate the effectiveness of LLM-based transformers in
refining medical image segmentation, highlighting their potential to
significantly boost model accuracy and robustness. The source code and our
implementation are available at: https://bit.ly/3zf2CVs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithms For Automatic Accentuation And Transcription Of Russian Texts
  In Speech Recognition Systems <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Iakovenko, Ivan Bondarenko, Mariya Borovikova, Daniil Vodolazsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an overview of rule-based system for automatic
accentuation and phonemic transcription of Russian texts for speech connected
tasks, such as Automatic Speech Recognition (ASR). Two parts of the developed
system, accentuation and transcription, use different approaches to achieve
correct phonemic representations of input phrases. Accentuation is based on
"Grammatical dictionary of the Russian language" of A.A. Zaliznyak and
wiktionary corpus. To distinguish homographs, the accentuation system also
utilises morphological information of the sentences based on Recurrent Neural
Networks (RNN). Transcription algorithms apply the rules presented in the
monograph of B.M. Lobanov and L.I. Tsirulnik "Computer Synthesis and Voice
Cloning". The rules described in the present paper are implemented in an
open-source module, which can be of use to any scientific study connected to
ASR or Speech To Text (STT) tasks. Automatically marked up text annotations of
the Russian Voxforge database were used as training data for an acoustic model
in CMU Sphinx. The resulting acoustic model was evaluated on cross-validation,
mean Word Accuracy being 71.2%. The developed toolkit is written in the Python
language and is accessible on GitHub for any researcher interested.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Speech and Computer 20th International Conference, SPECOM 2018,
  Leipzig, Germany, Proceedings 20</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methods for Automatic Matrix Language Determination of Code-Switched
  Speech <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Iakovenko, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching (CS) is the process of speakers interchanging between two or
more languages which in the modern world becomes increasingly common. In order
to better describe CS speech the Matrix Language Frame (MLF) theory introduces
the concept of a Matrix Language, which is the language that provides the
grammatical structure for a CS utterance. In this work the MLF theory was used
to develop systems for Matrix Language Identity (MLID) determination. The MLID
of English/Mandarin and English/Spanish CS text and speech was compared to
acoustic language identity (LID), which is a typical way to identify a language
in monolingual utterances. MLID predictors from audio show higher correlation
with the textual principles than LID in all cases while also outperforming LID
in an MLID recognition task based on F1 macro (60\%) and correlation score
(0.38). This novel approach has identified that non-English languages (Mandarin
and Spanish) are preferred over the English language as the ML contrary to the
monolingual choice of LID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">Large Language Model</span>s Grasp Legal Theories? Enhance Legal <span class="highlight-title">Reasoning</span>
  with Insights from Multi-<span class="highlight-title">Agent</span> Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikang Yuan, Junjie Cao, Zhuoren Jiang, Yangyang Kang, Jun Lin, Kaisong Song, tianqianjin lin, Pengwei Yan, Changlong Sun, Xiaozhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) could struggle to fully understand legal
theories and perform complex legal reasoning tasks. In this study, we introduce
a challenging task (confusing charge prediction) to better evaluate LLMs'
understanding of legal theories and reasoning capabilities. We also propose a
novel framework: Multi-Agent framework for improving complex Legal Reasoning
capability (MALR). MALR employs non-parametric learning, encouraging LLMs to
automatically decompose complex legal tasks and mimic human learning process to
extract insights from legal rules, helping LLMs better understand legal
theories and enhance their legal reasoning abilities. Extensive experiments on
multiple real-world datasets demonstrate that the proposed framework
effectively addresses complex reasoning issues in practical scenarios, paving
the way for more reliable applications in the legal domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed-Session Conversation with Egocentric Memory <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihyoung Jang, Taeyoung Kim, Hyounghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently introduced dialogue systems have demonstrated high usability.
However, they still fall short of reflecting real-world conversation scenarios.
Current dialogue systems exhibit an inability to replicate the dynamic,
continuous, long-term interactions involving multiple partners. This shortfall
arises because there have been limited efforts to account for both aspects of
real-world dialogues: deeply layered interactions over the long-term dialogue
and widely expanded conversation networks involving multiple participants. As
the effort to incorporate these aspects combined, we introduce Mixed-Session
Conversation, a dialogue system designed to construct conversations with
various partners in a multi-session dialogue setup. We propose a new dataset
called MiSC to implement this system. The dialogue episodes of MiSC consist of
6 consecutive sessions, with four speakers (one main speaker and three
partners) appearing in each episode. Also, we propose a new dialogue model with
a novel memory management mechanism, called Egocentric Memory Enhanced
Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories
from the main speaker's perspective during conversations with partners,
enabling seamless continuity in subsequent interactions. Extensive human
evaluations validate that the dialogues in MiSC demonstrate a seamless
conversational flow, even when conversation partners change in each session.
EMMA trained with MiSC is also evaluated to maintain high memorability without
contradiction throughout the entire conversation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024 (30 pages); Project website:
  https://mixed-session.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defining Knowledge: Bridging Epistemology and <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constanza Fierro, Ruchira Dhar, Filippos Stamatiou, Nicolas Garneau, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge claims are abundant in the literature on large language models
(LLMs); but can we say that GPT-4 truly "knows" the Earth is round? To address
this question, we review standard definitions of knowledge in epistemology and
we formalize interpretations applicable to LLMs. In doing so, we identify
inconsistencies and gaps in how current NLP research conceptualizes knowledge
with respect to epistemological frameworks. Additionally, we conduct a survey
of 100 professional philosophers and computer scientists to compare their
preferences in knowledge definitions and their views on whether LLMs can really
be said to know. Finally, we suggest evaluation protocols for testing knowledge
in accordance to the most relevant definitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Gradient Alignment for Online Data Mixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simin Fan, David Grangier, Pierre Ablin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The composition of training data mixtures is critical for effectively
training large language models (LLMs), as it directly impacts their performance
on downstream tasks. Our goal is to identify an optimal data mixture to
specialize an LLM for a specific task with access to only a few examples.
Traditional approaches to this problem include ad-hoc reweighting methods,
importance sampling, and gradient alignment techniques. This paper focuses on
gradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable
online gradient alignment algorithm. DGA dynamically estimates the pre-training
data mixture on which the models' gradients align as well as possible with
those of the model on the specific task. DGA is the first gradient alignment
approach that incurs minimal overhead compared to standard pre-training and
outputs a competitive model, eliminating the need for retraining the model.
Experimentally, we demonstrate significant improvements over importance
sampling in two key scenarios: (i) when the pre-training set is small and
importance sampling overfits due to limited data; and (ii) when there is
insufficient specialized data, trapping importance sampling on narrow pockets
of data. Our findings underscore the effectiveness of gradient alignment
methods in optimizing training data mixtures, particularly in data-constrained
environments, and offer a practical solution for enhancing LLM performance on
specific tasks with limited data availability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DTVLT: A Multi-modal Diverse Text <span class="highlight-title">Benchmark</span> for Visual Language Tracking
  Based on <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language tracking (VLT) has emerged as a cutting-edge research area,
harnessing linguistic data to enhance algorithms with multi-modal inputs and
broadening the scope of traditional single object tracking (SOT) to encompass
video understanding applications. Despite this, most VLT benchmarks still
depend on succinct, human-annotated text descriptions for each video. These
descriptions often fall short in capturing the nuances of video content
dynamics and lack stylistic variety in language, constrained by their uniform
level of detail and a fixed annotation frequency. As a result, algorithms tend
to default to a "memorize the answer" strategy, diverging from the core
objective of achieving a deeper understanding of video content. Fortunately,
the emergence of large language models (LLMs) has enabled the generation of
diverse text. This work utilizes LLMs to generate varied semantic annotations
(in terms of text lengths and granularities) for representative SOT benchmarks,
thereby establishing a novel multi-modal benchmark. Specifically, we (1)
propose a new visual language tracking benchmark with diverse texts, named
DTVLT, based on five prominent VLT and SOT benchmarks, including three
sub-tasks: short-term tracking, long-term tracking, and global instance
tracking. (2) We offer four granularity texts in our benchmark, considering the
extent and density of semantic information. We expect this multi-granular
generation strategy to foster a favorable environment for VLT and video
understanding research. (3) We conduct comprehensive experimental analyses on
DTVLT, evaluating the impact of diverse text on tracking performance and hope
the identified performance bottlenecks of existing algorithms can support
further research in VLT and video understanding. The proposed benchmark,
experimental results and toolkit will be released gradually on
http://videocube.aitestunion.com/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Generalization Complexity for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown exceptional capabilities in
understanding complex queries and performing sophisticated tasks, their
generalization abilities are often deeply entangled with memorization,
necessitating more precise evaluation. To address this challenge, we introduce
Scylla, a dynamic evaluation framework that quantitatively measures the
generalization abilities of LLMs. Scylla disentangles generalization from
memorization via assessing model performance on both in-distribution (ID) and
out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.
Through extensive experiments, we uncover a non-monotonic relationship between
task complexity and the performance gap between ID and OOD data, which we term
the generalization valley. Specifically, this phenomenon reveals a critical
threshold - referred to as critical complexity - where reliance on
non-generalizable behavior peaks, indicating the upper bound of LLMs'
generalization capabilities. As model size increases, the critical complexity
shifts toward higher levels of task complexity, suggesting that larger models
can handle more complex reasoning tasks before over-relying on memorization.
Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs
including both open-sourced models such as LLaMA and Qwen families, and
close-sourced models like Claude and GPT, providing a more robust evaluation
and establishing a clearer understanding of LLMs' generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leopard: A Vision Language Model For Text-Rich Multi-<span class="highlight-title">Image</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Meng Jiang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich images, where text serves as the central visual element guiding the
overall understanding, are prevalent in real-world applications, such as
presentation slides, scanned documents, and webpage snapshots. Tasks involving
multiple text-rich images are especially challenging, as they require not only
understanding the content of individual images but reasoning about
inter-relationships and logical flows across multiple visual inputs. Despite
the importance of these scenarios, current multimodal large language models
(MLLMs) struggle to handle such tasks due to two key challenges: (1) the
scarcity of high-quality instruction tuning datasets for text-rich multi-image
scenarios, and (2) the difficulty in balancing image resolution with visual
feature sequence length. To address these challenges, we propose Leopard, a
MLLM designed specifically for handling vision-language tasks involving
multiple text-rich images. First, we curated about one million high-quality
multimodal instruction-tuning data, tailored to text-rich, multi-image
scenarios. Second, we developed an adaptive high-resolution multi-image
encoding module to dynamically optimize the allocation of visual sequence
length based on the original aspect ratios and resolutions of the input images.
Experiments across a wide range of benchmarks demonstrate our model's superior
capabilities in text-rich, multi-image evaluations and competitive performance
in general domain evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/Jill0001/Leopard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrative Decoding: Improve Factuality via Implicit Self-consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Xiao Liang, Yeyun Gong, Wen Xiao, Song Wang, Yuji Zhang, Wenjun Hou, Kaishuai Xu, Wenge Liu, Wenjie Li, Jian Jiao, Qi Chen, Peng Cheng, Wayne Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-consistency-based approaches, which involve repeatedly sampling multiple
outputs and selecting the most consistent one as the final response, prove to
be remarkably effective in improving the factual accuracy of large language
models. Nonetheless, existing methods usually have strict constraints on the
task format, largely limiting their applicability. In this paper, we present
Integrative Decoding (ID), to unlock the potential of self-consistency in
open-ended generation tasks. ID operates by constructing a set of inputs, each
prepended with a previously sampled response, and then processes them
concurrently, with the next token being selected by aggregating of all their
corresponding predictions at each decoding step. In essence, this simple
approach implicitly incorporates self-consistency in the decoding objective.
Extensive evaluation shows that ID consistently enhances factuality over a wide
range of language models, with substantial improvements on the TruthfulQA
(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance
gains amplify progressively as the number of sampled responses increases,
indicating the potential of ID to scale up with repeated sampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extending Context Window of <span class="highlight-title">Large Language Model</span>s from a Distributional
  Perspective <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingsheng Wu, Yuxuan Gu, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling the rotary position embedding (RoPE) has become a common method for
extending the context window of RoPE-based large language models (LLMs).
However, existing scaling methods often rely on empirical approaches and lack a
profound understanding of the internal distribution within RoPE, resulting in
suboptimal performance in extending the context window length. In this paper,
we propose to optimize the context window extending task from the view of
rotary angle distribution. Specifically, we first estimate the distribution of
the rotary angles within the model and analyze the extent to which length
extension perturbs this distribution. Then, we present a novel extension
strategy that minimizes the disturbance between rotary angle distributions to
maintain consistency with the pre-training phase, enhancing the model's
capability to generalize to longer sequences. Experimental results compared to
the strong baseline methods demonstrate that our approach reduces by up to 72%
of the distributional disturbance when extending LLaMA2's context window to 8k,
and reduces by up to 32% when extending to 16k. On the LongBench-E benchmark,
our method achieves an average improvement of up to 4.33% over existing
state-of-the-art methods. Furthermore, Our method maintains the model's
performance on the Hugging Face Open LLM benchmark after context window
extension, with only an average performance fluctuation ranging from -0.12 to
+0.22.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, Accepted to EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Which questions should I answer? Salience Prediction of Inquisitive
  Questions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Wu, Ritika Mangla, Alexandros G. Dimakis, Greg Durrett, Junyi Jessy Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inquisitive questions -- open-ended, curiosity-driven questions people ask as
they read -- are an integral part of discourse processing (Kehler and Rohde,
2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has
taken advantage of question generation capabilities of LLMs to enhance a wide
range of applications. But the space of inquisitive questions is vast: many
questions can be evoked from a given context. So which of those should be
prioritized to find answers? Linguistic theories, unfortunately, have not yet
provided an answer to this question. This paper presents QSALIENCE, a salience
predictor of inquisitive questions. QSALIENCE is instruction-tuned over our
dataset of linguist-annotated salience scores of 1,766 (context, question)
pairs. A question scores high on salience if answering it would greatly enhance
the understanding of the text (Van Rooy, 2003). We show that highly salient
questions are empirically more likely to be answered in the same article,
bridging potential questions (Onea, 2016) with Questions Under Discussion
(Roberts, 2012). We further validate our findings by showing that answering
salient questions is an indicator of summarization quality in news.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready for EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LML-DAP: Language Model Learning a <span class="highlight-title">Dataset</span> for Data-Augmented Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification tasks are typically handled using Machine Learning (ML)
models, which lack a balance between accuracy and interpretability. This paper
introduces a new approach to using Large Language Models (LLMs) for
classification tasks in an explainable way. Unlike ML models that rely heavily
on data cleaning and feature engineering, this method streamlines the process
using LLMs. This paper proposes a new concept called "Language Model Learning
(LML)" powered by a new method called "Data-Augmented Prediction (DAP)". The
classification is performed by LLMs using a method similar to humans manually
exploring and understanding the data and deciding classifications using data as
a reference. In the LML process, a dataset is summarized and evaluated to
determine the features that lead to the classification of each label the most.
In the process of DAP, the system uses the data summary and a row of the
testing dataset to automatically generate a query, which is used to retrieve
relevant rows from the dataset. A classification is generated by the LLM using
data summary and relevant rows, ensuring satisfactory accuracy even with
complex data using context-aware decision-making. LML and DAP unlock the
possibilities of new applications. The proposed method uses the words "Act as
an Explainable Machine Learning Model" in the prompt to enhance the
interpretability of the predictions by allowing users to review the logic
behind each prediction. In some test cases, the system scored an accuracy above
90%, proving the effectiveness of the system and its potential to outperform
conventional ML models in various scenarios. The code is available at
https://github.com/Pro-GenAI/LML-DAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated title, abstract, and images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Falling Short: The Curse of Tokenization <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Yewei Fang, Qiwei Peng, Xuhong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models typically tokenize raw text into sequences of subword
identifiers from a predefined vocabulary, a process inherently sensitive to
typographical errors, length variations, and largely oblivious to the internal
structure of tokens--issues we term the curse of tokenization. In this study,
we delve into these drawbacks and demonstrate that large language models (LLMs)
remain susceptible to these problems. This study systematically investigates
these challenges and their impact on LLMs through three critical research
questions: (1) complex problem solving, (2) token structure probing, and (3)
resilience to typographical variation. Our findings reveal that scaling model
parameters can mitigate the issue of tokenization; however, LLMs still suffer
from biases induced by typos and other text format variations. Our experiments
show that subword regularization such as BPE-dropout can mitigate this issue.
We release our evaluation code and data at https://github.com/FloatAI/TKEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Training Data Influence of <span class="highlight-title">GPT</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07840v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07840v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Qingyi Liu, Shuohuan Wang, Yu Sun, Qiwei Peng, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst the rapid advancements in generative language models, the
investigation of how training data shapes the performance of GPT models is
still emerging. This paper presents GPTfluence, a novel approach that leverages
a featurized simulation to assess the impact of training examples on the
training dynamics of GPT models. Our approach not only traces the influence of
individual training instances on performance trajectories, such as loss and
other key metrics, on targeted test points but also enables a comprehensive
comparison with existing methods across various training scenarios in GPT
models, ranging from 14 million to 2.8 billion parameters, across a range of
downstream tasks. Contrary to earlier methods that struggle with generalization
to new data, GPTfluence introduces a parameterized simulation of training
dynamics, demonstrating robust generalization capabilities to unseen training
data. This adaptability is evident across both fine-tuning and
instruction-tuning scenarios, spanning tasks in natural language understanding
and generation. We make our code and data publicly available at
https://github.com/ernie-research/gptfluence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pharmacy<span class="highlight-title">GPT</span>: The AI Pharmacist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengliang Liu, Zihao Wu, Mengxuan Hu, Bokai Zhao, Lin Zhao, Tianyi Zhang, Haixing Dai, Xianyan Chen, Ye Shen, Sheng Li, Quanzheng Li, Xiang Li, Brian Murray, Tianming Liu, Andrea Sikora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce PharmacyGPT, a novel framework to assess the
capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in
emulating the role of clinical pharmacists. Our methodology encompasses the
utilization of LLMs to generate comprehensible patient clusters, formulate
medication plans, and forecast patient outcomes. We conduct our investigation
using real data acquired from the intensive care unit (ICU) at the University
of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable
insights into the potential applications and limitations of LLMs in the field
of clinical pharmacy, with implications for both patient care and the
development of future AI-driven healthcare solutions. By evaluating the
performance of PharmacyGPT, we aim to contribute to the ongoing discourse
surrounding the integration of artificial intelligence in healthcare settings,
ultimately promoting the responsible and efficacious use of such technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoregressive Pre-Training on Pixels and Texts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10710v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10710v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Qingyi Liu, Jingwu Xiao, Shuohuan Wang, Yu Sun, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of visual and textual information represents a promising
direction in the advancement of language models. In this paper, we explore the
dual modality of language--both visual and textual--within an autoregressive
framework, pre-trained on both document images and texts. Our method employs a
multimodal training strategy, utilizing visual data through next patch
prediction with a regression head and/or textual data through next token
prediction with a classification head. We focus on understanding the
interaction between these two modalities and their combined impact on model
performance. Our extensive evaluation across a wide range of benchmarks shows
that incorporating both visual and textual data significantly improves the
performance of pixel-based language models. Remarkably, we find that a
unidirectional pixel-based model trained solely on visual data can achieve
comparable results to state-of-the-art bidirectional models on several language
understanding tasks. This work uncovers the untapped potential of integrating
visual and textual modalities for more effective language modeling. We release
our code, data, and model checkpoints at
\url{https://github.com/ernie-research/pixelgpt}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Turning English-centric <span class="highlight-title">LLM</span>s Into Polyglots: How Much Multilinguality Is
  Needed? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tannon Kew, Florian Schottmann, Rico Sennrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast majority of today's large language models (LLMs) are
English-centric, having been pretrained predominantly on English text. Yet, in
order to meet user expectations, models need to be able to respond
appropriately in multiple languages once deployed in downstream applications.
This requires strong cross-lingual transfer abilities. In this work, we
investigate the minimal amount of multilinguality required during finetuning to
elicit cross-lingual generalisation in English-centric LLMs. In experiments
across four LLMs, we find that multilingual instruction tuning with as few as
two to three languages is both necessary and sufficient to elicit effective
cross-lingual generalisation, with the limiting factor being the degree to
which a target language is seen during pretraining. Evaluations on five
different tasks further reveal that multilingual instruction tuning is most
beneficial for generative tasks that assume input/output language agreement,
such as in chat settings, while being of less importance for highly structured
classification-style tasks. Our code and data is available at
https://github.com/ZurichNLP/multilingual-instruction-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookback Lens: Detecting and Mitigating Contextual Hallucinations in
  <span class="highlight-title">Large Language Model</span>s Using Only Attention Maps <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When asked to summarize articles or answer questions given a passage, large
language models (LLMs) can hallucinate details and respond with unsubstantiated
answers that are inaccurate with respect to the input context. This paper
describes a simple approach for detecting such contextual hallucinations. We
hypothesize that contextual hallucinations are related to the extent to which
an LLM attends to information in the provided context versus its own
generations. Based on this intuition, we propose a simple hallucination
detection model whose input features are given by the ratio of attention
weights on the context versus newly generated tokens (for each attention head).
We find that a linear classifier based on these lookback ratio features is as
effective as a richer detector that utilizes the entire hidden states of an LLM
or a text-based entailment model. The lookback ratio-based detector -- Lookback
Lens -- is found to transfer across tasks and even models, allowing a detector
that is trained on a 7B model to be applied (without retraining) to a larger
13B model. We further apply this detector to mitigate contextual
hallucinations, and find that a simple classifier-guided decoding approach is
able to reduce the amount of hallucination, for example by 9.6% in the XSum
summarization task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference long paper. The source code is available
  at https://github.com/voidism/Lookback-Lens</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Mystery of In-Context Learning: A Comprehensive <span class="highlight-title">Survey</span> on
  Interpretation and Analysis <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00237v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00237v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding in-context learning (ICL) capability that enables large
language models (LLMs) to excel in proficiency through demonstration examples
is of utmost importance. This importance stems not only from the better
utilization of this capability across various tasks, but also from the
proactive identification and mitigation of potential risks, including concerns
regarding truthfulness, bias, and toxicity, that may arise alongside the
capability. In this paper, we present a thorough survey on the interpretation
and analysis of in-context learning. First, we provide a concise introduction
to the background and definition of in-context learning. Then, we give an
overview of advancements from two perspectives: 1) a theoretical perspective,
emphasizing studies on mechanistic interpretability and delving into the
mathematical foundations behind ICL; and 2) an empirical perspective,
concerning studies that empirically analyze factors associated with ICL. We
conclude by highlighting the challenges encountered and suggesting potential
avenues for future research. We believe that our work establishes the basis for
further exploration into the interpretation of in-context learning.
Additionally, we have created a repository containing the resources referenced
in our survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of EMNLP 2024. Resources are
  available at https://github.com/zyxnlp/ICL-Interpretation-Analysis-Resources</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Automated Code Vulnerability Repair using <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David de-Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, Jose-Javier Martinez-Herraiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the complex challenge of automated repair of code
vulnerabilities, vital for enhancing digital security in an increasingly
technology-driven world. The study introduces a novel and efficient format for
the representation of code modification, using advanced Large Language Models
(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets
featuring C code vulnerabilities, significantly improve the accuracy and
adaptability of automated code repair techniques. A key finding is the enhanced
repair accuracy of these models when compared to previous methods such as
VulRepair, which underscores their practical utility and efficiency. The
research also offers a critical assessment of current evaluation metrics, such
as perfect predictions, and their limitations in reflecting the true
capabilities of automated repair models in real-world scenarios. Following
this, it underscores the importance of using test datasets devoid of train
samples, emphasizing the need for dataset integrity to enhance the
effectiveness of LLMs in code repair tasks. The significance of this work is
its contribution to digital security, setting new standards for automated code
vulnerability repair and paving the way for future advancements in the fields
of cybersecurity and artificial intelligence. The study does not only highlight
the potential of LLMs in enhancing code security but also fosters further
exploration and research in these crucial areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Limited Generalization Capability of the Implicit Reward Model
  Induced by Direct Preference Optimization <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is an effective approach
for aligning language models to human preferences. Central to RLHF is learning
a reward function for scoring human preferences. Two main approaches for
learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in
RLHF, and 2) using an implicit reward learned from preference data through
methods such as Direct Preference Optimization (DPO). Prior work has shown that
the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in
the limit. DPORM's effectiveness directly implies the optimality of the learned
policy, and also has practical implication for LLM alignment methods including
iterative DPO. However, it is unclear how well DPORM empirically matches the
performance of EXRM. This work studies the accuracy at distinguishing preferred
and rejected answers for both DPORM and EXRM. Our findings indicate that even
though DPORM fits the training dataset comparably, it generalizes less
effectively than EXRM, especially when the validation datasets contain
distribution shifts. Across five out-of-distribution settings, DPORM has a mean
drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that
DPORM has limited generalization ability and substantiates the integration of
an explicit reward model in iterative DPO approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking <span class="highlight-title">LLM</span>s with Arabic Transliteration and Arabizi <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study identifies the potential vulnerabilities of Large Language Models
(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and
its various forms. While most research has concentrated on English-based prompt
manipulation, our investigation broadens the scope to investigate the Arabic
language. We initially tested the AdvBench benchmark in Standardized Arabic,
finding that even with prompt manipulation techniques like prefix injection, it
was insufficient to provoke LLMs into generating unsafe content. However, when
using Arabic transliteration and chatspeak (or arabizi), we found that unsafe
content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3
Sonnet. Our findings suggest that using Arabic and its various forms could
expose information that might remain hidden, potentially increasing the risk of
jailbreak attacks. We hypothesize that this exposure could be due to the
model's learned connection to specific words, highlighting the need for more
comprehensive safety training across all language forms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for
  Children's Story-Based Learning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaju Chen, Yuxuan Lu, Shao Zhang, Bingsheng Yao, Yuanzhe Dong, Ying Xu, Yunyao Li, Qianwen Wang, Dakuo Wang, Yuling Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive story reading is a common parent-child activity, where parents
expect to teach both language skills and real-world knowledge beyond the story.
While increasing storytelling and reading systems have been developed for this
activity, they often fail to infuse real-world knowledge into the conversation.
This limitation can be attributed to the existing question-answering (QA)
datasets used for children's education, upon which the systems are built,
failing to capture the nuances of how education experts think when conducting
interactive story reading activities. To bridge this gap, we design an
annotation framework, empowered by existing knowledge graph to capture experts'
annotations and thinking process, and leverage this framework to construct
StorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with
real-world knowledge. We conduct automated and human expert evaluations across
various QA pair generation settings to demonstrate that our StorySparkQA can
effectively support models in generating QA pairs that target real-world
knowledge beyond story content. StorySparkQA is available at
https://huggingface.co/datasets/NEU-HAI/StorySparkQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM
  Reliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Nouha Dziri, Dan Jurafsky, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to communicate uncertainty, risk, and limitation is crucial for
the safety of large language models. However, current evaluations of these
abilities rely on simple calibration, asking whether the language generated by
the model matches appropriate probabilities. Instead, evaluation of this aspect
of LLM communication should focus on the behaviors of their human
interlocutors: how much do they rely on what the LLM says? Here we introduce an
interaction-centered evaluation framework called Rel-A.I. (pronounced "rely"})
that measures whether humans rely on LLM generations. We use this framework to
study how reliance is affected by contextual features of the interaction (e.g,
the knowledge domain that is being discussed), or the use of greetings
communicating warmth or competence (e.g., "I'm happy to help!"). We find that
contextual characteristics significantly affect human reliance behavior. For
example, people rely 10% more on LMs when responding to questions involving
calculations and rely 30% more on LMs that are perceived as more competent. Our
results show that calibration and language quality alone are insufficient in
evaluating the risks of human-LM interactions, and illustrate the need to
consider features of the interactional context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Leakage of Code <span class="highlight-title">Generation</span> Evaluation <span class="highlight-title">Dataset</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, Matthias Gallé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider contamination by code generation test sets, in
particular in their use in modern large language models. We discuss three
possible sources of such contamination and show findings supporting each of
them: (i) direct data leakage, (ii) indirect data leakage through the use of
synthetic data and (iii) overfitting to evaluation sets during model selection.
To address this, we release Less Basic Python Problems (LBPP): an
uncontaminated new benchmark of 161 prompts with their associated Python
solutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings. 5 main pages, 9 in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Refusal Training in <span class="highlight-title">LLM</span>s Generalize to the Past Tense? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Andriushchenko, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Refusal training is widely used to prevent LLMs from generating harmful,
undesirable, or illegal outputs. We reveal a curious generalization gap in the
current refusal training approaches: simply reformulating a harmful request in
the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make
a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art
LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,
GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,
o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For
example, the success rate of this simple attack on GPT-4o increases from 1%
using direct requests to 88% using 20 past tense reformulation attempts on
harmful requests from JailbreakBench with GPT-4 as a jailbreak judge.
Interestingly, we also find that reformulations in the future tense are less
effective, suggesting that refusal guardrails tend to consider past historical
questions more benign than hypothetical future questions. Moreover, our
experiments on fine-tuning GPT-3.5 Turbo show that defending against past
reformulations is feasible when past tense examples are explicitly included in
the fine-tuning data. Overall, our findings highlight that the widely used
alignment techniques -- such as SFT, RLHF, and adversarial training -- employed
to align the studied models can be brittle and do not always generalize as
intended. We provide code and jailbreak artifacts at
https://github.com/tml-epfl/llm-past-tense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update in v3: o1-mini and o1-preview results (on top of GPT-4o and
  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at
  https://github.com/tml-epfl/llm-past-tense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Immunization against harmful fine-tuning attacks <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are often trained with safety guards intended to
prevent harmful text generation. However, such safety training can be removed
by fine-tuning the LLM on harmful datasets. While this emerging threat (harmful
fine-tuning attacks) has been characterized by previous work, there is little
understanding of how we should proceed in constructing and validating defenses
against these attacks especially in the case where defenders would not have
control of the fine-tuning process. We introduce a formal framework based on
the training budget of an attacker which we call "Immunization" conditions.
Using a formal characterisation of the harmful fine-tuning problem, we provide
a thorough description of what a successful defense must comprise of and
establish a set of guidelines on how rigorous defense research that gives us
confidence should proceed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundations of <span class="highlight-title">Large Language Model</span> Compression -- Part 1: Weight
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean I. Young
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, compression of large language models (LLMs) has emerged as
an important problem to enable language model deployment on
resource-constrained devices, reduce computational costs, and mitigate the
environmental footprint of large-scale AI infrastructure. In this paper, we lay
down the foundation for LLM quantization from a convex optimization perspective
and propose a quantization technique that builds on this foundation for optimum
quantization outcomes. Our quantization framework, CVXQ, scales to models
containing hundreds of billions of weight parameters and provides users with
the flexibility to compress models to any specified model size, post-training.
A reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. 17 pages, 4 figures, 5 appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EIA: Environmental Injection Attack on Generalist Web <span class="highlight-title">Agent</span>s for Privacy
  Leakage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist web agents have demonstrated remarkable potential in autonomously
completing a wide range of tasks on real websites, significantly boosting human
productivity. However, web tasks, such as booking flights, usually involve
users' PII, which may be exposed to potential privacy risks if web agents
accidentally interact with compromised websites, a scenario that remains
largely unexplored in the literature. In this work, we narrow this gap by
conducting the first study on the privacy risks of generalist web agents in
adversarial environments. First, we present a realistic threat model for
attacks on the website, where we consider two adversarial targets: stealing
users' specific PII or the entire user request. Then, we propose a novel attack
method, termed Environmental Injection Attack (EIA). EIA injects malicious
content designed to adapt well to environments where the agents operate and our
work instantiates EIA specifically for privacy scenarios in web environments.
We collect 177 action steps that involve diverse PII categories on realistic
websites from the Mind2Web, and conduct experiments using one of the most
capable generalist web agent frameworks to date. The results demonstrate that
EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user
request. Additionally, by accessing the stealthiness and experimenting with a
defensive system prompt, we indicate that EIA is hard to detect and mitigate.
Notably, attacks that are not well adapted for a webpage can be detected via
human inspection, leading to our discussion about the trade-off between
security and autonomy. However, extra attackers' efforts can make EIA
seamlessly adapted, rendering such supervision ineffective. Thus, we further
discuss the defenses at the pre- and post-deployment stages of the websites
without relying on human supervision and call for more advanced defense
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human
  Feedback and Heuristic-based Sampling <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08702v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08702v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt optimization aims to find the best prompt to a large language model
(LLM) for a given task. LLMs have been successfully used to help find and
improve prompt candidates for single-step tasks. However, realistic tasks for
agents are multi-step and introduce new challenges: (1) Prompt content is
likely to be more extensive and complex, making it more difficult for LLMs to
analyze errors, (2) the impact of an individual step is difficult to evaluate,
and (3) different people may have varied preferences about task execution.
While humans struggle to optimize prompts, they are good at providing feedback
about LLM outputs; we therefore introduce a new LLM-driven discrete prompt
optimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that
incorporates human-designed feedback rules to automatically offer direct
suggestions for improvement. We also use an extra learned heuristic model that
predicts prompt performance to efficiently sample from prompt candidates. This
approach significantly outperforms both human-engineered prompts and several
other prompt optimization methods across 11 representative multi-step tasks (an
average 10.6\%-29.3\% improvement to current best methods on five LLMs
respectively). We believe our work can serve as a benchmark for automatic
prompt optimization for LLM-driven multi-step tasks. Datasets and Codes are
available at https://github.com/yongchao98/PROMST. Project Page is available at
https://yongchao98.github.io/MIT-REALM-PROMST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages, 14 figures, Published in EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of <span class="highlight-title">LLM</span>s for
  Legal Domain Adaptation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitodru Niyogi, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Paramanu-Ayn, a collection of legal language models
trained exclusively on Indian legal case documents. This 97-million-parameter
Auto-Regressive (AR) decoder-only model was pretrained from scratch with a
context size of 8192 on a single GPU for just 185 hours, achieving an efficient
MFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We
evaluated our model using perplexity and zero-shot tasks: case judgment
prediction with explanation and abstractive case summarization. Paramanu-Ayn
outperformed Llama-2 7B and Gemini-Pro in case judgment prediction with
explanation task on test accuracy by nearly 2 percentage points, despite being
72 times smaller. In zero-shot abstractive summarization, it surpassed
decoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10
percentage points in BLEU and METEOR metrics, and by nearly 4 percentage points
in BERTScore. Further evaluations on zero-shot commonsense and mathematical
benchmarks showed that Paramanu-Ayn excelled despite being trained exclusively
on legal documents, outperforming Llama-1, Llama-2, and Falcon on
AGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our
model on 10,763 diverse legal tasks, including legal clause generation, legal
drafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above
8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by
GPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge
and generalize to draft legal contracts and legal clauses with limited
instruction-tuning. Hence, we conclude that for a strong domain-specialized
generative language model (such as legal), domain specialized pretraining from
scratch is more cost effective, environmentally friendly, and remains
competitive with larger models or even better than adapting LLMs for legal
domain tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seemingly Plausible Distractors in Multi-Hop <span class="highlight-title">Reasoning</span>: Are Large
  Language Models Attentive Readers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeladri Bhuiya, Viktor Schlegel, Stefan Winkler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art Large Language Models (LLMs) are accredited with an
increasing number of different capabilities, ranging from reading
comprehension, over advanced mathematical and reasoning skills to possessing
scientific knowledge. In this paper we focus on their multi-hop reasoning
capability: the ability to identify and integrate information from multiple
textual sources.
  Given the concerns with the presence of simplifying cues in existing
multi-hop reasoning benchmarks, which allow models to circumvent the reasoning
requirement, we set out to investigate, whether LLMs are prone to exploiting
such simplifying cues. We find evidence that they indeed circumvent the
requirement to perform multi-hop reasoning, but they do so in more subtle ways
than what was reported about their fine-tuned pre-trained language model (PLM)
predecessors. Motivated by this finding, we propose a challenging multi-hop
reasoning benchmark, by generating seemingly plausible multi-hop reasoning
chains, which ultimately lead to incorrect answers. We evaluate multiple open
and proprietary state-of-the-art LLMs, and find that their performance to
perform multi-hop reasoning is affected, as indicated by up to 45% relative
decrease in F1 score when presented with such seemingly plausible alternatives.
We conduct a deeper analysis and find evidence that while LLMs tend to ignore
misleading lexical cues, misleading reasoning paths indeed present a
significant challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Qwen2-VL: Enhancing <span class="highlight-title">Vision-Language Model</span>'s Perception of the World at
  Any Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL
models that redefines the conventional predetermined-resolution approach in
visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,
which enables the model to dynamically process images of varying resolutions
into different numbers of visual tokens. This approach allows the model to
generate more efficient and accurate visual representations, closely aligning
with human perceptual processes. The model also integrates Multimodal Rotary
Position Embedding (M-RoPE), facilitating the effective fusion of positional
information across text, images, and videos. We employ a unified paradigm for
processing both images and videos, enhancing the model's visual perception
capabilities. To explore the potential of large multimodal models, Qwen2-VL
investigates the scaling laws for large vision-language models (LVLMs). By
scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the
amount of training data, the Qwen2-VL Series achieves highly competitive
performance. Notably, the Qwen2-VL-72B model achieves results comparable to
leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal
benchmarks, outperforming other generalist models. Code is available at
https://github.com/QwenLM/Qwen2-VL .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin
  note: text overlap with arXiv:2408.15262 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Matrix Multiplications for Lookup Table-Quantized <span class="highlight-title">LLM</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large language models (LLMs) is often constrained by memory
bandwidth, where the primary bottleneck is the cost of transferring model
parameters from the GPU's global memory to its registers. When coupled with
custom kernels that fuse the dequantization and matmul operations, weight-only
quantization can thus enable faster inference by reducing the amount of memory
movement. However, developing high-performance kernels for weight-quantized
LLMs presents substantial challenges, especially when the weights are
compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,
lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup
table engine for LUT-quantized LLMs, which uses offline restructuring of the
quantized weight matrix to minimize bit manipulations associated with
unpacking, and vectorization and duplication of the lookup table to mitigate
shared memory bandwidth constraints. At batch sizes < 32 and quantization group
size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster
than existing GEMM kernels. As an application of FLUTE, we explore a simple
extension to lookup table-based NormalFloat quantization and apply it to
quantize LLaMA3 to various configurations, obtaining competitive quantization
performance against strong baselines while obtaining an end-to-end throughput
increase of 1.5 to 2 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Llamipa: An Incremental Discourse Parser <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18256v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18256v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Thompson, Akshay Chaturvedi, Julie Hunter, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides the first discourse parsing experiments with a large
language model(LLM) finetuned on corpora annotated in the style of SDRT
(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,
2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),
that leverages discourse context, leading to substantial performance gains over
approaches that use encoder-only models to provide local, context-sensitive
representations of discourse units. Furthermore, it can process discourse data
incrementally, which is essential for the eventual use of discourse information
in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nebula: A discourse aware Minecraft Builder <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18164v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18164v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Chaturvedi, Kate Thompson, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When engaging in collaborative tasks, humans efficiently exploit the semantic
structure of a conversation to optimize verbal and nonverbal interactions. But
in recent "language to code" or "language to action" models, this information
is lacking. We show how incorporating the prior discourse and nonlinguistic
context of a conversation situated in a nonlinguistic environment can improve
the "language to action" component of such interactions. We finetune an LLM to
predict actions based on prior context; our model, Nebula, doubles the
net-action F1 score over the baseline on this task of Jayannavar et al.(2020).
We also investigate our model's ability to construct shapes and understand
location descriptions using a synthetic dataset
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongForm: Effective Instruction Tuning with Reverse Instructions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullatif Köksal, Timo Schick, Anna Korhonen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning enables language models to more effectively generalize and
better follow user intent. However, obtaining instruction data is costly and
challenging. Prior work employs methods such as expensive human annotation,
crowd-sourced datasets with alignment issues, and generating noisy examples via
LLMs. We introduce the LongForm-C dataset, which is created by reverse
instructions. We generate instructions via LLMs for human-written corpus
examples using reverse instructions. First we select a diverse set of
human-written documents from corpora such as C4 and Wikipedia; then we generate
instructions for these documents via LLMs. This approach provides a cheaper and
cleaner instruction-tuning dataset with natural output and one suitable for
long text generation. Our models outperform 10x larger language models without
instruction tuning on tasks such as story/recipe generation and long-form
question answering. Moreover, LongForm models outperform prior
instruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and
improve language understanding capabilities further. We publicly release our
data and models: https://github.com/akoksal/LongForm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings. This version extends the training with recent
  LLMs, evaluation with new metrics, and NLU tasks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TurkishMMLU: Measuring Massive Multitask Language Understanding in
  Turkish <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arda Yüksel, Abdullatif Köksal, Lütfi Kerem Şenel, Anna Korhonen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple choice question answering tasks evaluate the reasoning,
comprehension, and mathematical abilities of Large Language Models (LLMs).
While existing benchmarks employ automatic translation for multilingual
evaluation, this approach is error-prone and potentially introduces culturally
biased questions, especially in social sciences. We introduce the first
multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'
understanding of the Turkish language. TurkishMMLU includes over 10,000
questions, covering 9 different subjects from Turkish high-school education
curricula. These questions are written by curriculum experts, suitable for the
high-school curricula in Turkey, covering subjects ranging from natural
sciences and math questions to more culturally representative topics such as
Turkish Literature and the history of the Turkish Republic. We evaluate over 20
LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),
closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)
models. We provide an extensive evaluation, including zero-shot and few-shot
evaluation of LLMs, chain-of-thought reasoning, and question difficulty
analysis along with model performance. We provide an in-depth analysis of the
Turkish capabilities and limitations of current LLMs to provide insights for
future LLMs for the Turkish language. We publicly release our code for the
dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 - Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generate-on-Graph: Treat <span class="highlight-title">LLM</span> as both <span class="highlight-title">Agent</span> and KG in Incomplete
  Knowledge Graph Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Guang Liu, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the issues of insufficient knowledge and hallucination in Large
Language Models (LLMs), numerous studies have explored integrating LLMs with
Knowledge Graphs (KGs). However, these methods are typically evaluated on
conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where
all factual triples required for each question are entirely covered by the
given KG. In such cases, LLMs primarily act as an agent to find answer entities
within the KG, rather than effectively integrating the internal knowledge of
LLMs and external knowledge sources such as KGs. In fact, KGs are often
incomplete to cover all the knowledge required to answer questions. To simulate
these real-world scenarios and evaluate the ability of LLMs to integrate
internal and external knowledge, we propose leveraging LLMs for QA under
Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the
factual triples for each question, and construct corresponding datasets. To
handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),
which can generate new factual triples while exploring KGs. Specifically, GoG
performs reasoning through a Thinking-Searching-Generating framework, which
treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets
demonstrate that our GoG outperforms all previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-Instruct: Adapting Instruction Generators for Complex <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyun Cui, Qianle Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instructions augmentation is a crucial step for unleashing the full potential
of large language models (LLMs) in downstream tasks. Existing Self-Instruct
methods primarily simulate new instructions from a few initial instructions
with in-context learning. However, our study identifies a critical flaw in this
approach: even with GPT4o, Self-Instruct cannot generate complex instructions
of length $\ge 100$, which is necessary in complex tasks such as code
completion.
  To address this issue, our key insight is that fine-tuning open source LLMs
with only ten examples can produce complex instructions that maintain
distributional consistency for complex reasoning tasks. We introduce
Ada-Instruct, an adaptive instruction generator developed through fine-tuning.
We empirically validated Ada-Instruct's efficacy across different applications.
The results highlight Ada-Instruct's capacity to generate long, intricate, and
distributionally consistent instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context <span class="highlight-title">Editing</span>: Learning Knowledge from Self-Induced Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Qi, Bangcheng Yang, Kailin Jiang, Xiaobo Wang, Jiaqi Li, Yifan Zhong, Yaodong Yang, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In scenarios where language models must incorporate new information
efficiently without extensive retraining, traditional fine-tuning methods are
prone to overfitting, degraded generalization, and unnatural language
generation. To address these limitations, we introduce Consistent In-Context
Editing (ICE), a novel approach leveraging the model's in-context learning
capability to optimize toward a contextual distribution rather than a one-hot
target. ICE introduces a simple yet effective optimization framework for the
model to internalize new knowledge by aligning its output distributions with
and without additional context. This method enhances the robustness and
effectiveness of gradient-based tuning methods, preventing overfitting and
preserving the model's integrity. We analyze ICE across four critical aspects
of knowledge editing: accuracy, locality, generalization, and linguistic
quality, demonstrating its advantages. Experimental results confirm the
effectiveness of ICE and demonstrate its potential for continual editing,
ensuring that the integrity of the model is preserved while updating
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fighting Randomness with Randomness: Mitigating Optimisation Instability
  of Fine-Tuning using Delayed Ensemble and Noisy Interpolation <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Branislav Pecher, Jan Cegin, Robert Belanec, Jakub Simko, Ivan Srba, Maria Bielikova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While fine-tuning of pre-trained language models generally helps to overcome
the lack of labelled training samples, it also displays model performance
instability. This instability mainly originates from randomness in
initialisation or data shuffling. To address this, researchers either modify
the training process or augment the available samples, which typically results
in increased computational costs. We propose a new mitigation strategy, called
Delayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths
of ensembling, noise regularisation and model interpolation, while retaining
computational efficiency. We compare DENI with 9 representative mitigation
strategies across 3 models, 4 tuning strategies and 7 text classification
datasets. We show that: 1) DENI outperforms the best performing mitigation
strategy (Ensemble), while using only a fraction of its cost; 2) the mitigation
strategies are beneficial for parameter-efficient fine-tuning (PEFT) methods,
outperforming full fine-tuning in specific cases; and 3) combining DENI with
data augmentation often leads to even more effective instability mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Findings of the EMNLP'24 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Sensitivity of Learning with Limited Labelled Data to the Effects of
  Randomness: Impact of Interactions and Systematic Choices <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Branislav Pecher, Ivan Srba, Maria Bielikova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While learning with limited labelled data can improve performance when the
labels are lacking, it is also sensitive to the effects of uncontrolled
randomness introduced by so-called randomness factors (e.g., varying order of
data). We propose a method to systematically investigate the effects of
randomness factors while taking the interactions between them into
consideration. To measure the true effects of an individual randomness factor,
our method mitigates the effects of other factors and observes how the
performance varies across multiple runs. Applying our method to multiple
randomness factors across in-context learning and fine-tuning approaches on 7
representative text classification tasks and meta-learning on 3 tasks, we show
that: 1) disregarding interactions between randomness factors in existing works
caused inconsistent findings due to incorrect attribution of the effects of
randomness factors, such as disproving the consistent sensitivity of in-context
learning to sample order even with random sample selection; and 2) besides
mutual interactions, the effects of randomness factors, especially sample
order, are also dependent on more systematic choices unexplored in existing
works, such as number of classes, samples per class or choice of prompt format.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the EMNLP'24 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding
  for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Nan Li, Jian Guan, Wei Wu, Zhengtao Yu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tables are ubiquitous across various domains for concisely representing
structured information. Empowering large language models (LLMs) to reason over
tabular data represents an actively explored direction. However, since typical
LLMs only support one-dimensional~(1D) inputs, existing methods often flatten
the two-dimensional~(2D) table structure into a sequence of tokens, which can
severely disrupt the spatial relationships and result in an inevitable loss of
vital contextual information. In this paper, we first empirically demonstrate
the detrimental impact of such flattening operations on the performance of LLMs
in capturing the spatial information of tables through two elaborate proxy
tasks. Subsequently, we introduce a simple yet effective positional encoding
method, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to
address this challenge. 2D-TPE enables each attention head to dynamically
select a permutation order of tokens within the context for attending to them,
where each permutation represents a distinct traversal mode for the table, such
as column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of
losing essential spatial information while preserving computational efficiency,
thus better preserving the table structure. Extensive experiments across five
benchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring
the importance of preserving the table structure for accurate table
comprehension. Comprehensive analysis further reveals the substantially better
scalability of 2D-TPE to large tables than baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and
  Multi-Level Style Control <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15977v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15977v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot singing voice synthesis (SVS) with style transfer and style control
aims to generate high-quality singing voices with unseen timbres and styles
(including singing method, emotion, rhythm, technique, and pronunciation) from
audio and text prompts. However, the multifaceted nature of singing styles
poses a significant challenge for effective modeling, transfer, and control.
Furthermore, current SVS models often fail to generate singing voices rich in
stylistic nuances for unseen singers. To address these challenges, we introduce
TCSinger, the first zero-shot SVS model for style transfer across cross-lingual
speech and singing styles, along with multi-level style control. Specifically,
TCSinger proposes three primary modules: 1) the clustering style encoder
employs a clustering vector quantization model to stably condense style
information into a compact latent space; 2) the Style and Duration Language
Model (S\&D-LM) concurrently predicts style information and phoneme duration,
which benefits both; 3) the style adaptive decoder uses a novel mel-style
adaptive normalization method to generate singing voices with enhanced details.
Experimental results show that TCSinger outperforms all baseline models in
synthesis quality, singer similarity, and style controllability across various
tasks, including zero-shot style transfer, multi-level style control,
cross-lingual style transfer, and speech-to-singing style transfer. Singing
voice samples can be accessed at https://tcsinger.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-FAct: Assessing Factuality of Multilingual <span class="highlight-title">LLM</span>s using FActScore 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18045v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18045v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheikh Shafayat, Eunsu Kim, Juhyun Oh, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the factuality of long-form large language model (LLM)-generated
text is an important challenge. Recently there has been a surge of interest in
factuality evaluation for English, but little is known about the factuality
evaluation of multilingual LLMs, specially when it comes to long-form
generation. %This paper systematically evaluates multilingual LLMs' factual
accuracy across languages and geographic regions. We introduce a simple
pipeline for multilingual factuality evaluation, by applying FActScore (Min et
al., 2023) for diverse languages. In addition to evaluating multilingual
factual generation, we evaluate the factual accuracy of long-form text
generation in topics that reflect regional diversity. We also examine the
feasibility of running the FActScore pipeline using non-English Wikipedia and
provide comprehensive guidelines on multilingual factual evaluation for
regionally diverse topics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latte: Latent Attention for Linear Time Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rares Dolga, Marius Cobzarenco, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The time complexity of the standard attention mechanism in transformers
scales quadratically with sequence length. We propose a probabilistic framework
for attention, enabling us to derive a novel low-rank linear
re-parameterisation of both bidirectional and causal cases, based on defining a
latent variable model. Our method can be seamlessly integrated as a drop-in
replacement for the standard attention mechanism. Additionally, this framework
provides a natural extension for combining local standard attention with our
global linear attention. This approach allows us to extend the context length
of existing large pre-trained models with only a few additional training steps.
The resulting ``Latte Transformer'' achieves performance comparable to standard
attention and other state-of-the-art models, while maintaining linear time and
memory complexity, along with constant-time next-token prediction during
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Twists, Humps, and Pebbles: Multilingual Speech Recognition Models
  Exhibit Gender Performance Gaps <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17954v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17954v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Attanasio, Beatrice Savoldi, Dennis Fucci, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current automatic speech recognition (ASR) models are designed to be used
across many languages and tasks without substantial changes. However, this
broad language coverage hides performance gaps within languages, for example,
across genders. Our study systematically evaluates the performance of two
widely used multilingual ASR models on three datasets, encompassing 19
languages from eight language families and two speaking conditions. Our
findings reveal clear gender disparities, with the advantaged group varying
across languages and models. Surprisingly, those gaps are not explained by
acoustic or lexical properties. However, probing internal model states reveals
a correlation with gendered performance gap. That is, the easier it is to
distinguish speaker gender in a language using probes, the more the gap
reduces, favoring female speakers. Our results show that gender disparities
persist even in state-of-the-art models. Our findings have implications for the
improvement of multilingual ASR systems, underscoring the importance of
accessibility to training data and nuanced evaluation to predict and mitigate
gender gaps. We release all code and artifacts at
https://github.com/g8a9/multilingual-asr-gender-gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024. Code and artifacts at
  https://github.com/g8a9/multilingual-asr-gender-gap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational Feedback in Scripted versus Spontaneous Dialogues: A
  Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ildikó Pilán, Laurent Prévot, Hendrik Buschmeier, Pierre Lison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scripted dialogues such as movie and TV subtitles constitute a widespread
source of training data for conversational NLP models. However, there are
notable linguistic differences between these dialogues and spontaneous
interactions, especially regarding the occurrence of communicative feedback
such as backchannels, acknowledgments, or clarification requests. This paper
presents a quantitative analysis of such feedback phenomena in both subtitles
and spontaneous conversations. Based on conversational data spanning eight
languages and multiple genres, we extract lexical statistics, classifications
from a dialogue act tagger, expert annotations and labels derived from a
fine-tuned Large Language Model (LLM). Our main empirical findings are that (1)
communicative feedback is markedly less frequent in subtitles than in
spontaneous dialogues and (2) subtitles contain a higher proportion of negative
feedback. We also show that dialogues generated by standard LLMs lie much
closer to scripted dialogues than spontaneous interactions in terms of
communicative feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version for SIGdial 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ miniCTX: Neural Theorem Proving with (Long-)Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiewen Hu, Thomas Zhu, Sean Welleck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world formal theorem proving often depends on a wealth of context,
including definitions, lemmas, comments, file structure, and other information.
We introduce miniCTX, which tests a model's ability to prove formal
mathematical theorems that depend on new context that is not seen during
training. miniCTX contains theorems sourced from real Lean projects and
textbooks, each associated with a context that can span tens of thousands of
tokens. Models are tasked with proving a theorem given access to code from the
theorem's repository, which contains context that is needed for the proof. As a
baseline for miniCTX, we tested fine-tuning and prompting methods that
condition theorem proving on preceding context. Both approaches substantially
outperform traditional methods that rely solely on state information. We found
that this ability to use context is not captured by previous benchmarks such as
miniF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting
and annotating theorem proving data, making it easy to add new projects into
miniCTX to ensure that contexts are not seen during training. miniCTX offers a
challenging and realistic evaluation of neural theorem provers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Hardness of Code in <span class="highlight-title">Large Language Model</span>s -- A
  Probabilistic Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Wolf, Binyamin Rothberg, Dorin Shteyman, Amnon Shashua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common practice in large language model (LLM) usage for complex analytical
tasks such as code generation, is to sample a solution for the entire task
within the model's context window. Previous works have shown that subtask
decomposition within the model's context (chain of thought), is beneficial for
solving such tasks. In this work, we point a limitation of LLMs' ability to
perform several sub-tasks within the same context window - an in-context
hardness of composition, pointing to an advantage for distributing a decomposed
problem in a multi-agent system of LLMs. The hardness of composition is
quantified by a generation complexity metric, i.e., the number of LLM
generations required to sample at least one correct solution. We find a gap
between the generation complexity of solving a compositional problem within the
same context relative to distributing it among multiple agents, that increases
exponentially with the solution's length. We prove our results theoretically
and demonstrate them empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Automatic Metrics with Incremental Machine Translation
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guojun Wu, Shay B. Cohen, Rico Sennrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a dataset comprising commercial machine translations, gathered
weekly over six years across 12 translation directions. Since human A/B testing
is commonly used, we assume commercial systems improve over time, which enables
us to evaluate machine translation (MT) metrics based on their preference for
more recent translations. Our study not only confirms several prior findings,
such as the advantage of neural metrics over non-neural ones, but also explores
the debated issue of how MT quality affects metric reliability--an
investigation that smaller datasets in previous research could not sufficiently
explore. Overall, our research demonstrates the dataset's value as a testbed
for metric evaluation. We release our code at https://github.com/gjwubyron/Evo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph <span class="highlight-title">Chain-of-Thought</span>: Augmenting <span class="highlight-title">Large Language Model</span>s by <span class="highlight-title">Reasoning</span> on
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07103v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07103v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), while exhibiting exceptional performance,
suffer from hallucinations, especially on knowledge-intensive tasks. Existing
works propose to augment LLMs with individual text units retrieved from
external knowledge corpora to alleviate the issue. However, in many domains,
texts are interconnected (e.g., academic papers in a bibliographic graph are
linked by citations and co-authorships) which form a (text-attributed) graph.
The knowledge in such graphs is encoded not only in single texts/nodes but also
in their associated connections. To facilitate the research of augmenting LLMs
with graphs, we manually construct a Graph Reasoning Benchmark dataset called
GRBench, containing 1,740 questions that can be answered with the knowledge
from 10 domain graphs. Then, we propose a simple and effective framework called
Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging
LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of
three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We
conduct systematic experiments with three LLM backbones on GRBench, where
Graph-CoT outperforms the baselines consistently. The code is available at
https://github.com/PeterGriffinJin/Graph-CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Instruction-following Abilities of <span class="highlight-title">Large Language Model</span>s with
  Task-aware Curriculum Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Yue, Chengyu Wang, Jun Huang, Peng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning aims to align large language models (LLMs) with
open-domain instructions and human-preferred responses. While several studies
have explored autonomous approaches to distilling and annotating instructions
from powerful proprietary LLMs, such as ChatGPT, they often neglect the impact
of the distributions and characteristics of tasks, together with the varying
difficulty of instructions in training sets. This oversight can lead to
imbalanced knowledge capabilities and poor generalization powers of student
LLMs. To address these challenges, we introduce Task-Aware Curriculum Planning
for Instruction Refinement (TAPIR), a multi-round distillation framework that
utilizes an oracle LLM to select instructions that are difficult for a student
LLM to follow. To balance the student's capabilities, task distributions in
training sets are adjusted with responses automatically refined according to
their corresponding tasks. In addition, by incorporating curriculum planning,
our approach systematically escalates the difficulty levels of tasks,
progressively enhancing the student LLM's capabilities. We rigorously evaluate
TAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0,
MT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that
student LLMs, trained with our method and less training data, outperform larger
instruction-tuned models and strong distillation baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>emnlp 2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic <span class="highlight-title">Survey</span> and Critical <span class="highlight-title">Review</span> on Evaluating Large Language
  Models: Challenges, Limitations, and Recommendations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Tahmid Rahman Laskar, Sawsan Alqahtani, M Saiful Bari, Mizanur Rahman, Mohammad Abdullah Matin Khan, Haidar Khan, Israt Jahan, Amran Bhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty, Jimmy Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently gained significant attention due
to their remarkable capabilities in performing diverse tasks across various
domains. However, a thorough evaluation of these models is crucial before
deploying them in real-world applications to ensure they produce reliable
performance. Despite the well-established importance of evaluating LLMs in the
community, the complexity of the evaluation process has led to varied
evaluation setups, causing inconsistencies in findings and interpretations. To
address this, we systematically review the primary challenges and limitations
causing these inconsistencies and unreliable evaluations in various steps of
LLM evaluation. Based on our critical review, we present our perspectives and
recommendations to ensure LLM evaluations are reproducible, reliable, and
robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">102</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Xie, Haoming Cai, Sachin Shah, Yiran Xu, Brandon Y. Feng, Jia-Bin Huang, Christopher A. Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simple yet effective approach for separating transmitted and
reflected light. Our key insight is that the powerful novel view synthesis
capabilities provided by modern inverse rendering methods (e.g.,~3D Gaussian
splatting) allow one to perform flash/no-flash reflection separation using
unpaired measurements -- this relaxation dramatically simplifies image
acquisition over conventional paired flash/no-flash reflection separation
methods. Through extensive real-world experiments, we demonstrate our method,
Flash-Splat, accurately reconstructs both transmitted and reflected scenes in
3D. Our method outperforms existing 3D reflection separation methods, which do
not leverage illumination control, by a large margin. Our project webpage is at
https://flash-splat.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vinoground: Scrutinizing LMMs over Dense Temporal <span class="highlight-title">Reasoning</span> with Short
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrui Zhang, Mu Cai, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been growing sentiment recently that modern large multimodal models
(LMMs) have addressed most of the key challenges related to short video
comprehension. As a result, both academia and industry are gradually shifting
their attention towards the more complex challenges posed by understanding
long-form videos. However, is this really the case? Our studies indicate that
LMMs still lack many fundamental reasoning capabilities even when dealing with
short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation
benchmark encompassing 1000 short and natural video-caption pairs. We
demonstrate that existing LMMs severely struggle to distinguish temporal
differences between different actions and object transformations. For example,
the best model GPT-4o only obtains ~50% on our text and video scores, showing a
large gap compared to the human baseline of ~90%. All open-source multimodal
models and CLIP-based models perform much worse, producing mostly random chance
performance. Through this work, we shed light onto the fact that temporal
reasoning in short videos is a problem yet to be fully solved. The dataset and
evaluation code are available at https://vinoground.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://vinoground.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting and <span class="highlight-title">Editing</span> Vision-Language Representations to Mitigate
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Jiang, Anish Kachinthaya, Suzie Petryk, Yossi Gandelsman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the internal representations of vision-language models (VLMs)
to address hallucinations, a persistent challenge despite advances in model
size and training. We project VLMs' internal image representations to their
language vocabulary and observe more confident output probabilities on real
objects than hallucinated objects. We additionally use these output
probabilities to spatially localize real objects. Building on this approach, we
introduce a knowledge erasure algorithm that removes hallucinations by linearly
orthogonalizing image features with respect to hallucinated object features. We
show that targeted edits to a model's latent representations can reduce
hallucinations by up to 25.7% on the COCO2014 dataset while preserving
performance. Our findings demonstrate how a deeper understanding of VLMs'
latent representations can enhance reliability and enable novel capabilities,
such as zero-shot segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: http://anishk23733.github.io/vl-interp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FakeShield: Explainable <span class="highlight-title">Image</span> Forgery Detection and Localization via
  Multi-modal <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of generative AI is a double-edged sword, which not
only facilitates content creation but also makes image manipulation easier and
more difficult to detect. Although current image forgery detection and
localization (IFDL) methods are generally effective, they tend to face two
challenges: \textbf{1)} black-box nature with unknown detection principle,
\textbf{2)} limited generalization across diverse tampering methods (e.g.,
Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the
explainable IFDL task and design FakeShield, a multi-modal framework capable of
evaluating image authenticity, generating tampered region masks, and providing
a judgment basis based on pixel-level and image-level tampering clues.
Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating
the Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's
tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided
Explainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery
Localization Module (MFLM) to address various types of tamper detection
interpretation and achieve forgery localization guided by detailed textual
descriptions. Extensive experiments demonstrate that FakeShield effectively
detects and localizes various tampering techniques, offering an explainable and
superior solution compared to previous IFDL methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loong: Generating Minute-level Long Videos with Autoregressive Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is desirable but challenging to generate content-rich long videos in the
scale of minutes. Autoregressive large language models (LLMs) have achieved
great success in generating coherent and long sequences of tokens in the domain
of natural language processing, while the exploration of autoregressive LLMs
for video generation is limited to generating short videos of several seconds.
In this work, we conduct a deep analysis of the challenges that prevent
autoregressive LLM-based video generators from generating long videos. Based on
the observations and analysis, we propose Loong, a new autoregressive LLM-based
video generator that can generate minute-long videos. Specifically, we model
the text tokens and video tokens as a unified sequence for autoregressive LLMs
and train the model from scratch. We propose progressive short-to-long training
with a loss re-weighting scheme to mitigate the loss imbalance problem for long
video training. We further investigate inference strategies, including video
token re-encoding and sampling strategies, to diminish error accumulation
during inference. Our proposed Loong can be trained on 10-second videos and be
extended to generate minute-level long videos conditioned on text prompts, as
demonstrated by the results. More samples are available at:
https://epiphqny.github.io/Loong-video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://epiphqny.github.io/Loong-video/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Localized Language-<span class="highlight-title">Image</span> Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Zhang, Yinfei Yang, Zhe Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has been a celebrated method
for training vision encoders to generate image/text representations
facilitating various applications. Recently, CLIP has been widely adopted as
the vision backbone of multimodal large language models (MLLMs) to connect
image inputs for language interactions. The success of CLIP as a
vision-language foundation model relies on aligning web-crawled noisy text
annotations at image levels. Nevertheless, such criteria may become
insufficient for downstream tasks in need of fine-grained vision
representations, especially when region-level understanding is demanding for
MLLMs. In this paper, we improve the localization capability of CLIP with
several advances. We propose a pre-training method called Contrastive Localized
Language-Image Pre-training (CLOC) by complementing CLIP with region-text
contrastive loss and modules. We formulate a new concept, promptable
embeddings, of which the encoder produces image embeddings easy to transform
into region representations given spatial hints. To support large-scale
pre-training, we design a visually-enriched and spatially-localized captioning
framework to effectively generate region-text pseudo-labels at scale. By
scaling up to billions of annotated images, CLOC enables high-quality regional
embeddings for image region recognition and retrieval tasks, and can be a
drop-in replacement of CLIP to enhance MLLMs, especially on referring and
grounding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisit Large-Scale <span class="highlight-title">Image</span>-Caption Data in Pre-training <span class="highlight-title">Multimodal</span>
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, Meng Cao, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal models highlight the value of rewritten
captions for improving performance, yet key challenges remain. For example,
while synthetic captions often provide superior quality and image-text
alignment, it is not clear whether they can fully replace AltTexts: the role of
synthetic captions and their interaction with original web-crawled AltTexts in
pre-training is still not well understood. Moreover, different multimodal
foundation models may have unique preferences for specific caption formats, but
efforts to identify the optimal captions for each model remain limited. In this
work, we propose a novel, controllable, and scalable captioning pipeline
designed to generate diverse caption formats tailored to various multimodal
models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic
Captions (DSC+) as case studies, we systematically explore their effects and
interactions with AltTexts across models such as CLIP, multimodal LLMs, and
diffusion models. Our findings reveal that a hybrid approach that keeps both
synthetic captions and AltTexts can outperform the use of synthetic captions
alone, improving both alignment and performance, with each model demonstrating
preferences for particular caption formats. This comprehensive analysis
provides valuable insights into optimizing captioning strategies, thereby
advancing the pre-training of multimodal foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CV/ML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DivScene: <span class="highlight-title">Benchmark</span>ing LVLMs for Object Navigation with Diverse Scenes
  and Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object navigation in unknown environments is crucial for deploying embodied
agents in real-world applications. While we have witnessed huge progress due to
large-scale scene datasets, faster simulators, and stronger models, previous
studies mainly focus on limited scene types and target objects. In this paper,
we study a new task of navigating to diverse target objects in a large number
of scene types. To benchmark the problem, we present a large-scale scene
dataset, DivScene, which contains 4,614 scenes across 81 different types. With
the dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a
Large Vision Language Model (LVLM) through imitation learning. The LVLM is
trained to take previous observations from the environment and generate the
next actions. We also introduce CoT explanation traces of the action prediction
for better performance when tuning LVLMs. Our extensive experiments find that
we can build a performant LVLM-based agent through imitation learning on the
shortest paths constructed by a BFS planner without any human supervision. Our
agent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we
carry out various analyses showing the generalization ability of our agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curvature Diversity-Driven Deformation and Domain Alignment for Point
  Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxi Wu, Hao Huang, Yi Fang, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) is crucial for reducing the need for
extensive manual data annotation when training deep networks on point cloud
data. A significant challenge of UDA lies in effectively bridging the domain
gap. To tackle this challenge, we propose \textbf{C}urvature
\textbf{D}iversity-Driven \textbf{N}uclear-Norm Wasserstein \textbf{D}omain
Alignment (CDND). Our approach first introduces a \textit{\textbf{Curv}ature
Diversity-driven Deformation \textbf{Rec}onstruction (CurvRec)} task, which
effectively mitigates the gap between the source and target domains by enabling
the model to extract salient features from semantically rich regions of a given
point cloud. We then propose \textit{\textbf{D}eformation-based
\textbf{N}uclear-norm \textbf{W}asserstein \textbf{D}iscrepancy (D-NWD)}, which
applies the Nuclear-norm Wasserstein Discrepancy to both \textit{deformed and
original} data samples to align the source and target domains. Furthermore, we
contribute a theoretical justification for the effectiveness of D-NWD in
distribution alignment and demonstrate that it is \textit{generic} enough to be
applied to \textbf{any} deformations. To validate our method, we conduct
extensive experiments on two public domain adaptation datasets for point cloud
classification and segmentation tasks. Empirical experiment results show that
our CDND achieves state-of-the-art performance by a noticeable margin over
existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlzhiNet: Traversing from 2DCNN to 3DCNN, Towards Early Detection and
  Diagnosis of Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romoke Grace Akindele, Samuel Adebayo, Paul Shekonya Kanda, Ming Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) is a progressive neurodegenerative disorder with
increasing prevalence among the aging population, necessitating early and
accurate diagnosis for effective disease management. In this study, we present
a novel hybrid deep learning framework that integrates both 2D Convolutional
Neural Networks (2D-CNN) and 3D Convolutional Neural Networks (3D-CNN), along
with a custom loss function and volumetric data augmentation, to enhance
feature extraction and improve classification performance in AD diagnosis.
According to extensive experiments, AlzhiNet outperforms standalone 2D and 3D
models, highlighting the importance of combining these complementary
representations of data. The depth and quality of 3D volumes derived from the
augmented 2D slices also significantly influence the model's performance. The
results indicate that carefully selecting weighting factors in hybrid
predictions is imperative for achieving optimal results. Our framework has been
validated on the Magnetic Resonance Imaging (MRI) from Kaggle and MIRIAD
datasets, obtaining accuracies of 98.9% and 99.99%, respectively, with an AUC
of 100%. Furthermore, AlzhiNet was studied under a variety of perturbation
scenarios on the Alzheimer's Kaggle dataset, including Gaussian noise,
brightness, contrast, salt and pepper noise, color jitter, and occlusion. The
results obtained show that AlzhiNet is more robust to perturbations than
ResNet-18, making it an excellent choice for real-world applications. This
approach represents a promising advancement in the early diagnosis and
treatment planning for Alzheimer's disease.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Instruction Tuning With Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of video large multimodal models (LMMs) has been hindered by
the difficulty of curating large amounts of high-quality raw data from the web.
To address this, we propose an alternative approach by creating a high-quality
synthetic dataset specifically for video instruction-following, namely
LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,
open-ended question-answering (QA), and multiple-choice QA. By training on this
dataset, in combination with existing visual instruction tuning data, we
introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that
LLaVA-Video achieves strong performance across various video benchmarks,
highlighting the effectiveness of our dataset. We plan to release the dataset,
its generation pipeline, and the model checkpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-Critic: Learning to Evaluate <span class="highlight-title">Multimodal</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)
designed as a generalist evaluator to assess performance across a wide range of
multimodal tasks. LLaVA-Critic is trained using a high-quality critic
instruction-following dataset that incorporates diverse evaluation criteria and
scenarios. Our experiments demonstrate the model's effectiveness in two key
areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation
scores, performing on par with or surpassing GPT models on multiple evaluation
benchmarks; and (2) Preference Learning, where it generates reward signals for
preference learning, enhancing model alignment capabilities. This work
underscores the potential of open-source LMMs in self-critique and evaluation,
setting the stage for future research into scalable, superhuman alignment
feedback mechanisms for LMMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteerDiff: Steering towards Safe Text-to-<span class="highlight-title">Image</span> <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxiang Zhang, Yifeng He, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) diffusion models have drawn attention for their ability
to generate high-quality images with precise text alignment. However, these
models can also be misused to produce inappropriate content. Existing safety
measures, which typically rely on text classifiers or ControlNet-like
approaches, are often insufficient. Traditional text classifiers rely on
large-scale labeled datasets and can be easily bypassed by rephrasing. As
diffusion models continue to scale, fine-tuning these safeguards becomes
increasingly challenging and lacks flexibility. Recent red-teaming attack
researches further underscore the need for a new paradigm to prevent the
generation of inappropriate content. In this paper, we introduce SteerDiff, a
lightweight adaptor module designed to act as an intermediary between user
input and the diffusion model, ensuring that generated images adhere to ethical
and safety standards with little to no impact on usability. SteerDiff
identifies and manipulates inappropriate concepts within the text embedding
space to guide the model away from harmful outputs. We conduct extensive
experiments across various concept unlearning tasks to evaluate the
effectiveness of our approach. Furthermore, we benchmark SteerDiff against
multiple red-teaming strategies to assess its robustness. Finally, we explore
the potential of SteerDiff for concept forgetting tasks, demonstrating its
versatility in text-conditioned image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ControlAR: Controllable <span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> with Autoregressive Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongming Li, Tianheng Cheng, Shoufa Chen, Peize Sun, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) models have reformulated image generation as next-token
prediction, demonstrating remarkable potential and emerging as strong
competitors to diffusion models. However, control-to-image generation, akin to
ControlNet, remains largely unexplored within AR models. Although a natural
approach, inspired by advancements in Large Language Models, is to tokenize
control images into tokens and prefill them into the autoregressive model
before decoding image tokens, it still falls short in generation quality
compared to ControlNet and suffers from inefficiency. To this end, we introduce
ControlAR, an efficient and effective framework for integrating spatial
controls into autoregressive image generation models. Firstly, we explore
control encoding for AR models and propose a lightweight control encoder to
transform spatial inputs (e.g., canny edges or depth maps) into control tokens.
Then ControlAR exploits the conditional decoding method to generate the next
image token conditioned on the per-token fusion between control and image
tokens, similar to positional encodings. Compared to prefilling tokens, using
conditional decoding significantly strengthens the control capability of AR
models but also maintains the model's efficiency. Furthermore, the proposed
ControlAR surprisingly empowers AR models with arbitrary-resolution image
generation via conditional decoding and specific controls. Extensive
experiments can demonstrate the controllability of the proposed ControlAR for
the autoregressive control-to-image generation across diverse inputs, including
edges, depths, and segmentation masks. Furthermore, both quantitative and
qualitative results indicate that ControlAR surpasses previous state-of-the-art
controllable diffusion models, e.g., ControlNet++. Code, models, and demo will
soon be available at https://github.com/hustvl/ControlAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lie Algebra Canonicalization: Equivariant Neural Operators under
  arbitrary Lie Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakhar Shumaylov, Peter Zaika, James Rowbottom, Ferdia Sherry, Melanie Weber, Carola-Bibiane Schönlieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quest for robust and generalizable machine learning models has driven
recent interest in exploiting symmetries through equivariant neural networks.
In the context of PDE solvers, recent works have shown that Lie point
symmetries can be a useful inductive bias for Physics-Informed Neural Networks
(PINNs) through data and loss augmentation. Despite this, directly enforcing
equivariance within the model architecture for these problems remains elusive.
This is because many PDEs admit non-compact symmetry groups, oftentimes not
studied beyond their infinitesimal generators, making them incompatible with
most existing equivariant architectures. In this work, we propose Lie aLgebrA
Canonicalization (LieLAC), a novel approach that exploits only the action of
infinitesimal generators of the symmetry group, circumventing the need for
knowledge of the full group structure. To achieve this, we address existing
theoretical issues in the canonicalization literature, establishing connections
with frame averaging in the case of continuous non-compact groups. Operating
within the framework of canonicalization, LieLAC can easily be integrated with
unconstrained pre-trained models, transforming inputs to a canonical form
before feeding them into the existing model, effectively aligning the input for
model inference according to allowed symmetries. LieLAC utilizes standard Lie
group descent schemes, achieving equivariance in pre-trained models. Finally,
we showcase LieLAC's efficacy on tasks of invariant image classification and
Lie point symmetry equivariant neural PDE solvers using pre-trained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages; preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Point Cloud Completion through Unbalanced Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taekyung Lee, Jaemoo Choi, Jaewoong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unpaired point cloud completion explores methods for learning a completion
map from unpaired incomplete and complete point cloud data. In this paper, we
propose a novel approach for unpaired point cloud completion using the
unbalanced optimal transport map, called Unbalanced Optimal Transport Map for
Unpaired Point Cloud Completion (UOT-UPC). We demonstrate that the unpaired
point cloud completion can be naturally interpreted as the Optimal Transport
(OT) problem and introduce the Unbalanced Optimal Transport (UOT) approach to
address the class imbalance problem, which is prevalent in unpaired point cloud
completion datasets. Moreover, we analyze the appropriate cost function for
unpaired completion tasks. This analysis shows that the InfoCD cost function is
particularly well-suited for this task. Our model is the first attempt to
leverage UOT for unpaired point cloud completion, achieving competitive or
superior results on both single-category and multi-category datasets. In
particular, our model is especially effective in scenarios with class
imbalance, where the proportions of categories are different between the
incomplete and complete point cloud datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring and Improving Persuasiveness of Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somesh Singh, Yaman K Singla, Harini SI, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly being used in workflows involving generating content to
be consumed by humans (e.g., marketing) and also in directly interacting with
humans (e.g., through chatbots). The development of such systems that are
capable of generating verifiably persuasive messages presents both
opportunities and challenges for society. On the one hand, such systems could
positively impact domains like advertising and social good, such as addressing
drug addiction, and on the other, they could be misused for spreading
misinformation and shaping political opinions. To channel LLMs' impact on
society, we need to develop systems to measure and benchmark their
persuasiveness. With this motivation, we introduce PersuasionBench and
PersuasionArena, the first large-scale benchmark and arena containing a battery
of tasks to measure the persuasion ability of generative models automatically.
We investigate to what extent LLMs know and leverage linguistic patterns that
can help them generate more persuasive language. Our findings indicate that the
persuasiveness of LLMs correlates positively with model size, but smaller
models can also be made to have a higher persuasiveness than much larger
models. Notably, targeted training using synthetic and natural datasets
significantly enhances smaller models' persuasive capabilities, challenging
scale-dependent assumptions. Our findings carry key implications for both model
developers and policymakers. For instance, while the EU AI Act and California's
SB-1047 aim to regulate AI models based on the number of floating point
operations, we demonstrate that simple metrics like this alone fail to capture
the full scope of AI's societal impact. We invite the community to explore and
contribute to PersuasionArena and PersuasionBench, available at
https://bit.ly/measure-persuasion, to advance our understanding of AI-driven
persuasion and its societal implications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning 3D Perception from Others' Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinsu Yoo, Zhenyang Feng, Tai-Yu Pan, Yihong Sun, Cheng Perng Phoo, Xiangyu Chen, Mark Campbell, Kilian Q. Weinberger, Bharath Hariharan, Wei-Lun Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D object detection in real-world environments requires a huge
amount of annotated data with high quality. Acquiring such data is tedious and
expensive, and often needs repeated effort when a new sensor is adopted or when
the detector is deployed in a new environment. We investigate a new scenario to
construct 3D object detectors: learning from the predictions of a nearby unit
that is equipped with an accurate detector. For example, when a self-driving
car enters a new area, it may learn from other traffic participants whose
detectors have been optimized for that area. This setting is label-efficient,
sensor-agnostic, and communication-efficient: nearby units only need to share
the predictions with the ego agent (e.g., car). Naively using the received
predictions as ground-truths to train the detector for the ego car, however,
leads to inferior performance. We systematically study the problem and identify
viewpoint mismatches and mislocalization (due to synchronization and GPS
errors) as the main causes, which unavoidably result in false positives, false
negatives, and inaccurate pseudo labels. We propose a distance-based
curriculum, first learning from closer units with similar viewpoints and
subsequently improving the quality of other units' predictions via
self-training. We further demonstrate that an effective pseudo label refinement
module can be trained with a handful of annotated data, largely reducing the
data quantity necessary to train an object detector. We validate our approach
on the recently released real-world collaborative driving dataset, using
reference cars' predictions as pseudo labels for the ego car. Extensive
experiments including several scenarios (e.g., different sensors, detectors,
and domains) demonstrate the effectiveness of our approach toward
label-efficient learning of 3D perception from other units' predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based
  Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Stathoulopoulos, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in robotics are pushing real-world autonomy, enabling robots
to perform long-term and large-scale missions. A crucial component for
successful missions is the incorporation of loop closures through place
recognition, which effectively mitigates accumulated pose estimation drift.
Despite computational advancements, optimizing performance for real-time
deployment remains challenging, especially in resource-constrained mobile
robots and multi-robot systems since, conventional keyframe sampling practices
in place recognition often result in retaining redundant information or
overlooking relevant data, as they rely on fixed sampling intervals or work
directly in the 3D space instead of the feature space. To address these
concerns, we introduce the concept of sample space in place recognition and
demonstrate how different sampling techniques affect the query process and
overall performance. We then present a novel keyframe sampling approach for
LiDAR-based place recognition, which focuses on redundancy minimization and
information preservation in the hyper-dimensional descriptor space. This
approach is applicable to both learning-based and handcrafted descriptors, and
through the experimental validation across multiple datasets and descriptor
frameworks, we demonstrate the effectiveness of our proposed method, showing it
can jointly minimize redundancy and preserve essential information in
real-time. The proposed approach maintains robust performance across various
datasets without requiring parameter tuning, contributing to more efficient and
reliable place recognition for a wide range of robotic applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 15 figures. Submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span>-based Extreme <span class="highlight-title">Image</span> Compression with Compressed Feature
  Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Li, Yanhui Zhou, Hao Wei, Chenyang Ge, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based extreme image compression methods have achieved impressive
performance at extremely low bitrates. However, constrained by the iterative
denoising process that starts from pure noise, these methods are limited in
both fidelity and efficiency. To address these two issues, we present Relay
Residual Diffusion Extreme Image Compression (RDEIC), which leverages
compressed feature initialization and residual diffusion. Specifically, we
first use the compressed latent features of the image with added noise, instead
of pure noise, as the starting point to eliminate the unnecessary initial
stages of the denoising process. Second, we design a novel relay residual
diffusion that reconstructs the raw image by iteratively removing the added
noise and the residual between the compressed and target latent features.
Notably, our relay residual diffusion network seamlessly integrates pre-trained
stable diffusion to leverage its robust generative capability for high-quality
reconstruction. Third, we propose a fixed-step fine-tuning strategy to
eliminate the discrepancy between the training and inference phases, further
improving the reconstruction quality. Extensive experiments demonstrate that
the proposed RDEIC achieves state-of-the-art visual quality and outperforms
existing diffusion-based extreme image compression methods in both fidelity and
efficiency. The source code will be provided in
https://github.com/huai-chang/RDEIC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial-Temporal Multi-Cuts for Online Multiple-Camera Vehicle Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Herzog, Johannes Gilg, Philipp Wolters, Torben Teepe, Gerhard Rigoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate online multiple-camera vehicle tracking is essential for intelligent
transportation systems, autonomous driving, and smart city applications. Like
single-camera multiple-object tracking, it is commonly formulated as a graph
problem of tracking-by-detection. Within this framework, existing online
methods usually consist of two-stage procedures that cluster temporally first,
then spatially, or vice versa. This is computationally expensive and prone to
error accumulation. We introduce a graph representation that allows
spatial-temporal clustering in a single, combined step: New detections are
spatially and temporally connected with existing clusters. By keeping sparse
appearance and positional cues of all detections in a cluster, our method can
compare clusters based on the strongest available evidence. The final tracks
are obtained online using a simple multicut assignment procedure. Our method
does not require any training on the target scene, pre-extraction of
single-camera tracks, or additional annotations. Notably, we outperform the
online state-of-the-art on the CityFlow dataset in terms of IDF1 by more than
14%, and on the Synthehicle dataset by more than 25%, respectively. The code is
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plots Unlock Time-Series Understanding in <span class="highlight-title">Multimodal</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayank Daswani, Mathias M. J. Bellaiche, Marc Wilson, Desislav Ivanov, Mikhail Papkov, Eva Schnider, Jing Tang, Kay Lamerigts, Gabriela Botea, Michael A. Sanchez, Yojan Patel, Shruthi Prabhakara, Shravya Shetty, Umesh Telang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multimodal foundation models can now natively work with data beyond
text, they remain underutilized in analyzing the considerable amounts of
multi-dimensional time-series data in fields like healthcare, finance, and
social sciences, representing a missed opportunity for richer, data-driven
insights. This paper proposes a simple but effective method that leverages the
existing vision encoders of these models to "see" time-series data via plots,
avoiding the need for additional, potentially costly, model training. Our
empirical evaluations show that this approach outperforms providing the raw
time-series data as text, with the additional benefit that visual time-series
representations demonstrate up to a 90% reduction in model API costs. We
validate our hypothesis through synthetic data tasks of increasing complexity,
progressing from simple functional form identification on clean data, to
extracting trends from noisy scatter plots. To demonstrate generalizability
from synthetic tasks with clear reasoning steps to more complex, real-world
scenarios, we apply our approach to consumer health tasks - specifically fall
detection, activity recognition, and readiness assessment - which involve
heterogeneous, noisy data and multi-step reasoning. The overall success in plot
performance over text performance (up to an 120% performance increase on
zero-shot synthetic tasks, and up to 150% performance increase on real-world
tasks), across both GPT and Gemini model families, highlights our approach's
potential for making the best use of the native capabilities of foundation
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metrics Revolutions: Groundbreaking Insights into the Implementation of
  Metrics for Biomedical <span class="highlight-title">Image</span> Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gašper Podobnik, Tomaž Vrtovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of segmentation performance is a common task in biomedical
image analysis, with its importance emphasized in the recently released metrics
selection guidelines and computing frameworks. To quantitatively evaluate the
alignment of two segmentations, researchers commonly resort to counting
metrics, such as the Dice similarity coefficient, or distance-based metrics,
such as the Hausdorff distance, which are usually computed by publicly
available open-source tools with an inherent assumption that these tools
provide consistent results. In this study we questioned this assumption, and
performed a systematic implementation analysis along with quantitative
experiments on real-world clinical data to compare 11 open-source tools for
distance-based metrics computation against our highly accurate mesh-based
reference implementation. The results revealed that statistically significant
differences among all open-source tools are both surprising and concerning,
since they question the validity of existing studies. Besides identifying the
main sources of variation, we also provide recommendations for distance-based
metrics computation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GI-GS: Global Illumination Decomposition on Gaussian Splatting for
  Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongze Chen, Zehong Lin, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GI-GS, a novel inverse rendering framework that leverages 3D
Gaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel
view synthesis and relighting. In inverse rendering, accurately modeling the
shading processes of objects is essential for achieving high-fidelity results.
Therefore, it is critical to incorporate global illumination to account for
indirect lighting that reaches an object after multiple bounces across the
scene. Previous 3DGS-based methods have attempted to model indirect lighting by
characterizing indirect illumination as learnable lighting volumes or
additional attributes of each Gaussian, while using baked occlusion to
represent shadow effects. These methods, however, fail to accurately model the
complex physical interactions between light and objects, making it impossible
to construct realistic indirect illumination during relighting. To address this
limitation, we propose to calculate indirect lighting using efficient path
tracing with deferred shading. In our framework, we first render a G-buffer to
capture the detailed geometry and material properties of the scene. Then, we
perform physically-based rendering (PBR) only for direct lighting. With the
G-buffer and previous rendering results, the indirect lighting can be
calculated through a lightweight path tracing. Our method effectively models
indirect lighting under any given lighting conditions, thereby achieving better
novel view synthesis and relighting. Quantitative and qualitative results show
that our GI-GS outperforms existing baselines in both rendering quality and
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-Eye: Abductive NLI for <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mor Ventura, Michael Toker, Nitay Calderon, Zorik Gekhman, Yonatan Bitton, Roi Reichart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Will a Visual Language Model (VLM)-based bot warn us about slipping if it
detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet
their ability to infer outcomes and causes remains underexplored. To address
this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual
abductive reasoning skills. NL-Eye adapts the abductive Natural Language
Inference (NLI) task to the visual domain, requiring models to evaluate the
plausibility of hypothesis images based on a premise image and explain their
decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050
images) spanning diverse reasoning categories: physical, functional, logical,
emotional, cultural, and social. The data curation process involved two steps -
writing textual descriptions and generating images using text-to-image models,
both requiring substantial human involvement to ensure high-quality and
challenging scenes. Our experiments show that VLMs struggle significantly on
NL-Eye, often performing at random baseline levels, while humans excel in both
plausibility prediction and explanation quality. This demonstrates a deficiency
in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a
crucial step toward developing VLMs capable of robust multimodal reasoning for
real-world applications, including accident-prevention bots and generated video
verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Efficiency Neural Video Compression via Hierarchical Predictive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Lu, Zhihao Duan, Wuyang Cong, Dandan Ding, Fengqing Zhu, Zhan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enhanced Deep Hierarchical Video Compression-DHVC 2.0-has been
introduced. This single-model neural video codec operates across a broad range
of bitrates, delivering not only superior compression performance to
representative methods but also impressive complexity efficiency, enabling
real-time processing with a significantly smaller memory footprint on standard
GPUs. These remarkable advancements stem from the use of hierarchical
predictive coding. Each video frame is uniformly transformed into multiscale
representations through hierarchical variational autoencoders. For a specific
scale's feature representation of a frame, its corresponding latent residual
variables are generated by referencing lower-scale spatial features from the
same frame and then conditionally entropy-encoded using a probabilistic model
whose parameters are predicted using same-scale temporal reference from
previous frames and lower-scale spatial reference of the current frame. This
feature-space processing operates from the lowest to the highest scale of each
frame, completely eliminating the need for the complexity-intensive motion
estimation and compensation techniques that have been standard in video codecs
for decades. The hierarchical approach facilitates parallel processing,
accelerating both encoding and decoding, and supports transmission-friendly
progressive decoding, making it particularly advantageous for networked video
applications in the presence of packet loss. Source codes will be made
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IC3M: In-Car <span class="highlight-title">Multimodal</span> Multi-object Monitoring for Abnormal Status of
  Both Driver and Passengers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Fang, Zheng Lin, Senkang Hu, Hangcheng Cao, Yiqin Deng, Xianhao Chen, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, in-car monitoring has emerged as a promising technology for
detecting early-stage abnormal status of the driver and providing timely alerts
to prevent traffic accidents. Although training models with multimodal data
enhances the reliability of abnormal status detection, the scarcity of labeled
data and the imbalance of class distribution impede the extraction of critical
abnormal state features, significantly deteriorating training performance.
Furthermore, missing modalities due to environment and hardware limitations
further exacerbate the challenge of abnormal status identification. More
importantly, monitoring abnormal health conditions of passengers, particularly
in elderly care, is of paramount importance but remains underexplored. To
address these challenges, we introduce our IC3M, an efficient
camera-rotation-based multimodal framework for monitoring both driver and
passengers in a car. Our IC3M comprises two key modules: an adaptive threshold
pseudo-labeling strategy and a missing modality reconstruction. The former
customizes pseudo-labeling thresholds for different classes based on the class
distribution, generating class-balanced pseudo labels to guide model training
effectively, while the latter leverages crossmodality relationships learned
from limited labels to accurately recover missing modalities by distribution
transferring from available modalities. Extensive experimental results
demonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,
precision, and recall while exhibiting superior robustness under limited
labeled data and severe missing modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Variational Method for <span class="highlight-title">Image</span> Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing-En Huang, Jia-Wei Liao, Ku-Te Lin, Yu-Ju Tsai, Mei-Heng Yueh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The total variation (TV) method is an image denoising technique that aims to
reduce noise by minimizing the total variation of the image, which measures the
variation in pixel intensities. The TV method has been widely applied in image
processing and computer vision for its ability to preserve edges and enhance
image quality. In this paper, we propose an improved TV model for image
denoising and the associated numerical algorithm to carry out the procedure,
which is particularly effective in removing several types of noises and their
combinations. Our improved model admits a unique solution and the associated
numerical algorithm guarantees the convergence. Numerical experiments are
demonstrated to show improved effectiveness and denoising quality compared to
other TV models. Such encouraging results further enhance the utility of the TV
method in image processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Pre- and Post-Demosaicking Noise Removal for RAW Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Sánchez-Beeckman, Antoni Buades, Nicola Brandonisio, Bilel Kanoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising is one of the fundamental steps of the processing pipeline that
converts data captured by a camera sensor into a display-ready image or video.
It is generally performed early in the pipeline, usually before demosaicking,
although studies swapping their order or even conducting them jointly have been
proposed. With the advent of deep learning, the quality of denoising algorithms
has steadily increased. Even so, modern neural networks still have a hard time
adapting to new noise levels and scenes, which is indispensable for real-world
applications. With those in mind, we propose a self-similarity-based denoising
scheme that weights both a pre- and a post-demosaicking denoiser for
Bayer-patterned CFA video data. We show that a balance between the two leads to
better image quality, and we empirically find that higher noise levels benefit
from a higher influence pre-demosaicking. We also integrate temporal trajectory
prefiltering steps before each denoiser, which further improve texture
reconstruction. The proposed method only requires an estimation of the noise
model at the sensor, accurately adapts to any noise level, and is competitive
with the state of the art, making it suitable for real-world videography.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperGS: Super-Resolution 3D Gaussian Splatting via Latent Feature Field
  and Gradient-guided Splitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyun Xie, Zhiru Wang, Yinghao Zhu, Chengwei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis
with its real-time rendering capabilities and superior quality. However, it
faces challenges for high-resolution novel view synthesis (HRNVS) due to the
coarse nature of primitives derived from low-resolution input views. To address
this issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion
of 3DGS designed with a two-stage coarse-to-fine training framework, utilizing
pretrained low-resolution scene representation as an initialization for
super-resolution optimization. Moreover, we introduce Multi-resolution Feature
Gaussian Splatting (MFGS) to incorporates a latent feature field for flexible
feature sampling and Gradient-guided Selective Splitting (GSS) for effective
Gaussian upsampling. By integrating these strategies within the coarse-to-fine
framework ensure both high fidelity and memory efficiency. Extensive
experiments demonstrate that SuperGS surpasses state-of-the-art HRNVS methods
on challenging real-world datasets using only low-resolution inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NestedMorph: Enhancing Deformable Medical <span class="highlight-title">Image</span> Registration with Nested
  Attention Mechanisms <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gurucharan Marthi Krishna Kumar, Janine Mendola, Amir Shmuel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable image registration is crucial for aligning medical images in a
non-linear fashion across different modalities, allowing for precise spatial
correspondence between varying anatomical structures. This paper presents
NestedMorph, a novel network utilizing a Nested Attention Fusion approach to
improve intra-subject deformable registration between T1-weighted (T1w) MRI and
diffusion MRI (dMRI) data. NestedMorph integrates high-resolution spatial
details from an encoder with semantic information from a decoder using a
multi-scale framework, enhancing both local and global feature extraction. Our
model notably outperforms existing methods, including CNN-based approaches like
VoxelMorph, MIDIR, and CycleMorph, as well as Transformer-based models such as
TransMorph and ViT-V-Net, and traditional techniques like NiftyReg and SyN.
Evaluations on the HCP dataset demonstrate that NestedMorph achieves superior
performance across key metrics, including SSIM, HD95, and SDlogJ, with the
highest SSIM of 0.89, and the lowest HD95 of 2.5 and SDlogJ of 0.22. These
results highlight NestedMorph's ability to capture both local and global image
features effectively, leading to superior registration performance. The
promising outcomes of this study underscore NestedMorph's potential to
significantly advance deformable medical image registration, providing a robust
framework for future research and clinical applications. The source code and
our implementation are available at: https://bit.ly/3zdVqcg
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedVisionLlama: Leveraging Pre-Trained <span class="highlight-title">Large Language Model</span> Layers to
  Enhance Medical <span class="highlight-title">Image</span> Segmentation <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), known for their versatility in textual data,
are increasingly being explored for their potential to enhance medical image
segmentation, a crucial task for accurate diagnostic imaging. This study
explores enhancing Vision Transformers (ViTs) for medical image segmentation by
integrating pre-trained LLM transformer blocks. Our approach, which
incorporates a frozen LLM transformer block into the encoder of a ViT-based
model, leads to substantial improvements in segmentation performance across
various medical imaging modalities. We propose a Hybrid Attention Mechanism
that combines global and local feature learning with a Multi-Scale Fusion Block
for aggregating features across different scales. The enhanced model shows
significant performance gains, including an average Dice score increase from
0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.
These results demonstrate the effectiveness of LLM-based transformers in
refining medical image segmentation, highlighting their potential to
significantly boost model accuracy and robustness. The source code and our
implementation are available at: https://bit.ly/3zf2CVs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo-Stereo Inputs: A Solution to the Occlusion Challenge in
  Self-Supervised Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhi Yang, Xingqiang Li, Jiajun Bai, Jinsong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised stereo matching holds great promise for application and
research due to its independence from expensive labeled data. However, direct
self-supervised stereo matching paradigms based on photometric loss functions
have consistently struggled with performance issues due to the occlusion
challenge. The crux of the occlusion challenge lies in the fact that the
positions of occluded pixels consistently align with the epipolar search
direction defined by the input stereo images, leading to persistent information
loss and erroneous feedback at fixed locations during self-supervised training.
In this work, we propose a simple yet highly effective pseudo-stereo inputs
strategy to address the core occlusion challenge. This strategy decouples the
input and feedback images, compelling the network to probabilistically sample
information from both sides of the occluding objects. As a result, the
persistent lack of information in the aforementioned fixed occlusion areas is
mitigated. Building upon this, we further address feedback conflicts and
overfitting issues arising from the strategy. By integrating these components,
our method achieves stable and significant performance improvements compared to
existing methods. Quantitative experiments are conducted to evaluate the
performance. Qualitative experiments further demonstrate accurate disparity
inference even at occluded regions. These results demonstrate a significant
advancement over previous methods in the field of direct self-supervised stereo
matching based on photometric loss. The proposed pseudo-stereo inputs strategy,
due to its simplicity and effectiveness, has the potential to serve as a new
paradigm for direct self-supervised stereo matching. Code is available at
https://github.com/qrzyang/Pseudo-Stereo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Image Processing (TIP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Foundation Model for the Solar Dynamics Observatory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Walsh, Daniel G. Gass, Raul Ramos Pollan, Paul J. Wright, Richard Galvez, Noah Kasmanoff, Jason Naradowsky, Anne Spalding, James Parr, Atılım Güneş Baydin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SDO-FM is a foundation model using data from NASA's Solar Dynamics
Observatory (SDO) spacecraft; integrating three separate instruments to
encapsulate the Sun's complex physical interactions into a multi-modal
embedding space. This model can be used to streamline scientific investigations
involving SDO by making the enormous datasets more computationally accessible
for heliophysics research and enable investigations that require instrument
fusion. We discuss four key components: an ingestion pipeline to create machine
learning ready datasets, the model architecture and training approach,
resultant embeddings and fine-tunable models, and finally downstream fine-tuned
applications. A key component of this effort has been to include subject matter
specialists at each stage of development; reviewing the scientific value and
providing guidance for model architecture, dataset, and training paradigm
decisions. This paper marks release of our pretrained models and embedding
datasets, available to the community on Hugging Face and sdofm.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiFiSeg: High-Frequency Information Enhanced Polyp Segmentation with
  Global-Local Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Ren, Xiaoyong Zhang, Lina Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous studies have demonstrated the strong performance of Vision
Transformer (ViT)-based methods across various computer vision tasks. However,
ViT models often struggle to effectively capture high-frequency components in
images, which are crucial for detecting small targets and preserving edge
details, especially in complex scenarios. This limitation is particularly
challenging in colon polyp segmentation, where polyps exhibit significant
variability in structure, texture, and shape. High-frequency information, such
as boundary details, is essential for achieving precise semantic segmentation
in this context. To address these challenges, we propose HiFiSeg, a novel
network for colon polyp segmentation that enhances high-frequency information
processing through a global-local vision transformer framework. HiFiSeg
leverages the pyramid vision transformer (PVT) as its encoder and introduces
two key modules: the global-local interaction module (GLIM) and the selective
aggregation module (SAM). GLIM employs a parallel structure to fuse global and
local information at multiple scales, effectively capturing fine-grained
features. SAM selectively integrates boundary details from low-level features
with semantic information from high-level features, significantly improving the
model's ability to accurately detect and segment polyps. Extensive experiments
on five widely recognized benchmark datasets demonstrate the effectiveness of
HiFiSeg for polyp segmentation. Notably, the mDice scores on the challenging
CVC-ColonDB and ETIS datasets reached 0.826 and 0.822, respectively,
underscoring the superior performance of HiFiSeg in handling the specific
complexities of this task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Offline Foundation Features with Tensor Augmentations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emir Konuk, Christos Matsoukas, Moein Sorkhei, Phitchapha Lertsiravaramet, Kevin Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Learning from Offline Foundation Features with Tensor
Augmentations (LOFF-TA), an efficient training scheme designed to harness the
capabilities of foundation models in limited resource settings where their
direct development is not feasible. LOFF-TA involves training a compact
classifier on cached feature embeddings from a frozen foundation model,
resulting in up to $37\times$ faster training and up to $26\times$ reduced GPU
memory usage. Because the embeddings of augmented images would be too numerous
to store, yet the augmentation process is essential for training, we propose to
apply tensor augmentations to the cached embeddings of the original
non-augmented images. LOFF-TA makes it possible to leverage the power of
foundation models, regardless of their size, in settings with limited
computational capacity. Moreover, LOFF-TA can be used to apply foundation
models to high-resolution images without increasing compute. In certain
scenarios, we find that training with LOFF-TA yields better results than
directly fine-tuning the foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Med-TTT: Vision Test-Time Training model for Medical <span class="highlight-title">Image</span> Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation plays a crucial role in clinical diagnosis and
treatment planning. Although models based on convolutional neural networks
(CNNs) and Transformers have achieved remarkable success in medical image
segmentation tasks, they still face challenges such as high computational
complexity and the loss of local features when capturing long-range
dependencies. To address these limitations, we propose Med-TTT, a visual
backbone network integrated with Test-Time Training (TTT) layers, which
incorporates dynamic adjustment capabilities. Med-TTT introduces the Vision-TTT
layer, which enables effective modeling of long-range dependencies with linear
computational complexity and adaptive parameter adjustment during inference.
Furthermore, we designed a multi-resolution fusion mechanism to combine image
features at different scales, facilitating the identification of subtle lesion
characteristics in complex backgrounds. At the same time, we adopt a frequency
domain feature enhancement strategy based on high pass filtering, which can
better capture texture and fine-grained details in images. Experimental results
demonstrate that Med-TTT significantly outperforms existing methods on multiple
medical image datasets, exhibiting strong segmentation capabilities,
particularly in complex image backgrounds. The model achieves leading
performance in terms of accuracy, sensitivity, and Dice coefficient, providing
an efficient and robust solution for the field of medical image
segmentation.The code is available at https://github.com/Jiashu-Xu/Med-TTT .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dog-IQA: Standard-guided Zero-shot <span class="highlight-title">MLLM</span> for Mix-grained <span class="highlight-title">Image</span> Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Liu, Ziqing Zhang, Wenbo Li, Renjing Pei, Fenglong Song, Xiaohong Liu, Linghe Kong, Yulun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality assessment (IQA) serves as the golden standard for all models'
performance in nearly all computer vision fields. However, it still suffers
from poor out-of-distribution generalization ability and expensive training
costs. To address these problems, we propose Dog-IQA, a standard-guided
zero-shot mix-grained IQA method, which is training-free and utilizes the
exceptional prior knowledge of multimodal large language models (MLLMs). To
obtain accurate IQA scores, namely scores consistent with humans, we design an
MLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA
applies two techniques. First, Dog-IQA objectively scores with specific
standards that utilize MLLM's behavior pattern and minimize the influence of
subjective factors. Second, Dog-IQA comprehensively takes local semantic
objects and the whole image as input and aggregates their scores, leveraging
local and global information. Our proposed Dog-IQA achieves state-of-the-art
(SOTA) performance compared with training-free methods, and competitive
performance compared with training-based methods in cross-dataset scenarios.
Our code and models will be available at https://github.com/Kai-Liu001/Dog-IQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures. The code and models will be available at
  https://github.com/Kai-Liu001/Dog-IQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DTVLT: A Multi-modal Diverse Text <span class="highlight-title">Benchmark</span> for Visual Language Tracking
  Based on <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language tracking (VLT) has emerged as a cutting-edge research area,
harnessing linguistic data to enhance algorithms with multi-modal inputs and
broadening the scope of traditional single object tracking (SOT) to encompass
video understanding applications. Despite this, most VLT benchmarks still
depend on succinct, human-annotated text descriptions for each video. These
descriptions often fall short in capturing the nuances of video content
dynamics and lack stylistic variety in language, constrained by their uniform
level of detail and a fixed annotation frequency. As a result, algorithms tend
to default to a "memorize the answer" strategy, diverging from the core
objective of achieving a deeper understanding of video content. Fortunately,
the emergence of large language models (LLMs) has enabled the generation of
diverse text. This work utilizes LLMs to generate varied semantic annotations
(in terms of text lengths and granularities) for representative SOT benchmarks,
thereby establishing a novel multi-modal benchmark. Specifically, we (1)
propose a new visual language tracking benchmark with diverse texts, named
DTVLT, based on five prominent VLT and SOT benchmarks, including three
sub-tasks: short-term tracking, long-term tracking, and global instance
tracking. (2) We offer four granularity texts in our benchmark, considering the
extent and density of semantic information. We expect this multi-granular
generation strategy to foster a favorable environment for VLT and video
understanding research. (3) We conduct comprehensive experimental analyses on
DTVLT, evaluating the impact of diverse text on tracking performance and hope
the identified performance bottlenecks of existing algorithms can support
further research in VLT and video understanding. The proposed benchmark,
experimental results and toolkit will be released gradually on
http://videocube.aitestunion.com/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-Customized <span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Wang, Yilei Jiang, Dong Zheng, Jun Xiao, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized Image Generation, generating customized images with user-specified
concepts, has raised significant attention due to its creativity and novelty.
With impressive progress achieved in subject customization, some pioneer works
further explored the customization of action and interaction beyond entity
(i.e., human, animal, and object) appearance. However, these approaches only
focus on basic actions and interactions between two entities, and their effects
are limited by insufficient ''exactly same'' reference images. To extend
customized image generation to more complex scenes for general real-world
applications, we propose a new task: event-customized image generation. Given a
single reference image, we define the ''event'' as all specific actions, poses,
relations, or interactions between different entities in the scene. This task
aims at accurately capturing the complex event and generating customized images
with various target entities. To solve this task, we proposed a novel
training-free event customization method: FreeEvent. Specifically, FreeEvent
introduces two extra paths alongside the general diffusion denoising process:
1) Entity switching path: it applies cross-attention guidance and regulation
for target entity generation. 2) Event transferring path: it injects the
spatial feature and self-attention maps from the reference image to the target
image for event generation. To further facilitate this new task, we collected
two evaluation benchmarks: SWiG-Event and Real-Event. Extensive experiments and
ablations have demonstrated the effectiveness of FreeEvent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Towards a Theoretical Understanding of Memorization in <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Chen, Xingjun Ma, Difan Zou, <span class="highlight-author">Yu-Gang Jiang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As diffusion probabilistic models (DPMs) are being employed as mainstream
models for Generative Artificial Intelligence (GenAI), the study of their
memorization of training data has attracted growing attention. Existing works
in this direction aim to establish an understanding of whether or to what
extent DPMs learn via memorization. Such an understanding is crucial for
identifying potential risks of data leakage and copyright infringement in
diffusion models and, more importantly, for trustworthy application of GenAI.
Existing works revealed that conditional DPMs are more prone to training data
memorization than unconditional DPMs, and the motivated data extraction methods
are mostly for conditional DPMs. However, these understandings are primarily
empirical, and extracting training data from unconditional models has been
found to be extremely challenging. In this work, we provide a theoretical
understanding of memorization in both conditional and unconditional DPMs under
the assumption of model convergence. Our theoretical analysis indicates that
extracting data from unconditional models can also be effective by constructing
a proper surrogate condition. Based on this result, we propose a novel data
extraction method named \textbf{Surrogate condItional Data Extraction (SIDE)}
that leverages a time-dependent classifier trained on the generated data as a
surrogate condition to extract training data from unconditional DPMs. Empirical
results demonstrate that our SIDE can extract training data in challenging
scenarios where previous methods fail, and it is, on average, over 50\% more
effective across different scales of the CelebA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2406.12752</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recurrent Few-Shot model for Document Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Talarmain, Carlos Boned, Sanket Biswas, Oriol Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose ID, or travel, document image- and video-based verification
systems have yet to achieve good enough performance to be considered a solved
problem. There are several factors that negatively impact their performance,
including low-resolution images and videos and a lack of sufficient data to
train the models. This task is particularly challenging when dealing with
unseen class of ID, or travel, documents. In this paper we address this task by
proposing a recurrent-based model able to detect forged documents in a few-shot
scenario. The recurrent architecture makes the model robust to document
resolution variability. Moreover, the few-shot approach allow the model to
perform well even for unseen class of documents. Preliminary results on the
SIDTD and Findit datasets show good performance of this model for this task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinnova Federated Learning Proof of Concept: Key Takeaways from a
  Cross-border Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Alekseenko, Bram Stieltjes, Michael Bach, Melanie Boerries, Oliver Opitz, Alexandros Karargyris, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinnova, a collaborative initiative involving France, Germany, Switzerland,
and Luxembourg, is dedicated to unlocking the power of precision medicine
through data federation, standardization, and interoperability. This European
Greater Region initiative seeks to create an interoperable European standard
using artificial intelligence (AI) and data science to enhance healthcare
outcomes and efficiency. Key components include multidisciplinary research
centers, a federated biobanking strategy, a digital health innovation platform,
and a federated AI strategy. It targets inflammatory bowel disease, rheumatoid
diseases, and multiple sclerosis (MS), emphasizing data quality to develop AI
algorithms for personalized treatment and translational research.
  The IHU Strasbourg (Institute of Minimal-invasive Surgery) has the lead in
this initiative to develop the federated learning (FL) proof of concept (POC)
that will serve as a foundation for advancing AI in healthcare. At its core,
Clinnova-MS aims to enhance MS patient care by using FL to develop more
accurate models that detect disease progression, guide interventions, and
validate digital biomarkers across multiple sites. This technical report
presents insights and key takeaways from the first cross-border federated POC
on MS segmentation of MRI images within the Clinnova framework. While our work
marks a significant milestone in advancing MS segmentation through cross-border
collaboration, it also underscores the importance of addressing technical,
logistical, and ethical considerations to realize the full potential of FL in
healthcare settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Attractor Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramy Mounir, Sudeep Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential memory, the ability to form and accurately recall a sequence of
events or stimuli in the correct order, is a fundamental prerequisite for
biological and artificial intelligence as it underpins numerous cognitive
functions (e.g., language comprehension, planning, episodic memory formation,
etc.) However, existing methods of sequential memory suffer from catastrophic
forgetting, limited capacity, slow iterative learning procedures, low-order
Markov memory, and, most importantly, the inability to represent and generate
multiple valid future possibilities stemming from the same context. Inspired by
biologically plausible neuroscience theories of cognition, we propose
\textit{Predictive Attractor Models (PAM)}, a novel sequence memory
architecture with desirable generative properties. PAM is a streaming model
that learns a sequence in an online, continuous manner by observing each input
\textit{only once}. Additionally, we find that PAM avoids catastrophic
forgetting by uniquely representing past context through lateral inhibition in
cortical minicolumns, which prevents new memories from overwriting previously
learned knowledge. PAM generates future predictions by sampling from a union
set of predicted possibilities; this generative ability is realized through an
attractor model trained alongside the predictor. We show that PAM is trained
with local computations through Hebbian plasticity rules in a biologically
plausible framework. Other desirable traits (e.g., noise tolerance, CPU-based
learning, capacity scaling) are discussed throughout the paper. Our findings
suggest that PAM represents a significant step forward in the pursuit of
biologically plausible and computationally efficient sequential memory models,
with broad implications for cognitive science and artificial intelligence
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PnP-Flow: Plug-and-Play <span class="highlight-title">Image</span> Restoration with Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ségolène Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm
for solving imaging inverse problems. PnP methods leverage the strength of
pre-trained denoisers, often deep neural networks, by integrating them in
optimization schemes. While they achieve state-of-the-art performance on
various inverse problems in imaging, PnP approaches face inherent limitations
on more generative tasks like inpainting. On the other hand, generative models
such as Flow Matching pushed the boundary in image sampling yet lack a clear
method for efficient use in image restoration. We propose to combine the PnP
framework with Flow Matching (FM) by defining a time-dependent denoiser using a
pre-trained FM model. Our algorithm alternates between gradient descent steps
on the data-fidelity term, reprojections onto the learned FM path, and
denoising. Notably, our method is computationally efficient and
memory-friendly, as it avoids backpropagation through ODEs and trace
computations. We evaluate its performance on denoising, super-resolution,
deblurring, and inpainting tasks, demonstrating superior results compared to
existing PnP algorithms and Flow Matching based state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoGDesc: Local geometric features aggregation for robust point cloud
  registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Slimani, Brahim Tamadazte, Catherine Achard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new hybrid descriptor for 3D point matching and point
cloud registration, combining local geometrical properties and learning-based
feature propagation for each point's neighborhood structure description. The
proposed architecture first extracts prior geometrical information by computing
each point's planarity, anisotropy, and omnivariance using a Principal
Components Analysis (PCA). This prior information is completed by a descriptor
based on the normal vectors estimated thanks to constructing a neighborhood
based on triangles. The final geometrical descriptor is propagated between the
points using local graph convolutions and attention mechanisms. The new feature
extractor is evaluated on ModelNet40, Bunny Stanford dataset, KITTI and MVP
(Multi-View Partial)-RG for point cloud registration and shows interesting
results, particularly on noisy and low overlapping point clouds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eliminating Oversaturation and Artifacts of High Guidance Scales in
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedmorteza Sadat, Otmar Hilliges, Romann M. Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier-free guidance (CFG) is crucial for improving both generation
quality and alignment between the input condition and final output in diffusion
models. While a high guidance scale is generally required to enhance these
aspects, it also causes oversaturation and unrealistic artifacts. In this
paper, we revisit the CFG update rule and introduce modifications to address
this issue. We first decompose the update term in CFG into parallel and
orthogonal components with respect to the conditional model prediction and
observe that the parallel component primarily causes oversaturation, while the
orthogonal component enhances image quality. Accordingly, we propose
down-weighting the parallel component to achieve high-quality generations
without oversaturation. Additionally, we draw a connection between CFG and
gradient ascent and introduce a new rescaling and momentum method for the CFG
update rule based on this insight. Our approach, termed adaptive projected
guidance (APG), retains the quality-boosting advantages of CFG while enabling
the use of higher guidance scales without oversaturation. APG is easy to
implement and introduces practically no additional computational overhead to
the sampling process. Through extensive experiments, we demonstrate that APG is
compatible with various conditional diffusion models and samplers, leading to
improved FID, recall, and saturation scores while maintaining precision
comparable to CFG, making our method a superior plug-and-play alternative to
standard classifier-free guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynCo: Synthetic Hard Negatives in Contrastive Learning for Better
  Unsupervised Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a dominant approach in self-supervised visual
representation learning, with hard negatives-samples that closely resemble the
anchor-being key to enhancing the discriminative power of learned
representations. However, efficiently leveraging hard negatives remains a
challenge due to the difficulty in identifying and incorporating them without
significantly increasing computational costs. To address this, we introduce
SynCo (Synthetic Negatives in Contrastive learning), a novel contrastive
learning approach that improves model performance by generating synthetic hard
negatives. Built on the MoCo framework, SynCo introduces six novel strategies
for creating diverse synthetic hard negatives that can be generated on-the-fly
with minimal computational overhead. SynCo achieves faster training and better
representation learning, achieving a top-1 accuracy of 68.1% in ImageNet linear
evaluation after only 200 epochs on pretraining, surpassing MoCo's 67.5% with
the same ResNet-50 encoder. Additionally, it transfers more effectively to
detection tasks: on the PASCAL VOC, it outperforms both the supervised baseline
and MoCo, achieving an AP of 82.5%; on the COCO dataset, it sets a new
benchmark with 40.4% AP for bounding box detection and 35.4% AP for instance
segmentation. Our synthetic hard negative generation procedure significantly
enhances the quality of visual representations learned through self-supervised
contrastive learning. Code is available at
https://github.com/giakoumoglou/synco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:2010.01028 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter Competition Balancing for Model Merging <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While fine-tuning pretrained models has become common practice, these models
often underperform outside their specific domains. Recently developed model
merging techniques enable the direct integration of multiple models, each
fine-tuned for distinct tasks, into a single model. This strategy promotes
multitasking capabilities without requiring retraining on the original
datasets. However, existing methods fall short in addressing potential
conflicts and complex correlations between tasks, especially in parameter-level
adjustments, posing a challenge in effectively balancing parameter competition
across various tasks. This paper introduces an innovative technique named
PCB-Merging (Parameter Competition Balancing), a lightweight and training-free
technique that adjusts the coefficients of each parameter for effective model
merging. PCB-Merging employs intra-balancing to gauge parameter significance
within individual tasks and inter-balancing to assess parameter similarities
across different tasks. Parameters with low importance scores are dropped, and
the remaining ones are rescaled to form the final merged model. We assessed our
approach in diverse merging scenarios, including cross-task, cross-domain, and
cross-training configurations, as well as out-of-domain generalization. The
experimental results reveal that our approach achieves substantial performance
enhancements across multiple modalities, domains, model sizes, number of tasks,
fine-tuning forms, and large language models, outperforming existing model
merging methods. The code is publicly available at:
\url{https://github.com/duguodong7/pcb-merging}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaMetrics: Calibrating Metrics For <span class="highlight-title">Generation</span> Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Potential of the <span class="highlight-title">Diffusion</span> Model in Few-shot Semantic
  Segmentation <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muzhi Zhu, Yang Liu, Zekai Luo, Chenchen Jing, Hao Chen, Guangkai Xu, Xinlong Wang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Diffusion Model has not only garnered noteworthy achievements in the
realm of image generation but has also demonstrated its potential as an
effective pretraining method utilizing unlabeled data. Drawing from the
extensive potential unveiled by the Diffusion Model in both semantic
correspondence and open vocabulary segmentation, our work initiates an
investigation into employing the Latent Diffusion Model for Few-shot Semantic
Segmentation. Recently, inspired by the in-context learning ability of large
language models, Few-shot Semantic Segmentation has evolved into In-context
Segmentation tasks, morphing into a crucial element in assessing generalist
segmentation models. In this context, we concentrate on Few-shot Semantic
Segmentation, establishing a solid foundation for the future development of a
Diffusion-based generalist model for segmentation. Our initial focus lies in
understanding how to facilitate interaction between the query image and the
support image, resulting in the proposal of a KV fusion method within the
self-attention framework. Subsequently, we delve deeper into optimizing the
infusion of information from the support mask and simultaneously re-evaluating
how to provide reasonable supervision from the query mask. Based on our
analysis, we establish a simple and effective framework named DiffewS,
maximally retaining the original Latent Diffusion Model's generative framework
and effectively utilizing the pre-training prior. Experimental results
demonstrate that our method significantly outperforms the previous SOTA models
in multiple settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Proc. Annual Conference on Neural Information Processing
  Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> of Mamba Architectures for Medical <span class="highlight-title">Image</span>
  Analysis: Classification, Segmentation, Restoration and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhi Bansal, Sreeharish A, Madhava Prasath J, Manikandan S, Sreekanth Madisetty, Mohammad Zia Ur Rehman, Chandravardhan Singh Raghaw, Gaurav Duggal, Nagendra Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mamba, a special case of the State Space Model, is gaining popularity as an
alternative to template-based deep learning approaches in medical image
analysis. While transformers are powerful architectures, they have drawbacks,
including quadratic computational complexity and an inability to address
long-range dependencies efficiently. This limitation affects the analysis of
large and complex datasets in medical imaging, where there are many spatial and
temporal relationships. In contrast, Mamba offers benefits that make it
well-suited for medical image analysis. It has linear time complexity, which is
a significant improvement over transformers. Mamba processes longer sequences
without attention mechanisms, enabling faster inference and requiring less
memory. Mamba also demonstrates strong performance in merging multimodal data,
improving diagnosis accuracy and patient outcomes. The organization of this
paper allows readers to appreciate the capabilities of Mamba in medical imaging
step by step. We begin by defining core concepts of SSMs and models, including
S4, S5, and S6, followed by an exploration of Mamba architectures such as pure
Mamba, U-Net variants, and hybrid models with convolutional neural networks,
transformers, and Graph Neural Networks. We also cover Mamba optimizations,
techniques and adaptations, scanning, datasets, applications, experimental
results, and conclude with its challenges and future directions in medical
imaging. This review aims to demonstrate the transformative potential of Mamba
in overcoming existing barriers within medical imaging while paving the way for
innovative advancements in the field. A comprehensive list of Mamba
architectures applied in the medical field, reviewed in this work, is available
at Github.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProtoSeg: A Prototype-Based Point Cloud Instance Segmentation Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remco Royen, Leon Denis, Adrian Munteanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D instance segmentation is crucial for obtaining an understanding of a point
cloud scene. This paper presents a novel neural network architecture for
performing instance segmentation on 3D point clouds. We propose to jointly
learn coefficients and prototypes in parallel which can be combined to obtain
the instance predictions. The coefficients are computed using an overcomplete
set of sampled points with a novel multi-scale module, dubbed dilated point
inception. As the set of obtained instance mask predictions is overcomplete, we
employ a non-maximum suppression algorithm to retrieve the final predictions.
This approach allows to omit the time-expensive clustering step and leads to a
more stable inference time. The proposed method is not only 28% faster than the
state-of-the-art, it also exhibits the lowest standard deviation. Our
experiments have shown that the standard deviation of the inference time is
only 1.0% of the total time while it ranges between 10.8 and 53.1% for the
state-of-the-art methods. Lastly, our method outperforms the state-of-the-art
both on S3DIS-blocks (4.9% in mRec on Fold-5) and PartNet (2.0% on average in
mAP).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-eXplainable AI for Medical <span class="highlight-title">Image</span> Analysis: A <span class="highlight-title">Survey</span> and New
  Outlooks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Hou, Sicen Liu, Yequan Bie, Hongmei Wang, Andong Tan, Luyang Luo, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for transparent and reliable models, particularly in
high-stakes decision-making areas such as medical image analysis, has led to
the emergence of eXplainable Artificial Intelligence (XAI). Post-hoc XAI
techniques, which aim to explain black-box models after training, have been
controversial in recent works concerning their fidelity to the models'
predictions. In contrast, Self-eXplainable AI (S-XAI) offers a compelling
alternative by incorporating explainability directly into the training process
of deep learning models. This approach allows models to generate inherent
explanations that are closely aligned with their internal decision-making
processes. Such enhanced transparency significantly supports the
trustworthiness, robustness, and accountability of AI systems in real-world
medical applications. To facilitate the development of S-XAI methods for
medical image analysis, this survey presents an comprehensive review across
various image modalities and clinical applications. It covers more than 200
papers from three key perspectives: 1) input explainability through the
integration of explainable feature engineering and knowledge graph, 2) model
explainability via attention-based learning, concept-based learning, and
prototype-based learning, and 3) output explainability by providing
counterfactual explanation and textual explanation. Additionally, this paper
outlines the desired characteristics of explainability and existing evaluation
methods for assessing explanation quality. Finally, it discusses the major
challenges and future research directions in developing S-XAI for medical image
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RESSCAL3D++: Joint Acquisition and Semantic Segmentation of 3D Point
  Clouds <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remco Royen, Kostas Pataridis, Ward van der Tempel, Adrian Munteanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D scene understanding is crucial for facilitating seamless interaction
between digital devices and the physical world. Real-time capturing and
processing of the 3D scene are essential for achieving this seamless
integration. While existing approaches typically separate acquisition and
processing for each frame, the advent of resolution-scalable 3D sensors offers
an opportunity to overcome this paradigm and fully leverage the otherwise
wasted acquisition time to initiate processing. In this study, we introduce
VX-S3DIS, a novel point cloud dataset accurately simulating the behavior of a
resolution-scalable 3D sensor. Additionally, we present RESSCAL3D++, an
important improvement over our prior work, RESSCAL3D, by incorporating an
update module and processing strategy. By applying our method to the new
dataset, we practically demonstrate the potential of joint acquisition and
semantic segmentation of 3D point clouds. Our resolution-scalable approach
significantly reduces scalability costs from 2% to just 0.2% in mIoU while
achieving impressive speed-ups of 15.6 to 63.9% compared to the non-scalable
baseline. Furthermore, our scalable approach enables early predictions, with
the first one occurring after only 7% of the total inference time of the
baseline. The new VX-S3DIS dataset is available at
https://github.com/remcoroyen/vx-s3dis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Image Processing (ICIP). IEEE,
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTARR: A fast and robust method for identifying anatomical regions on CT
  <span class="highlight-title">image</span>s via atlas registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Buddenkotte, Roland Opfer, Julia Krüger, Alessa Hering, Mireia Crispin-Ortuzar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image analysis tasks often focus on regions or structures located in
a particular location within the patient's body. Often large parts of the image
may not be of interest for the image analysis task. When using deep-learning
based approaches, this causes an unnecessary increases the computational burden
during inference and raises the chance of errors. In this paper, we introduce
CTARR, a novel generic method for CT Anatomical Region Recognition. The method
serves as a pre-processing step for any deep learning-based CT image analysis
pipeline by automatically identifying the pre-defined anatomical region that is
relevant for the follow-up task and removing the rest. It can be used in (i)
image segmentation to prevent false positives in anatomically implausible
regions and speeding up the inference, (ii) image classification to produce
image crops that are consistent in their anatomical context, and (iii) image
registration by serving as a fast pre-registration step. Our proposed method is
based on atlas registration and provides a fast and robust way to crop any
anatomical region encoded as one or multiple bounding box(es) from any
unlabeled CT scan of the brain, chest, abdomen and/or pelvis. We demonstrate
the utility and robustness of the proposed method in the context of medical
image segmentation by evaluating it on six datasets of public segmentation
challenges. The foreground voxels in the regions of interest are preserved in
the vast majority of cases and tasks (97.45-100%) while taking only fractions
of a seconds to compute (0.1-0.21s) on a deep learning workstation and greatly
reducing the segmentation runtime (2.0-12.7x). Our code is available at
https://github.com/ThomasBudd/ctarr.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling Layout from Glyph in Online Chinese Handwriting <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren-Min Si, Yan-Ming Zhang, Yi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text plays a crucial role in the transmission of human civilization, and
teaching machines to generate online handwritten text in various styles
presents an interesting and significant challenge. However, most prior work has
concentrated on generating individual Chinese fonts, leaving {complete text
line generation largely unexplored}. In this paper, we identify that text lines
can naturally be divided into two components: layout and glyphs. Based on this
division, we designed a text line layout generator coupled with a
diffusion-based stylized font synthesizer to address this challenge
hierarchically. More concretely, the layout generator performs in-context-like
learning based on the text content and the provided style references to
generate positions for each glyph autoregressively. Meanwhile, the font
synthesizer which consists of a character embedding dictionary, a multi-scale
calligraphy style encoder, and a 1D U-Net based diffusion denoiser will
generate each font on its position while imitating the calligraphy style
extracted from the given style references. Qualitative and quantitative
experiments on the CASIA-OLHWDB demonstrate that our method is capable of
generating structurally correct and indistinguishable imitation samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Comparison of Individual Cat Recognition Using Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Li, Kai Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial recognition using deep learning has been widely used in social life
for applications such as authentication, smart door locks, and photo grouping,
etc. More and more networks have been developed to facilitate computer vision
tasks, such as ResNet, DenseNet, EfficientNet, ConvNeXt, and Siamese networks.
However, few studies have systematically compared the advantages and
disadvantages of such neural networks in identifying individuals from images,
especially for pet animals like cats. In the present study, by systematically
comparing the efficacy of different neural networks in cat recognition, we
found traditional CNNs trained with transfer learning have better performance
than models trained with the fine-tuning method or Siamese networks in
individual cat recognition. In addition, ConvNeXt and DenseNet yield
significant results which could be further optimized for individual cat
recognition in pet stores and in the wild. These results provide a method to
improve cat management in pet stores and monitoring of cats in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Method for Accurate & Real-time Food Classification: The
  Synergistic Integration of EfficientNetB7, CBAM, Transfer Learning, and Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Rokhva, Babak Teimourpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating artificial intelligence into modern society is profoundly
transformative, significantly enhancing productivity by streamlining various
daily tasks. AI-driven recognition systems provide notable advantages in the
food sector, including improved nutrient tracking, tackling food waste, and
boosting food production and consumption efficiency. Accurate food
classification is a crucial initial step in utilizing advanced AI models, as
the effectiveness of this process directly influences the success of subsequent
operations; therefore, achieving high accuracy at a reasonable speed is
essential. Despite existing research efforts, a gap persists in improving
performance while ensuring rapid processing times, prompting researchers to
pursue cost-effective and precise models. This study addresses this gap by
employing the state-of-the-art EfficientNetB7 architecture, enhanced through
transfer learning, data augmentation, and the CBAM attention module. This
methodology results in a robust model that surpasses previous studies in
accuracy while maintaining rapid processing suitable for real-world
applications. The Food11 dataset from Kaggle was utilized, comprising 16643
imbalanced images across 11 diverse classes with significant intra-category
diversities and inter-category similarities. Furthermore, the proposed
methodology, bolstered by various deep learning techniques, consistently
achieves an impressive average accuracy of 96.40%. Notably, it can classify
over 60 images within one second during inference on unseen data, demonstrating
its ability to deliver high accuracy promptly. This underscores its potential
for practical applications in accurate food classification and enhancing
efficiency in subsequent processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, six figures, two tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computer-aided Colorization State-of-the-science: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Cao, Xin Duan, Xiangqiao Meng, P. Y. Mok, Ping Li, Tong-Yee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews published research in the field of computer-aided
colorization technology. We argue that the colorization task originates from
computer graphics, prospers by introducing computer vision, and tends to the
fusion of vision and graphics, so we put forward our taxonomy and organize the
whole paper chronologically. We extend the existing reconstruction-based
colorization evaluation techniques, considering that aesthetic assessment of
colored images should be introduced to ensure that colorization satisfies human
visual-related requirements and emotions more closely. We perform the
colorization aesthetic assessment on seven representative unconditional
colorization models and discuss the difference between our assessment and the
existing reconstruction-based metrics. Finally, this paper identifies
unresolved issues and proposes fruitful areas for future research and
development. Access to the project associated with this survey can be obtained
at https://github.com/DanielCho-HK/Colorization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structural-Entropy-Based Sample Selection for Efficient and Effective
  Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchi Xie, Jiangning Zhu, Guozu Ma, Minzhi Lin, Wei Chen, Weikai Yang, Shixia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample selection improves the efficiency and effectiveness of machine
learning models by providing informative and representative samples. Typically,
samples can be modeled as a sample graph, where nodes are samples and edges
represent their similarities. Most existing methods are based on local
information, such as the training difficulty of samples, thereby overlooking
global information, such as connectivity patterns. This oversight can result in
suboptimal selection because global information is crucial for ensuring that
the selected samples well represent the structural properties of the graph. To
address this issue, we employ structural entropy to quantify global information
and losslessly decompose it from the whole graph to individual nodes using the
Shapley value. Based on the decomposition, we present
$\textbf{S}$tructural-$\textbf{E}$ntropy-based sample $\textbf{S}$election
($\textbf{SES}$), a method that integrates both global and local information to
select informative and representative samples. SES begins by constructing a
$k$NN-graph among samples based on their similarities. It then measures sample
importance by combining structural entropy (global metric) with training
difficulty (local metric). Finally, SES applies importance-biased blue noise
sampling to select a set of diverse and representative samples. Comprehensive
experiments on three learning scenarios -- supervised learning, active
learning, and continual learning -- clearly demonstrate the effectiveness of
our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic road classification in historical maps using synthetic
  data and deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik J. Mühlematter, Sebastian Schweizer, Chenjing Jiao, Xue Xia, Magnus Heitzler, Lorenz Hurni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historical maps are invaluable for analyzing long-term changes in
transportation and spatial development, offering a rich source of data for
evolutionary studies. However, digitizing and classifying road networks from
these maps is often expensive and time-consuming, limiting their widespread
use. Recent advancements in deep learning have made automatic road extraction
from historical maps feasible, yet these methods typically require large
amounts of labeled training data. To address this challenge, we introduce a
novel framework that integrates deep learning with geoinformation,
computer-based painting, and image processing methodologies. This framework
enables the extraction and classification of roads from historical maps using
only road geometries without needing road class labels for training. The
process begins with training of a binary segmentation model to extract road
geometries, followed by morphological operations, skeletonization,
vectorization, and filtering algorithms. Synthetic training data is then
generated by a painting function that artificially re-paints road segments
using predefined symbology for road classes. Using this synthetic data, a deep
ensemble is trained to generate pixel-wise probabilities for road classes to
mitigate distribution shift. These predictions are then discretized along the
extracted road geometries. Subsequently, further processing is employed to
classify entire roads, enabling the identification of potential changes in road
classes and resulting in a labeled road class dataset. Our method achieved
completeness and correctness scores of over 94% and 92%, respectively, for road
class 2, the most prevalent class in the two Siegfried Map sheets from
Switzerland used for testing. This research offers a powerful tool for urban
planning and transportation decision-making by efficiently extracting and
classifying roads from historical maps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spiking Neural Network as Adaptive Event Stream Slicer <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahang Cao, Mingyuan Sun, Ziqing Wang, Hao Cheng, Qiang Zhang, Shibo Zhou, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based cameras are attracting significant interest as they provide rich
edge information, high dynamic range, and high temporal resolution. Many
state-of-the-art event-based algorithms rely on splitting the events into fixed
groups, resulting in the omission of crucial temporal information, particularly
when dealing with diverse motion scenarios (e.g., high/low speed). In this
work, we propose SpikeSlicer, a novel-designed plug-and-play event processing
method capable of splitting events stream adaptively. SpikeSlicer utilizes a
lightweight (0.41M) and low-energy spiking neural network (SNN) to trigger
event slicing. To guide the SNN to fire spikes at optimal time steps, we
propose the Spiking Position-aware Loss (SPA-Loss) to modulate the neuron's
state. Additionally, we develop a Feedback-Update training strategy that
refines the slicing decisions using feedback from the downstream artificial
neural network (ANN). Extensive experiments demonstrate that our method yields
significant performance improvements in event-based object tracking and
recognition. Notably, SpikeSlicer provides a brand-new SNN-ANN cooperation
paradigm, where the SNN acts as an efficient, low-energy data processor to
assist the ANN in improving downstream performance, injecting new perspectives
and potential avenues of exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leopard: A Vision Language Model For Text-Rich Multi-<span class="highlight-title">Image</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Meng Jiang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich images, where text serves as the central visual element guiding the
overall understanding, are prevalent in real-world applications, such as
presentation slides, scanned documents, and webpage snapshots. Tasks involving
multiple text-rich images are especially challenging, as they require not only
understanding the content of individual images but reasoning about
inter-relationships and logical flows across multiple visual inputs. Despite
the importance of these scenarios, current multimodal large language models
(MLLMs) struggle to handle such tasks due to two key challenges: (1) the
scarcity of high-quality instruction tuning datasets for text-rich multi-image
scenarios, and (2) the difficulty in balancing image resolution with visual
feature sequence length. To address these challenges, we propose Leopard, a
MLLM designed specifically for handling vision-language tasks involving
multiple text-rich images. First, we curated about one million high-quality
multimodal instruction-tuning data, tailored to text-rich, multi-image
scenarios. Second, we developed an adaptive high-resolution multi-image
encoding module to dynamically optimize the allocation of visual sequence
length based on the original aspect ratios and resolutions of the input images.
Experiments across a wide range of benchmarks demonstrate our model's superior
capabilities in text-rich, multi-image evaluations and competitive performance
in general domain evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/Jill0001/Leopard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOREL: Enhancing Adversarial Robustness through Multi-Objective
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedjro Salomon Hotegni, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive research has shown that deep neural networks (DNNs) are vulnerable
to slight adversarial perturbations$-$small changes to the input data that
appear insignificant but cause the model to produce drastically different
outputs. In addition to augmenting training data with adversarial examples
generated from a specific attack method, most of the current defense strategies
necessitate modifying the original model architecture components to improve
robustness or performing test-time data purification to handle adversarial
attacks. In this work, we demonstrate that strong feature representation
learning during training can significantly enhance the original model's
robustness. We propose MOREL, a multi-objective feature representation learning
approach, encouraging classification models to produce similar features for
inputs within the same class, despite perturbations. Our training method
involves an embedding space where cosine similarity loss and multi-positive
contrastive loss are used to align natural and adversarial features from the
model encoder and ensure tight clustering. Concurrently, the classifier is
motivated to achieve accurate predictions. Through extensive experiments, we
demonstrate that our approach significantly enhances the robustness of DNNs
against white-box and black-box adversarial attacks, outperforming other
methods that similarly require no architectural changes or test-time data
purification. Our code is available at https://github.com/salomonhotegni/MOREL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Releasing the Parameter Latency of Neural Representation for
  High-Efficiency Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gai Zhang, Xinfeng Zhang, Lv Tang, Yue Li, Kai Zhang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, video compression technology has been a prominent research area.
Traditional hybrid video compression framework and end-to-end frameworks
continue to explore various intra- and inter-frame reference and prediction
strategies based on discrete transforms and deep learning techniques. However,
the emerging implicit neural representation (INR) technique models entire
videos as basic units, automatically capturing intra-frame and inter-frame
correlations and obtaining promising performance. INR uses a compact neural
network to store video information in network parameters, effectively
eliminating spatial and temporal redundancy in the original video. However, in
this paper, our exploration and verification reveal that current INR video
compression methods do not fully exploit their potential to preserve
information. We investigate the potential of enhancing network parameter
storage through parameter reuse. By deepening the network, we designed a
feasible INR parameter reuse scheme to further improve compression performance.
Extensive experimental results show that our method significantly enhances the
rate-distortion performance of INR video compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMOD: A Large <span class="highlight-title">Multimodal</span> Ophthalmology <span class="highlight-title">Dataset</span> and <span class="highlight-title">Benchmark</span> for Large
  <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyue Qin, Yu Yin, Dylan Campbell, Xuansheng Wu, Ke Zou, Yih-Chung Tham, Ninghao Liu, Xiuzhen Zhang, Qingyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ophthalmology relies heavily on detailed image analysis for diagnosis and
treatment planning. While large vision-language models (LVLMs) have shown
promise in understanding complex visual information, their performance on
ophthalmology images remains underexplored. We introduce LMOD, a dataset and
benchmark for evaluating LVLMs on ophthalmology images, covering anatomical
understanding, diagnostic analysis, and demographic extraction. LMODincludes
21,993 images spanning optical coherence tomography, scanning laser
ophthalmoscopy, eye photos, surgical scenes, and color fundus photographs. We
benchmark 13 state-of-the-art LVLMs and find that they are far from perfect for
comprehending ophthalmology images. Models struggle with diagnostic analysis
and demographic extraction, reveal weaknesses in spatial reasoning, diagnostic
analysis, handling out-of-domain queries, and safeguards for handling
biomarkers of ophthalmology images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fake It Until You Break It: On the Adversarial Robustness of
  AI-generated <span class="highlight-title">Image</span> Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Mavali, Jonas Ricker, David Pape, Yash Sharma, Asja Fischer, Lea Schönherr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative AI (GenAI) offers countless possibilities for creative and
productive tasks, artificially generated media can be misused for fraud,
manipulation, scams, misinformation campaigns, and more. To mitigate the risks
associated with maliciously generated media, forensic classifiers are employed
to identify AI-generated content. However, current forensic classifiers are
often not evaluated in practically relevant scenarios, such as the presence of
an attacker or when real-world artifacts like social media degradations affect
images. In this paper, we evaluate state-of-the-art AI-generated image (AIGI)
detectors under different attack scenarios. We demonstrate that forensic
classifiers can be effectively attacked in realistic settings, even when the
attacker does not have access to the target model and post-processing occurs
after the adversarial examples are created, which is standard on social media
platforms. These attacks can significantly reduce detection accuracy to the
extent that the risks of relying on detectors outweigh their benefits. Finally,
we propose a simple defense mechanism to make CLIP-based detectors, which are
currently the best-performing detectors, robust against these attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EUFCC-CIR: a Composed <span class="highlight-title">Image</span> Retrieval <span class="highlight-title">Dataset</span> for GLAM Collections <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesc Net, Lluis Gomez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intersection of Artificial Intelligence and Digital Humanities enables
researchers to explore cultural heritage collections with greater depth and
scale. In this paper, we present EUFCC-CIR, a dataset designed for Composed
Image Retrieval (CIR) within Galleries, Libraries, Archives, and Museums (GLAM)
collections. Our dataset is built on top of the EUFCC-340K image labeling
dataset and contains over 180K annotated CIR triplets. Each triplet is composed
of a multi-modal query (an input image plus a short text describing the desired
attribute manipulations) and a set of relevant target images. The EUFCC-CIR
dataset fills an existing gap in CIR-specific resources for Digital Humanities.
We demonstrate the value of the EUFCC-CIR dataset by highlighting its unique
qualities in comparison to other existing CIR datasets and evaluating the
performance of several zero-shot CIR baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV Workshop (AI4DH2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexuan Ding, Lei Wang, Liyun Zhu, Tom Gedeon, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision tasks, features often come from diverse representations,
domains, and modalities, such as text, images, and videos. Effectively fusing
these features is essential for robust performance, especially with the
availability of powerful pre-trained models like vision-language models.
However, common fusion methods, such as concatenation, element-wise operations,
and non-linear techniques, often fail to capture structural relationships, deep
feature interactions, and suffer from inefficiency or misalignment of features
across domains. In this paper, we shift from high-dimensional feature space to
a lower-dimensional, interpretable graph space by constructing similarity
graphs that encode feature relationships at different levels, e.g., clip,
frame, patch, token, etc. To capture deeper interactions, we use graph power
expansions and introduce a learnable graph fusion operator to combine these
graph powers for more effective fusion. Our approach is relationship-centric,
operates in a homogeneous space, and is mathematically principled, resembling
element-wise similarity score aggregation via multilinear polynomials. We
demonstrate the effectiveness of our graph-based fusion method on video anomaly
detection, showing strong performance across multi-representational,
multi-modal, and multi-domain feature fusion tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Research paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SurgPointTransformer: Vertebrae Shape Completion with RGB-D Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidana Massalimova, Florentin Liebmann, Sascha Jecklin, Fabio Carrillo, Farshad Mazda, Philipp Fürnstahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art computer- and robot-assisted surgery systems heavily depend
on intraoperative imaging technologies such as CT and fluoroscopy to generate
detailed 3D visualization of the patient's anatomy. While imaging techniques
are highly accurate, they are based on ionizing radiation and expose patients
and clinicians. This study introduces an alternative, radiation-free approach
for reconstructing the 3D spine anatomy using RGB-D data. Drawing inspiration
from the 3D "mental map" that surgeons form during surgeries, we introduce
SurgPointTransformer, a shape completion approach for surgical applications
that can accurately reconstruct the unexposed spine regions from sparse
observations of the exposed surface.
  Our method involves two main steps: segmentation and shape completion. The
segmentation step includes spinal column localization and segmentation,
followed by vertebra-wise segmentation. The segmented vertebra point clouds are
then subjected to SurgPointTransformer, which leverages an attention mechanism
to learn patterns between visible surface features and the underlying anatomy.
For evaluation, we utilize an ex-vivo dataset of nine specimens. Their CT data
is used to establish ground truth data that were used to compare to the outputs
of our methods. Our method significantly outperforms the state-of-the-art
baselines, achieving an average Chamfer Distance of 5.39, an F-Score of 0.85,
an Earth Mover's Distance of 0.011, and a Signal-to-Noise Ratio of 22.90 dB.
  This study demonstrates the potential of our reconstruction method for 3D
vertebral shape completion. It enables 3D reconstruction of the entire lumbar
spine and surgical guidance without ionizing radiation or invasive imaging. Our
work contributes to computer-aided and robot-assisted surgery, advancing the
perception and intelligence of these systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMP: Cooperative Motion Prediction with Multi-<span class="highlight-title">Agent</span> Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehao Wang, Yuping Wang, Zhuoyuan Wu, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as model input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies on the OPV2V and V2V4Real datasets, we demonstrate the effectiveness of
our method in cooperative perception, tracking, and motion prediction. In
particular, CMP reduces the average prediction error by 16.4\% with fewer
missing detections compared with the no cooperation setting and by 12.3\%
compared with the strongest baseline. Our work marks a significant step forward
in the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios. The code can be found on the project website:
https://cmp-cooperative-prediction.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://cmp-cooperative-prediction.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NVDS+: Towards Efficient and Versatile Neural Stabilizer for Video Depth
  Estimation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08695v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08695v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Wang, Min Shi, Jiaqi Li, Chaoyi Hong, Zihao Huang, Juewen Peng, Zhiguo Cao, Jianming Zhang, Ke Xian, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video depth estimation aims to infer temporally consistent depth. One
approach is to finetune a single-image model on each video with geometry
constraints, which proves inefficient and lacks robustness. An alternative is
learning to enforce consistency from data, which requires well-designed models
and sufficient video depth data. To address both challenges, we introduce NVDS+
that stabilizes inconsistent depth estimated by various single-image models in
a plug-and-play manner. We also elaborate a large-scale Video Depth in the Wild
(VDW) dataset, which contains 14,203 videos with over two million frames,
making it the largest natural-scene video depth dataset. Additionally, a
bidirectional inference strategy is designed to improve consistency by
adaptively fusing forward and backward predictions. We instantiate a model
family ranging from small to large scales for different applications. The
method is evaluated on VDW dataset and three public benchmarks. To further
prove the versatility, we extend NVDS+ to video semantic segmentation and
several downstream applications like bokeh rendering, novel view synthesis, and
3D reconstruction. Experimental results show that our method achieves
significant improvements in consistency, accuracy, and efficiency. Our work
serves as a solid baseline and data foundation for learning-based video depth
estimation. Code and dataset are available at:
https://github.com/RaymondWang987/NVDS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V1/V2: ICCV 2023 accepted; V3: the journal extension accepted by IEEE
  TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual
  <span class="highlight-title">Editing</span> <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05717v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05717v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Gu, Nanxuan Zhao, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, Yilin Wang, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective editing of personal content holds a pivotal role in enabling
individuals to express their creativity, weaving captivating narratives within
their visual stories, and elevate the overall quality and impact of their
visual content. Therefore, in this work, we introduce SwapAnything, a novel
framework that can swap any objects in an image with personalized concepts
given by the reference, while keeping the context unchanged. Compared with
existing methods for personalized subject swapping, SwapAnything has three
unique advantages: (1) precise control of arbitrary objects and parts rather
than the main subject, (2) more faithful preservation of context pixels, (3)
better adaptation of the personalized concept to the image. First, we propose
targeted variable swapping to apply region control over latent feature maps and
swap masked variables for faithful context preservation and initial semantic
concept swapping. Then, we introduce appearance adaptation, to seamlessly adapt
the semantic concept into the original image in terms of target location,
shape, style, and content during the image generation process. Extensive
results on both human and automatic evaluation demonstrate significant
improvements of our approach over baseline methods on personalized swapping.
Furthermore, SwapAnything shows its precise and faithful swapping abilities
across single object, multiple objects, partial object, and cross-domain
swapping tasks. SwapAnything also achieves great performance on text-based
swapping and tasks beyond swapping such as object insertion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024, 23 pages, 14 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning
  for Volumetric Organ Segmentation <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julio Silva-Rodríguez, Jose Dolz, Ismail Ben Ayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent popularity of foundation models and the pre-train-and-adapt
paradigm, where a large-scale model is transferred to downstream tasks, is
gaining attention for volumetric medical image segmentation. However, current
transfer learning strategies devoted to full fine-tuning for transfer learning
may require significant resources and yield sub-optimal results when the
labeled data of the target task is scarce. This makes its applicability in real
clinical settings challenging since these institutions are usually constrained
on data and computational resources to develop proprietary solutions. To
address this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a
novel and realistic scenario for adapting medical image segmentation foundation
models. This setting considers the key role of both data- and parameter-
efficiency during adaptation. Building on a foundation model pre-trained on
open-access CT organ segmentation sources, we propose leveraging
Parameter-Efficient Fine-Tuning and black-box Adapters to address such
challenges. Furthermore, novel efficient adaptation methodologies are
introduced in this work, which include Spatial black-box Adapters that are more
appropriate for dense prediction tasks and constrained transductive inference,
leveraging task-specific prior knowledge. Our comprehensive transfer learning
experiments confirm the suitability of foundation models in medical image
segmentation and unveil the limitations of popular fine-tuning strategies in
few-shot scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal Extension of MICCAI - MedAGI Workshop 2023. Code in
  https://github.com/jusiro/fewshot-finetuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoregressive Pre-Training on Pixels and Texts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10710v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10710v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Qingyi Liu, Jingwu Xiao, Shuohuan Wang, Yu Sun, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of visual and textual information represents a promising
direction in the advancement of language models. In this paper, we explore the
dual modality of language--both visual and textual--within an autoregressive
framework, pre-trained on both document images and texts. Our method employs a
multimodal training strategy, utilizing visual data through next patch
prediction with a regression head and/or textual data through next token
prediction with a classification head. We focus on understanding the
interaction between these two modalities and their combined impact on model
performance. Our extensive evaluation across a wide range of benchmarks shows
that incorporating both visual and textual data significantly improves the
performance of pixel-based language models. Remarkably, we find that a
unidirectional pixel-based model trained solely on visual data can achieve
comparable results to state-of-the-art bidirectional models on several language
understanding tasks. This work uncovers the untapped potential of integrating
visual and textual modalities for more effective language modeling. We release
our code, data, and model checkpoints at
\url{https://github.com/ernie-research/pixelgpt}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoPhy: Evaluating Physical Commonsense for Video <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in internet-scale video data pretraining have led to the
development of text-to-video generative models that can create high-quality
videos across a broad range of visual concepts, synthesize realistic motions
and render complex objects. Hence, these generative models have the potential
to become general-purpose simulators of the physical world. However, it is
unclear how far we are from this goal with the existing text-to-video
generative models. To this end, we present VideoPhy, a benchmark designed to
assess whether the generated videos follow physical commonsense for real-world
activities (e.g. marbles will roll down when placed on a slanted surface).
Specifically, we curate diverse prompts that involve interactions between
various material types in the physical world (e.g., solid-solid, solid-fluid,
fluid-fluid). We then generate videos conditioned on these captions from
diverse state-of-the-art text-to-video generative models, including open models
(e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our human
evaluation reveals that the existing models severely lack the ability to
generate videos adhering to the given text prompts, while also lack physical
commonsense. Specifically, the best performing model, CogVideoX-5B, generates
videos that adhere to the caption and physical laws for 39.6% of the instances.
VideoPhy thus highlights that the video generative models are far from
accurately simulating the physical world. Finally, we propose an
auto-evaluator, VideoCon-Physics, to assess the performance reliably for the
newly released models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 29 figures, 12 tables. Added CogVideo and Dream Machine in
  v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing Medical <span class="highlight-title">Image</span> Representations via Quaternion Wavelet
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10224v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10224v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network generalizability is becoming a broad research field due to the
increasing availability of datasets from different sources and for various
tasks. This issue is even wider when processing medical data, where a lack of
methodological standards causes large variations being provided by different
imaging centers or acquired with various devices and cofactors. To overcome
these limitations, we introduce a novel, generalizable, data- and task-agnostic
framework able to extract salient features from medical images. The proposed
quaternion wavelet network (QUAVE) can be easily integrated with any
pre-existing medical image analysis or synthesis task, and it can be involved
with real, quaternion, or hypercomplex-valued models, generalizing their
adoption to single-channel data. QUAVE first extracts different sub-bands
through the quaternion wavelet transform, resulting in both
low-frequency/approximation bands and high-frequency/fine-grained features.
Then, it weighs the most representative set of sub-bands to be involved as
input to any other neural model for image processing, replacing standard data
samples. We conduct an extensive experimental evaluation comprising different
datasets, diverse image analysis, and synthesis tasks including reconstruction,
segmentation, and modality translation. We also evaluate QUAVE in combination
with both real and quaternion-valued models. Results demonstrate the
effectiveness and the generalizability of the proposed framework that improves
network performance while being flexible to be adopted in manifold scenarios
and robust to domain shifts. The full code is available at:
https://github.com/ispamm/QWT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Perceptual Distance Models by Fitting Binomial Distributions
  to Two-Alternative Forced Choice Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Hepburn, Raul Santos-Rodriguez, Javier Portilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The two-alternative forced choice (2AFC) experimental method is popular in
the visual perception literature, where practitioners aim to understand how
human observers perceive distances within triplets made of a reference image
and two distorted versions. In the past, this had been conducted in controlled
environments, with triplets sharing images, so it was possible to rank the
perceived quality. This ranking would then be used to evaluate perceptual
distance models against the experimental data. Recently, crowd-sourced
perceptual datasets have emerged, with no images shared between triplets,
making ranking infeasible. Evaluating perceptual distance models using this
data reduces the judgements on a triplet to a binary decision, namely, whether
the distance model agrees with the human decision - which is suboptimal and
prone to misleading conclusions. Instead, we statistically model the underlying
decision-making process during 2AFC experiments using a binomial distribution.
Having enough empirical data, we estimate a smooth and consistent distribution
of the judgements on the reference-distorted distance plane, according to each
distance model. By applying maximum likelihood, we estimate the parameter of
the local binomial distribution, and a global measurement of the expected
log-likelihood of the measured responses. We calculate meaningful and
well-founded metrics for the distance model, beyond the mere prediction
accuracy as percentage agreement, even with variable numbers of judgements per
triplet -- key advantages over both classical and neural network methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context and Geometry Aware Voxel Transformer for Semantic Scene
  Completion <span class="chip">NIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Yu, Runmin Zhang, Jiacheng Ying, Junchen Yu, Xiaohai Hu, Lun Luo, Si-Yuan Cao, Hui-Liang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based Semantic Scene Completion (SSC) has gained much attention due to
its widespread applications in various 3D perception tasks. Existing
sparse-to-dense approaches typically employ shared context-independent queries
across various input images, which fails to capture distinctions among them as
the focal regions of different inputs vary and may result in undirected feature
aggregation of cross-attention. Additionally, the absence of depth information
may lead to points projected onto the image plane sharing the same 2D position
or similar sampling points in the feature map, resulting in depth ambiguity. In
this paper, we present a novel context and geometry aware voxel transformer. It
utilizes a context aware query generator to initialize context-dependent
queries tailored to individual input images, effectively capturing their unique
characteristics and aggregating information within the region of interest.
Furthermore, it extend deformable cross-attention from 2D to 3D pixel space,
enabling the differentiation of points with similar image coordinates based on
their depth coordinates. Building upon this module, we introduce a neural
network named CGFormer to achieve semantic scene completion. Simultaneously,
CGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost
the semantic and geometric representation abilities of the transformed 3D
volume from both local and global perspectives. Experimental results
demonstrate that CGFormer achieves state-of-the-art performance on the
SemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and
20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer
even outperforms approaches employing temporal images as inputs or much larger
image backbone networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NIPS 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Qwen2-VL: Enhancing <span class="highlight-title">Vision-Language Model</span>'s Perception of the World at
  Any Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL
models that redefines the conventional predetermined-resolution approach in
visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,
which enables the model to dynamically process images of varying resolutions
into different numbers of visual tokens. This approach allows the model to
generate more efficient and accurate visual representations, closely aligning
with human perceptual processes. The model also integrates Multimodal Rotary
Position Embedding (M-RoPE), facilitating the effective fusion of positional
information across text, images, and videos. We employ a unified paradigm for
processing both images and videos, enhancing the model's visual perception
capabilities. To explore the potential of large multimodal models, Qwen2-VL
investigates the scaling laws for large vision-language models (LVLMs). By
scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the
amount of training data, the Qwen2-VL Series achieves highly competitive
performance. Notably, the Qwen2-VL-72B model achieves results comparable to
leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal
benchmarks, outperforming other generalist models. Code is available at
https://github.com/QwenLM/Qwen2-VL .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin
  note: text overlap with arXiv:2408.15262 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BinaryDM: Accurate Weight Binarization for Efficient <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05662v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05662v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Zheng, Xianglong Liu, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of diffusion models (DMs) and the substantially
increased computational requirements, quantization emerges as a practical
solution to obtain compact and efficient low-bit DMs. However, the highly
discrete representation leads to severe accuracy degradation, hindering the
quantization of diffusion models to ultra-low bit-widths. This paper proposes a
novel weight binarization approach for DMs, namely BinaryDM, pushing binarized
DMs to be accurate and efficient by improving the representation and
optimization. From the representation perspective, we present an
Evolvable-Basis Binarizer (EBB) to enable a smooth evolution of DMs from
full-precision to accurately binarized. EBB enhances information representation
in the initial stage through the flexible combination of multiple binary bases
and applies regularization to evolve into efficient single-basis binarization.
The evolution only occurs in the head and tail of the DM architecture to retain
the stability of training. From the optimization perspective, a Low-rank
Representation Mimicking (LRM) is applied to assist the optimization of
binarized DMs. The LRM mimics the representations of full-precision DMs in
low-rank space, alleviating the direction ambiguity of the optimization process
caused by fine-grained alignment. Comprehensive experiments demonstrate that
BinaryDM achieves significant accuracy and efficiency gains compared to SOTA
quantization methods of DMs under ultra-low bit-widths. With 1-bit weight and
4-bit activation (W1A4), BinaryDM achieves as low as 7.74 FID and saves the
performance from collapse (baseline FID 10.87). As the first binarization
method for diffusion models, W1A4 BinaryDM achieves impressive 15.2x OPs and
29.2x model size savings, showcasing its substantial potential for edge
deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is available at https://github.com/Xingyu-Zheng/BinaryDM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning an Actionable Discrete <span class="highlight-title">Diffusion</span> Policy via Large-Scale
  Actionless Video Pre-Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. However, it remains a challenge due to the domain gap between
humans and robots. Moreover, it is difficult to extract useful information
representing the dynamic world from human videos, because of its noisy and
multimodal data structure. In this paper, we introduce a novel framework to
tackle these challenges, which leverages a unified discrete diffusion to
combine generative pre-training on human videos and policy fine-tuning on a
small number of action-labeled robot videos. We start by compressing both human
and robot videos into unified video tokens. In the pre-training stage, we
employ a discrete diffusion model with a mask-and-replace diffusion strategy to
predict future video tokens in the latent space. In the fine-tuning stage, we
harness the imagined future videos to guide low-level action learning with a
limited set of robot data. Experiments demonstrate that our method generates
high-fidelity future videos for planning and enhances the fine-tuned policies
compared to previous state-of-the-art approaches with superior performance. Our
project website is available at https://video-diff.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. 24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARVIS: Motion & Geometry Aware Real and Virtual <span class="highlight-title">Image</span> Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Wu, Xiaomin Lin, Shahriar Negahdaripour, Cornelia Fermüller, Yiannis Aloimonos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tasks such as autonomous navigation, 3D reconstruction, and object
recognition near the water surfaces are crucial in marine robotics
applications. However, challenges arise due to dynamic disturbances, e.g.,
light reflections and refraction from the random air-water interface, irregular
liquid flow, and similar factors, which can lead to potential failures in
perception and navigation systems. Traditional computer vision algorithms
struggle to differentiate between real and virtual image regions, significantly
complicating tasks. A virtual image region is an apparent representation formed
by the redirection of light rays, typically through reflection or refraction,
creating the illusion of an object's presence without its actual physical
location. This work proposes a novel approach for segmentation on real and
virtual image regions, exploiting synthetic images combined with
domain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric
Consistency. Our segmentation network does not need to be re-trained if the
domain changes. We show this by deploying the same segmentation network in two
different domains: simulation and the real world. By creating realistic
synthetic images that mimic the complexities of the water surface, we provide
fine-grained training data for our network (MARVIS) to discern between real and
virtual images effectively. By motion & geometry-aware design choices and
through comprehensive experimental analysis, we achieve state-of-the-art
real-virtual image segmentation performance in unseen real world domain,
achieving an IoU over 78% and a F1-Score over 86% while ensuring a small
computational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a
single GPU (CPU core). Our code and dataset are available here
https://github.com/jiayi-wu-umd/MARVIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Regularized Multi-Modal <span class="highlight-title">Image</span> Assimilation for Brain Tumor
  Localization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Balcerak, Tamaz Amiranashvili, Andreas Wagner, Jonas Weidner, Petr Karnakov, Johannes C. Paetzold, Ivan Ezhov, Petros Koumoutsakos, Benedikt Wiestler, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical models in the form of partial differential equations represent an
important prior for many under-constrained problems. One example is tumor
treatment planning, which heavily depends on accurate estimates of the spatial
distribution of tumor cells in a patient's anatomy. Medical imaging scans can
identify the bulk of the tumor, but they cannot reveal its full spatial
distribution. Tumor cells at low concentrations remain undetectable, for
example, in the most frequent type of primary brain tumors, glioblastoma.
Deep-learning-based approaches fail to estimate the complete tumor cell
distribution due to a lack of reliable training data. Most existing works
therefore rely on physics-based simulations to match observed tumors, providing
anatomically and physiologically plausible estimations. However, these
approaches struggle with complex and unknown initial conditions and are limited
by overly rigid physical models. In this work, we present a novel method that
balances data-driven and physics-based cost functions. In particular, we
propose a unique discretization scheme that quantifies the adherence of our
learned spatiotemporal tumor and brain tissue distributions to their
corresponding growth and elasticity equations. This quantification, serving as
a regularization term rather than a hard constraint, enables greater
flexibility and proficiency in assimilating patient data than existing models.
We demonstrate improved coverage of tumor recurrence areas compared to existing
techniques on real-world data from a cohort of patients. The method holds the
potential to enhance clinical adoption of model-driven treatment planning for
glioblastoma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Source-Free Domain Adaptation Guided by Vision and Vision-Language
  Pre-Training <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02954v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02954v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Zhang, Li Shen, Chuan-Sheng Foo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation (SFDA) aims to adapt a source model trained on
a fully-labeled source domain to a related but unlabeled target domain. While
the source model is a key avenue for acquiring target pseudolabels, the
generated pseudolabels may exhibit source bias. In the conventional SFDA
pipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to
initialize the source model at the start of source training, and subsequently
discarded. Despite having diverse features important for generalization, the
pre-trained feature extractor can overfit to the source data distribution
during source training and forget relevant target domain knowledge. Rather than
discarding this valuable knowledge, we introduce an integrated framework to
incorporate pre-trained networks into the target adaptation process. The
proposed framework is flexible and allows us to plug modern pre-trained
networks into the adaptation process to leverage their stronger representation
learning capabilities. For adaptation, we propose the Co-learn algorithm to
improve target pseudolabel quality collaboratively through the source model and
a pre-trained feature extractor. Building on the recent success of the
vision-language model CLIP in zero-shot image recognition, we present an
extension Co-learn++ to further incorporate CLIP's zero-shot classification
decisions. We evaluate on 4 benchmark datasets and include more challenging
scenarios such as open-set, partial-set and open-partial SFDA. Experimental
results demonstrate that our proposed strategy improves adaptation performance
and can be successfully integrated with existing SFDA methods. Project code is
available at https://github.com/zwenyu/colearn-plus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extension of ICCV paper arXiv:2212.07585; Published at IJCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransRef: Multi-Scale Reference Embedding Transformer for
  Reference-Guided <span class="highlight-title">Image</span> Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11528v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11528v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taorong Liu, Liang Liao, Delin Chen, Jing Xiao, Zheng Wang, Chia-Wen Lin, Shin'ichi Satoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image inpainting for completing complicated semantic environments and diverse
hole patterns of corrupted images is challenging even for state-of-the-art
learning-based inpainting methods trained on large-scale data. A reference
image capturing the same scene of a corrupted image offers informative guidance
for completing the corrupted image as it shares similar texture and structure
priors to that of the holes of the corrupted image. In this work, we propose a
transformer-based encoder-decoder network, named TransRef, for reference-guided
image inpainting. Specifically, the guidance is conducted progressively through
a reference embedding procedure, in which the referencing features are
subsequently aligned and fused with the features of the corrupted image. For
precise utilization of the reference features for guidance, a reference-patch
alignment (Ref-PA) module is proposed to align the patch features of the
reference and corrupted images and harmonize their style differences, while a
reference-patch transformer (Ref-PT) module is proposed to refine the embedded
reference feature. Moreover, to facilitate the research of reference-guided
image restoration tasks, we construct a publicly accessible benchmark dataset
containing 50K pairs of input and reference images. Both quantitative and
qualitative evaluations demonstrate the efficacy of the reference information
and the proposed method over the state-of-the-art methods in completing complex
holes. Code and dataset can be accessed at https://github.com/Cameltr/TransRef.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional <span class="highlight-title">Image</span> Synthesis with <span class="highlight-title">Diffusion</span> Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Zhan, Defang Chen, Jian-Ping Mei, Zhenghe Zhao, Jiawei Chen, Chun Chen, Siwei Lyu, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional image synthesis based on user-specified requirements is a key
component in creating complex visual content. In recent years, diffusion-based
generative modeling has become a highly effective way for conditional image
synthesis, leading to exponential growth in the literature. However, the
complexity of diffusion-based modeling, the wide range of image synthesis
tasks, and the diversity of conditioning mechanisms present significant
challenges for researchers to keep up with rapid developments and understand
the core concepts on this topic. In this survey, we categorize existing works
based on how conditions are integrated into the two fundamental components of
diffusion-based modeling, i.e., the denoising network and the sampling process.
We specifically highlight the underlying principles, advantages, and potential
challenges of various conditioning approaches in the training, re-purposing,
and specialization stages to construct a desired denoising network. We also
summarize six mainstream conditioning mechanisms in the essential sampling
process. All discussions are centered around popular applications. Finally, we
pinpoint some critical yet still open problems to be solved in the future and
suggest some possible solutions. Our reviewed works are itemized at
https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forecasting Disease Progression with Parallel Hyperplanes in
  Longitudinal Retinal OCT <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arunava Chakravarty, Taha Emre, Dmitrii Lachinov, Antoine Rivail, Hendrik Scholl, Lars Fritsche, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Ursula Schmidt-Erfurth, Hrvoje Bogunović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting future disease progression risk from medical images is challenging
due to patient heterogeneity, and subtle or unknown imaging biomarkers.
Moreover, deep learning (DL) methods for survival analysis are susceptible to
image domain shifts across scanners. We tackle these issues in the task of
predicting late dry Age-related Macular Degeneration (dAMD) onset from retinal
OCT scans. We propose a novel DL method for survival prediction to jointly
predict from the current scan a risk score, inversely related to
time-to-conversion, and the probability of conversion within a time interval
$t$. It uses a family of parallel hyperplanes generated by parameterizing the
bias term as a function of $t$. In addition, we develop unsupervised losses
based on intra-subject image pairs to ensure that risk scores increase over
time and that future conversion predictions are consistent with AMD stage
prediction using actual scans of future visits. Such losses enable
data-efficient fine-tuning of the trained model on new unlabeled datasets
acquired with a different scanner. Extensive evaluation on two large datasets
acquired with different scanners resulted in a mean AUROCs of 0.82 for
Dataset-1 and 0.83 for Dataset-2, across prediction intervals of 6,12 and 24
months.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliciting In-Context Learning in <span class="highlight-title">Vision-Language Model</span>s for Videos
  Through Curated Data Distributional Properties <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17041v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17041v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Shane Storks, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major reason behind the recent success of large language models (LLMs) is
their \textit{in-context learning} capability, which makes it possible to
rapidly adapt them to downstream text-based tasks by prompting them with a
small number of relevant demonstrations. While large vision-language models
(VLMs) have recently been developed for tasks requiring both text and images,
they largely lack in-context learning over visual information, especially in
understanding and generating text about videos. In this work, we implement
\textbf{E}mergent \textbf{I}n-context \textbf{Le}arning on \textbf{V}ideos
(\eilev{}), a novel training paradigm that induces in-context learning over
video and text by capturing key properties of pre-training data found by prior
work to be essential for in-context learning in transformers. In our
experiments, we show that \eilev-trained models outperform other off-the-shelf
VLMs in few-shot video narration for novel, rare actions. Furthermore, we
demonstrate that these key properties of bursty distributions, skewed marginal
distributions, and dynamic meaning each contribute to varying degrees to VLMs'
in-context learning capability in narrating procedural videos. Our results,
analysis, and \eilev{}-trained models yield numerous insights about the
emergence of in-context learning over video and text, creating a foundation for
future work to optimize and scale VLMs for open-domain video understanding and
reasoning. Our code and demo are available at
\url{https://github.com/yukw777/EILEV}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, LaTeX; Accepted to EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> <span class="highlight-title">Multimodal</span> Self-Instruct: Synthetic Abstract <span class="highlight-title">Image</span> and Visual <span class="highlight-title">Reasoning</span>
  Instruction Using Language Model <span class="chip">EMNLP-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07053v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07053v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen<span class="highlight-author">qi Zhang</span>, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although most current large multimodal models (LMMs) can already understand
photos of natural scenes and portraits, their understanding of abstract images,
e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite
rudimentary. They often struggle with simple daily tasks, such as reading time
from a clock, understanding a flowchart, or planning a route using a road map.
In light of this, we design a multi-modal self-instruct, utilizing large
language models and their code capabilities to synthesize massive abstract
images and visual reasoning instructions across daily scenarios. Our strategy
effortlessly creates a multimodal benchmark with 11,193 instructions for eight
visual scenarios: charts, tables, simulated maps, dashboards, flowcharts,
relation graphs, floor plans, and visual puzzles. \textbf{This benchmark,
constructed with simple lines and geometric elements, exposes the shortcomings
of most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image
understanding, spatial relations reasoning, and visual element induction.
Besides, to verify the quality of our synthetic data, we fine-tune an LMM using
62,476 synthetic chart, table and road map instructions. The results
demonstrate improved chart understanding and map navigation performance, and
also demonstrate potential benefits for other visual reasoning tasks. Our code
is available at: \url{https://github.com/zwq2018/Multi-modal-Self-instruct}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted by EMNLP-24. Code:
  https://github.com/zwq2018/Multi-modal-Self-instruct dataset:
  https://huggingface.co/datasets/zwq2018/Multi-modal-Self-instruct
  Leaderboard: https://multi-modal-self-instruct.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SRIF: Semantic Shape Registration Empowered by <span class="highlight-title">Diffusion</span>-based <span class="highlight-title">Image</span>
  Morphing and Flow Estimation <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Sun, Chen Guo, Puhua Jiang, Shiwei Mao, Yurun Chen, Ruqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose SRIF, a novel Semantic shape Registration framework
based on diffusion-based Image morphing and Flow estimation. More concretely,
given a pair of extrinsically aligned shapes, we first render them from
multi-views, and then utilize an image interpolation framework based on
diffusion models to generate sequences of intermediate images between them. The
images are later fed into a dynamic 3D Gaussian splatting framework, with which
we reconstruct and post-process for intermediate point clouds respecting the
image morphing processing. In the end, tailored for the above, we propose a
novel registration module to estimate continuous normalizing flow, which
deforms source shape consistently towards the target, with intermediate point
clouds as weak guidance. Our key insight is to leverage large vision models
(LVMs) to associate shapes and therefore obtain much richer semantic
information on the relationship between shapes than the ad-hoc feature
extraction and alignment. As a consequence, SRIF achieves high-quality dense
correspondences on challenging shape pairs, but also delivers smooth,
semantically meaningful interpolation in between. Empirical evidence justifies
the effectiveness and superiority of our method as well as specific design
choices. The code is released at https://github.com/rqhuang88/SRIF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper of SIGGRAPH Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizable Human Gaussians from Single-View <span class="highlight-title">Image</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinnan Chen, Chen Li, Jianfeng Zhang, Lingting Zhu, Buzhen Huang, Hanlin Chen, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the task of learning generalizable 3D human Gaussians
from a single image. The main challenge for this task is to recover detailed
geometry and appearance, especially for the unobserved regions. To this end, we
propose single-view generalizable Human Gaussian model (HGM), a
diffusion-guided framework for 3D human modeling from a single image. We design
a diffusion-based coarse-to-fine pipeline, where the diffusion model is adapted
to refine novel-view images rendered from a coarse human Gaussian model. The
refined images are then used together with the input image to learn a refined
human Gaussian model. Although effective in hallucinating the unobserved views,
the approach may generate unrealistic human pose and shapes due to the lack of
supervision. We circumvent this problem by further encoding the geometric
priors from SMPL model. Specifically, we propagate geometric features from SMPL
volume to the predicted Gaussians via sparse convolution and attention
mechanism. We validate our approach on publicly available datasets and
demonstrate that it significantly surpasses state-of-the-art methods in terms
of PSNR and SSIM. Additionally, our method exhibits strong generalization for
in-the-wild images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://jinnan-chen.github.io/projects/HGM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongLLaVA: Scaling Multi-modal <span class="highlight-title">LLM</span>s to 1000 <span class="highlight-title">Image</span>s Efficiently via a
  Hybrid Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expanding the long-context capabilities of Multi-modal Large Language
Models~(MLLMs) is crucial for video understanding, high-resolution image
understanding, and multi-modal agents. This involves a series of systematic
optimizations, including model architecture, data construction and training
strategy, particularly addressing challenges such as \textit{degraded
performance with more images} and \textit{high computational costs}. In this
paper, we adapt the model architecture to a hybrid of Mamba and Transformer
blocks, approach data construction with both temporal and spatial dependencies
among multiple images and employ a progressive training strategy. The released
model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge
\textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first
hybrid MLLM, which achieved a better balance between efficiency and
effectiveness. LongLLaVA not only achieves competitive results across various
benchmarks, but also maintains high throughput and low memory consumption.
Especially, it could process nearly a thousand images on a single A100 80GB
GPU, showing promising application prospects for a wide range of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Projections for Discriminative Dictionary Learning using the
  JL-lemma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13991v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13991v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Madhuri, Atul Negi, Kaluri V. Rangarao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction-based dictionary learning methods in the literature
have often used iterative random projections. The dimensionality of such a
random projection matrix is a random number that might not lead to a separable
subspace structure in the transformed space. The convergence of such methods
highly depends on the initial seed values used. Also, gradient descent-based
updates might result in local minima. This paper proposes a constructive
approach to derandomize the projection matrix using the Johnson-Lindenstrauss
lemma. Rather than reducing dimensionality via random projections, a projection
matrix derived from the proposed Modified Supervised PC analysis is used. A
heuristic is proposed to decide the data perturbation levels and the dictionary
atom's corresponding suitable description length. The projection matrix is
derived in a single step, provides maximum feature-label consistency of the
transformed space, and preserves the geometry of the original data. The
projection matrix thus constructed is proved to be a JL-embedding. Despite
confusing classes in the OCR datasets, the dictionary trained in the
transformed space generates discriminative sparse coefficients with reduced
complexity. Empirical study demonstrates that the proposed method performs well
even when the number of classes and dimensionality increase. Experimentation on
OCR and face recognition datasets shows better classification performance than
other algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Temporal Extrapolation of <span class="highlight-title">Multimodal</span> <span class="highlight-title">Large Language Model</span>s
  with Temporal Grounding Bridge <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Yang Liu, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite progress in multimodal large language models (MLLMs), the challenge
of interpreting long-form videos in response to linguistic queries persists,
largely due to the inefficiency in temporal grounding and limited pre-trained
context window size. In this work, we introduce Temporal Grounding Bridge
(TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding
capabilities and broadens their contextual scope. Our framework significantly
enhances the temporal capabilities of current MLLMs through three key
innovations: an efficient multi-span temporal grounding algorithm applied to
low-dimension temporal features projected from flow; a multimodal length
extrapolation training paradigm that utilizes low-dimension temporal features
to extend the training context window size; and a bootstrapping framework that
bridges our model with pluggable MLLMs without requiring annotation. We
validate TGB across seven video benchmarks and demonstrate substantial
performance improvements compared with prior MLLMs. Notably, our model,
initially trained on sequences of four frames, effectively handles sequences up
to 16 longer without sacrificing performance, highlighting its scalability and
effectiveness in real-world applications. Our code is publicly available at
https://github.com/bigai-nlco/VideoTGB
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-to-Sticker: Style Tailoring Latent <span class="highlight-title">Diffusion</span> Models for Human
  Expression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Animesh Sinha, Bo Sun, Anmol Kalia, Arantxa Casanova, Elliot Blanchard, David Yan, Winnie Zhang, Tony Nelli, Jiahui Chen, Hardik Shah, Licheng Yu, Mitesh Kumar Singh, Ankit Ramchandani, Maziar Sanjabi, Sonal Gupta, Amy Bearman, Dhruv Mahajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Style Tailoring, a recipe to finetune Latent Diffusion Models
(LDMs) in a distinct domain with high visual quality, prompt alignment and
scene diversity. We choose sticker image generation as the target domain, as
the images significantly differ from photorealistic samples typically generated
by large-scale LDMs. We start with a competent text-to-image model, like Emu,
and show that relying on prompt engineering with a photorealistic model to
generate stickers leads to poor prompt alignment and scene diversity. To
overcome these drawbacks, we first finetune Emu on millions of sticker-like
images collected using weak supervision to elicit diversity. Next, we curate
human-in-the-loop (HITL) Alignment and Style datasets from model generations,
and finetune to improve prompt alignment and style alignment respectively.
Sequential finetuning on these datasets poses a tradeoff between better style
alignment and prompt alignment gains. To address this tradeoff, we propose a
novel fine-tuning method called Style Tailoring, which jointly fits the content
and style distribution and achieves best tradeoff. Evaluation results show our
method improves visual quality by 14%, prompt alignment by 16.2% and scene
diversity by 15.3%, compared to prompt engineering the base Emu model for
stickers generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CTSpine1K: A Large-Scale <span class="highlight-title">Dataset</span> for Spinal Vertebrae Segmentation in
  Computed Tomography <span class="chip">MICCAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.14711v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.14711v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Deng, Ce Wang, Yuan Hui, Qian Li, Jun Li, Shiwei Luo, Mengke Sun, Quan Quan, Shuxin Yang, You Hao, Pengbo Liu, Honghu Xiao, Chunpeng Zhao, Xinbao Wu, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spine-related diseases have high morbidity and cause a huge burden of social
cost. Spine imaging is an essential tool for noninvasively visualizing and
assessing spinal pathology. Segmenting vertebrae in computed tomography (CT)
images is the basis of quantitative medical image analysis for clinical
diagnosis and surgery planning of spine diseases. Current publicly available
annotated datasets on spinal vertebrae are small in size. Due to the lack of a
large-scale annotated spine image dataset, the mainstream deep learning-based
segmentation methods, which are data-driven, are heavily restricted. In this
paper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated
from multiple sources for vertebra segmentation, which contains 1,005 CT
volumes with over 11,100 labeled vertebrae belonging to different spinal
conditions. Based on this dataset, we conduct several spinal vertebrae
segmentation experiments to set the first benchmark. We believe that this
large-scale dataset will facilitate further research in many spine-related
image analysis tasks, including but not limited to vertebrae segmentation,
labeling, 3D spine reconstruction from biplanar radiographs, image
super-resolution, and enhancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI2024 Open Data for oral presentation and will be
  published as a part of the journal MELBA special issue</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of Hallucination in Large Language, <span class="highlight-title">Image</span>, Video
  and Audio Foundation Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09589v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09589v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of foundation models (FMs) across language, image,
audio, and video domains has shown remarkable capabilities in diverse tasks.
However, the proliferation of FMs brings forth a critical challenge: the
potential to generate hallucinated outputs, particularly in high-stakes
applications. The tendency of foundation models to produce hallucinated content
arguably represents the biggest hindrance to their widespread adoption in
real-world scenarios, especially in domains where reliability and accuracy are
paramount. This survey paper presents a comprehensive overview of recent
developments that aim to identify and mitigate the problem of hallucination in
FMs, spanning text, image, video, and audio modalities. By synthesizing recent
advancements in detecting and mitigating hallucination across various
modalities, the paper aims to provide valuable insights for researchers,
developers, and practitioners. Essentially, it establishes a clear framework
encompassing definition, taxonomy, and detection strategies for addressing
hallucination in multimodal foundation models, laying the foundation for future
research in this pivotal area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Identifying and Solving Conditional <span class="highlight-title">Image</span> Leakage in <span class="highlight-title">Image</span>-to-Video
  <span class="highlight-title">Diffusion</span> Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, <span class="highlight-author">Jun Zhu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have obtained substantial progress in image-to-video
generation. However, in this paper, we find that these models tend to generate
videos with less motion than expected. We attribute this to the issue called
conditional image leakage, where the image-to-video diffusion models (I2V-DMs)
tend to over-rely on the conditional image at large time steps. We further
address this challenge from both inference and training aspects. First, we
propose to start the generation process from an earlier time step to avoid the
unreliable large-time steps of I2V-DMs, as well as an initial noise
distribution with optimal analytic expressions (Analytic-Init) by minimizing
the KL divergence between it and the actual marginal distribution to bridge the
training-inference gap. Second, we design a time-dependent noise distribution
(TimeNoise) for the conditional image during training, applying higher noise
levels at larger time steps to disrupt it and reduce the model's dependency on
it. We validate these general strategies on various I2V-DMs on our collected
open-domain image benchmark and the UCF101 dataset. Extensive results show that
our methods outperform baselines by producing higher motion scores with lower
errors while maintaining image alignment and temporal consistency, thereby
yielding superior overall performance and enabling more accurate motion
control. The project page: \url{https://cond-image-leak.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project page: https://cond-image-leak.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrap3D: Improving Multi-view <span class="highlight-title">Diffusion</span> Model with Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Sun, Tong Wu, Pan Zhang, Yuhang Zang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed remarkable progress in multi-view diffusion
models for 3D content creation. However, there remains a significant gap in
image quality and prompt-following ability compared to 2D diffusion models. A
critical bottleneck is the scarcity of high-quality 3D objects with detailed
captions. To address this challenge, we propose Bootstrap3D, a novel framework
that automatically generates an arbitrary quantity of multi-view images to
assist in training multi-view diffusion models. Specifically, we introduce a
data generation pipeline that employs (1) 2D and video diffusion models to
generate multi-view images based on constructed text prompts, and (2) our
fine-tuned 3D-aware MV-LLaVA for filtering high-quality data and rewriting
inaccurate captions. Leveraging this pipeline, we have generated 1 million
high-quality synthetic multi-view images with dense descriptive captions to
address the shortage of high-quality 3D data. Furthermore, we present a
Training Timestep Reschedule (TTR) strategy that leverages the denoising
process to learn multi-view consistency while maintaining the original 2D
diffusion prior. Extensive experiments demonstrate that Bootstrap3D can
generate high-quality multi-view images with superior aesthetic quality,
image-text alignment, and maintained view consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sunzey.github.io/Bootstrap3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Emanuele Cinà, Jérôme Rony, Maura Pintor, Luca Demetrio, Ambra Demontis, Battista Biggio, Ismail Ben Ayed, Fabio Roli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples are typically optimized with gradient-based attacks.
While novel attacks are continuously proposed, each is shown to outperform its
predecessors using different experimental setups, hyperparameter settings, and
number of forward and backward calls to the target models. This provides
overly-optimistic and even biased evaluations that may unfairly favor one
particular attack over the others. In this work, we aim to overcome these
limitations by proposing AttackBench, i.e., the first evaluation framework that
enables a fair comparison among different attacks. To this end, we first
propose a categorization of gradient-based attacks, identifying their main
components and differences. We then introduce our framework, which evaluates
their effectiveness and efficiency. We measure these characteristics by (i)
defining an optimality metric that quantifies how close an attack is to the
optimal solution, and (ii) limiting the number of forward and backward queries
to the model, such that all attacks are compared within a given maximum query
budget. Our extensive experimental analysis compares more than $100$ attack
implementations with a total of over $800$ different configurations against
CIFAR-10 and ImageNet models, highlighting that only very few attacks
outperform all the competing approaches. Within this analysis, we shed light on
several implementation issues that prevent many attacks from finding better
solutions or running at all. We release AttackBench as a publicly-available
benchmark, aiming to continuously update it to include and evaluate novel
gradient-based attacks for optimizing adversarial examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://attackbench.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">105</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Xie, Haoming Cai, Sachin Shah, Yiran Xu, Brandon Y. Feng, Jia-Bin Huang, Christopher A. Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simple yet effective approach for separating transmitted and
reflected light. Our key insight is that the powerful novel view synthesis
capabilities provided by modern inverse rendering methods (e.g.,~3D Gaussian
splatting) allow one to perform flash/no-flash reflection separation using
unpaired measurements -- this relaxation dramatically simplifies image
acquisition over conventional paired flash/no-flash reflection separation
methods. Through extensive real-world experiments, we demonstrate our method,
Flash-Splat, accurately reconstructs both transmitted and reflected scenes in
3D. Our method outperforms existing 3D reflection separation methods, which do
not leverage illumination control, by a large margin. Our project webpage is at
https://flash-splat.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vinoground: Scrutinizing LMMs over Dense Temporal <span class="highlight-title">Reasoning</span> with Short
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrui Zhang, Mu Cai, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been growing sentiment recently that modern large multimodal models
(LMMs) have addressed most of the key challenges related to short video
comprehension. As a result, both academia and industry are gradually shifting
their attention towards the more complex challenges posed by understanding
long-form videos. However, is this really the case? Our studies indicate that
LMMs still lack many fundamental reasoning capabilities even when dealing with
short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation
benchmark encompassing 1000 short and natural video-caption pairs. We
demonstrate that existing LMMs severely struggle to distinguish temporal
differences between different actions and object transformations. For example,
the best model GPT-4o only obtains ~50% on our text and video scores, showing a
large gap compared to the human baseline of ~90%. All open-source multimodal
models and CLIP-based models perform much worse, producing mostly random chance
performance. Through this work, we shed light onto the fact that temporal
reasoning in short videos is a problem yet to be fully solved. The dataset and
evaluation code are available at https://vinoground.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://vinoground.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting and <span class="highlight-title">Editing</span> Vision-Language Representations to Mitigate
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Jiang, Anish Kachinthaya, Suzie Petryk, Yossi Gandelsman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the internal representations of vision-language models (VLMs)
to address hallucinations, a persistent challenge despite advances in model
size and training. We project VLMs' internal image representations to their
language vocabulary and observe more confident output probabilities on real
objects than hallucinated objects. We additionally use these output
probabilities to spatially localize real objects. Building on this approach, we
introduce a knowledge erasure algorithm that removes hallucinations by linearly
orthogonalizing image features with respect to hallucinated object features. We
show that targeted edits to a model's latent representations can reduce
hallucinations by up to 25.7% on the COCO2014 dataset while preserving
performance. Our findings demonstrate how a deeper understanding of VLMs'
latent representations can enhance reliability and enable novel capabilities,
such as zero-shot segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: http://anishk23733.github.io/vl-interp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Erasing Conceptual Knowledge from Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept erasure in language models has traditionally lacked a comprehensive
evaluation framework, leading to incomplete assessments of effectiveness of
erasure methods. We propose an evaluation paradigm centered on three critical
criteria: innocence (complete knowledge removal), seamlessness (maintaining
conditional fluent generation), and specificity (preserving unrelated task
performance). Our evaluation metrics naturally motivate the development of
Erasure of Language Memory (ELM), a new method designed to address all three
dimensions. ELM employs targeted low-rank updates to alter output distributions
for erased concepts while preserving overall model capabilities including
fluency when prompted for an erased concept. We demonstrate ELM's efficacy on
biosecurity, cybersecurity, and literary domain erasure tasks. Comparative
analysis shows that ELM achieves superior performance across our proposed
metrics, including near-random scores on erased topic assessments, generation
fluency, maintained accuracy on unrelated benchmarks, and robustness under
adversarial attacks. Our code, data, and trained models are available at
https://elm.baulab.info
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://elm.baulab.info</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting Smog Clouds With Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentijn Oldenburg, Juan Cardenas-Cartagena, Matias Valdenegro-Toro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this proof-of-concept study, we conduct multivariate timeseries
forecasting for the concentrations of nitrogen dioxide (NO2), ozone (O3), and
(fine) particulate matter (PM10 & PM2.5) with meteorological covariates between
two locations using various deep learning models, with a focus on long
short-term memory (LSTM) and gated recurrent unit (GRU) architectures. In
particular, we propose an integrated, hierarchical model architecture inspired
by air pollution dynamics and atmospheric science that employs multi-task
learning and is benchmarked by unidirectional and fully-connected models.
Results demonstrate that, above all, the hierarchical GRU proves itself as a
competitive and efficient method for forecasting the concentration of
smog-related pollutants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIEVE: General Purpose Data Filtering System Matching <span class="highlight-title">GPT</span>-4o Accuracy at
  1% the Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifan Zhang, Robert Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating specialized large language models requires vast amounts of clean,
special purpose data for training and fine-tuning. With only a handful of
existing large-scale, domain-specific datasets, creation of new datasets is
required in most applications. This requires the development of new
application-specific filtering of web-scale data. Filtering with a
high-performance, general-purpose LLM such as GPT-4o can be highly effective,
but this is extremely expensive at web-scale. This paper proposes SIEVE, a
lightweight alternative that matches GPT-4o accuracy at a fraction of the cost.
SIEVE can perform up to 500 filtering operations for the cost of one GPT-4o
filtering call. The key to SIEVE is a seamless integration of GPT-4o and
lightweight T5 models, using active learning to fine-tune T5 in the background
with a small number of calls to GPT-4o. Once trained, it performs as well as
GPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the
OpenWebText dataset, using five highly customized filter tasks targeting high
quality and domain-specific content. Our results demonstrate the effectiveness
and efficiency of our method in curating large, high-quality datasets for
language model training at a substantially lower cost (1%) than existing
techniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o
achieve similar accuracy, with human evaluators preferring SIEVE's filtering
results to those of GPT-4o.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for
  Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Elawady, Gunjan Chhablani, Ram Ramrakhya, Karmesh Yadav, Dhruv Batra, Zsolt Kira, Andrew Szot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent embodied agents need to quickly adapt to new scenarios by
integrating long histories of experience into decision-making. For instance, a
robot in an unfamiliar house initially wouldn't know the locations of objects
needed for tasks and might perform inefficiently. However, as it gathers more
experience, it should learn the layout of its environment and remember where
objects are, allowing it to complete new tasks more efficiently. To enable such
rapid adaptation to new tasks, we present ReLIC, a new approach for in-context
reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable
of adapting to new environments using 64,000 steps of in-context experience
with full attention while being trained through self-generated experience via
RL. We achieve this by proposing a novel policy update scheme for on-policy RL
called "partial updates'' as well as a Sink-KV mechanism that enables effective
utilization of a long observation history for embodied agents. Our method
outperforms a variety of meta-RL baselines in adapting to unseen houses in an
embodied multi-object navigation task. In addition, we find that ReLIC is
capable of few-shot imitation learning despite never being trained with expert
demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting
that the combination of large-scale RL training, the proposed partial updates
scheme, and the Sink-KV are essential for effective in-context learning. The
code for ReLIC and all our experiments is at https://github.com/aielawady/relic
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Online Automatic Modulation Classification Scheme Based on Isolation
  Distributional Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Li, Zile Jiang, Kai Ming Ting, Ye Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Modulation Classification (AMC), as a crucial technique in modern
non-cooperative communication networks, plays a key role in various civil and
military applications. However, existing AMC methods usually are complicated
and can work in batch mode only due to their high computational complexity.
This paper introduces a new online AMC scheme based on Isolation Distributional
Kernel. Our method stands out in two aspects. Firstly, it is the first proposal
to represent baseband signals using a distributional kernel. Secondly, it
introduces a pioneering AMC technique that works well in online settings under
realistic time-varying channel conditions. Through extensive experiments in
online settings, we demonstrate the effectiveness of the proposed classifier.
Our results indicate that the proposed approach outperforms existing baseline
models, including two state-of-the-art deep learning classifiers. Moreover, it
distinguishes itself as the first online classifier for AMC with linear time
complexity, which marks a significant efficiency boost for real-time
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Language Models on Synthetic Edit Sequences Improves Code
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ulyana Piterbarg, Lerrel Pinto, Rob Fergus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineers mainly write code by editing existing programs. In
contrast, large language models (LLMs) autoregressively synthesize programs in
a single pass. One explanation for this is the scarcity of open-sourced edit
data. While high-quality instruction data for code synthesis is already scarce,
high-quality edit data is even scarcer. To fill this gap, we develop a
synthetic data generation algorithm called LintSeq. This algorithm refactors
existing code into a sequence of code edits by using a linter to procedurally
sample across the error-free insertions that can be used to sequentially write
programs. It outputs edit sequences as text strings consisting of consecutive
program diffs. To test LintSeq, we use it to refactor a dataset of instruction
+ program pairs into instruction + program-diff-sequence tuples. Then, we
instruction finetune a series of smaller LLMs ranging from 2.6B to 14B
parameters on both the re-factored and original versions of this dataset,
comparing zero-shot performance on code synthesis benchmarks. We show that
during repeated sampling, edit sequence finetuned models produce more diverse
programs than baselines. This results in better inference-time scaling for
benchmark coverage as a function of samples, i.e. the fraction of problems
"pass@k" solved by any attempt given "k" tries. For example, on HumanEval
pass@50, small LLMs finetuned on synthetic edit sequences are competitive with
GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)
in absolute score. Finally, we also pretrain our own tiny LMs for code
understanding. We show that finetuning tiny models on synthetic code edits
results in state-of-the-art code synthesis for the on-device model class. Our
150M parameter edit sequence LM matches or outperforms code models with twice
as many parameters, both with and without repeated sampling, including Codex
and AlphaCode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt
  Optimization for Text <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned
to extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Localized Language-<span class="highlight-title">Image</span> Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Zhang, Yinfei Yang, Zhe Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has been a celebrated method
for training vision encoders to generate image/text representations
facilitating various applications. Recently, CLIP has been widely adopted as
the vision backbone of multimodal large language models (MLLMs) to connect
image inputs for language interactions. The success of CLIP as a
vision-language foundation model relies on aligning web-crawled noisy text
annotations at image levels. Nevertheless, such criteria may become
insufficient for downstream tasks in need of fine-grained vision
representations, especially when region-level understanding is demanding for
MLLMs. In this paper, we improve the localization capability of CLIP with
several advances. We propose a pre-training method called Contrastive Localized
Language-Image Pre-training (CLOC) by complementing CLIP with region-text
contrastive loss and modules. We formulate a new concept, promptable
embeddings, of which the encoder produces image embeddings easy to transform
into region representations given spatial hints. To support large-scale
pre-training, we design a visually-enriched and spatially-localized captioning
framework to effectively generate region-text pseudo-labels at scale. By
scaling up to billions of annotated images, CLOC enables high-quality regional
embeddings for image region recognition and retrieval tasks, and can be a
drop-in replacement of CLIP to enhance MLLMs, especially on referring and
grounding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neutral residues: revisiting adapters for model extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franck Signe Talla, Herve Jegou, Edouard Grave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of extending a pretrained large language model to a
new domain that was not seen at training time, like adding a language for which
the original model has seen no or little training data. Popular solutions like
fine-tuning or low-rank adaptation are successful at domain adaptation, but
formally they do not add any extra capacity and degrade the performance in the
original domain.
  Our paper analyzes this extension problem under three angles: data,
architecture and training procedure, which are advantageously considered
jointly. In particular, we improve adapters and make it possible to learn an
entire new language while ensuring that the output of the neural network is
almost unchanged in the original domain. For this purpose, we modify the new
residual blocks in a way that leads each new residual block to output
near-zeros in the original domain.
  This solution of neutral residues, which borrows architectural components
from mixture of experts, is effective: with only 20% extra learnable weights
compared to an original model trained on English, we get results that are
significantly better than concurrent approaches (fine-tuning, low-rank or
vanilla adapters) in terms of the trade-off between learning a new language and
not forgetting English.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding <span class="highlight-title">Large Language Model</span>s In Embodied Environment With Imperfect
  World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolan Liu, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite a widespread success in various applications, large language models
(LLMs) often stumble when tackling basic physical reasoning or executing
robotics tasks, due to a lack of direct experience with the physical nuances of
the real world. To address these issues, we propose a Grounding Large language
model with Imperfect world MOdel (GLIMO), which utilizes proxy world models
such as simulators to collect and synthesize trining data. GLIMO incorporates
an LLM agent-based data generator to automatically create high-quality and
diverse instruction datasets. The generator includes an iterative self-refining
module for temporally consistent experience sampling, a diverse set of
question-answering instruction seeds, and a retrieval-augmented generation
module for reflecting on prior experiences. Comprehensive experiments show that
our approach improve the performance of strong open-source LLMs like LLaMA-3
with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$
across three different benchmarks, respectively. The performance is able to
compete with or surpass their larger counterparts such as GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Salient Information Prompting to Steer Content in Prompt-based
  Abstractive Summarization <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Xu, Mohammed Asad Karim, Saket Dingliwal, Aparna Elangovan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned
to extract salient keyphrases. By using SigExt, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisit Large-Scale <span class="highlight-title">Image</span>-Caption Data in Pre-training <span class="highlight-title">Multimodal</span>
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, Meng Cao, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal models highlight the value of rewritten
captions for improving performance, yet key challenges remain. For example,
while synthetic captions often provide superior quality and image-text
alignment, it is not clear whether they can fully replace AltTexts: the role of
synthetic captions and their interaction with original web-crawled AltTexts in
pre-training is still not well understood. Moreover, different multimodal
foundation models may have unique preferences for specific caption formats, but
efforts to identify the optimal captions for each model remain limited. In this
work, we propose a novel, controllable, and scalable captioning pipeline
designed to generate diverse caption formats tailored to various multimodal
models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic
Captions (DSC+) as case studies, we systematically explore their effects and
interactions with AltTexts across models such as CLIP, multimodal LLMs, and
diffusion models. Our findings reveal that a hybrid approach that keeps both
synthetic captions and AltTexts can outperform the use of synthetic captions
alone, improving both alignment and performance, with each model demonstrating
preferences for particular caption formats. This comprehensive analysis
provides valuable insights into optimizing captioning strategies, thereby
advancing the pre-training of multimodal foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CV/ML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OOD-Chameleon: Is Algorithm Selection for OOD Generalization Learnable? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangze Jiang, Damien Teney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) generalization is challenging because distribution
shifts come in many forms. A multitude of learning algorithms exist and each
can improve performance in specific OOD situations. We posit that much of the
challenge of OOD generalization lies in choosing the right algorithm for the
right dataset. However, such algorithm selection is often elusive under complex
real-world shifts. In this work, we formalize the task of algorithm selection
for OOD generalization and investigate whether it could be approached by
learning. We propose a solution, dubbed OOD-Chameleon that treats the task as a
supervised classification over candidate algorithms. We construct a dataset of
datasets to learn from, which represents diverse types, magnitudes and
combinations of shifts (covariate shift, label shift, spurious correlations).
We train the model to predict the relative performance of algorithms given a
dataset's characteristics. This enables a priori selection of the best learning
strategy, i.e. without training various models as needed with traditional model
selection. Our experiments show that the adaptive selection outperforms any
individual algorithm and simple selection heuristics, on unseen datasets of
controllable and realistic image data. Inspecting the model shows that it
learns non-trivial data/algorithms interactions, and reveals the conditions for
any one algorithm to surpass another. This opens new avenues for (1) enhancing
OOD generalization with existing algorithms instead of designing new ones, and
(2) gaining insights into the applicability of existing algorithms with respect
to datasets' properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Similarity-Based One-Shot Clustering for Multi-Task Hierarchical
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulmoneam Ali, Ahmed Arafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of cluster identity estimation in a hierarchical
federated learning setting in which users work toward learning different tasks.
To overcome the challenge of task heterogeneity, users need to be grouped in a
way such that users with the same task are in the same group, conducting
training together, while sharing the weights of feature extraction layers with
the other groups. Toward that end, we propose a one-shot clustering algorithm
that can effectively identify and group users based on their data similarity.
This enables more efficient collaboration and sharing of a common layer
representation within the federated learning system. Our proposed algorithm not
only enhances the clustering process, but also overcomes challenges related to
privacy concerns, communication overhead, and the need for prior knowledge
about learning models or loss function behaviors. We validate our proposed
algorithm using various datasets such as CIFAR-10 and Fashion MNIST, and show
that it outperforms the baseline in terms of accuracy and variance reduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Asilomar 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Inference-Time Compute: <span class="highlight-title">LLM</span>s Can Predict if They Can Do Better,
  Even Mid-<span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohin Manvi, Anikait Singh, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time computation is a powerful paradigm to enhance the performance
of large language models (LLMs), with Best-of-N sampling being a widely used
technique. However, this method is computationally expensive, requiring both
(1) an external reward model and (2) the generation of multiple samples. In
this work, we introduce a new generative self-evaluation scheme designed to
adaptively reduce the number of generated samples while maintaining or even
improving performance. We use a generative reward model formulation, allowing
the LLM to predict mid-generation the probability that restarting the
generation will yield a better response. These predictions are obtained without
an external reward model and can be used to decide whether or not to generate
more samples, prune unpromising samples early on, or to pick the best sample.
This capability is very inexpensive as it involves generating a single
predefined token. Trained using a dataset constructed with real unfiltered
LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval
increases from 21% to 34% with 16 samples and math performance on GSM8K
improves from 84% to 91%. By sampling only when the LLM determines that it is
beneficial to do so and adaptively adjusting temperature annealing, we
demonstrate that 74% of the improvement from using 16 samples can be achieved
with only 1.2 samples on average. We further demonstrate that 50-75% of samples
can be pruned early in generation with minimal degradation in performance.
Overall, our methods enable more efficient and scalable compute utilization
during inference for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>s as Markov Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boullé, Ievgen Redko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have proven to be remarkably efficient, both
across a wide range of natural language processing tasks and well beyond them.
However, a comprehensive theoretical analysis of the origins of their
impressive performance remains elusive. In this paper, we approach this
challenging task by drawing an equivalence between generic autoregressive
language models with vocabulary of size $T$ and context window of size $K$ and
Markov chains defined on a finite state space of size $\mathcal{O}(T^K)$. We
derive several surprising findings related to the existence of a stationary
distribution of Markov chains that capture the inference power of LLMs, their
speed of convergence to it, and the influence of the temperature on the latter.
We then prove pre-training and in-context generalization bounds and show how
the drawn equivalence allows us to enrich their interpretation. Finally, we
illustrate our theoretical guarantees with experiments on several recent LLMs
to highlight how they capture the behavior observed in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynthFormer: Equivariant Pharmacophore-based <span class="highlight-title">Generation</span> of Molecules for
  Ligand-Based Drug Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zygimantas Jocys, Henriette M. G. Willems, Katayoun Farrahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug discovery is a complex and resource-intensive process, with significant
time and cost investments required to bring new medicines to patients. Recent
advancements in generative machine learning (ML) methods offer promising
avenues to accelerate early-stage drug discovery by efficiently exploring
chemical space. This paper addresses the gap between in silico generative
approaches and practical in vitro methodologies, highlighting the need for
their integration to optimize molecule discovery. We introduce SynthFormer, a
novel ML model that utilizes a 3D equivariant encoder for pharmacophores to
generate fully synthesizable molecules, constructed as synthetic trees. Unlike
previous methods, SynthFormer incorporates 3D information and provides
synthetic paths, enhancing its ability to produce molecules with good docking
scores across various proteins. Our contributions include a new methodology for
efficient chemical space exploration using 3D information, a novel architecture
called Synthformer for translating 3D pharmacophore representations into
molecules, and a meaningful embedding space that organizes reagents for drug
discovery optimization. Synthformer generates molecules that dock well and
enables effective late-stage optimization restricted by synthesis paths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measurements with Noise: Bayesian Optimization for Co-optimizing Noise
  and Property Discovery in Automated Experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris N. Slautin, Yu Liu, Jan Dec, Vladimir V. Shvartsman, Doru C. Lupascu, Maxim Ziatdinov, Sergei V. Kalinin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have developed a Bayesian optimization (BO) workflow that integrates
intra-step noise optimization into automated experimental cycles. Traditional
BO approaches in automated experiments focus on optimizing experimental
trajectories but often overlook the impact of measurement noise on data quality
and cost. Our proposed framework simultaneously optimizes both the target
property and the associated measurement noise by introducing time as an
additional input parameter, thereby balancing the signal-to-noise ratio and
experimental duration. Two approaches are explored: a reward-driven noise
optimization and a double-optimization acquisition function, both enhancing the
efficiency of automated workflows by considering noise and cost within the
optimization process. We validate our method through simulations and real-world
experiments using Piezoresponse Force Microscopy (PFM), demonstrating the
successful optimization of measurement duration and property exploration. Our
approach offers a scalable solution for optimizing multiple variables in
automated experimental workflows, improving data quality, and reducing resource
expenditure in materials science and beyond.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlzhiNet: Traversing from 2DCNN to 3DCNN, Towards Early Detection and
  Diagnosis of Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romoke Grace Akindele, Samuel Adebayo, Paul Shekonya Kanda, Ming Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) is a progressive neurodegenerative disorder with
increasing prevalence among the aging population, necessitating early and
accurate diagnosis for effective disease management. In this study, we present
a novel hybrid deep learning framework that integrates both 2D Convolutional
Neural Networks (2D-CNN) and 3D Convolutional Neural Networks (3D-CNN), along
with a custom loss function and volumetric data augmentation, to enhance
feature extraction and improve classification performance in AD diagnosis.
According to extensive experiments, AlzhiNet outperforms standalone 2D and 3D
models, highlighting the importance of combining these complementary
representations of data. The depth and quality of 3D volumes derived from the
augmented 2D slices also significantly influence the model's performance. The
results indicate that carefully selecting weighting factors in hybrid
predictions is imperative for achieving optimal results. Our framework has been
validated on the Magnetic Resonance Imaging (MRI) from Kaggle and MIRIAD
datasets, obtaining accuracies of 98.9% and 99.99%, respectively, with an AUC
of 100%. Furthermore, AlzhiNet was studied under a variety of perturbation
scenarios on the Alzheimer's Kaggle dataset, including Gaussian noise,
brightness, contrast, salt and pepper noise, color jitter, and occlusion. The
results obtained show that AlzhiNet is more robust to perturbations than
ResNet-18, making it an excellent choice for real-world applications. This
approach represents a promising advancement in the early diagnosis and
treatment planning for Alzheimer's disease.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NETS: A Non-Equilibrium Transport Sampler 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael S. Albergo, Eric Vanden-Eijnden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an algorithm, termed the Non-Equilibrium Transport Sampler (NETS),
to sample from unnormalized probability distributions. NETS can be viewed as a
variant of annealed importance sampling (AIS) based on Jarzynski's equality, in
which the stochastic differential equation used to perform the non-equilibrium
sampling is augmented with an additional learned drift term that lowers the
impact of the unbiasing weights used in AIS. We show that this drift is the
minimizer of a variety of objective functions, which can all be estimated in an
unbiased fashion without backpropagating through solutions of the stochastic
differential equations governing the sampling. We also prove that some these
objectives control the Kullback-Leibler divergence of the estimated
distribution from its target. NETS is shown to be unbiased and, in addition,
has a tunable diffusion coefficient which can be adjusted post-training to
maximize the effective sample size. We demonstrate the efficacy of the method
on standard benchmarks, high-dimensional Gaussian mixture distributions, and a
model from statistical lattice field theory, for which it surpasses the
performances of related work and existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Attention Improves Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaniv Leviathan, Matan Kalman, Yossi Matias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unneeded elements in the attention's context degrade performance. We
introduce Selective Attention, a simple parameter-free change to the standard
attention mechanism which reduces attention to unneeded elements. Selective
attention improves language modeling performance in a variety of model sizes
and context lengths. For example, a range of transformers trained with the
language modeling objective on C4 with selective attention perform equivalently
to standard transformers with ~2X more heads and parameters in their attention
modules. Selective attention also allows decreasing the size of the attention's
context buffer, leading to meaningful reductions in the memory and compute
requirements during inference. For example, transformers with 100M parameters
trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and
47X less memory for their attention module, respectively, when equipped with
selective attention, as those without selective attention, with the same
validation perplexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lie Algebra Canonicalization: Equivariant Neural Operators under
  arbitrary Lie Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakhar Shumaylov, Peter Zaika, James Rowbottom, Ferdia Sherry, Melanie Weber, Carola-Bibiane Schönlieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quest for robust and generalizable machine learning models has driven
recent interest in exploiting symmetries through equivariant neural networks.
In the context of PDE solvers, recent works have shown that Lie point
symmetries can be a useful inductive bias for Physics-Informed Neural Networks
(PINNs) through data and loss augmentation. Despite this, directly enforcing
equivariance within the model architecture for these problems remains elusive.
This is because many PDEs admit non-compact symmetry groups, oftentimes not
studied beyond their infinitesimal generators, making them incompatible with
most existing equivariant architectures. In this work, we propose Lie aLgebrA
Canonicalization (LieLAC), a novel approach that exploits only the action of
infinitesimal generators of the symmetry group, circumventing the need for
knowledge of the full group structure. To achieve this, we address existing
theoretical issues in the canonicalization literature, establishing connections
with frame averaging in the case of continuous non-compact groups. Operating
within the framework of canonicalization, LieLAC can easily be integrated with
unconstrained pre-trained models, transforming inputs to a canonical form
before feeding them into the existing model, effectively aligning the input for
model inference according to allowed symmetries. LieLAC utilizes standard Lie
group descent schemes, achieving equivariance in pre-trained models. Finally,
we showcase LieLAC's efficacy on tasks of invariant image classification and
Lie point symmetry equivariant neural PDE solvers using pre-trained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages; preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Clues of Spoofed LM Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaud Gloaguen, Nikola Jovanović, Robin Staab, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM watermarks stand out as a promising way to attribute ownership of
LLM-generated text. One threat to watermark credibility comes from spoofing
attacks, where an unauthorized third party forges the watermark, enabling it to
falsely attribute arbitrary texts to a particular LLM. While recent works have
demonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,
they lack deeper qualitative analysis of the texts produced by spoofing
methods. In this work, we for the first time reveal that there are observable
differences between genuine and spoofed watermark texts. Namely, we show that
regardless of their underlying approach, all current spoofing methods
consistently leave observable artifacts in spoofed texts, indicative of
watermark forgery. We build upon these findings to propose rigorous statistical
tests that reliably reveal the presence of such artifacts, effectively
discovering that a watermark was spoofed. Our experimental evaluation shows
high test power across all current spoofing methods, providing insights into
their fundamental limitations, and suggesting a way to mitigate this threat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DailyDilemmas: Revealing Value Preferences of <span class="highlight-title">LLM</span>s with Quandaries of
  Daily Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we increasingly seek guidance from LLMs for decision-making in daily life,
many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of the users. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
includes two possible actions and with each action, the affected parties and
human values invoked. Based on these dilemmas, we consolidated a set of human
values across everyday topics e.g., interpersonal relationships, workplace, and
environmental issues. We evaluated LLMs on these dilemmas to determine what
action they will take and the values represented by these actions. Then, we
analyzed these values through the lens of five popular theories inspired by
sociology, psychology and philosophy. These theories are: World Value Survey,
Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and
Plutchik Wheel of Emotion. We find that LLMs are most aligned with the
self-expression over survival values in terms of World Value Survey, care over
loyalty in Moral Foundation Theory. Interestingly, we find large preferences
differences in models for some core values such as truthfulness e.g.,
Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to
select it by 9.4%. We also study the recent guidance released by OpenAI
(ModelSpec), and Anthropic (Constitutional AI) to understand how their released
principles reflect their actual value prioritization when facing nuanced moral
reasoning in daily-life settings. We find that end users cannot effectively
steer such prioritization using system prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Mitigating Miscalibration in Prompt Tuning for
  <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuoyuan Wang, Yixuan Li, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confidence calibration is critical for the safe deployment of machine
learning models in the real world. However, such issue in vision-language
models like CLIP, particularly after fine-tuning, has not been fully addressed.
In this work, we demonstrate that existing prompt tuning methods usually lead
to a trade-off of calibration between base and new classes: the cross-entropy
loss in CoOp causes overconfidence in new classes by increasing textual label
divergence, whereas the regularization of KgCoOp maintains the confidence level
but results in underconfidence in base classes due to the improved accuracy.
Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR)
to ensure the confidence calibration on both base and new classes after
fine-tuning. In particular, we propose to minimize the feature deviation of
novel textual labels (instead of base classes) sampled from a large vocabulary.
In effect, DOR prevents the increase in textual divergence for new labels while
easing restrictions on base classes. Extensive experiments demonstrate that DOR
can enhance the calibration performance of current fine-tuning methods on base
and new classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Highly Adaptive Ridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Schuler, Alexander Hagemeister, Mark van der Laan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose the Highly Adaptive Ridge (HAR): a regression method
that achieves a $n^{-1/3}$ dimension-free L2 convergence rate in the class of
right-continuous functions with square-integrable sectional derivatives. This
is a large nonparametric function class that is particularly appropriate for
tabular data. HAR is exactly kernel ridge regression with a specific
data-adaptive kernel based on a saturated zero-order tensor-product spline
basis expansion. We use simulation and real data to confirm our theory. We
demonstrate empirical performance better than state-of-the-art algorithms for
small datasets in particular.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CulturalBench: a Robust, Diverse and Challenging <span class="highlight-title">Benchmark</span> on Measuring
  the (Lack of) Cultural Knowledge of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To make large language models (LLMs) more helpful across diverse cultures, it
is essential to have effective cultural knowledge benchmarks to measure and
track our progress. Effective benchmarks need to be robust, diverse, and
challenging. We introduce CulturalBench: a set of 1,227 human-written and
human-verified questions for effectively assessing LLMs' cultural knowledge,
covering 45 global regions including the underrepresented ones like Bangladesh,
Zimbabwe, and Peru. Questions - each verified by five independent annotators -
span 17 diverse topics ranging from food preferences to greeting etiquettes. We
evaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which
share the same questions but asked differently. We find that LLMs are sensitive
to such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to
human performance (92.6% accuracy), CulturalBench-Hard is more challenging for
frontier LLMs with the best performing model (GPT-4o) at only 61.5% and the
worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with
tricky questions that have multiple correct answers (e.g., What utensils do the
Chinese usually use?), revealing a tendency to converge to a single answer. Our
results also indicate that OpenAI GPT-4o substantially outperform other
proprietary and open source models in questions related to all but one region
(Oceania). Nonetheless, all models consistently underperform on questions
related to South America and the Middle East.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAN: Fourier Analysis Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jing Su, Jun Zhang, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success achieved by neural networks, particularly
those represented by MLP and Transformer, we reveal that they exhibit potential
flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize
the periodic data rather than genuinely understanding the underlying principles
of periodicity. However, periodicity is a crucial trait in various forms of
reasoning and generalization, underpinning predictability across natural and
engineered systems through recurring patterns in observations. In this paper,
we propose FAN, a novel network architecture based on Fourier Analysis, which
empowers the ability to efficiently model and reason about periodic phenomena.
By introducing Fourier Series, the periodicity is naturally integrated into the
structure and computational processes of the neural network, thus achieving a
more accurate expression and prediction of periodic patterns. As a promising
substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in
various models with fewer parameters and FLOPs. Through extensive experiments,
we demonstrate the effectiveness of FAN in modeling and reasoning about
periodic functions, and the superiority and generalizability of FAN across a
range of real-world tasks, including symbolic formula representation, time
series forecasting, and language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUD: <span class="highlight-title">Generation</span> with Unified <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathis Gerdes, Max Welling, Miranda C. N. Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion generative models transform noise into data by inverting a process
that progressively adds noise to data samples. Inspired by concepts from the
renormalization group in physics, which analyzes systems across different
scales, we revisit diffusion models by exploring three key design aspects: 1)
the choice of representation in which the diffusion process operates (e.g.
pixel-, PCA-, Fourier-, or wavelet-basis), 2) the prior distribution that data
is transformed into during diffusion (e.g. Gaussian with covariance $\Sigma$),
and 3) the scheduling of noise levels applied separately to different parts of
the data, captured by a component-wise noise schedule. Incorporating the
flexibility in these choices, we develop a unified framework for diffusion
generative models with greatly enhanced design freedom. In particular, we
introduce soft-conditioning models that smoothly interpolate between standard
diffusion models and autoregressive models (in any basis), conceptually
bridging these two approaches. Our framework opens up a wide design space which
may lead to more efficient training and data generation, and paves the way to
novel architectures integrating different generative approaches and generation
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaIntegrator: Transformer Action Search for Symbolic Integration
  Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Ünsal, Timon Gehr, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first correct-by-construction learning-based system for
step-by-step mathematical integration. The key idea is to learn a policy,
represented by a GPT transformer model, which guides the search for the right
mathematical integration rule, to be carried out by a symbolic solver.
Concretely, we introduce a symbolic engine with axiomatically correct actions
on mathematical expressions, as well as the first dataset for step-by-step
integration. Our GPT-style transformer model, trained on this synthetic data,
demonstrates strong generalization by surpassing its own data generator in
accuracy and efficiency, using 50% fewer search steps. Our experimental results
with SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs
on a set of question-answer pairs is insufficient for solving this mathematical
task. This motivates the importance of discovering creative methods for
combining LLMs with symbolic reasoning engines, of which our work is an
instance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Train Long-Context Language Models (Effectively) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study continued training and supervised fine-tuning (SFT) of a language
model (LM) to make effective use of long-context information. We first
establish a reliable evaluation protocol to guide model development -- Instead
of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set
of long-context tasks, and we evaluate models after SFT with instruction data
as this better reveals long-context abilities. Supported by our robust
evaluations, we run thorough experiments to decide the data mix for continued
pre-training, the instruction tuning dataset, and many other design choices. We
find that (1) code repositories and books are excellent sources of long data,
but it is crucial to combine them with high-quality short data; (2) training
with a sequence length beyond the evaluation length boosts long-context
performance; (3) for SFT, using only short instruction datasets yields strong
performance on long-context tasks. Our final model, ProLong-8B, which is
initialized from Llama-3 and trained on 40B tokens, demonstrates
state-of-the-art long-context performance among similarly sized models at a
length of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of
long-context tasks despite having seen only 5% as many tokens during
long-context training. Additionally, ProLong can effectively process up to 512K
tokens, one of the longest context windows of publicly available LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code, data, and models are available at
  https://github.com/princeton-nlp/ProLong</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Simulation-free Entropic Unbalanced Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemoo Choi, Jaewoong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Optimal Transport (OT) problem investigates a transport map that connects
two distributions while minimizing a given cost function. Finding such a
transport map has diverse applications in machine learning, such as generative
modeling and image-to-image translation. In this paper, we introduce a scalable
and simulation-free approach for solving the Entropic Unbalanced Optimal
Transport (EUOT) problem. We derive the dynamical form of this EUOT problem,
which is a generalization of the Schr\"odinger bridges (SB) problem. Based on
this, we derive dual formulation and optimality conditions of the EUOT problem
from the stochastic optimal control interpretation. By leveraging these
properties, we propose a simulation-free algorithm to solve EUOT, called
Simulation-free EUOT (SF-EUOT). While existing SB models require expensive
simulation costs during training and evaluation, our model achieves
simulation-free training and one-step generation by utilizing the reciprocal
property. Our model demonstrates significantly improved scalability in
generative modeling and image-to-image translation tasks compared to previous
SB methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deconstructing Recurrence, Attention, and Gating: Investigating the
  transferability of Transformers and Gated Recurrent Neural Networks in
  forecasting of dynamical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hunter S. Heidenreich, Pantelis R. Vlachas, Petros Koumoutsakos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning architectures, including transformers and recurrent neural
networks (RNNs) have revolutionized forecasting in applications ranging from
text processing to extreme weather. Notably, advanced network architectures,
tuned for applications such as natural language processing, are transferable to
other tasks such as spatiotemporal forecasting tasks. However, there is a
scarcity of ablation studies to illustrate the key components that enable this
forecasting accuracy. The absence of such studies, although explainable due to
the associated computational cost, intensifies the belief that these models
ought to be considered as black boxes. In this work, we decompose the key
architectural components of the most powerful neural architectures, namely
gating and recurrence in RNNs, and attention mechanisms in transformers. Then,
we synthesize and build novel hybrid architectures from the standard blocks,
performing ablation studies to identify which mechanisms are effective for each
task. The importance of considering these components as hyper-parameters that
can augment the standard architectures is exhibited on various forecasting
datasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96
system, the Kuramoto-Sivashinsky equation, as well as standard real world
time-series benchmarks. A key finding is that neural gating and attention
improves the performance of all standard RNNs in most tasks, while the addition
of a notion of recurrence in transformers is detrimental. Furthermore, our
study reveals that a novel, sparsely used, architecture which integrates
Recurrent Highway Networks with neural gating and attention mechanisms, emerges
as the best performing architecture in high-dimensional spatiotemporal
forecasting of dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAX: Cellular Automata Accelerated in JAX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxence Faldor, Antoine Cully
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cellular automata have become a cornerstone for investigating emergence and
self-organization across diverse scientific disciplines, spanning neuroscience,
artificial life, and theoretical physics. However, the absence of a
hardware-accelerated cellular automata library limits the exploration of new
research directions, hinders collaboration, and impedes reproducibility. In
this work, we introduce CAX (Cellular Automata Accelerated in JAX), a
high-performance and flexible open-source library designed to accelerate
cellular automata research. CAX offers cutting-edge performance and a modular
design through a user-friendly interface, and can support both discrete and
continuous cellular automata with any number of dimensions. We demonstrate
CAX's performance and flexibility through a wide range of benchmarks and
applications. From classic models like elementary cellular automata and
Conway's Game of Life to advanced applications such as growing neural cellular
automata and self-classifying MNIST digits, CAX speeds up simulations up to
2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate
research by presenting a collection of three novel cellular automata
experiments, each implemented in just a few lines of code thanks to the
library's modular architecture. Notably, we show that a simple one-dimensional
cellular automaton can outperform GPT-4 on the 1D-ARC challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Immunogenicity Prediction with Dual Attention Enables Vaccine Target
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Li, Yang Tan, Song Ke, Liang Hong, Bingxin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Immunogenicity prediction is a central topic in reverse vaccinology for
finding candidate vaccines that can trigger protective immune responses.
Existing approaches typically rely on highly compressed features and simple
model architectures, leading to limited prediction accuracy and poor
generalizability. To address these challenges, we introduce ProVaccine, a novel
deep learning solution with a dual attention mechanism that integrates
pre-trained latent vector representations of protein sequences and structures.
We also compile the most comprehensive immunogenicity dataset to date,
encompassing over 9,500 antigen sequences, structures, and immunogenicity
labels from bacteria, viruses, and tumors. Extensive experiments demonstrate
that ProVaccine outperforms existing methods across a wide range of evaluation
metrics. Furthermore, we establish a post-hoc validation protocol to assess the
practical significance of deep learning models in tackling vaccine design
challenges. Our work provides an effective tool for vaccine design and sets
valuable benchmarks for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Labor Migration Modeling through Large-scale Job Query Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Guo, Le Zhang, Hengshu Zhu, Weijia Zhang, Hui Xiong, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and timely modeling of labor migration is crucial for various urban
governance and commercial tasks, such as local policy-making and business site
selection. However, existing studies on labor migration largely rely on limited
survey data with statistical methods, which fail to deliver timely and
fine-grained insights for time-varying regional trends. To this end, we propose
a deep learning-based spatial-temporal labor migration analysis framework,
DHG-SIL, by leveraging large-scale job query data. Specifically, we first
acquire labor migration intention as a proxy of labor migration via job queries
from one of the world's largest search engines. Then, a Disprepant Homophily
co-preserved Graph Convolutional Network (DH-GCN) and an interpretable temporal
module are respectively proposed to capture cross-city and sequential labor
migration dependencies. Besides, we introduce four interpretable variables to
quantify city migration properties, which are co-optimized with city
representations via tailor-designed contrastive losses. Extensive experiments
on three real-world datasets demonstrate the superiority of our DHG-SIL.
Notably, DHG-SIL has been deployed as a core component of a cooperative
partner's intelligent human resource system, and the system supported a series
of city talent attraction reports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Generalization Performance Along the Trajectory of Proximal
  SGD in Robust Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Tan, Pierre C. Bellec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the generalization performance of iterates obtained by
Gradient Descent (GD), Stochastic Gradient Descent (SGD) and their proximal
variants in high-dimensional robust regression problems. The number of features
is comparable to the sample size and errors may be heavy-tailed. We introduce
estimators that precisely track the generalization error of the iterates along
the trajectory of the iterative algorithm. These estimators are provably
consistent under suitable conditions. The results are illustrated through
several examples, including Huber regression, pseudo-Huber regression, and
their penalized variants with non-smooth regularizer. We provide explicit
generalization error estimates for iterates generated from GD and SGD, or from
proximal SGD in the presence of a non-smooth regularizer. The proposed risk
estimates serve as effective proxies for the actual generalization error,
allowing us to determine the optimal stopping iteration that minimizes the
generalization error. Extensive simulations confirm the effectiveness of the
proposed generalization error estimates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse Entropic Optimal Transport Solves Semi-supervised Learning via
  Data Likelihood Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Persiianov, Arip Asadulaev, Nikita Andreev, Nikita Starodubcev, Dmitry Baranchuk, Anastasis Kratsios, Evgeny Burnaev, Alexander Korotin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning conditional distributions $\pi^*(\cdot|x)$ is a central problem in
machine learning, which is typically approached via supervised methods with
paired data $(x,y) \sim \pi^*$. However, acquiring paired data samples is often
challenging, especially in problems such as domain translation. This
necessitates the development of $\textit{semi-supervised}$ models that utilize
both limited paired data and additional unpaired i.i.d. samples $x \sim
\pi^*_x$ and $y \sim \pi^*_y$ from the marginal distributions. The usage of
such combined data is complex and often relies on heuristic approaches. To
tackle this issue, we propose a new learning paradigm that integrates both
paired and unpaired data $\textbf{seamlessly}$ through the data likelihood
maximization techniques. We demonstrate that our approach also connects
intriguingly with inverse entropic optimal transport (OT). This finding allows
us to apply recent advances in computational OT to establish a $\textbf{light}$
learning algorithm to get $\pi^*(\cdot|x)$. Furthermore, we demonstrate through
empirical tests that our method effectively learns conditional distributions
using paired and unpaired data simultaneously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Learning Guided Quasi-Newton Methods with Global Non-Asymptotic
  Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Jiang, Aryan Mokhtari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a quasi-Newton method for solving smooth and
monotone nonlinear equations, including unconstrained minimization and minimax
optimization as special cases. For the strongly monotone setting, we establish
two global convergence bounds: (i) a linear convergence rate that matches the
rate of the celebrated extragradient method, and (ii) an explicit global
superlinear convergence rate that provably surpasses the linear convergence
rate after at most ${O}(d)$ iterations, where $d$ is the problem's dimension.
In addition, for the case where the operator is only monotone, we prove a
global convergence rate of ${O}(\min\{{1}/{k},{\sqrt{d}}/{k^{1.25}}\})$ in
terms of the duality gap. This matches the rate of the extragradient method
when $k = {O}(d^2)$ and is faster when $k = \Omega(d^2)$. These results are the
first global convergence results to demonstrate a provable advantage of a
quasi-Newton method over the extragradient method, without querying the
Jacobian of the operator. Unlike classical quasi-Newton methods, we achieve
this by using the hybrid proximal extragradient framework and a novel online
learning approach for updating the Jacobian approximation matrices.
Specifically, guided by the convergence analysis, we formulate the Jacobian
approximation update as an online convex optimization problem over
non-symmetric matrices, relating the regret of the online problem to the
convergence rate of our method. To facilitate efficient implementation, we
further develop a tailored online learning algorithm based on an approximate
separation oracle, which preserves structures such as symmetry and sparsity in
the Jacobian matrices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diss-l-ECT: Dissecting Graph Data with local Euler Characteristic
  Transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius von Rohrscheidt, Bastian Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Euler Characteristic Transform (ECT) is an efficiently-computable
geometrical-topological invariant that characterizes the global shape of data.
In this paper, we introduce the Local Euler Characteristic Transform
($\ell$-ECT), a novel extension of the ECT particularly designed to enhance
expressivity and interpretability in graph representation learning. Unlike
traditional Graph Neural Networks (GNNs), which may lose critical local details
through aggregation, the $\ell$-ECT provides a lossless representation of local
neighborhoods. This approach addresses key limitations in GNNs by preserving
nuanced local structures while maintaining global interpretability. Moreover,
we construct a rotation-invariant metric based on $\ell$-ECTs for spatial
alignment of data spaces. Our method exhibits superior performance than
standard GNNs on a variety of node classification tasks, particularly in graphs
with high heterophily.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving Fairness in Predictive Process Analytics via Adversarial
  Learning (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimiliano de Leoni, Alessandro Padella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive business process analytics has become important for organizations,
offering real-time operational support for their processes. However, these
algorithms often perform unfair predictions because they are based on biased
variables (e.g., gender or nationality), namely variables embodying
discrimination. This paper addresses the challenge of integrating a debiasing
phase into predictive business process analytics to ensure that predictions are
not influenced by biased variables. Our framework leverages on adversial
debiasing is evaluated on four case studies, showing a significant reduction in
the contribution of biased variables to the predicted value. The proposed
technique is also compared with the state of the art in fairness in process
mining, illustrating that our framework allows for a more enhanced level of
fairness, while retaining a better prediction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoGra-Med: Long Context Multi-Graph Alignment for Medical
  <span class="highlight-title">Vision-Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy M. H. Nguyen, Nghiem T. Diep, Trung Q. Nguyen, Hoang-Bao Le, Tai Nguyen, Tien Nguyen, TrungTin Nguyen, Nhat Ho, Pengtao Xie, Roger Wattenhofer, James Zhou, Daniel Sonntag, Mathias Niepert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art medical multi-modal large language models (med-MLLM), like
LLaVA-Med or BioMedGPT, leverage instruction-following data in pre-training.
However, those models primarily focus on scaling the model size and data volume
to boost performance while mainly relying on the autoregressive learning
objectives. Surprisingly, we reveal that such learning schemes might result in
a weak alignment between vision and language modalities, making these models
highly reliant on extensive pre-training datasets - a significant challenge in
medical domains due to the expensive and time-consuming nature of curating
high-quality instruction-following instances. We address this with LoGra-Med, a
new multi-graph alignment algorithm that enforces triplet correlations across
image modalities, conversation-based descriptions, and extended captions. This
helps the model capture contextual meaning, handle linguistic variability, and
build cross-modal associations between visuals and text. To scale our approach,
we designed an efficient end-to-end learning scheme using black-box gradient
estimation, enabling faster LLaMa 7B training. Our results show LoGra-Med
matches LLAVA-Med performance on 600K image-text pairs for Medical VQA and
significantly outperforms it when trained on 10% of the data. For example, on
VQA-RAD, we exceed LLAVA-Med by 20.13% and nearly match the 100% pre-training
score (72.52% vs. 72.64%). We also surpass SOTA methods like BiomedGPT on
visual chatbots and RadFM on zero-shot image classification with VQA,
highlighting the effectiveness of multi-graph alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndicSentEval: How Effectively do Multilingual Transformer Models encode
  Linguistic Properties for Indic Languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhilesh Aravapalli, Mounika Marreddy, Subba Reddy Oota, Radhika Mamidi, Manish Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have revolutionized the field of natural language
processing. To understand why they perform so well and to assess their
reliability, several studies have focused on questions such as: Which
linguistic properties are encoded by these models, and to what extent? How
robust are these models in encoding linguistic properties when faced with
perturbations in the input text? However, these studies have mainly focused on
BERT and the English language. In this paper, we investigate similar questions
regarding encoding capability and robustness for 8 linguistic properties across
13 different perturbations in 6 Indic languages, using 9 multilingual
Transformer models (7 universal and 2 Indic-specific). To conduct this study,
we introduce a novel multilingual benchmark dataset, IndicSentEval, containing
approximately $\sim$47K sentences. Surprisingly, our probing analysis of
surface, syntactic, and semantic properties reveals that while almost all
multilingual models demonstrate consistent encoding performance for English,
they show mixed results for Indic languages. As expected, Indic-specific
multilingual models capture linguistic properties in Indic languages better
than universal models. Intriguingly, universal models broadly exhibit better
robustness compared to Indic-specific models, particularly under perturbations
such as dropping both nouns and verbs, dropping only verbs, or keeping only
nouns. Overall, this study provides valuable insights into probing and
perturbation-specific strengths and weaknesses of popular multilingual
Transformer-based models for different Indic languages. We make our code and
dataset publicly available [https://tinyurl.com/IndicSentEval}].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative
  Prospect Theoretic Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Lepel, Anas Barakat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widely used expected utility theory has been shown to be empirically
inconsistent with human preferences in the psychology and behavioral economy
literatures. Cumulative Prospect Theory (CPT) has been developed to fill in
this gap and provide a better model for human-based decision-making supported
by empirical evidence. It allows to express a wide range of attitudes and
perceptions towards risk, gains and losses. A few years ago, CPT has been
combined with Reinforcement Learning (RL) to formulate a CPT policy
optimization problem where the goal of the agent is to search for a policy
generating long-term returns which are aligned with their preferences. In this
work, we revisit this policy optimization problem and provide new insights on
optimal policies and their nature depending on the utility function under
consideration. We further derive a novel policy gradient theorem for the CPT
policy optimization objective generalizing the seminal corresponding result in
standard RL. This result enables us to design a model-free policy gradient
algorithm to solve the CPT-RL problem. We illustrate the performance of our
algorithm in simple examples motivated by traffic control and electricity
management applications. We also demonstrate that our policy gradient algorithm
scales better to larger state spaces compared to the existing zeroth order
algorithm for solving the same problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Sequence Recommendation Models Need Decoupled Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningya Feng, Junwei Pan, Jialong Wu, Baixu Chen, Ximei Wang, Qian Li, Xian Hu, Jie Jiang, Mingsheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong user behavior sequences, comprising up to tens of thousands of
history behaviors, are crucial for capturing user interests and predicting user
responses in modern recommendation systems. A two-stage paradigm is typically
adopted to handle these long sequences: a few relevant behaviors are first
searched from the original long sequences via an attention mechanism in the
first stage and then aggregated with the target item to construct a
discriminative representation for prediction in the second stage. In this work,
we identify and characterize, for the first time, a neglected deficiency in
existing long-sequence recommendation models: a single set of embeddings
struggles with learning both attention and representation, leading to
interference between these two processes. Initial attempts to address this
issue using linear projections -- a technique borrowed from language processing
-- proved ineffective, shedding light on the unique challenges of
recommendation models. To overcome this, we propose the Decoupled Attention and
Representation Embeddings (DARE) model, where two distinct embedding tables are
initialized and learned separately to fully decouple attention and
representation. Extensive experiments and analysis demonstrate that DARE
provides more accurate search of correlated behaviors and outperforms baselines
with AUC gains up to 0.9% on public datasets and notable online system
improvements. Furthermore, decoupling embedding spaces allows us to reduce the
attention embedding dimension and accelerate the search procedure by 50%
without significant performance impact, enabling more efficient,
high-performance online serving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Agent</span>s' Room: Narrative <span class="highlight-title">Generation</span> through Multi-step Collaboration <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing compelling fiction is a multifaceted process combining elements such
as crafting a plot, developing interesting characters, and using evocative
language. While large language models (LLMs) show promise for story writing,
they currently rely heavily on intricate prompting, which limits their use. We
propose Agents' Room, a generation framework inspired by narrative theory, that
decomposes narrative writing into subtasks tackled by specialized agents. To
illustrate our method, we introduce Tell Me A Story, a high-quality dataset of
complex writing prompts and human-written stories, and a novel evaluation
framework designed specifically for assessing long narratives. We show that
Agents' Room generates stories that are preferred by expert evaluators over
those produced by baseline systems by leveraging collaboration and
specialization to decompose the complex story writing task into tractable
components. We provide extensive analysis with automated and human-based
metrics of the generated output.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span> & Adversarial Schrödinger Bridges via Iterative Proportional
  Markovian Fitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergei Kholkin, Grigoriy Ksenofontov, David Li, Nikita Kornilov, Nikita Gushchin, Evgeny Burnaev, Alexander Korotin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Iterative Markovian Fitting (IMF) procedure based on iterative reciprocal
and Markovian projections has recently been proposed as a powerful method for
solving the Schr\"odinger Bridge problem. However, it has been observed that
for the practical implementation of this procedure, it is crucial to alternate
between fitting a forward and backward time diffusion at each iteration. Such
implementation is thought to be a practical heuristic, which is required to
stabilize training and obtain good results in applications such as unpaired
domain translation. In our work, we show that this heuristic closely connects
with the pioneer approaches for the Schr\"odinger Bridge based on the Iterative
Proportional Fitting (IPF) procedure. Namely, we find that the practical
implementation of IMF is, in fact, a combination of IMF and IPF procedures, and
we call this combination the Iterative Proportional Markovian Fitting (IPMF)
procedure. We show both theoretically and practically that this combined IPMF
procedure can converge under more general settings, thus, showing that the IPMF
procedure opens a door towards developing a unified framework for solving
Schr\"odinger Bridge problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hainan Xu, Travis M. Bartley, Vladimir Bataev, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present \textbf{H}ybrid-\textbf{A}utoregressive \textbf{IN}ference
Tr\textbf{AN}sducers (HAINAN), a novel architecture for speech recognition that
extends the Token-and-Duration Transducer (TDT) model. Trained with randomly
masked predictor network outputs, HAINAN supports both autoregressive inference
with all network components and non-autoregressive inference without the
predictor. Additionally, we propose a novel semi-autoregressive inference
paradigm that first generates an initial hypothesis using non-autoregressive
inference, followed by refinement steps where each token prediction is
regenerated using parallelized autoregression on the initial hypothesis.
Experiments on multiple datasets across different languages demonstrate that
HAINAN achieves efficiency parity with CTC in non-autoregressive mode and with
TDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN
outperforms TDT and RNN-T, while non-autoregressive HAINAN significantly
outperforms CTC. Semi-autoregressive inference further enhances the model's
accuracy with minimal computational overhead, and even outperforms TDT results
in some cases. These results highlight HAINAN's flexibility in balancing
accuracy and speed, positioning it as a strong candidate for real-world speech
recognition applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Squared Error: Exploring Loss Design for Enhanced Training of
  Generative Flow Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Hu, Yifan Zhang, Zhuoran Li, Longbo Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (GFlowNets) are a novel class of generative models
designed to sample from unnormalized distributions and have found applications
in various important tasks, attracting great research interest in their
training algorithms. In general, GFlowNets are trained by fitting the forward
flow to the backward flow on sampled training objects. Prior work focused on
the choice of training objects, parameterizations, sampling and resampling
strategies, and backward policies, aiming to enhance credit assignment,
exploration, or exploitation of the training process. However, the choice of
regression loss, which can highly influence the exploration and exploitation
behavior of the under-training policy, has been overlooked. Due to the lack of
theoretical understanding for choosing an appropriate regression loss, most
existing algorithms train the flow network by minimizing the squared error of
the forward and backward flows in log-space, i.e., using the quadratic
regression loss. In this work, we rigorously prove that distinct regression
losses correspond to specific divergence measures, enabling us to design and
analyze regression losses according to the desired properties of the
corresponding divergence measures. Specifically, we examine two key properties:
zero-forcing and zero-avoiding, where the former promotes exploitation and
higher rewards, and the latter encourages exploration and enhances diversity.
Based on our theoretical framework, we propose three novel regression losses,
namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three
benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our
proposed losses are compatible with most existing training algorithms, and
significantly improve the performances of the algorithms concerning convergence
speed, sample diversity, and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IC3M: In-Car <span class="highlight-title">Multimodal</span> Multi-object Monitoring for Abnormal Status of
  Both Driver and Passengers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Fang, Zheng Lin, Senkang Hu, Hangcheng Cao, Yiqin Deng, Xianhao Chen, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, in-car monitoring has emerged as a promising technology for
detecting early-stage abnormal status of the driver and providing timely alerts
to prevent traffic accidents. Although training models with multimodal data
enhances the reliability of abnormal status detection, the scarcity of labeled
data and the imbalance of class distribution impede the extraction of critical
abnormal state features, significantly deteriorating training performance.
Furthermore, missing modalities due to environment and hardware limitations
further exacerbate the challenge of abnormal status identification. More
importantly, monitoring abnormal health conditions of passengers, particularly
in elderly care, is of paramount importance but remains underexplored. To
address these challenges, we introduce our IC3M, an efficient
camera-rotation-based multimodal framework for monitoring both driver and
passengers in a car. Our IC3M comprises two key modules: an adaptive threshold
pseudo-labeling strategy and a missing modality reconstruction. The former
customizes pseudo-labeling thresholds for different classes based on the class
distribution, generating class-balanced pseudo labels to guide model training
effectively, while the latter leverages crossmodality relationships learned
from limited labels to accurately recover missing modalities by distribution
transferring from available modalities. Extensive experimental results
demonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,
precision, and recall while exhibiting superior robustness under limited
labeled data and severe missing modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization emerges from local optimization in a self-organized
  learning network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Barland, L. Gil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We design and analyze a new paradigm for building supervised learning
networks, driven only by local optimization rules without relying on a global
error function. Traditional neural networks with a fixed topology are made up
of identical nodes and derive their expressiveness from an appropriate
adjustment of connection weights. In contrast, our network stores new knowledge
in the nodes accurately and instantaneously, in the form of a lookup table.
Only then is some of this information structured and incorporated into the
network geometry. The training error is initially zero by construction and
remains so throughout the network topology transformation phase. The latter
involves a small number of local topological transformations, such as splitting
or merging of nodes and adding binary connections between them. The choice of
operations to be carried out is only driven by optimization of expressivity at
the local scale. What we are primarily looking for in a learning network is its
ability to generalize, i.e. its capacity to correctly answer questions for
which it has never learned the answers. We show on numerous examples of
classification tasks that the networks generated by our algorithm
systematically reach such a state of perfect generalization when the number of
learned examples becomes sufficiently large. We report on the dynamics of the
change of state and show that it is abrupt and has the distinctive
characteristics of a first order phase transition, a phenomenon already
observed for traditional learning networks and known as grokking. In addition
to proposing a non-potential approach for the construction of learning
networks, our algorithm makes it possible to rethink the grokking transition in
a new light, under which acquisition of training data and topological
structuring of data are completely decoupled phenomena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is submitted to Phys. Rev. X. It's a physicist's study
  that focus on a new paradigm for deep learning networks. We would have liked
  to choose other keywords for arXiv to reach a wider community, but don't have
  the rights to do so</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Sample Efficiency and Generalization in Multi-<span class="highlight-title">agent</span>
  Reinforcement Learning via Equivariance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua McClellan, Naveed Haghani, John Winder, Furong Huang, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency
and poor generalization [1]. These challenges are partially due to a lack of
structure or inductive bias in the neural networks typically used in learning
the policy. One such form of structure that is commonly observed in multi-agent
scenarios is symmetry. The field of Geometric Deep Learning has developed
Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to
rotations, translations, and reflections of nodes. Incorporating equivariance
has been shown to improve learning efficiency and decrease error [ 2 ]. In this
paper, we demonstrate that EGNNs improve the sample efficiency and
generalization in MARL. However, we also show that a naive application of EGNNs
to MARL results in poor early exploration due to a bias in the EGNN structure.
To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural
Networks or E2GN2. We compare E2GN2 to other common function approximators
using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant
improvement in sample efficiency, greater final reward convergence, and a 2x-5x
gain in over standard GNNs in our generalization tests. These results pave the
way for more reliable and effective solutions in complex multi-agent systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted as a poster at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Prediction of Suspension Dynamics Performance in
  Multi-Axle Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Chun Lin, Bo-Yi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a deep learning-based framework for predicting the
dynamic performance of suspension systems in multi-axle vehicles, emphasizing
the integration of machine learning with traditional vehicle dynamics modeling.
A Multi-Task Deep Belief Network Deep Neural Network (MTL-DBN-DNN) was
developed to capture the relationships between key vehicle parameters and
suspension performance metrics. The model was trained on data generated from
numerical simulations and demonstrated superior prediction accuracy compared to
conventional DNN models. A comprehensive sensitivity analysis was conducted to
assess the impact of various vehicle and suspension parameters on dynamic
suspension performance. Additionally, the Suspension Dynamic Performance Index
(SDPI) was introduced as a holistic measure to quantify overall suspension
performance, accounting for the combined effects of multiple parameters. The
findings highlight the effectiveness of multitask learning in improving
predictive models for complex vehicle systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Benefit of Being Bayesian in Online Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Zhang, Zhou Lu, Heng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on the framework of Conformal Prediction (CP), we study the online
construction of valid confidence sets given a black-box machine learning model.
By converting the target confidence levels into quantile levels, the problem
can be reduced to predicting the quantiles (in hindsight) of a sequentially
revealed data sequence. Two very different approaches have been studied
previously. (i) Direct approach: Assuming the data sequence is iid or
exchangeable, one could maintain the empirical distribution of the observed
data as an algorithmic belief, and directly predict its quantiles. (ii)
Indirect approach: As statistical assumptions often do not hold in practice, a
recent trend is to consider the adversarial setting and apply first-order
online optimization to moving quantile losses (Gibbs & Cand\`es, 2021). It
requires knowing the target quantile level beforehand, and suffers from certain
validity issues on the obtained confidence sets, due to the associated loss
linearization.
  This paper presents a novel Bayesian CP framework that combines their
strengths. Without any statistical assumption, it is able to both: (i) answer
multiple arbitrary confidence level queries online, with provably low regret;
and (ii) overcome the validity issues suffered by first-order optimization
baselines, due to being "data-centric" rather than "iterate-centric".
  From a technical perspective, our key idea is to regularize the algorithmic
belief of the above direct approach by a Bayesian prior, which "robustifies" it
by simulating a non-linearized Follow the Regularized Leader (FTRL) algorithm
on the output. For statisticians, this can be regarded as an online adversarial
view of Bayesian inference. Importantly, the proposed belief update backbone is
shared by prediction heads targeting different confidence levels, bringing
practical benefits analogous to U-calibration (Kleinberg et al., 2023).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Obtaining Lower Query Complexities through Lightweight Zeroth-Order
  Proximal Gradient Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Gu, Xiyuan Wei, Hualin Zhang, Yi Chang, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zeroth-order (ZO) optimization is one key technique for machine learning
problems where gradient calculation is expensive or impossible. Several
variance reduced ZO proximal algorithms have been proposed to speed up ZO
optimization for non-smooth problems, and all of them opted for the coordinated
ZO estimator against the random ZO estimator when approximating the true
gradient, since the former is more accurate. While the random ZO estimator
introduces bigger error and makes convergence analysis more challenging
compared to coordinated ZO estimator, it requires only $\mathcal{O}(1)$
computation, which is significantly less than $\mathcal{O}(d)$ computation of
the coordinated ZO estimator, with $d$ being dimension of the problem space. To
take advantage of the computationally efficient nature of the random ZO
estimator, we first propose a ZO objective decrease (ZOOD) property which can
incorporate two different types of errors in the upper bound of convergence
rate. Next, we propose two generic reduction frameworks for ZO optimization
which can automatically derive the convergence results for convex and
non-convex problems respectively, as long as the convergence rate for the inner
solver satisfies the ZOOD property. With the application of two reduction
frameworks on our proposed ZOR-ProxSVRG and ZOR-ProxSAGA, two variance reduced
ZO proximal algorithms with fully random ZO estimators, we improve the
state-of-the-art function query complexities from
$\mathcal{O}\left(\min\{\frac{dn^{1/2}}{\epsilon^2},
\frac{d}{\epsilon^3}\}\right)$ to
$\tilde{\mathcal{O}}\left(\frac{n+d}{\epsilon^2}\right)$ under $d >
n^{\frac{1}{2}}$ for non-convex problems, and from
$\mathcal{O}\left(\frac{d}{\epsilon^2}\right)$ to
$\tilde{\mathcal{O}}\left(n\log\frac{1}{\epsilon}+\frac{d}{\epsilon}\right)$
for convex problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neural Computation 36 (5), 897-935</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColaCare: Enhancing Electronic Health Record Modeling through Large
  Language Model-Driven Multi-<span class="highlight-title">Agent</span> Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Wang, Yinghao Zhu, Huiya Zhao, Xiaochen Zheng, Tianlong Wang, Wen Tang, Yasha Wang, Chengwei Pan, Ewen M. Harrison, Junyi Gao, Liantao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by clinical consultations, ColaCare employs two types of
agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.
Expert models process and generate predictions from numerical EHR data, while
LLM agents produce reasoning references and decision-making reports within the
collaborative consultation framework. We additionally incorporate the Merck
Manual of Diagnosis and Therapy (MSD) medical guideline within a
retrieval-augmented generation (RAG) module for authoritative evidence support.
Extensive experiments conducted on four distinct EHR datasets demonstrate
ColaCare's superior performance in mortality prediction tasks, underscoring its
potential to revolutionize clinical decision support systems and advance
personalized precision medicine. The code, complete prompt templates, more case
studies, etc. are publicly available at the anonymous link:
https://colacare.netlify.app.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Flow Matching Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Xiuyuan Cheng, Yao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow Matching (FM) is a simulation-free method for learning a continuous and
invertible flow to interpolate between two distributions, and in particular to
generate data from noise in generative modeling. In this paper, we introduce
Local Flow Matching (LFM), which learns a sequence of FM sub-models and each
matches a diffusion process up to the time of the step size in the
data-to-noise direction. In each step, the two distributions to be interpolated
by the sub-model are closer to each other than data vs. noise, and this enables
the use of smaller models with faster training. The stepwise structure of LFM
is natural to be distilled and different distillation techniques can be adopted
to speed up generation. Theoretically, we prove a generation guarantee of the
proposed flow model in terms of the $\chi^2$-divergence between the generated
and true data distributions. In experiments, we demonstrate the improved
training efficiency and competitive generative performance of LFM compared to
FM on the unconditional generation of tabular data and image datasets, and also
on the conditional generation of robotic manipulation policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span> Models are Evolutionary Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbo Zhang, Benedikt Hartl, Hananel Hazan, Michael Levin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a convergence of machine learning and biology, we reveal that diffusion
models are evolutionary algorithms. By considering evolution as a denoising
process and reversed evolution as diffusion, we mathematically demonstrate that
diffusion models inherently perform evolutionary algorithms, naturally
encompassing selection, mutation, and reproductive isolation. Building on this
equivalence, we propose the Diffusion Evolution method: an evolutionary
algorithm utilizing iterative denoising -- as originally introduced in the
context of diffusion models -- to heuristically refine solutions in parameter
spaces. Unlike traditional approaches, Diffusion Evolution efficiently
identifies multiple optimal solutions and outperforms prominent mainstream
evolutionary algorithms. Furthermore, leveraging advanced concepts from
diffusion models, namely latent space diffusion and accelerated sampling, we
introduce Latent Space Diffusion Evolution, which finds solutions for
evolutionary tasks in high-dimensional complex parameter space while
significantly reducing computational steps. This parallel between diffusion and
evolution not only bridges two different fields but also opens new avenues for
mutual enhancement, raising questions about open-ended evolution and
potentially utilizing non-Gaussian or discrete diffusion models in the context
of Diffusion Evolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Decentralized Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayan Biswas, Anne-Marie Kermarrec, Rishi Sharma, Thibaud Trinca, Martijn de Vos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized learning (DL) is an emerging approach that enables nodes to
collaboratively train a machine learning model without sharing raw data. In
many application domains, such as healthcare, this approach faces challenges
due to the high level of heterogeneity in the training data's feature space.
Such feature heterogeneity lowers model utility and negatively impacts
fairness, particularly for nodes with under-represented training data. In this
paper, we introduce \textsc{Facade}, a clustering-based DL algorithm
specifically designed for fair model training when the training data exhibits
several distinct features. The challenge of \textsc{Facade} is to assign nodes
to clusters, one for each feature, based on the similarity in the features of
their local data, without requiring individual nodes to know apriori which
cluster they belong to. \textsc{Facade} (1) dynamically assigns nodes to their
appropriate clusters over time, and (2) enables nodes to collaboratively train
a specialized model for each cluster in a fully decentralized manner. We
theoretically prove the convergence of \textsc{Facade}, implement our
algorithm, and compare it against three state-of-the-art baselines. Our
experimental results on three datasets demonstrate the superiority of our
approach in terms of model accuracy and fairness compared to all three
competitors. Compared to the best-performing baseline, \textsc{Facade} on the
CIFAR-10 dataset also reduces communication costs by 32.3\% to reach a target
accuracy when cluster sizes are imbalanced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TopER: Topological Embeddings in Graph Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astrit Tola, Funmilola Mary Taiwo, Cuneyt Gurcan Akcora, Baris Coskunuzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph embeddings play a critical role in graph representation learning,
allowing machine learning models to explore and interpret graph-structured
data. However, existing methods often rely on opaque, high-dimensional
embeddings, limiting interpretability and practical visualization.
  In this work, we introduce Topological Evolution Rate (TopER), a novel,
low-dimensional embedding approach grounded in topological data analysis. TopER
simplifies a key topological approach, Persistent Homology, by calculating the
evolution rate of graph substructures, resulting in intuitive and interpretable
visualizations of graph data. This approach not only enhances the exploration
of graph datasets but also delivers competitive performance in graph clustering
and classification tasks. Our TopER-based models achieve or surpass
state-of-the-art results across molecular, biological, and social network
datasets in tasks such as classification, clustering, and visualization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOREL: Enhancing Adversarial Robustness through Multi-Objective
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedjro Salomon Hotegni, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive research has shown that deep neural networks (DNNs) are vulnerable
to slight adversarial perturbations$-$small changes to the input data that
appear insignificant but cause the model to produce drastically different
outputs. In addition to augmenting training data with adversarial examples
generated from a specific attack method, most of the current defense strategies
necessitate modifying the original model architecture components to improve
robustness or performing test-time data purification to handle adversarial
attacks. In this work, we demonstrate that strong feature representation
learning during training can significantly enhance the original model's
robustness. We propose MOREL, a multi-objective feature representation learning
approach, encouraging classification models to produce similar features for
inputs within the same class, despite perturbations. Our training method
involves an embedding space where cosine similarity loss and multi-positive
contrastive loss are used to align natural and adversarial features from the
model encoder and ensure tight clustering. Concurrently, the classifier is
motivated to achieve accurate predictions. Through extensive experiments, we
demonstrate that our approach significantly enhances the robustness of DNNs
against white-box and black-box adversarial attacks, outperforming other
methods that similarly require no architectural changes or test-time data
purification. Our code is available at https://github.com/salomonhotegni/MOREL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Quantification with Bayesian Higher Order ReLU KANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Giroux, Cristiano Fanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the first method of uncertainty quantification in the domain of
Kolmogorov-Arnold Networks, specifically focusing on (Higher Order) ReLUKANs to
enhance computational efficiency given the computational demands of Bayesian
methods. The method we propose is general in nature, providing access to both
epistemic and aleatoric uncertainties. It is also capable of generalization to
other various basis functions. We validate our method through a series of
closure tests, including simple one-dimensional functions and application to
the domain of (Stochastic) Partial Differential Equations. Referring to the
latter, we demonstrate the method's ability to correctly identify functional
dependencies introduced through the inclusion of a stochastic term. The code
supporting this work can be found at
https://github.com/wmdataphys/Bayesian-HR-KAN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fake It Until You Break It: On the Adversarial Robustness of
  AI-generated <span class="highlight-title">Image</span> Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Mavali, Jonas Ricker, David Pape, Yash Sharma, Asja Fischer, Lea Schönherr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative AI (GenAI) offers countless possibilities for creative and
productive tasks, artificially generated media can be misused for fraud,
manipulation, scams, misinformation campaigns, and more. To mitigate the risks
associated with maliciously generated media, forensic classifiers are employed
to identify AI-generated content. However, current forensic classifiers are
often not evaluated in practically relevant scenarios, such as the presence of
an attacker or when real-world artifacts like social media degradations affect
images. In this paper, we evaluate state-of-the-art AI-generated image (AIGI)
detectors under different attack scenarios. We demonstrate that forensic
classifiers can be effectively attacked in realistic settings, even when the
attacker does not have access to the target model and post-processing occurs
after the adversarial examples are created, which is standard on social media
platforms. These attacks can significantly reduce detection accuracy to the
extent that the risks of relying on detectors outweigh their benefits. Finally,
we propose a simple defense mechanism to make CLIP-based detectors, which are
currently the best-performing detectors, robust against these attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrative Decoding: Improve Factuality via Implicit Self-consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Xiao Liang, Yeyun Gong, Wen Xiao, Song Wang, Yuji Zhang, Wenjun Hou, Kaishuai Xu, Wenge Liu, Wenjie Li, Jian Jiao, Qi Chen, Peng Cheng, Wayne Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-consistency-based approaches, which involve repeatedly sampling multiple
outputs and selecting the most consistent one as the final response, prove to
be remarkably effective in improving the factual accuracy of large language
models. Nonetheless, existing methods usually have strict constraints on the
task format, largely limiting their applicability. In this paper, we present
Integrative Decoding (ID), to unlock the potential of self-consistency in
open-ended generation tasks. ID operates by constructing a set of inputs, each
prepended with a previously sampled response, and then processes them
concurrently, with the next token being selected by aggregating of all their
corresponding predictions at each decoding step. In essence, this simple
approach implicitly incorporates self-consistency in the decoding objective.
Extensive evaluation shows that ID consistently enhances factuality over a wide
range of language models, with substantial improvements on the TruthfulQA
(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance
gains amplify progressively as the number of sampled responses increases,
indicating the potential of ID to scale up with repeated sampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMP: Cooperative Motion Prediction with Multi-<span class="highlight-title">Agent</span> Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehao Wang, Yuping Wang, Zhuoyuan Wu, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as model input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies on the OPV2V and V2V4Real datasets, we demonstrate the effectiveness of
our method in cooperative perception, tracking, and motion prediction. In
particular, CMP reduces the average prediction error by 16.4\% with fewer
missing detections compared with the no cooperation setting and by 12.3\%
compared with the strongest baseline. Our work marks a significant step forward
in the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios. The code can be found on the project website:
https://cmp-cooperative-prediction.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://cmp-cooperative-prediction.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Training with Neuron Interaction and Nowcasting Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Knyazev, Abhinav Moudgil, Guillaume Lajoie, Eugene Belilovsky, Simon Lacoste-Julien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network training can be accelerated when a learnable update rule is
used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable
update rules can be costly and unstable to train and use. Recently, Jang et al.
(2023) proposed a simpler approach to accelerate training based on weight
nowcaster networks (WNNs). In their approach, Adam is used for most of the
optimization steps and periodically, only every few steps, a WNN nowcasts
(predicts near future) parameters. We improve WNNs by proposing neuron
interaction and nowcasting (NiNo) networks. In contrast to WNNs, NiNo leverages
neuron connectivity and graph neural networks to more accurately nowcast
parameters. We further show that in some networks, such as Transformers,
modeling neuron connectivity accurately is challenging. We address this and
other limitations, which allows NiNo to accelerate Adam training by up to 50%
in vision and language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>added Llama3-based results and other updates, code is
  https://github.com/SamsungSAILMontreal/nino</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LML-DAP: Language Model Learning a <span class="highlight-title">Dataset</span> for Data-Augmented Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification tasks are typically handled using Machine Learning (ML)
models, which lack a balance between accuracy and interpretability. This paper
introduces a new approach to using Large Language Models (LLMs) for
classification tasks in an explainable way. Unlike ML models that rely heavily
on data cleaning and feature engineering, this method streamlines the process
using LLMs. This paper proposes a new concept called "Language Model Learning
(LML)" powered by a new method called "Data-Augmented Prediction (DAP)". The
classification is performed by LLMs using a method similar to humans manually
exploring and understanding the data and deciding classifications using data as
a reference. In the LML process, a dataset is summarized and evaluated to
determine the features that lead to the classification of each label the most.
In the process of DAP, the system uses the data summary and a row of the
testing dataset to automatically generate a query, which is used to retrieve
relevant rows from the dataset. A classification is generated by the LLM using
data summary and relevant rows, ensuring satisfactory accuracy even with
complex data using context-aware decision-making. LML and DAP unlock the
possibilities of new applications. The proposed method uses the words "Act as
an Explainable Machine Learning Model" in the prompt to enhance the
interpretability of the predictions by allowing users to review the logic
behind each prediction. In some test cases, the system scored an accuracy above
90%, proving the effectiveness of the system and its potential to outperform
conventional ML models in various scenarios. The code is available at
https://github.com/Pro-GenAI/LML-DAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated title, abstract, and images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Training Data Influence of <span class="highlight-title">GPT</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07840v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07840v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Qingyi Liu, Shuohuan Wang, Yu Sun, Qiwei Peng, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst the rapid advancements in generative language models, the
investigation of how training data shapes the performance of GPT models is
still emerging. This paper presents GPTfluence, a novel approach that leverages
a featurized simulation to assess the impact of training examples on the
training dynamics of GPT models. Our approach not only traces the influence of
individual training instances on performance trajectories, such as loss and
other key metrics, on targeted test points but also enables a comprehensive
comparison with existing methods across various training scenarios in GPT
models, ranging from 14 million to 2.8 billion parameters, across a range of
downstream tasks. Contrary to earlier methods that struggle with generalization
to new data, GPTfluence introduces a parameterized simulation of training
dynamics, demonstrating robust generalization capabilities to unseen training
data. This adaptability is evident across both fine-tuning and
instruction-tuning scenarios, spanning tasks in natural language understanding
and generation. We make our code and data publicly available at
https://github.com/ernie-research/gptfluence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preble: Efficient Distributed Prompt Scheduling for <span class="highlight-title">LLM</span> Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikranth Srivatsa, Zijian He, Reyna Abhyankar, Dongming Li, Yiying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompts to large language models (LLMs) have evolved beyond simple user
questions. For LLMs to solve complex problems, today's practices are to include
domain-specific instructions, illustration of tool usages, and/or long context
such as textbook chapters in prompts. As such, many parts of prompts are
repetitive across requests. Recent works propose to cache and reuse KV state of
prompts. However, they are all confined to a single-GPU optimization, while
production LLM serving systems are distributed by nature.
  This paper proposes Preble, the first distributed LLM serving platform that
targets and optimizes for prompt sharing. We designed a distributed scheduling
system that co-optimizes KV state reuse and computation load-balancing with a
new scheduling algorithm and a hierarchical scheduling mechanism. Our
evaluation of Preble with real workloads and request arrival patterns on two
open-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X
to 14.5X on average latency and 2X to 10X on p99 latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E(n) Equivariant Topological Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15429v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15429v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Battiloro, Ege Karaismailoğlu, Mauricio Tec, George Dasoulas, Michelle Audirac, Francesca Dominici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks excel at modeling pairwise interactions, but they
cannot flexibly accommodate higher-order interactions and features. Topological
deep learning (TDL) has emerged recently as a promising tool for addressing
this issue. TDL enables the principled modeling of arbitrary multi-way,
hierarchical higher-order interactions by operating on combinatorial
topological spaces, such as simplicial or cell complexes, instead of graphs.
However, little is known about how to leverage geometric features such as
positions and velocities for TDL. This paper introduces E(n)-Equivariant
Topological Neural Networks (ETNNs), which are E(n)-equivariant message-passing
networks operating on combinatorial complexes, formal objects unifying graphs,
hypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric
node features while respecting rotation, reflection, and translation
equivariance. Moreover, ETNNs are natively ready for settings with
heterogeneous interactions. We provide a theoretical analysis to show the
improved expressiveness of ETNNs over architectures for geometric graphs. We
also show how E(n)-equivariant variants of TDL models can be directly derived
from our framework. The broad applicability of ETNNs is demonstrated through
two tasks of vastly different scales: i) molecular property prediction on the
QM9 benchmark and ii) land-use regression for hyper-local estimation of air
pollution with multi-resolution irregular geospatial data. The results indicate
that ETNNs are an effective tool for learning from diverse types of richly
structured data, as they match or surpass SotA equivariant TDL models with a
significantly smaller computational burden, thus highlighting the benefits of a
principled geometric inductive bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 11 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of
  Average-Reward Restless Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the infinite-horizon, average-reward restless bandit problem in
discrete time. We propose a new class of policies that are designed to drive a
progressively larger subset of arms toward the optimal distribution. We show
that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality
gap for an $N$-armed problem, assuming only a unichain and aperiodicity
assumption. Our approach departs from most existing work that focuses on index
or priority policies, which rely on the Global Attractor Property (GAP) to
guarantee convergence to the optimum, or a recently developed simulation-based
policy, which requires a Synchronization Assumption (SA).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 14 figures. This version includes a restructured main
  result section and new experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookback Lens: Detecting and Mitigating Contextual Hallucinations in
  <span class="highlight-title">Large Language Model</span>s Using Only Attention Maps <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When asked to summarize articles or answer questions given a passage, large
language models (LLMs) can hallucinate details and respond with unsubstantiated
answers that are inaccurate with respect to the input context. This paper
describes a simple approach for detecting such contextual hallucinations. We
hypothesize that contextual hallucinations are related to the extent to which
an LLM attends to information in the provided context versus its own
generations. Based on this intuition, we propose a simple hallucination
detection model whose input features are given by the ratio of attention
weights on the context versus newly generated tokens (for each attention head).
We find that a linear classifier based on these lookback ratio features is as
effective as a richer detector that utilizes the entire hidden states of an LLM
or a text-based entailment model. The lookback ratio-based detector -- Lookback
Lens -- is found to transfer across tasks and even models, allowing a detector
that is trained on a 7B model to be applied (without retraining) to a larger
13B model. We further apply this detector to mitigate contextual
hallucinations, and find that a simple classifier-guided decoding approach is
able to reduce the amount of hallucination, for example by 9.6% in the XSum
summarization task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference long paper. The source code is available
  at https://github.com/voidism/Lookback-Lens</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoPhy: Evaluating Physical Commonsense for Video <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in internet-scale video data pretraining have led to the
development of text-to-video generative models that can create high-quality
videos across a broad range of visual concepts, synthesize realistic motions
and render complex objects. Hence, these generative models have the potential
to become general-purpose simulators of the physical world. However, it is
unclear how far we are from this goal with the existing text-to-video
generative models. To this end, we present VideoPhy, a benchmark designed to
assess whether the generated videos follow physical commonsense for real-world
activities (e.g. marbles will roll down when placed on a slanted surface).
Specifically, we curate diverse prompts that involve interactions between
various material types in the physical world (e.g., solid-solid, solid-fluid,
fluid-fluid). We then generate videos conditioned on these captions from
diverse state-of-the-art text-to-video generative models, including open models
(e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our human
evaluation reveals that the existing models severely lack the ability to
generate videos adhering to the given text prompts, while also lack physical
commonsense. Specifically, the best performing model, CogVideoX-5B, generates
videos that adhere to the caption and physical laws for 39.6% of the instances.
VideoPhy thus highlights that the video generative models are far from
accurately simulating the physical world. Finally, we propose an
auto-evaluator, VideoCon-Physics, to assess the performance reliably for the
newly released models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 29 figures, 12 tables. Added CogVideo and Dream Machine in
  v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative learning of common latent representations in routinely
  collected multivariate ICU physiological signals <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hollan Haule, Ian Piper, Patricia Jones, Tsz-Yan Milly Lo, Javier Escudero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Intensive Care Units (ICU), the abundance of multivariate time series
presents an opportunity for machine learning (ML) to enhance patient
phenotyping. In contrast to previous research focused on electronic health
records (EHR), here we propose an ML approach for phenotyping using routinely
collected physiological time series data. Our new algorithm integrates Long
Short-Term Memory (LSTM) networks with collaborative filtering concepts to
identify common physiological states across patients. Tested on real-world ICU
clinical data for intracranial hypertension (IH) detection in patients with
brain injury, our method achieved an area under the curve (AUC) of 0.889 and
average precision (AP) of 0.725. Moreover, our algorithm outperforms
autoencoders in learning more structured latent representations of the
physiological signals. These findings highlight the promise of our methodology
for patient phenotyping, leveraging routinely collected multivariate time
series to improve clinical care practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2024 IEEE International Conference on Acoustics, Speech,
  and Signal Processing Workshops (ICASSPW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing Medical <span class="highlight-title">Image</span> Representations via Quaternion Wavelet
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10224v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10224v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network generalizability is becoming a broad research field due to the
increasing availability of datasets from different sources and for various
tasks. This issue is even wider when processing medical data, where a lack of
methodological standards causes large variations being provided by different
imaging centers or acquired with various devices and cofactors. To overcome
these limitations, we introduce a novel, generalizable, data- and task-agnostic
framework able to extract salient features from medical images. The proposed
quaternion wavelet network (QUAVE) can be easily integrated with any
pre-existing medical image analysis or synthesis task, and it can be involved
with real, quaternion, or hypercomplex-valued models, generalizing their
adoption to single-channel data. QUAVE first extracts different sub-bands
through the quaternion wavelet transform, resulting in both
low-frequency/approximation bands and high-frequency/fine-grained features.
Then, it weighs the most representative set of sub-bands to be involved as
input to any other neural model for image processing, replacing standard data
samples. We conduct an extensive experimental evaluation comprising different
datasets, diverse image analysis, and synthesis tasks including reconstruction,
segmentation, and modality translation. We also evaluate QUAVE in combination
with both real and quaternion-valued models. Results demonstrate the
effectiveness and the generalizability of the proposed framework that improves
network performance while being flexible to be adopted in manifold scenarios
and robust to domain shifts. The full code is available at:
https://github.com/ispamm/QWT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Limited Generalization Capability of the Implicit Reward Model
  Induced by Direct Preference Optimization <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is an effective approach
for aligning language models to human preferences. Central to RLHF is learning
a reward function for scoring human preferences. Two main approaches for
learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in
RLHF, and 2) using an implicit reward learned from preference data through
methods such as Direct Preference Optimization (DPO). Prior work has shown that
the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in
the limit. DPORM's effectiveness directly implies the optimality of the learned
policy, and also has practical implication for LLM alignment methods including
iterative DPO. However, it is unclear how well DPORM empirically matches the
performance of EXRM. This work studies the accuracy at distinguishing preferred
and rejected answers for both DPORM and EXRM. Our findings indicate that even
though DPORM fits the training dataset comparably, it generalizes less
effectively than EXRM, especially when the validation datasets contain
distribution shifts. Across five out-of-distribution settings, DPORM has a mean
drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that
DPORM has limited generalization ability and substantiates the integration of
an explicit reward model in iterative DPO approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking <span class="highlight-title">LLM</span>s with Arabic Transliteration and Arabizi <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study identifies the potential vulnerabilities of Large Language Models
(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and
its various forms. While most research has concentrated on English-based prompt
manipulation, our investigation broadens the scope to investigate the Arabic
language. We initially tested the AdvBench benchmark in Standardized Arabic,
finding that even with prompt manipulation techniques like prefix injection, it
was insufficient to provoke LLMs into generating unsafe content. However, when
using Arabic transliteration and chatspeak (or arabizi), we found that unsafe
content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3
Sonnet. Our findings suggest that using Arabic and its various forms could
expose information that might remain hidden, potentially increasing the risk of
jailbreak attacks. We hypothesize that this exposure could be due to the
model's learned connection to specific words, highlighting the need for more
comprehensive safety training across all language forms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Allocation in Dynamic Mechanism Design <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Fallah, Michael I. Jordan, Annie Ulichney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a dynamic mechanism design problem where an auctioneer sells an
indivisible good to groups of buyers in every round, for a total of $T$ rounds.
The auctioneer aims to maximize their discounted overall revenue while adhering
to a fairness constraint that guarantees a minimum average allocation for each
group. We begin by studying the static case ($T=1$) and establish that the
optimal mechanism involves two types of subsidization: one that increases the
overall probability of allocation to all buyers, and another that favors the
groups which otherwise have a lower probability of winning the item. We then
extend our results to the dynamic case by characterizing a set of recursive
functions that determine the optimal allocation and payments in each round.
Notably, our results establish that in the dynamic case, the seller, on the one
hand, commits to a participation bonus to incentivize truth-telling, and on the
other hand, charges an entry fee for every round. Moreover, the optimal
allocation once more involves subsidization, which its extent depends on the
difference in future utilities for both the seller and buyers when allocating
the item to one group versus the others. Finally, we present an approximation
scheme to solve the recursive equations and determine an approximately optimal
and fair allocation efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter conference version has been accepted at the Advances in
  Neural Information Processing Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signature Isolation Forest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Campi, Guillaume Staerman, Gareth W. Peters, Tomoko Matsui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly
Detection (AD) algorithm designed for functional data. It relies on a tree
partition procedure where an abnormality score is computed by projecting each
curve observation on a drawn dictionary through a linear inner product. Such
linear inner product and the dictionary are a priori choices that highly
influence the algorithm's performances and might lead to unreliable results,
particularly with complex datasets. This work addresses these challenges by
introducing \textit{Signature Isolation Forest}, a novel AD algorithm class
leveraging the rough path theory's signature transform. Our objective is to
remove the constraints imposed by FIF through the proposition of two algorithms
which specifically target the linearity of the FIF inner product and the choice
of the dictionary. We provide several numerical experiments, including a
real-world applications benchmark showing the relevance of our methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DyGPrompt: Learning Feature and Time Prompts on Dynamic Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13937v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13937v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingtong Yu, Zhenghao Liu, Yuan Fang, Xinming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graphs capture evolving interactions between entities, such as in
social networks, online learning platforms, and crowdsourcing projects. For
dynamic graph modeling, dynamic graph neural networks (DGNNs) have emerged as a
mainstream technique. However, they are generally pre-trained on the link
prediction task, leaving a significant gap from the objectives of downstream
tasks such as node classification. To bridge the gap, prompt-based learning has
gained traction on graphs, but most existing efforts focus on static graphs,
neglecting the evolution of dynamic graphs. In this paper, we propose
DYGPROMPT, a novel pre-training and prompt learning framework for dynamic graph
modeling. First, we design dual prompts to address the gap in both task
objectives and temporal variations across pre-training and downstream tasks.
Second, we recognize that node and time features mutually characterize each
other, and propose dual condition-nets to model the evolving node-time patterns
in downstream tasks. Finally, we thoroughly evaluate and analyze DYGPROMPT
through extensive experiments on four public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Refusal Training in <span class="highlight-title">LLM</span>s Generalize to the Past Tense? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Andriushchenko, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Refusal training is widely used to prevent LLMs from generating harmful,
undesirable, or illegal outputs. We reveal a curious generalization gap in the
current refusal training approaches: simply reformulating a harmful request in
the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make
a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art
LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,
GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,
o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For
example, the success rate of this simple attack on GPT-4o increases from 1%
using direct requests to 88% using 20 past tense reformulation attempts on
harmful requests from JailbreakBench with GPT-4 as a jailbreak judge.
Interestingly, we also find that reformulations in the future tense are less
effective, suggesting that refusal guardrails tend to consider past historical
questions more benign than hypothetical future questions. Moreover, our
experiments on fine-tuning GPT-3.5 Turbo show that defending against past
reformulations is feasible when past tense examples are explicitly included in
the fine-tuning data. Overall, our findings highlight that the widely used
alignment techniques -- such as SFT, RLHF, and adversarial training -- employed
to align the studied models can be brittle and do not always generalize as
intended. We provide code and jailbreak artifacts at
https://github.com/tml-epfl/llm-past-tense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update in v3: o1-mini and o1-preview results (on top of GPT-4o and
  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at
  https://github.com/tml-epfl/llm-past-tense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Label Distribution Learning for Multi-Label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Zhao, Yuexuan An, Lei Qi, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label classification (MLC) refers to the problem of tagging a given
instance with a set of relevant labels. Most existing MLC methods are based on
the assumption that the correlation of two labels in each label pair is
symmetric, which is violated in many real-world scenarios. Moreover, most
existing methods design learning processes associated with the number of
labels, which makes their computational complexity a bottleneck when scaling up
to large-scale output space. To tackle these issues, we propose a novel method
named Scalable Label Distribution Learning (SLDL) for multi-label
classification which can describe different labels as distributions in a latent
space, where the label correlation is asymmetric and the dimension is
independent of the number of labels. Specifically, SLDL first converts labels
into continuous distributions within a low-dimensional latent space and
leverages the asymmetric metric to establish the correlation between different
labels. Then, it learns the mapping from the feature space to the latent space,
resulting in the computational complexity is no longer related to the number of
labels. Finally, SLDL leverages a nearest-neighbor-based strategy to decode the
latent representations and obtain the final predictions. Extensive experiments
illustrate that SLDL achieves very competitive classification performances with
little computational consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundations of <span class="highlight-title">Large Language Model</span> Compression -- Part 1: Weight
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean I. Young
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, compression of large language models (LLMs) has emerged as
an important problem to enable language model deployment on
resource-constrained devices, reduce computational costs, and mitigate the
environmental footprint of large-scale AI infrastructure. In this paper, we lay
down the foundation for LLM quantization from a convex optimization perspective
and propose a quantization technique that builds on this foundation for optimum
quantization outcomes. Our quantization framework, CVXQ, scales to models
containing hundreds of billions of weight parameters and provides users with
the flexibility to compress models to any specified model size, post-training.
A reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. 17 pages, 4 figures, 5 appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NECOMIMI: Neural-Cognitive <span class="highlight-title">Multimodal</span> EEG-informed <span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> with
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Sheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NECOMIMI (NEural-COgnitive MultImodal EEG-Informed Image Generation with
Diffusion Models) introduces a novel framework for generating images directly
from EEG signals using advanced diffusion models. Unlike previous works that
focused solely on EEG-image classification through contrastive learning,
NECOMIMI extends this task to image generation. The proposed NERV EEG encoder
demonstrates state-of-the-art (SoTA) performance across multiple zero-shot
classification tasks, including 2-way, 4-way, and 200-way, and achieves top
results in our newly proposed Category-based Assessment Table (CAT) Score,
which evaluates the quality of EEG-generated images based on semantic concepts.
A key discovery of this work is that the model tends to generate abstract or
generalized images, such as landscapes, rather than specific objects,
highlighting the inherent challenges of translating noisy and low-resolution
EEG data into detailed visual outputs. Additionally, we introduce the CAT Score
as a new metric tailored for EEG-to-image evaluation and establish a benchmark
on the ThingsEEG dataset. This study underscores the potential of EEG-to-image
generation while revealing the complexities and challenges that remain in
bridging neural activity with visual representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EIA: Environmental Injection Attack on Generalist Web <span class="highlight-title">Agent</span>s for Privacy
  Leakage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist web agents have demonstrated remarkable potential in autonomously
completing a wide range of tasks on real websites, significantly boosting human
productivity. However, web tasks, such as booking flights, usually involve
users' PII, which may be exposed to potential privacy risks if web agents
accidentally interact with compromised websites, a scenario that remains
largely unexplored in the literature. In this work, we narrow this gap by
conducting the first study on the privacy risks of generalist web agents in
adversarial environments. First, we present a realistic threat model for
attacks on the website, where we consider two adversarial targets: stealing
users' specific PII or the entire user request. Then, we propose a novel attack
method, termed Environmental Injection Attack (EIA). EIA injects malicious
content designed to adapt well to environments where the agents operate and our
work instantiates EIA specifically for privacy scenarios in web environments.
We collect 177 action steps that involve diverse PII categories on realistic
websites from the Mind2Web, and conduct experiments using one of the most
capable generalist web agent frameworks to date. The results demonstrate that
EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user
request. Additionally, by accessing the stealthiness and experimenting with a
defensive system prompt, we indicate that EIA is hard to detect and mitigate.
Notably, attacks that are not well adapted for a webpage can be detected via
human inspection, leading to our discussion about the trade-off between
security and autonomy. However, extra attackers' efforts can make EIA
seamlessly adapted, rendering such supervision ineffective. Thus, we further
discuss the defenses at the pre- and post-deployment stages of the websites
without relying on human supervision and call for more advanced defense
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph <span class="highlight-title">Diffusion</span> Transformers for Multi-Conditional Molecular <span class="highlight-title">Generation</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13858v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13858v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse molecular design with diffusion models holds great potential for
advancements in material and drug discovery. Despite success in unconditional
molecular generation, integrating multiple properties such as synthetic score
and gas permeability as condition constraints into diffusion models remains
unexplored. We present the Graph Diffusion Transformer (Graph DiT) for
multi-conditional molecular generation. Graph DiT integrates an encoder to
learn numerical and categorical property representations with the
Transformer-based denoiser. Unlike previous graph diffusion models that add
noise separately on the atoms and bonds in the forward diffusion process, Graph
DiT is trained with a novel graph-dependent noise model for accurate estimation
of graph-related noise in molecules. We extensively validate Graph DiT for
multi-conditional polymer and small molecule generation. Results demonstrate
the superiority of Graph DiT across nine metrics from distribution learning to
condition control for molecular properties. A polymer inverse design task for
gas separation with feedback from domain experts further demonstrates its
practical utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (Oral). 21 pages, 11 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample and Oracle Efficient Reinforcement Learning for MDPs with
  Linearly-Realizable Value Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakaria Mhammedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing sample-efficient and computationally feasible reinforcement
learning (RL) algorithms is particularly challenging in environments with large
or infinite state and action spaces. In this paper, we advance this effort by
presenting an efficient algorithm for Markov Decision Processes (MDPs) where
the state-action value function of any policy is linear in a given feature map.
This challenging setting can model environments with infinite states and
actions, strictly generalizes classic linear MDPs, and currently lacks a
computationally efficient algorithm under online access to the MDP.
Specifically, we introduce a new RL algorithm that efficiently finds a
near-optimal policy in this setting, using a number of episodes and calls to a
cost-sensitive classification (CSC) oracle that are both polynomial in the
problem parameters. Notably, our CSC oracle can be efficiently implemented when
the feature dimension is constant, representing a clear improvement over
state-of-the-art methods, which require solving non-convex problems with
horizon-many variables and can incur computational costs that are exponential
in the horizon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adjusted Expected Improvement for Cumulative Regret Minimization in
  Noisy Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.04901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.04901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shouri Hu, Haowei Wang, Zhongxiang Dai, Bryan Kian Hsiang Low, Szu Hui Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expected improvement (EI) is one of the most popular acquisition
functions for Bayesian optimization (BO) and has demonstrated good empirical
performances in many applications for the minimization of simple regret.
However, under the evaluation metric of cumulative regret, the performance of
EI may not be competitive, and its existing theoretical regret upper bound
still has room for improvement. To adapt the EI for better performance under
cumulative regret, we introduce a novel quantity called the evaluation cost
which is compared against the acquisition function, and with this, develop the
expected improvement-cost (EIC) algorithm. In each iteration of EIC, a new
point with the largest acquisition function value is sampled, only if that
value exceeds its evaluation cost. If none meets this criteria, the current
best point is resampled. This evaluation cost quantifies the potential downside
of sampling a point, which is important under the cumulative regret metric as
the objective function value in every iteration affects the performance
measure. We establish in theory a high-probability regret upper bound of EIC
based on the maximum information gain, which is tighter than the bound of
existing EI-based algorithms. It is also comparable to the regret bound of
other popular BO algorithms such as Thompson sampling (GP-TS) and upper
confidence bound (GP-UCB). We further perform experiments to illustrate the
improvement of EIC over several popular BO algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohua Lu, Leshanshui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore different approaches to anomaly detection on
dynamic knowledge graphs, specifically in a microservices environment for
Kubernetes applications. Our approach explores three dynamic knowledge graph
representations: sequential data, one-hop graph structure, and two-hop graph
structure, with each representation incorporating increasingly complex
structural information. Each phase includes different machine learning and deep
learning models. We empirically analyse their performance and propose an
approach based on ensemble learning of these models. Our approach significantly
outperforms the baseline on the ISWC 2024 Dynamic Knowledge Graph Anomaly
Detection dataset, providing a robust solution for anomaly detection in dynamic
complex data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Generative Learning Approach for Two-stage Adaptive Robust
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aron Brenner, Rahman Khorramfar, Jennifer Sun, Saurabh Amin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-stage adaptive robust optimization (ARO) is a powerful approach for
planning under uncertainty, balancing first-stage decisions with recourse
decisions made after uncertainty is realized. To account for uncertainty,
modelers typically define a simple uncertainty set over which potential
outcomes are considered. However, classical methods for defining these sets
unintentionally capture a wide range of unrealistic outcomes, resulting in
overly-conservative and costly planning in anticipation of unlikely
contingencies. In this work, we introduce AGRO, a solution algorithm that
performs adversarial generation for two-stage adaptive robust optimization
using a variational autoencoder. AGRO generates high-dimensional contingencies
that are simultaneously adversarial and realistic, improving the robustness of
first-stage decisions at a lower planning cost than standard methods. To ensure
generated contingencies lie in high-density regions of the uncertainty
distribution, AGRO defines a tight uncertainty set as the image of "latent"
uncertainty sets under the VAE decoding transformation. Projected gradient
ascent is then used to maximize recourse costs over the latent uncertainty sets
by leveraging differentiable optimization methods. We demonstrate the
cost-efficiency of AGRO by applying it to both a synthetic
production-distribution problem and a real-world power system expansion
setting. We show that AGRO outperforms the standard column-and-constraint
algorithm by up to 1.8% in production-distribution planning and up to 11.6% in
power system expansion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of <span class="highlight-title">LLM</span>s for
  Legal Domain Adaptation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitodru Niyogi, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Paramanu-Ayn, a collection of legal language models
trained exclusively on Indian legal case documents. This 97-million-parameter
Auto-Regressive (AR) decoder-only model was pretrained from scratch with a
context size of 8192 on a single GPU for just 185 hours, achieving an efficient
MFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We
evaluated our model using perplexity and zero-shot tasks: case judgment
prediction with explanation and abstractive case summarization. Paramanu-Ayn
outperformed Llama-2 7B and Gemini-Pro in case judgment prediction with
explanation task on test accuracy by nearly 2 percentage points, despite being
72 times smaller. In zero-shot abstractive summarization, it surpassed
decoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10
percentage points in BLEU and METEOR metrics, and by nearly 4 percentage points
in BERTScore. Further evaluations on zero-shot commonsense and mathematical
benchmarks showed that Paramanu-Ayn excelled despite being trained exclusively
on legal documents, outperforming Llama-1, Llama-2, and Falcon on
AGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our
model on 10,763 diverse legal tasks, including legal clause generation, legal
drafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above
8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by
GPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge
and generalize to draft legal contracts and legal clauses with limited
instruction-tuning. Hence, we conclude that for a strong domain-specialized
generative language model (such as legal), domain specialized pretraining from
scratch is more cost effective, environmentally friendly, and remains
competitive with larger models or even better than adapting LLMs for legal
domain tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advantage Alignment Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Razvan Ciuca, Tianyu Zhang, Gauthier Gidel, Aaron Courville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificially intelligent agents are increasingly being integrated into human
decision-making: from large language model (LLM) assistants to autonomous
vehicles. These systems often optimize their individual objective, leading to
conflicts, particularly in general-sum games where naive reinforcement learning
agents empirically converge to Pareto-suboptimal Nash equilibria. To address
this issue, opponent shaping has emerged as a paradigm for finding socially
beneficial equilibria in general-sum games. In this work, we introduce
Advantage Alignment, a family of algorithms derived from first principles that
perform opponent shaping efficiently and intuitively. We achieve this by
aligning the advantages of interacting agents, increasing the probability of
mutually beneficial actions when their interaction has been positive. We prove
that existing opponent shaping methods implicitly perform Advantage Alignment.
Compared to these methods, Advantage Alignment simplifies the mathematical
formulation of opponent shaping, reduces the computational burden and extends
to continuous action domains. We demonstrate the effectiveness of our
algorithms across a range of social dilemmas, achieving state-of-the-art
cooperation and robustness against exploitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 Pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Matrix Multiplications for Lookup Table-Quantized <span class="highlight-title">LLM</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large language models (LLMs) is often constrained by memory
bandwidth, where the primary bottleneck is the cost of transferring model
parameters from the GPU's global memory to its registers. When coupled with
custom kernels that fuse the dequantization and matmul operations, weight-only
quantization can thus enable faster inference by reducing the amount of memory
movement. However, developing high-performance kernels for weight-quantized
LLMs presents substantial challenges, especially when the weights are
compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,
lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup
table engine for LUT-quantized LLMs, which uses offline restructuring of the
quantized weight matrix to minimize bit manipulations associated with
unpacking, and vectorization and duplication of the lookup table to mitigate
shared memory bandwidth constraints. At batch sizes < 32 and quantization group
size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster
than existing GEMM kernels. As an application of FLUTE, we explore a simple
extension to lookup table-based NormalFloat quantization and apply it to
quantize LLaMA3 to various configurations, obtaining competitive quantization
performance against strong baselines while obtaining an end-to-end throughput
increase of 1.5 to 2 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nebula: A discourse aware Minecraft Builder <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18164v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18164v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Chaturvedi, Kate Thompson, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When engaging in collaborative tasks, humans efficiently exploit the semantic
structure of a conversation to optimize verbal and nonverbal interactions. But
in recent "language to code" or "language to action" models, this information
is lacking. We show how incorporating the prior discourse and nonlinguistic
context of a conversation situated in a nonlinguistic environment can improve
the "language to action" component of such interactions. We finetune an LLM to
predict actions based on prior context; our model, Nebula, doubles the
net-action F1 score over the baseline on this task of Jayannavar et al.(2020).
We also investigate our model's ability to construct shapes and understand
location descriptions using a synthetic dataset
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongForm: Effective Instruction Tuning with Reverse Instructions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullatif Köksal, Timo Schick, Anna Korhonen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning enables language models to more effectively generalize and
better follow user intent. However, obtaining instruction data is costly and
challenging. Prior work employs methods such as expensive human annotation,
crowd-sourced datasets with alignment issues, and generating noisy examples via
LLMs. We introduce the LongForm-C dataset, which is created by reverse
instructions. We generate instructions via LLMs for human-written corpus
examples using reverse instructions. First we select a diverse set of
human-written documents from corpora such as C4 and Wikipedia; then we generate
instructions for these documents via LLMs. This approach provides a cheaper and
cleaner instruction-tuning dataset with natural output and one suitable for
long text generation. Our models outperform 10x larger language models without
instruction tuning on tasks such as story/recipe generation and long-form
question answering. Moreover, LongForm models outperform prior
instruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and
improve language understanding capabilities further. We publicly release our
data and models: https://github.com/akoksal/LongForm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings. This version extends the training with recent
  LLMs, evaluation with new metrics, and NLU tasks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual-based Attention Physics-informed Neural Networks for
  Spatio-Temporal Ageing Assessment of Transformers Operated in Renewable Power
  Plants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibai Ramirez, Joel Pino, David Pardo, Mikel Sanz, Luis del Rio, Alvaro Ortiz, Kateryna Morozovska, Jose I. Aizpurua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are crucial for reliable and efficient power system operations,
particularly in supporting the integration of renewable energy. Effective
monitoring of transformer health is critical to maintain grid stability and
performance. Thermal insulation ageing is a key transformer failure mode, which
is generally tracked by monitoring the hotspot temperature (HST). However, HST
measurement is complex, costly, and often estimated from indirect measurements.
Existing HST models focus on space-agnostic thermal models, providing
worst-case HST estimates. This article introduces a spatio-temporal model for
transformer winding temperature and ageing estimation, which leverages
physics-based partial differential equations (PDEs) with data-driven Neural
Networks (NN) in a Physics Informed Neural Networks (PINNs) configuration to
improve prediction accuracy and acquire spatio-temporal resolution. The
computational accuracy of the PINN model is improved through the implementation
of the Residual-Based Attention (PINN-RBA) scheme that accelerates the PINN
model convergence. The PINN-RBA model is benchmarked against self-adaptive
attention schemes and classical vanilla PINN configurations. For the first
time, PINN based oil temperature predictions are used to estimate
spatio-temporal transformer winding temperature values, validated through PDE
numerical solution and fiber optic sensor measurements. Furthermore, the
spatio-temporal transformer ageing model is inferred, which supports
transformer health management decision-making. Results are validated with a
distribution transformer operating on a floating photovoltaic power plant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and
  <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18313v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18313v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanting Xie, So Yeon Min, Tianyi Zhang, Aarav Bajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson, Yonatan Bisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is no limit to how much a robot might explore and learn, but all of
that knowledge needs to be searchable and actionable. Within language research,
retrieval augmented generation (RAG) has become the workhouse of large-scale
non-parametric knowledge, however existing techniques do not directly transfer
to the embodied domain, which is multimodal, data is highly correlated, and
perception requires abstraction.
  To address these challenges, we introduce Embodied-RAG, a framework that
enhances the foundational model of an embodied agent with a non-parametric
memory system capable of autonomously constructing hierarchical knowledge for
both navigation and language generation. Embodied-RAG handles a full range of
spatial and semantic resolutions across diverse environments and query types,
whether for a specific object or a holistic description of ambiance. At its
core, Embodied-RAG's memory is structured as a semantic forest, storing
language descriptions at varying levels of detail. This hierarchical
organization allows the system to efficiently generate context-sensitive
outputs across different robotic platforms. We demonstrate that Embodied-RAG
effectively bridges RAG to the robotics domain, successfully handling over 200
explanation and navigation queries across 19 environments, highlighting its
promise for general-purpose non-parametric system for embodied agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Web: https://quanting-xie.github.io/Embodied-RAG-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDMol: Text-to-Molecule <span class="highlight-title">Diffusion</span> Model with Structurally Informative
  Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinho Chang, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of diffusion models as the frontline of generative models,
many researchers have proposed molecule generation techniques with conditional
diffusion models. However, the unavoidable discreteness of a molecule makes it
difficult for a diffusion model to connect raw data with highly complex
conditions like natural language. To address this, we present a novel latent
diffusion model dubbed LDMol for text-conditioned molecule generation. LDMol
comprises a molecule autoencoder that produces a learnable and structurally
informative feature space, and a natural language-conditioned latent diffusion
model. In particular, recognizing that multiple SMILES notations can represent
the same molecule, we employ a contrastive learning strategy to extract feature
space that is aware of the unique characteristics of the molecule structure.
LDMol outperforms the existing baselines on the text-to-molecule generation
benchmark, suggesting a potential for diffusion models can outperform
autoregressive models in text data generation with a better choice of the
latent domain. Furthermore, we show that LDMol can be applied to downstream
tasks such as molecule-to-text retrieval and text-guided molecule editing,
demonstrating its versatility as a diffusion model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning an Actionable Discrete <span class="highlight-title">Diffusion</span> Policy via Large-Scale
  Actionless Video Pre-Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. However, it remains a challenge due to the domain gap between
humans and robots. Moreover, it is difficult to extract useful information
representing the dynamic world from human videos, because of its noisy and
multimodal data structure. In this paper, we introduce a novel framework to
tackle these challenges, which leverages a unified discrete diffusion to
combine generative pre-training on human videos and policy fine-tuning on a
small number of action-labeled robot videos. We start by compressing both human
and robot videos into unified video tokens. In the pre-training stage, we
employ a discrete diffusion model with a mask-and-replace diffusion strategy to
predict future video tokens in the latent space. In the fine-tuning stage, we
harness the imagined future videos to guide low-level action learning with a
limited set of robot data. Experiments demonstrate that our method generates
high-fidelity future videos for planning and enhances the fine-tuned policies
compared to previous state-of-the-art approaches with superior performance. Our
project website is available at https://video-diff.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. 24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Sensitivity of Learning with Limited Labelled Data to the Effects of
  Randomness: Impact of Interactions and Systematic Choices <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Branislav Pecher, Ivan Srba, Maria Bielikova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While learning with limited labelled data can improve performance when the
labels are lacking, it is also sensitive to the effects of uncontrolled
randomness introduced by so-called randomness factors (e.g., varying order of
data). We propose a method to systematically investigate the effects of
randomness factors while taking the interactions between them into
consideration. To measure the true effects of an individual randomness factor,
our method mitigates the effects of other factors and observes how the
performance varies across multiple runs. Applying our method to multiple
randomness factors across in-context learning and fine-tuning approaches on 7
representative text classification tasks and meta-learning on 3 tasks, we show
that: 1) disregarding interactions between randomness factors in existing works
caused inconsistent findings due to incorrect attribution of the effects of
randomness factors, such as disproving the consistent sensitivity of in-context
learning to sample order even with random sample selection; and 2) besides
mutual interactions, the effects of randomness factors, especially sample
order, are also dependent on more systematic choices unexplored in existing
works, such as number of classes, samples per class or choice of prompt format.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the EMNLP'24 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AtomSurf : Surface Representation for Learning on Protein Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16519v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16519v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Mallet, Souhaib Attaiki, Yangyang Miao, Bruno Correia, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While there has been significant progress in evaluating and comparing
different representations for learning on protein data, the role of
surface-based learning approaches remains not well-understood. In particular,
there is a lack of direct and fair benchmark comparison between the best
available surface-based learning methods against alternative representations
such as graphs. Moreover, the few existing surface-based approaches either use
surface information in isolation or, at best, perform global pooling between
surface and graph-based architectures.
  In this work, we fill this gap by first adapting a state-of-the-art surface
encoder for protein learning tasks. We then perform a direct and fair
comparison of the resulting method against alternative approaches within the
Atom3D benchmark, highlighting the limitations of pure surface-based learning.
Finally, we propose an integrated approach, which allows learned feature
sharing between graphs and surface representations on the level of nodes and
vertices $\textit{across all layers}$.
  We demonstrate that the resulting architecture achieves state-of-the-art
results on all tasks in the Atom3D benchmark, while adhering to the strict
benchmark protocol, as well as more broadly on binding site identification and
binding pocket classification. Furthermore, we use coarsened surfaces and
optimize our approach for efficiency, making our tool competitive in training
and inference time with existing techniques. Our code and data can be found
online: $\texttt{github.com/Vincentx15/atomsurf}$
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PowerPM: Foundation Model for Power Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Tu, Yupeng Zhang, Jing Zhang, Zhendong Fu, Yin Zhang, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of abundant electricity time series (ETS) data provides ample
opportunities for various applications in the power systems, including
demand-side management, grid stability, and consumer behavior analysis. Deep
learning models have advanced ETS modeling by effectively capturing sequence
dependence. Nevertheless, learning a generic representation of ETS data for
various applications remains challenging due to the inherently complex
hierarchical structure of ETS data. Moreover, ETS data exhibits intricate
temporal dependencies and is suscepti ble to the influence of exogenous
variables. Furthermore, different instances exhibit diverse electricity
consumption behavior. In this paper, we propose a foundation model PowerPM to
model ETS data, providing a large-scale, off-the-shelf model for power systems.
PowerPM consists of a temporal encoder and a hierarchical encoder. The temporal
encoder captures both temporal dependencies in ETS data, considering exogenous
variables. The hierarchical encoder models the correlation between hierarchy.
Furthermore, PowerPM leverages a novel self-supervised pretraining framework
consisting of masked ETS modeling and dual-view contrastive learning, which
enable PowerPM to capture temporal dependency within ETS windows and aware the
discrepancy across ETS windows, providing two different perspectives to learn
generic representation. Our experiments involve five real world scenario
datasets, comprising private and public data. Through pre-training on massive
ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within
the private dataset. Impressively, when transferred to the public datasets,
PowerPM maintains its superiority, showcasing its remarkable generalization
ability across various tasks and domains. Moreover, ablation studies, few-shot
experiments provide additional evidence of the effectiveness of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BadCM: Invisible Backdoor Attack Against Cross-Modal Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhang, Xu Yuan, Lei Zhu, Jingkuan Song, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable successes in unimodal learning tasks, backdoor attacks
against cross-modal learning are still underexplored due to the limited
generalization and inferior stealthiness when involving multiple modalities.
Notably, since works in this area mainly inherit ideas from unimodal visual
attacks, they struggle with dealing with diverse cross-modal attack
circumstances and manipulating imperceptible trigger samples, which hinders
their practicability in real-world applications. In this paper, we introduce a
novel bilateral backdoor to fill in the missing pieces of the puzzle in the
cross-modal backdoor and propose a generalized invisible backdoor framework
against cross-modal learning (BadCM). Specifically, a cross-modal mining scheme
is developed to capture the modality-invariant components as target poisoning
areas, where well-designed trigger patterns injected into these regions can be
efficiently recognized by the victim models. This strategy is adapted to
different image-text cross-modal models, making our framework available to
various attack scenarios. Furthermore, for generating poisoned samples of high
stealthiness, we conceive modality-specific generators for visual and
linguistic modalities that facilitate hiding explicit trigger patterns in
modality-invariant regions. To the best of our knowledge, BadCM is the first
invisible backdoor method deliberately designed for diverse cross-modal attacks
within one unified framework. Comprehensive experimental evaluations on two
typical applications, i.e., cross-modal retrieval and VQA, demonstrate the
effectiveness and generalization of our method under multiple kinds of attack
scenarios. Moreover, we show that BadCM can robustly evade existing backdoor
defenses. Our code is available at https://github.com/xandery-geek/BadCM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CalliffusionV2: Personalized Natural Calligraphy <span class="highlight-title">Generation</span> with
  Flexible Multi-modal Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qisheng Liao, Liang Li, Yulang Fei, Gus Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce CalliffusionV2, a novel system designed to
produce natural Chinese calligraphy with flexible multi-modal control. Unlike
previous approaches that rely solely on image or text inputs and lack
fine-grained control, our system leverages both images to guide generations at
fine-grained levels and natural language texts to describe the features of
generations. CalliffusionV2 excels at creating a broad range of characters and
can quickly learn new styles through a few-shot learning approach. It is also
capable of generating non-Chinese characters without prior training.
Comprehensive tests confirm that our system produces calligraphy that is both
stylistically accurate and recognizable by neural network classifiers and human
evaluators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Releasing the Parameter Latency of Neural Representation for
  High-Efficiency Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gai Zhang, Xinfeng Zhang, Lv Tang, Yue Li, Kai Zhang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, video compression technology has been a prominent research area.
Traditional hybrid video compression framework and end-to-end frameworks
continue to explore various intra- and inter-frame reference and prediction
strategies based on discrete transforms and deep learning techniques. However,
the emerging implicit neural representation (INR) technique models entire
videos as basic units, automatically capturing intra-frame and inter-frame
correlations and obtaining promising performance. INR uses a compact neural
network to store video information in network parameters, effectively
eliminating spatial and temporal redundancy in the original video. However, in
this paper, our exploration and verification reveal that current INR video
compression methods do not fully exploit their potential to preserve
information. We investigate the potential of enhancing network parameter
storage through parameter reuse. By deepening the network, we designed a
feasible INR parameter reuse scheme to further improve compression performance.
Extensive experimental results show that our method significantly enhances the
rate-distortion performance of INR video compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SonicSense: Object Perception from In-Hand Acoustic Vibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17932v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17932v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxun Liu, Boyuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SonicSense, a holistic design of hardware and software to enable
rich robot object perception through in-hand acoustic vibration sensing. While
previous studies have shown promising results with acoustic sensing for object
perception, current solutions are constrained to a handful of objects with
simple geometries and homogeneous materials, single-finger sensing, and mixing
training and testing on the same objects. SonicSense enables container
inventory status differentiation, heterogeneous material prediction, 3D shape
reconstruction, and object re-identification from a diverse set of 83
real-world objects. Our system employs a simple but effective heuristic
exploration policy to interact with the objects as well as end-to-end
learning-based algorithms to fuse vibration signals to infer object properties.
Our framework underscores the significance of in-hand acoustic vibration
sensing in advancing robot tactile perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project website is at: http://generalroboticslab.com/SonicSense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongLLaVA: Scaling Multi-modal <span class="highlight-title">LLM</span>s to 1000 <span class="highlight-title">Image</span>s Efficiently via a
  Hybrid Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expanding the long-context capabilities of Multi-modal Large Language
Models~(MLLMs) is crucial for video understanding, high-resolution image
understanding, and multi-modal agents. This involves a series of systematic
optimizations, including model architecture, data construction and training
strategy, particularly addressing challenges such as \textit{degraded
performance with more images} and \textit{high computational costs}. In this
paper, we adapt the model architecture to a hybrid of Mamba and Transformer
blocks, approach data construction with both temporal and spatial dependencies
among multiple images and employ a progressive training strategy. The released
model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge
\textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first
hybrid MLLM, which achieved a better balance between efficiency and
effectiveness. LongLLaVA not only achieves competitive results across various
benchmarks, but also maintains high throughput and low memory consumption.
Especially, it could process nearly a thousand images on a single A100 80GB
GPU, showing promising application prospects for a wide range of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrap3D: Improving Multi-view <span class="highlight-title">Diffusion</span> Model with Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Sun, Tong Wu, Pan Zhang, Yuhang Zang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed remarkable progress in multi-view diffusion
models for 3D content creation. However, there remains a significant gap in
image quality and prompt-following ability compared to 2D diffusion models. A
critical bottleneck is the scarcity of high-quality 3D objects with detailed
captions. To address this challenge, we propose Bootstrap3D, a novel framework
that automatically generates an arbitrary quantity of multi-view images to
assist in training multi-view diffusion models. Specifically, we introduce a
data generation pipeline that employs (1) 2D and video diffusion models to
generate multi-view images based on constructed text prompts, and (2) our
fine-tuned 3D-aware MV-LLaVA for filtering high-quality data and rewriting
inaccurate captions. Leveraging this pipeline, we have generated 1 million
high-quality synthetic multi-view images with dense descriptive captions to
address the shortage of high-quality 3D data. Furthermore, we present a
Training Timestep Reschedule (TTR) strategy that leverages the denoising
process to learn multi-view consistency while maintaining the original 2D
diffusion prior. Extensive experiments demonstrate that Bootstrap3D can
generate high-quality multi-view images with superior aesthetic quality,
image-text alignment, and maintained view consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sunzey.github.io/Bootstrap3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Yuan, Zheng Zhang, Xunguang Wang, Lin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing has been intensively studied and successfully applied in
large-scale image retrieval systems due to its efficiency and effectiveness.
Recent studies have recognized that the existence of adversarial examples poses
a security threat to deep hashing models, that is, adversarial vulnerability.
Notably, it is challenging to efficiently distill reliable semantic
representatives for deep hashing to guide adversarial learning, and thereby it
hinders the enhancement of adversarial robustness of deep hashing-based
retrieval models. Moreover, current researches on adversarial training for deep
hashing are hard to be formalized into a unified minimax structure. In this
paper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the
adversarial robustness of deep hashing models. Specifically, we conceive a
discriminative mainstay features learning (DMFL) scheme to construct semantic
representatives for guiding adversarial learning in deep hashing. Particularly,
our DMFL with the strict theoretical guarantee is adaptively optimized in a
discriminative learning manner, where both discriminative and semantic
properties are jointly considered. Moreover, adversarial examples are
fabricated by maximizing the Hamming distance between the hash codes of
adversarial samples and mainstay features, the efficacy of which is validated
in the adversarial attack trials. Further, we, for the first time, formulate
the formalized adversarial training of deep hashing into a unified minimax
optimization under the guidance of the generated mainstay codes. Extensive
experiments on benchmark datasets show superb attack performance against the
state-of-the-art algorithms, meanwhile, the proposed adversarial training can
effectively eliminate adversarial perturbations for trustworthy deep
hashing-based retrieval. Our code is available at
https://github.com/xandery-geek/SAAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating <span class="highlight-title">Large Language Model</span>s into a Tri-Modal Architecture for
  Automated Depression Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19340v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19340v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santosh V. Patapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Major Depressive Disorder (MDD) is a pervasive mental health condition that
affects 300 million people worldwide. This work presents a novel, BiLSTM-based
tri-modal model-level fusion architecture for the binary classification of
depression from clinical interview recordings. The proposed architecture
incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses
a two-shot learning based GPT-4 model to process text data. This is the first
work to incorporate large language models into a multi-modal architecture for
this task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge
cross-validation split and Leave-One-Subject-Out cross-validation split,
surpassing all baseline models and multiple state-of-the-art models. In
Leave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score
of 85.95%, a precision of 80%, and a recall of 92.86%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: Multi-Modal Neural Networks, Deep Learning, Large Language
  Models, Depression Diagnosis, Biomedical Informatics, DAIC-WOZ</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-02T00:00:00Z">2024-10-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Locret: Enhancing Eviction in Long-Context <span class="highlight-title">LLM</span> Inference with Trained
  Retaining Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, <span class="highlight-author">Zhiyuan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable advances in supporting
long-context comprehension and processing tasks. However, scaling the
generation inference of LLMs to such long contexts incurs significant
additional computation load, and demands a substantial GPU memory footprint to
maintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache
compression methods, such as quantization, face memory bottlenecks as context
length increases, while static-sized caches, such as eviction, suffer from
inefficient policies. These limitations restrict deployment on consumer-grade
devices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a
framework for long-context LLM inference that introduces retaining heads to
evaluate the causal importance of KV cache units, allowing for more accurate
eviction within a fixed cache size. Locret is fine-tuned on top of the frozen
backbone LLM using a minimal amount of data from standard long-context SFT
datasets. During inference, we evict low-importance cache units along with a
chunked prefill pattern, significantly reducing peak GPU memory usage. We
conduct an extensive empirical study to evaluate Locret, where the experimental
results show that Locret outperforms the recent competitive approaches,
including InfLLM, Quantization, SirLLM, and MInference, in terms of memory
efficiency and the quality of generated contents -- Locret achieves over a 20x
and 8x KV cache compression ratio compared to the full KV cache for
Phi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined
with other methods, such as quantization and token merging. To our knowledge,
Locret is the first framework capable of deploying Llama-3.1-8B or similar
models on a single Nvidia 4090 GPU, enabling 128K long-context inference
without compromising generation quality, and requiring little additional system
optimizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprints</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Driven Feature Selection and Engineering for Genotype Data
  with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Lee, Shu Yang, Jae Young Baik, Xiaoxi Liu, Zhen Tan, Dawei Li, Zixuan Wen, Bojian Hou, Duy Duong-Tran, Tianlong Chen, Li Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting phenotypes with complex genetic bases based on a small,
interpretable set of variant features remains a challenging task.
Conventionally, data-driven approaches are utilized for this task, yet the high
dimensional nature of genotype data makes the analysis and prediction
difficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and
their success in processing complex biomedical concepts, we set to examine the
ability of LLMs in feature selection and engineering for tabular genotype data,
with a novel knowledge-driven framework. We develop FREEFORM, Free-flow
Reasoning and Ensembling for Enhanced Feature Output and Robust Modeling,
designed with chain-of-thought and ensembling principles, to select and
engineer features with the intrinsic knowledge of LLMs. Evaluated on two
distinct genotype-phenotype datasets, genetic ancestry and hereditary hearing
loss, we find this framework outperforms several data-driven methods,
particularly on low-shot regimes. FREEFORM is available as open-source
framework at GitHub: https://github.com/PennShenLab/FREEFORM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loki: An Open-Source Tool for Fact Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Li, Xudong Han, Hao Wang, Yuxia Wang, Minghan Wang, Rui Xing, Yilin Geng, Zenan Zhai, Preslav Nakov, Timothy Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Loki, an open-source tool designed to address the growing
problem of misinformation. Loki adopts a human-centered approach, striking a
balance between the quality of fact-checking and the cost of human involvement.
It decomposes the fact-checking task into a five-step pipeline: breaking down
long texts into individual claims, assessing their check-worthiness, generating
queries, retrieving evidence, and verifying the claims. Instead of fully
automating the claim verification process, Loki provides essential information
at each step to assist human judgment, especially for general users such as
journalists and content moderators. Moreover, it has been optimized for
latency, robustness, and cost efficiency at a commercially usable level. Loki
is released under an MIT license and is available on GitHub. We also provide a
video presenting the system and its capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When a language model is optimized for <span class="highlight-title">reasoning</span>, does it still show
  embers of autoregression? An analysis of OpenAI o1 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Thomas McCoy, Shunyu Yao, Dan Friedman, Mathew D. Hardy, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In "Embers of Autoregression" (McCoy et al., 2023), we showed that several
large language models (LLMs) have some important limitations that are
attributable to their origins in next-word prediction. Here we investigate
whether these issues persist with o1, a new system from OpenAI that differs
from previous LLMs in that it is optimized for reasoning. We find that o1
substantially outperforms previous LLMs in many cases, with particularly large
improvements on rare variants of common tasks (e.g., forming acronyms from the
second letter of each word in a list, rather than the first letter). Despite
these quantitative improvements, however, o1 still displays the same
qualitative trends that we observed in previous systems. Specifically, o1 -
like previous LLMs - is sensitive to the probability of examples and tasks,
performing better and requiring fewer "thinking tokens" in high-probability
settings than in low-probability ones. These results show that optimizing a
language model for reasoning can mitigate but might not fully overcome the
language model's probability sensitivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamGarden: A Designer Assistant for Growing Games from a Single Prompt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Earle, Samyak Parajuli, Andrzej Banburski-Fahey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coding assistants are increasingly leveraged in game design, both generating
code and making high-level plans. To what degree can these tools align with
developer workflows, and what new modes of human-computer interaction can
emerge from their use? We present DreamGarden, an AI system capable of
assisting with the development of diverse game environments in Unreal Engine.
At the core of our method is an LLM-driven planner, capable of breaking down a
single, high-level prompt -- a dream, memory, or imagined scenario provided by
a human user -- into a hierarchical action plan, which is then distributed
across specialized submodules facilitating concrete implementation. This system
is presented to the user as a garden of plans and actions, both growing
independently and responding to user intervention via seed prompts, pruning,
and feedback. Through a user study, we explore design implications of this
system, charting courses for future work in semi-autonomous assistants and
open-ended simulation design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages + appendix, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniGenBench: Automating Large-scale in-silico <span class="highlight-title">Benchmark</span>ing for Genomic
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Yang, Jack Cole, Ke Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements in artificial intelligence in recent years, such as Large
Language Models (LLMs), have fueled expectations for breakthroughs in genomic
foundation models (GFMs). The code of nature, hidden in diverse genomes since
the very beginning of life's evolution, holds immense potential for impacting
humans and ecosystems through genome modeling. Recent breakthroughs in GFMs,
such as Evo, have attracted significant investment and attention to genomic
modeling, as they address long-standing challenges and transform in-silico
genomic studies into automated, reliable, and efficient paradigms. In the
context of this flourishing era of consecutive technological revolutions in
genomics, GFM studies face two major challenges: the lack of GFM benchmarking
tools and the absence of open-source software for diverse genomics. These
challenges hinder the rapid evolution of GFMs and their wide application in
tasks such as understanding and synthesizing genomes, problems that have
persisted for decades. To address these challenges, we introduce GFMBench, a
framework dedicated to GFM-oriented benchmarking. GFMBench standardizes
benchmark suites and automates benchmarking for a wide range of open-source
GFMs. It integrates millions of genomic sequences across hundreds of genomic
tasks from four large-scale benchmarks, democratizing GFMs for a wide range of
in-silico genomic applications. Additionally, GFMBench is released as
open-source software, offering user-friendly interfaces and diverse tutorials,
applicable for AutoBench and complex tasks like RNA design and structure
prediction. To facilitate further advancements in genome modeling, we have
launched a public leaderboard showcasing the benchmark performance derived from
AutoBench. GFMBench represents a step toward standardizing GFM benchmarking and
democratizing GFM applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yangheng95/OmniGenomeBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-RAG: Enhanced Retrieval-Augmented <span class="highlight-title">Reasoning</span> with Open-Source Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, Md Rizwan Parvez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual
accuracy of Large Language Models (LLMs), but existing methods often suffer
from limited reasoning capabilities in effectively using the retrieved
evidence, particularly when using open-source LLMs. To mitigate this gap, we
introduce a novel framework, Open-RAG, designed to enhance reasoning
capabilities in RAG with open-source LLMs. Our framework transforms an
arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)
model capable of handling complex reasoning tasks, including both single- and
multi-hop queries. Open-RAG uniquely trains the model to navigate challenging
distractors that appear relevant but are misleading. As a result, Open-RAG
leverages latent learning, dynamically selecting relevant experts and
integrating external knowledge effectively for more accurate and contextually
relevant responses. In addition, we propose a hybrid adaptive retrieval method
to determine retrieval necessity and balance the trade-off between performance
gain and inference speed. Experimental results show that the Llama2-7B-based
Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,
Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source
our code and models at https://openragmoe.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings. Website:
  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composing Global Optimizers to <span class="highlight-title">Reasoning</span> Tasks via Algebraic Objects in
  Neural Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove rich algebraic structures of the solution space for 2-layer neural
networks with quadratic activation and $L_2$ loss, trained on reasoning tasks
in Abelian group (e.g., modular addition). Such a rich structure enables
analytical construction of global optimal solutions from partial solutions that
only satisfy part of the loss, despite its high nonlinearity. We coin the
framework as CoGO (Composing Global Optimizers). Specifically, we show that the
weight space over different numbers of hidden nodes of the 2-layer network is
equipped with a semi-ring algebraic structure, and the loss function to be
optimized consists of monomial potentials, which are ring homomorphism,
allowing partial solutions to be composed into global ones by ring addition and
multiplication. Our experiments show that around $95\%$ of the solutions
obtained by gradient descent match exactly our theoretical constructions.
Although the global optimizers constructed only required a small number of
hidden nodes, our analysis on gradient dynamics shows that
over-parameterization asymptotically decouples training dynamics and is
beneficial. We further show that training dynamics favors simpler solutions
under weight decay, and thus high-order global optimizers such as perfect
memorization are unfavorable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeFine: Enhancing <span class="highlight-title">LLM</span> Decision-Making with Factor Profiles and
  Analogical <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yebowen Hu, Xiaoyang Wang, Wenlin Yao, Yiming Lu, Daoan Zhang, Hassan Foroosh, Dong Yu, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are ideal for decision-making due to their ability to reason over long
contexts and identify critical factors. However, challenges arise when
processing transcripts of spoken speech describing complex scenarios. These
transcripts often contain ungrammatical or incomplete sentences, repetitions,
hedging, and vagueness. For example, during a company's earnings call, an
executive might project a positive revenue outlook to reassure investors,
despite significant uncertainty regarding future earnings. It is crucial for
LLMs to incorporate this uncertainty systematically when making decisions. In
this paper, we introduce DeFine, a new framework that constructs probabilistic
factor profiles from complex scenarios. DeFine then integrates these profiles
with analogical reasoning, leveraging insights from similar past experiences to
guide LLMs in making critical decisions in novel situations. Our framework
separates the tasks of quantifying uncertainty in complex scenarios and
incorporating it into LLM decision-making. This approach is particularly useful
in fields such as medical consultations, negotiations, and political debates,
where making decisions under uncertainty is vital.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Generalization Complexity for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown exceptional capabilities in
understanding complex queries and performing sophisticated tasks, their
generalization abilities are often deeply entangled with memorization,
necessitating more precise evaluation. To address this challenge, we introduce
Scylla, a dynamic evaluation framework that quantitatively measures the
generalization abilities of LLMs. Scylla disentangles generalization from
memorization via assessing model performance on both in-distribution (ID) and
out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.
Through extensive experiments, we uncover a non-monotonic relationship between
task complexity and the performance gap between ID and OOD data, which we term
the generalization valley. Specifically, this phenomenon reveals a critical
threshold - referred to as critical complexity - where reliance on
non-generalizable behavior peaks, indicating the upper bound of LLMs'
generalization capabilities. As model size increases, the critical complexity
shifts toward higher levels of task complexity, suggesting that larger models
can handle more complex reasoning tasks before over-relying on memorization.
Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs
including both open-sourced models such as LLaMA and Qwen families, and
close-sourced models like Claude and GPT, providing a more robust evaluation
and establishing a clearer understanding of LLMs' generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEOPARD : A Vision Language Model For Text-Rich Multi-<span class="highlight-title">Image</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Meng Jiang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich images, where text serves as the central visual element guiding the
overall understanding, are prevalent in real-world applications, such as
presentation slides, scanned documents, and webpage snapshots. Tasks involving
multiple text-rich images are especially challenging, as they require not only
understanding the content of individual images but reasoning about
inter-relationships and logical flows across multiple visual inputs. Despite
the importance of these scenarios, current multimodal large language models
(MLLMs) struggle to handle such tasks due to two key challenges: (1) the
scarcity of high-quality instruction tuning datasets for text-rich multi-image
scenarios, and (2) the difficulty in balancing image resolution with visual
feature sequence length. To address these challenges, we propose \OurMethod, a
MLLM designed specifically for handling vision-language tasks involving
multiple text-rich images. First, we curated about one million high-quality
multimodal instruction-tuning data, tailored to text-rich, multi-image
scenarios. Second, we developed an adaptive high-resolution multi-image
encoding module to dynamically optimize the allocation of visual sequence
length based on the original aspect ratios and resolutions of the input images.
Experiments across a wide range of benchmarks demonstrate our model's superior
capabilities in text-rich, multi-image evaluations and competitive performance
in general domain evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/Jill0001/Leopard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recursive Abstractive Processing for Retrieval in Dynamic <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charbel Chucri, Rami Azouz, Joachim Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent retrieval-augmented models enhance basic methods by building a
hierarchical structure over retrieved text chunks through recursive embedding,
clustering, and summarization. The most relevant information is then retrieved
from both the original text and generated summaries. However, such approaches
face limitations with dynamic datasets, where adding or removing documents over
time complicates the updating of hierarchical representations formed through
clustering. We propose a new algorithm to efficiently maintain the
recursive-abstractive tree structure in dynamic datasets, without compromising
performance. Additionally, we introduce a novel post-retrieval method that
applies query-focused recursive abstractive processing to substantially improve
context quality. Our method overcomes the limitations of other approaches by
functioning as a black-box post-retrieval layer compatible with any retrieval
algorithm. Both algorithms are validated through extensive experiments on
real-world datasets, demonstrating their effectiveness in handling dynamic data
and improving retrieval performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASeR: Learning to Adaptively Select Reward Models with Multi-Armed
  Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward Models (RMs) play a crucial role in aligning LLMs with human
preferences, enhancing their performance by ranking outputs during inference or
iterative training. However, the degree to which an RM generalizes to new tasks
is often not known a priori (e.g. some RMs may excel at scoring creative
writing vs. math reasoning). Therefore, using only one fixed RM while training
LLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs
simultaneously can be prohibitively computationally-intensive and challenging
due to conflicting signals from different RMs, potentially degrading
performance. To address these challenges, we introduce LASeR (Learning to
Adaptively Select Rewards), which iteratively trains LLMs using multiple RMs,
selecting and utilizing the most well-suited RM for each instance to rank
outputs and generate preference data, framed as a multi-armed bandit problem.
Our results on commonsense and math reasoning tasks demonstrate that LASeR can
boost iterative LLM optimization by optimizing for multiple RMs, improving the
absolute average accuracy of Llama-3-8B over three datasets by 2.67% over
training with ensemble RM scores while also showing superior training
efficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of
instruction-following prompts, we find that using Llama-3-8B LASeR leads to a
71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending
to long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an
average improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA
over random RM selection when used with best-of-n sampling. LASeR is robust to
noisy rewards and generalizes to multiple settings. Finally, LASeR's RM
selection changes depending on the underlying task or instance and we verify
the presence of conflicting preferences from multiple RMs that can be mitigated
using LASeR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages; First two authors contributed equally. Code:
  https://github.com/duykhuongnguyen/LASeR-MAB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Perception in Text Strings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Jia, Xiang Yue, Shanshan Huang, Ziheng Qin, Yizhu Liu, Bill Yuchen Lin, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding visual semantics embedded in consecutive characters is a
crucial capability for both large language models (LLMs) and multi-modal large
language models (MLLMs). This type of artifact possesses the unique
characteristic that identical information can be readily formulated in both
texts and images, making them a significant proxy for analyzing modern LLMs'
and MLLMs' capabilities in modality-agnostic vision understanding. In this
work, we select ASCII art as a representative artifact, where the lines and
brightness used to depict each concept are rendered by characters, and we frame
the problem as an ASCII art recognition task. We benchmark model performance on
this task by constructing an evaluation dataset with an elaborate
categorization tree and also collect a training set to elicit the models'
visual perception ability. Through a comprehensive analysis of dozens of
models, results reveal that although humans can achieve nearly 100% accuracy,
the state-of-the-art LLMs and MLLMs lag far behind. Models are capable of
recognizing concepts depicted in the ASCII arts given only text inputs
indicated by over 60% accuracy for some concepts, but most of them achieves
merely around 30% accuracy when averaged across all categories. When provided
with images as inputs, GPT-4o gets 82.68%, outperforming the strongest
open-source MLLM by 21.95%. Although models favor different kinds of ASCII art
depending on the modality provided, none of the MLLMs successfully benefit when
both modalities are supplied simultaneously. Moreover, supervised fine-tuning
helps improve models' accuracy especially when provided with the image
modality, but also highlights the need for better training techniques to
enhance the information fusion among modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComfyGen: Prompt-Adaptive Workflows for Text-to-<span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rinon Gal, Adi Haviv, Yuval Alaluf, Amit H. Bermano, Daniel Cohen-Or, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The practical use of text-to-image generation has evolved from simple,
monolithic models to complex workflows that combine multiple specialized
components. While workflow-based approaches can lead to improved image quality,
crafting effective workflows requires significant expertise, owing to the large
number of available components, their complex inter-dependence, and their
dependence on the generation prompt. Here, we introduce the novel task of
prompt-adaptive workflow generation, where the goal is to automatically tailor
a workflow to each user prompt. We propose two LLM-based approaches to tackle
this task: a tuning-based method that learns from user-preference data, and a
training-free method that uses the LLM to select existing flows. Both
approaches lead to improved image quality when compared to monolithic models or
generic, prompt-independent workflows. Our work shows that prompt-dependent
flow prediction offers a new pathway to improving text-to-image generation
quality, complementing existing research directions in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://comfygen-paper.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Robustness of Reward Models for Mathematical <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghwan Kim, Dongjin Kang, Taeyoon Kwon, Hyungjoo Chae, Jungsoo Won, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are key in reinforcement learning from human feedback (RLHF)
systems, aligning the model behavior with human preferences. Particularly in
the math domain, there have been plenty of studies using reward models to align
policies for improving reasoning capabilities. Recently, as the importance of
reward models has been emphasized, RewardBench is proposed to understand their
behavior. However, we figure out that the math subset of RewardBench has
different representations between chosen and rejected completions, and relies
on a single comparison, which may lead to unreliable results as it only see an
isolated case. Therefore, it fails to accurately present the robustness of
reward models, leading to a misunderstanding of its performance and potentially
resulting in reward hacking. In this work, we introduce a new design for
reliable evaluation of reward models, and to validate this, we construct
RewardMATH, a benchmark that effectively represents the robustness of reward
models in mathematical reasoning tasks. We demonstrate that the scores on
RewardMATH strongly correlate with the results of optimized policy and
effectively estimate reward overoptimization, whereas the existing benchmark
shows almost no correlation. The results underscore the potential of our design
to enhance the reliability of evaluation, and represent the robustness of
reward model. We make our code and data publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Knowledge Concept Annotation and Question Representation
  Learning for Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilmazcan Ozyurt, Stefan Feuerriegel, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge tracing (KT) is a popular approach for modeling students' learning
progress over time, which can enable more personalized and adaptive learning.
However, existing KT approaches face two major limitations: (1) they rely
heavily on expert-defined knowledge concepts (KCs) in questions, which is
time-consuming and prone to errors; and (2) KT methods tend to overlook the
semantics of both questions and the given KCs. In this work, we address these
challenges and present KCQRL, a framework for automated knowledge concept
annotation and question representation learning that can improve the
effectiveness of any existing KT model. First, we propose an automated KC
annotation process using large language models (LLMs), which generates question
solutions and then annotates KCs in each solution step of the questions.
Second, we introduce a contrastive learning approach to generate semantically
rich embeddings for questions and solution steps, aligning them with their
associated KCs via a tailored false negative elimination approach. These
embeddings can be readily integrated into existing KT models, replacing their
randomly initialized embeddings. We demonstrate the effectiveness of KCQRL
across 15 KT algorithms on two large real-world Math learning datasets, where
we achieve consistent performance improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for
  Enhanced Batch Prompting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longyu Feng, Mengze Hong, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Batch prompting is a common technique in large language models (LLMs) used to
process multiple inputs simultaneously, aiming to improve computational
efficiency. However, as batch sizes increase, performance degradation often
occurs due to the model's difficulty in handling lengthy context inputs.
Existing methods that attempt to mitigate these issues rely solely on batch
data arrangement and majority voting rather than improving the design of the
batch prompt itself. In this paper, we address these limitations by proposing
"Auto-Demo Prompting," a novel approach that leverages the question-output
pairs from earlier questions within a batch as demonstrations for subsequent
answer inference. We provide a formal theoretical analysis of how Auto-Demo
Prompting functions within the autoregressive generation process of LLMs,
illustrating how it utilizes prior outputs to optimize the model's internal
representations. Our method effectively bridges the gap between batch prompting
and few-shot prompting, enhancing performance with only a slight compromise in
token usage. Experimental results across five NLP tasks demonstrate its
effectiveness in mitigating performance degradation and occasionally
outperforming single prompts. Furthermore, it opens new avenues for applying
few-shot learning techniques, such as demonstration selection, within batch
prompting, making it a robust solution for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Theoretical Understanding of Synthetic Data in <span class="highlight-title">LLM</span>
  Post-Training: A Reverse-Bottleneck Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Gan, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data has become a pivotal resource in post-training tasks for large
language models (LLMs) due to the scarcity of high-quality, specific data.
While various methods have been developed to generate synthetic data, there
remains a discernible gap between the practical effects of synthetic data and
our theoretical comprehension. To address this challenge, we commence by
presenting a detailed modeling of the prevalent synthetic data generation
process. Building upon this modeling, we demonstrate that the generalization
capability of the post-trained model is critically determined by the
information gain derived from the generative model, as analyzed from a novel
reverse-bottleneck perspective. Moreover, we introduce the concept of
Generalization Gain via Mutual Information (GGMI) and elucidate the
relationship between generalization gain and information gain. This analysis
serves as a theoretical foundation for synthetic data generation and further
highlights its connection with the generalization capability of post-trained
models, offering an understanding about the design of synthetic data generation
techniques and the optimization of the post-training process. We open source
our code through an anonymous GitHub repository at
https://anonymous.4open.science/r/Understanding-Synthetic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining the Role of Relationship Alignment in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristen M. Altenburger, Hongda Jiang, Robert E. Kraut, Yi-Chia Wang, Jane Dwivedi-Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development and deployment of Generative AI in social settings
raise important questions about how to optimally personalize them for users
while maintaining accuracy and realism. Based on a Facebook public post-comment
dataset, this study evaluates the ability of Llama 3.0 (70B) to predict the
semantic tones across different combinations of a commenter's and poster's
gender, age, and friendship closeness and to replicate these differences in
LLM-generated comments.
  The study consists of two parts: Part I assesses differences in semantic
tones across social relationship categories, and Part II examines the
similarity between comments generated by Llama 3.0 (70B) and human comments
from Part I given public Facebook posts as input. Part I results show that
including social relationship information improves the ability of a model to
predict the semantic tone of human comments. However, Part II results show that
even without including social context information in the prompt, LLM-generated
comments and human comments are equally sensitive to social context, suggesting
that LLMs can comprehend semantics from the original post alone. When we
include all social relationship information in the prompt, the similarity
between human comments and LLM-generated comments decreases. This inconsistency
may occur because LLMs did not include social context information as part of
their training data. Together these results demonstrate the ability of LLMs to
comprehend semantics from the original post and respond similarly to human
comments, but also highlights their limitations in generalizing personalized
comments through prompting alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Contrastive Monte Carlo Tree Search <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, Lijie Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning
algorithm for Large Language Models (LLMs), significantly improves both
reasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM
reasoning works often overlooked its biggest drawback--slower speed compared to
CoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on
various tasks with limited quantitative analysis or ablation studies of its
components from reasoning interpretability perspective. 3. The reward model is
the most crucial component in MCTS, however previous work has rarely conducted
in-depth study or improvement of MCTS's reward models. Thus, we conducted
extensive ablation studies and quantitative analysis on components of MCTS,
revealing the impact of each component on the MCTS reasoning performance of
LLMs. Building on this, (i) we designed a highly interpretable reward model
based on the principle of contrastive decoding and (ii) achieved an average
speed improvement of 51.9% per node using speculative decoding. Additionally,
(iii) we improved UCT node selection strategy and backpropagation used in
previous works, resulting in significant performance improvement. We
outperformed o1-mini by an average of 17.4% on the Blocksworld multi-step
reasoning dataset using Llama-3.1-70B with SC-MCTS*.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Exploration of Self-Supervised Mutual Information Alignment for
  Multi-Task Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Govande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing need for pluralistic alignment methods that can steer
language models towards individual attributes and preferences. One such method,
Self-Supervised Alignment with Mutual Information (SAMI), uses conditional
mutual information to encourage the connection between behavioral preferences
and model responses. We conduct two experiments exploring SAMI in multi-task
settings. First, we compare SAMI to Direct Preference Optimization (DPO) on a
multi-task benchmark (MT-Bench), using a stronger model to generate training
data for a weaker one across diverse categories (humanities, STEM, extraction,
coding, math, reasoning, and roleplay). Our results indicate that one iteration
of SAMI has a 57% win rate against DPO, with significant variation in
performance between task categories. Second, we examine SAMI's impact on
mathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While
SAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2%
boost. However, SAMI shows interesting scaling trends. When given 10 attempts,
SAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining
SAMI with SFT yields an additional improvement of 1.3% in multi-attempt
settings, though single-attempt accuracy remains unchanged.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CreDes: Causal <span class="highlight-title">Reasoning</span> Enhancement and Dual-End Searching for Solving
  Long-Range <span class="highlight-title">Reasoning</span> Problems using <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangsheng Wang, Xiao Zhang, Hao Liu, Songde Han, Huimin Ma, Tianyu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated limitations in handling
combinatorial optimization problems involving long-range reasoning, partially
due to causal hallucinations and huge search space. As for causal
hallucinations, i.e., the inconsistency between reasoning and corresponding
state transition, this paper introduces the Causal Relationship Enhancement
(CRE) mechanism combining cause-effect interventions and the Individual
Treatment Effect (ITE) to guarantee the solid causal rightness between each
step of reasoning and state transition. As for the long causal range and huge
search space limiting the performances of existing models featuring
single-direction search, a Dual-End Searching (DES) approach is proposed to
seek solutions by simultaneously starting from both the initial and goal states
on the causal probability tree. By integrating CRE and DES (CreDes), our model
has realized simultaneous multi-step reasoning, circumventing the
inefficiencies from cascading multiple one-step reasoning like the
Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly
outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning
tasks in terms of both accuracy and time efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-shaped and Inverted-U Scaling behind Emergent Abilities of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tung-Yu Wu, Pei-Yu Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been shown to exhibit emergent abilities in
some downstream tasks, where performance seems to stagnate at first and then
improve sharply and unpredictably with scale beyond a threshold. By dividing
questions in the datasets according to difficulty level by average performance,
we observe U-shaped scaling for hard questions, and inverted-U scaling followed
by steady improvement for easy questions. Moreover, the emergence threshold
roughly coincides with the point at which performance on easy questions reverts
from inverse scaling to standard scaling. Capitalizing on the observable though
opposing scaling trend on easy and hard questions, we propose a simple yet
effective pipeline, called Slice-and-Sandwich, to predict both the emergence
threshold and model performance beyond the threshold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FactAlign: Long-form Factuality Alignment of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Wei Huang, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated significant potential as the
next-generation information access engines. However, their reliability is
hindered by issues of hallucination and generating non-factual content. This is
particularly problematic in long-form responses, where assessing and ensuring
factual accuracy is complex. In this paper, we address this gap by proposing
FactAlign, a novel alignment framework designed to enhance the factuality of
LLMs' long-form responses while maintaining their helpfulness. We introduce
fKTO, a fine-grained, sentence-level alignment algorithm that extends the
Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent
advances in automatic factuality evaluation, FactAlign utilizes fine-grained
factuality assessments to guide the alignment process. Our experiments on
open-domain prompts and information-seeking questions demonstrate that
FactAlign significantly improves the factual accuracy of LLM responses while
also improving their helpfulness. Further analyses identify that FactAlign is
capable of training LLMs to provide more information without losing factual
precision, thus improving the factual F1 score. Our source code, datasets, and
trained models are publicly available at https://github.com/MiuLab/FactAlign
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VinePPO: Unlocking RL Potential For <span class="highlight-title">LLM</span> <span class="highlight-title">Reasoning</span> Through Refined Credit
  Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly applied to complex reasoning
tasks that require executing several complex steps before receiving any reward.
Properly assigning credit to these steps is essential for enhancing model
performance. Proximal Policy Optimization (PPO), a state-of-the-art
reinforcement learning (RL) algorithm used for LLM finetuning, employs value
networks to tackle credit assignment. However, value networks face challenges
in predicting the expected cumulative rewards accurately in complex reasoning
tasks, often leading to high-variance updates and suboptimal performance. In
this work, we systematically evaluate the efficacy of value networks and reveal
their significant shortcomings in reasoning-heavy LLM tasks, showing that they
barely outperform a random baseline when comparing alternative steps. To
address this, we propose VinePPO, a straightforward approach that leverages the
flexibility of language environments to compute unbiased Monte Carlo-based
estimates, bypassing the need for large value networks. Our method consistently
outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with
fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These
results emphasize the importance of accurate credit assignment in RL finetuning
of LLM and demonstrate VinePPO's potential as a superior alternative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trying to be human: Linguistic traces of stochastic empathy in language
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bennett Kleinberg, Jari Zegers, Jonas Festor, Stefana Vida, Julian Präsent, Riccardo Loconte, Sanne Peereboom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiating between generated and human-written content is important for
navigating the modern world. Large language models (LLMs) are crucial drivers
behind the increased quality of computer-generated content. Reportedly, humans
find it increasingly difficult to identify whether an AI model generated a
piece of text. Our work tests how two important factors contribute to the human
vs AI race: empathy and an incentive to appear human. We address both aspects
in two experiments: human participants and a state-of-the-art LLM wrote
relationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),
either instructed to be as human as possible or not. New samples of humans
(n=428 and n=408) then judged the texts' source. Our findings show that when
empathy is required, humans excel. Contrary to expectations, instructions to
appear human were only effective for the LLM, so the human advantage
diminished. Computational text analysis revealed that LLMs become more human
because they may have an implicit representation of what makes a text human and
effortlessly apply these heuristics. The model resorts to a conversational,
self-referential, informal tone with a simpler vocabulary to mimic stochastic
empathy. We discuss these findings in light of recent claims on the on-par
performance of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Context Gaps: Leveraging Coreference Resolution for Long
  Contextual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yanxin Shen, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capabilities in natural
language processing; however, they still face difficulties when tasked with
understanding lengthy contexts and executing effective question answering.
These challenges often arise due to the complexity and ambiguity present in
longer texts. To enhance the performance of LLMs in such scenarios, we
introduce the Long Question Coreference Adaptation (LQCA) method. This
innovative framework focuses on coreference resolution tailored to long
contexts, allowing the model to identify and manage references effectively. The
LQCA method encompasses four key steps: resolving coreferences within
sub-documents, computing the distances between mentions, defining a
representative mention for coreference, and answering questions through mention
replacement. By processing information systematically, the framework provides
easier-to-handle partitions for LLMs, promoting better understanding.
Experimental evaluations on a range of LLMs and datasets have yielded positive
results, with a notable improvements on OpenAI-o1-mini and GPT-4o models,
highlighting the effectiveness of leveraging coreference resolution to bridge
context gaps in question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Underreview version of LQCA, Bridge context gap for long context</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Long-range Language Modeling with Self-supervised Causal
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Zhihao Teng, Wei Wu, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, retrieval-based language models (RLMs) have received much
attention. However, most of them leverage a pre-trained retriever with fixed
parameters, which may not adapt well to causal language models. In this work,
we propose Grouped Cross-Attention, a novel module enabling joint pre-training
of the retriever and causal LM, and apply it to long-context modeling. For a
given input sequence, we split it into chunks and use the current chunk to
retrieve past chunks for subsequent text generation. Our innovation allows the
retriever to learn how to retrieve past chunks that better minimize the
auto-regressive loss of subsequent tokens in an end-to-end manner. By
integrating top-$k$ retrieval, our model can be pre-trained efficiently from
scratch with context lengths up to 64K tokens. Our experiments show our model,
compared with long-range LM baselines, can achieve lower perplexity with
comparable or lower pre-training and inference costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeIDClinic: A Multi-Layered Framework for De-identification of Clinical
  Free-text Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel Paul, Dhivin Shaji, Lifeng Han, Warren Del-Pinto, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  De-identification is important in protecting patients' privacy for healthcare
text analytics. The MASK framework is one of the best on the de-identification
shared task organised by n2c2/i2b2 challenges. This work enhances the MASK
framework by integrating ClinicalBERT, a deep learning model specifically
fine-tuned on clinical texts, alongside traditional de-identification methods
like dictionary lookup and rule-based approaches. The system effectively
identifies and either redacts or replaces sensitive identifiable entities
within clinical documents, while also allowing users to customise the masked
documents according to their specific needs. The integration of ClinicalBERT
significantly improves the performance of entity recognition, achieving 0.9732
F1-score, especially for common entities such as names, dates, and locations.
  A risk assessment feature has also been developed, which analyses the
uniqueness of context within documents to classify them into risk levels,
guiding further de-identification efforts. While the system demonstrates strong
overall performance, this work highlights areas for future improvement,
including handling more complex entity occurrences and enhancing the system's
adaptability to different clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ongoing work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On The Adaptation of Unlimiformer for Decoder-Only Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kian Ahrabian, Alon Benhaim, Barun Patra, Jay Pujara, Saksham Singhal, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the prominent issues stifling the current generation of large language
models is their limited context length. Recent proprietary models such as GPT-4
and Claude 2 have introduced longer context lengths, 8k/32k and 100k,
respectively; however, despite the efforts in the community, most common
models, such as LLama-2, have a context length of 4k or less. Unlimiformer
(Bertsch et al., 2023) is a recently popular vector-retrieval augmentation
method that offloads cross-attention computations to a kNN index. However, its
main limitation is incompatibility with decoder-only transformers out of the
box. In this work, we explore practical considerations of adapting Unlimiformer
to decoder-only transformers and introduce a series of modifications to
overcome this limitation. Moreover, we expand the original experimental setup
on summarization to include a new task (i.e., free-form Q&A) and an
instruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase
the effectiveness of these modifications on summarization, performing on par
with a model with 2x the context length. Moreover, we discuss limitations and
future directions for free-form Q&A and instruction-tuned models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Thematic Framework for Analyzing Large-scale Self-reported Social
  Media Data on Opioid Use Disorder Treatment Using Buprenorphine Product 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhusudan Basak, Omar Sharif, Sarah E. Lord, Jacob T. Borodovsky, Lisa A. Marsch, Sandra A. Springer, Edward Nunes, Charlie D. Brackett, Luke J. ArchiBald, Sarah M. Preum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: One of the key FDA-approved medications for Opioid Use Disorder
(OUD) is buprenorphine. Despite its popularity, individuals often report
various information needs regarding buprenorphine treatment on social media
platforms like Reddit. However, the key challenge is to characterize these
needs. In this study, we propose a theme-based framework to curate and analyze
large-scale data from social media to characterize self-reported treatment
information needs (TINs).
  Methods: We collected 15,253 posts from r/Suboxone, one of the largest Reddit
sub-community for buprenorphine products. Following the standard protocol, we
first identified and defined five main themes from the data and then coded
6,000 posts based on these themes, where one post can be labeled with
applicable one to three themes. Finally, we determined the most frequently
appearing sub-themes (topics) for each theme by analyzing samples from each
group.
  Results: Among the 6,000 posts, 40.3% contained a single theme, 36% two
themes, and 13.9% three themes. The most frequent topics for each theme or
theme combination came with several key findings - prevalent reporting of
psychological and physical effects during recovery, complexities in accessing
buprenorphine, and significant information gaps regarding medication
administration, tapering, and usage of substances during different stages of
recovery. Moreover, self-treatment strategies and peer-driven advice reveal
valuable insights and potential misconceptions.
  Conclusions: The findings obtained using our proposed framework can inform
better patient education and patient-provider communication, design systematic
interventions to address treatment-related misconceptions and rumors, and
streamline the generation of hypotheses for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent Detection in the Age of <span class="highlight-title">LLM</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Arora, Shreya Jain, Srujana Merugu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent detection is a critical component of task-oriented dialogue systems
(TODS) which enables the identification of suitable actions to address user
utterances at each dialog turn. Traditional approaches relied on
computationally efficient supervised sentence transformer encoder models, which
require substantial training data and struggle with out-of-scope (OOS)
detection. The emergence of generative large language models (LLMs) with
intrinsic world knowledge presents new opportunities to address these
challenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context
learning and chain-of-thought prompting for intent detection, and compare their
performance with contrastively fine-tuned sentence transformer (SetFit) models
to highlight prediction quality and latency tradeoff. We propose a hybrid
system using uncertainty based routing strategy to combine the two approaches
that along with negative data augmentation results in achieving the best of
both worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To
better understand LLM OOS detection capabilities, we perform controlled
experiments revealing that this capability is significantly influenced by the
scope of intent labels and the size of the label space. We also introduce a
two-step approach utilizing internal LLM representations, demonstrating
empirical gains in OOS detection accuracy and F1-score by >5% for the
Mistral-7B model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upcycling Instruction Tuning from Dense to Mixture-of-Experts via
  Parameter Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Yu Sun, Hua Wu, Sen Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and
demonstrates outstanding performance in plentiful natural language processing
tasks. However, existing methods transforming LLMs from dense to MoE face
significant data requirements and typically rely on large-scale post-training.
In this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient
approach for tuning a dense pre-trained model into a MoE instruction model.
Specifically, we first point out that intermediate checkpoints during
instruction tuning of the dense model are naturally suitable for specialized
experts, and then propose an expert expansion stage to flexibly achieve models
with flexible numbers of experts, where genetic algorithm and parameter merging
are introduced to ensure sufficient diversity of new extended experts. To
ensure that each specialized expert in the MoE model works as expected, we
select a small amount of seed data that each expert excels to pre-optimize the
router. Extensive experiments with various data scales and upcycling settings
demonstrate the outstanding performance and data efficiency of UpIT, as well as
stable improvement in expert or data scaling. Further analysis reveals the
importance of ensuring expert diversity in upcycling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ENTP: Encoder-only Next Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Ewer, Daewon Chae, Thomas Zeng, Jinkyu Kim, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next-token prediction models have predominantly relied on decoder-only
Transformers with causal attention, driven by the common belief that causal
attention is essential to prevent "cheating" by masking future tokens. We
challenge this widely accepted notion and argue that this design choice is
about efficiency rather than necessity. While decoder-only Transformers are
still a good choice for practical reasons, they are not the only viable option.
In this work, we introduce Encoder-only Next Token Prediction (ENTP). We
explore the differences between ENTP and decoder-only Transformers in
expressive power and complexity, highlighting potential advantages of ENTP. We
introduce the Triplet-Counting task and show, both theoretically and
experimentally, that while ENTP can perform this task easily, a decoder-only
Transformer cannot. Finally, we empirically demonstrate ENTP's superior
performance across various realistic tasks, such as length generalization and
in-context learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spoken Grammar Assessment Using <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunil Kumar Kopparapu, Chitralekha Bhat, Ashish Panda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken language assessment (SLA) systems restrict themselves to evaluating
the pronunciation and oral fluency of a speaker by analysing the read and
spontaneous spoken utterances respectively. The assessment of language grammar
or vocabulary is relegated to written language assessment (WLA) systems. Most
WLA systems present a set of sentences from a curated finite-size database of
sentences thereby making it possible to anticipate the test questions and train
oneself. In this paper, we propose a novel end-to-end SLA system to assess
language grammar from spoken utterances thus making WLA systems redundant;
additionally, we make the assessment largely unteachable by employing a large
language model (LLM) to bring in variations in the test. We further demonstrate
that a hybrid automatic speech recognition (ASR) with a custom-built language
model outperforms the state-of-the-art ASR engine for spoken grammar
assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source
  Instruction Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, Igor Gitman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning continues to be a critical challenge in large language
model (LLM) development with significant interest. However, most of the
cutting-edge progress in mathematical reasoning with LLMs has become
\emph{closed-source} due to lack of access to training data. This lack of data
access limits researchers from understanding the impact of different choices
for synthesizing and utilizing the data. With the goal of creating a
high-quality finetuning (SFT) dataset for math reasoning, we conduct careful
ablation experiments on data synthesis using the recently released
\texttt{Llama3.1} family of models. Our experiments show that: (a) solution
format matters, with excessively verbose solutions proving detrimental to SFT
performance, (b) data generated by a strong teacher outperforms
\emph{on-policy} data generated by a weak student model, (c) SFT is robust to
low-quality solutions, allowing for imprecise data filtering, and (d) question
diversity is crucial for achieving data scaling gains. Based on these insights,
we create the OpenMathInstruct-2 dataset, which consists of 14M
question-solution pairs ($\approx$ 600K unique questions), making it nearly
eight times larger than the previous largest open-source math reasoning
dataset. Finetuning the \texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2
outperforms \texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\% (51.9\%
$\rightarrow$ 67.8\%). Finally, to accelerate the open-source efforts, we
release the code, the finetuned models, and the OpenMathInstruct-2 dataset
under a commercially permissive license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrative Decoding: Improve Factuality via Implicit Self-consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Xiao Liang, Yeyun Gong, Wen Xiao, Song Wang, Yuji Zhang, Wenjun Hou, Kaishuai Xu, Wenge Liu, Wenjie Li, Jian Jiao, Qi Chen, Peng Cheng, Wayne Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-consistency-based approaches, which involve repeatedly sampling multiple
outputs and selecting the most consistent one as the final response, prove to
be remarkably effective in improving the factual accuracy of large language
models. Nonetheless, existing methods usually have strict constraints on the
task format, largely limiting their applicability. In this paper, we present
Integrative Decoding (ID), to unlock the potential of self-consistency in
open-ended generation tasks. ID operates by constructing a set of inputs, each
prepended with a previously sampled response, and then processes them
concurrently, with the next token being selected by aggregating of all their
corresponding predictions at each decoding step. In essence, this simple
approach implicitly incorporates self-consistency in the decoding objective.
Extensive evaluation shows that ID consistently enhances factuality over a wide
range of language models, with substantial improvements on the TruthfulQA
(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance
gains amplify progressively as the number of sampled responses increases,
indicating the potential of ID to scale up with repeated sampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACE: A <span class="highlight-title">LLM</span>-based Negotiation Coaching System <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Shea, Aymen Kallala, Xin Lucy Liu, Michael W. Morris, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing prominence of LLMs has led to an increase in the development of
AI tutoring systems. These systems are crucial in providing underrepresented
populations with improved access to valuable education. One important area of
education that is unavailable to many learners is strategic bargaining related
to negotiation. To address this, we develop a LLM-based Assistant for Coaching
nEgotiation (ACE). ACE not only serves as a negotiation partner for users but
also provides them with targeted feedback for improvement. To build our system,
we collect a dataset of negotiation transcripts between MBA students. These
transcripts come from trained negotiators and emulate realistic bargaining
scenarios. We use the dataset, along with expert consultations, to design an
annotation scheme for detecting negotiation mistakes. ACE employs this scheme
to identify mistakes and provide targeted feedback to users. To test the
effectiveness of ACE-generated feedback, we conducted a user experiment with
two consecutive trials of negotiation and found that it improves negotiation
performances significantly compared to a system that doesn't provide feedback
and one which uses an alternative method of providing feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedQA-CS: <span class="highlight-title">Benchmark</span>ing <span class="highlight-title">Large Language Model</span>s Clinical Skills Using an
  AI-SCE Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghai Yao, Zihao Zhang, Chaolong Tang, Xingyu Bian, Youxia Zhao, Zhichao Yang, Junda Wang, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) and large language models (LLMs) in healthcare
require advanced clinical skills (CS), yet current benchmarks fail to evaluate
these comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by
medical education's Objective Structured Clinical Examinations (OSCEs), to
address this gap. MedQA-CS evaluates LLMs through two instruction-following
tasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real
clinical scenarios. Our contributions include developing MedQA-CS, a
comprehensive evaluation framework with publicly available data and expert
annotations, and providing the quantitative and qualitative assessment of LLMs
as reliable judges in CS evaluation. Our experiments show that MedQA-CS is a
more challenging benchmark for evaluating clinical skills than traditional
multiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks,
MedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities
for both open- and closed-source LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> In-Context Transfer Learning: Demonstration Synthesis by Transferring
  Similar Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingzirui Wang, Xuangliang Zhang, Qiguang Chen, Longxu Dou, Xiao Xu, Rongyu Cao, Yingwei Ma, Qingfu Zhu, <span class="highlight-author">Wanxiang Che</span>, Binhua Li, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is an effective approach to help large language
models (LLMs) adapt to various tasks by providing demonstrations of the target
task. Considering the high cost of labeling demonstrations, many methods
propose synthesizing demonstrations from scratch using LLMs. However, the
quality of the demonstrations synthesized from scratch is limited by the
capabilities and knowledge of LLMs. To address this, inspired by transfer
learning, we propose In-Context Transfer Learning (ICTL), which synthesizes
target task demonstrations by transferring labeled demonstrations from similar
source tasks. ICTL consists of two steps: source sampling and target transfer.
First, we define an optimization objective, which minimizes transfer error to
sample source demonstrations similar to the target task. Then, we employ LLMs
to transfer the sampled source demonstrations to the target task, matching the
definition and format of the target task. Experiments on Super-NI show that
ICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the
effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angela Lopez-Cardona, Carlos Segura, Alexandros Karatzoglou, Sergi Abadal, Ioannis Arapakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in Natural Language Processing (NLP), have led to the emergence
of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which
excel across a range of tasks but require extensive fine-tuning to align their
outputs with human expectations. A widely used method for achieving this
alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite
its success, faces challenges in accurately modelling human preferences. In
this paper, we introduce GazeReward, a novel framework that integrates implicit
feedback -- and specifically eye-tracking (ET) data -- into the Reward Model
(RM). In addition, we explore how ET-based features can provide insights into
user preferences. Through ablation studies we test our framework with different
integration methods, LLMs, and ET generator models, demonstrating that our
approach significantly improves the accuracy of the RM on established human
preference datasets. This work advances the ongoing discussion on optimizing AI
alignment with human values, exploring the potential of cognitive data for
shaping future NLP research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HarmAug: Effective Data Augmentation for Knowledge Distillation of
  Safety Guard Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seanie Lee, Haebin Seong, Dong Bok Lee, Minki Kang, Xiaoyin Chen, Dominik Wagner, Yoshua Bengio, Juho Lee, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety guard models that detect malicious queries aimed at large language
models (LLMs) are essential for ensuring the secure and responsible deployment
of LLMs in real-world applications. However, deploying existing safety guard
models with billions of parameters alongside LLMs on mobile devices is
impractical due to substantial memory requirements and latency. To reduce this
cost, we distill a large teacher safety guard model into a smaller one using a
labeled dataset of instruction-response pairs with binary harmfulness labels.
Due to the limited diversity of harmful instructions in the existing labeled
dataset, naively distilled models tend to underperform compared to larger
models. To bridge the gap between small and large models, we propose HarmAug, a
simple yet effective data augmentation method that involves jailbreaking an LLM
and prompting it to generate harmful instructions. Given a prompt such as,
"Make a single harmful instruction prompt that would elicit offensive content",
we add an affirmative prefix (e.g., "I have an idea for a prompt:") to the
LLM's response. This encourages the LLM to continue generating the rest of the
response, leading to sampling harmful instructions. Another LLM generates a
response to the harmful instruction, and the teacher model labels the
instruction-response pair. We empirically show that our HarmAug outperforms
other relevant baselines. Moreover, a 435-million-parameter safety guard model
trained with HarmAug achieves an F1 score comparable to larger models with over
7 billion parameters, and even outperforms them in AUPRC, while operating at
less than 25% of their computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiniPot: Infinite Context Processing on Memory-Constrained <span class="highlight-title">LLM</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling long input contexts remains a significant challenge for Large
Language Models (LLMs), particularly in resource-constrained environments such
as mobile devices. Our work aims to address this limitation by introducing
InfiniPot, a novel KV cache control framework designed to enable pre-trained
LLMs to manage extensive sequences within fixed memory constraints efficiently,
without requiring additional training. InfiniPot leverages Continual Context
Distillation (CCD), an iterative process that compresses and retains essential
information through novel importance metrics, effectively maintaining critical
data even without access to future context. Our comprehensive evaluations
indicate that InfiniPot significantly outperforms models trained for long
contexts in various NLP tasks, establishing its efficacy and versatility. This
work represents a substantial advancement toward making LLMs applicable to a
broader range of real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstaTrans: An Instruction-Aware Translation Framework for Non-English
  Instruction <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yungi Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is challenging to generate high-quality instruction datasets for
non-English languages due to tail phenomena, which limit performance on less
frequently observed data. To mitigate this issue, we propose translating
existing high-quality English instruction datasets as a solution, emphasizing
the need for complete and instruction-aware translations to maintain the
inherent attributes of these datasets. We claim that fine-tuning LLMs with
datasets translated in this way can improve their performance in the target
language. To this end, we introduces a new translation framework tailored for
instruction datasets, named InstaTrans (INSTruction-Aware TRANSlation). Through
extensive experiments, we demonstrate the superiority of InstaTrans over other
competitors in terms of completeness and instruction-awareness of translation,
highlighting its potential to broaden the accessibility of LLMs across diverse
languages at a relatively low cost. Furthermore, we have validated that
fine-tuning LLMs with datasets translated by InstaTrans can effectively improve
their performance in the target language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Latent Shifts of In-Context Learning Through Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josip Jukić, Jan Šnajder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) has become essential in natural language
processing, particularly with autoregressive large language models capable of
learning from demonstrations provided within the prompt. However, ICL faces
challenges with stability and long contexts, especially as the number of
demonstrations grows, leading to poor generalization and inefficient inference.
To address these issues, we introduce STICL (Self-Training ICL), an approach
that disentangles the latent shifts of demonstrations from the latent shift of
the query through self-training. STICL employs a teacher model to generate
pseudo-labels and trains a student model using these labels, encoded in an
adapter module. The student model exhibits weak-to-strong generalization,
progressively refining its predictions over time. Our empirical results show
that STICL improves generalization and stability, consistently outperforming
traditional ICL methods and other disentangling strategies across both
in-domain and out-of-domain data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PersonaMath: Enhancing Math <span class="highlight-title">Reasoning</span> through Persona-Driven Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Luo, Run Luo, Longze Chen, Liang Zhu, Chang Ao, Jiaming Li, Yukun Chen, Xin Cheng, Wen Yang, Jiayuan Su, Chengming Li, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While closed-source Large Language Models (LLMs) demonstrate strong
mathematical problem-solving abilities, open-source models continue to struggle
with such tasks. To bridge this gap, we propose a data augmentation approach
and introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we
train the PersonaMath models. Our approach consists of two stages: the first
stage is learning from Persona Diversification, and the second stage is
learning from Reflection. In the first stage, we regenerate detailed
chain-of-thought (CoT) solutions as instructions using a closed-source LLM and
introduce a novel persona-driven data augmentation technique to enhance the
dataset's quantity and diversity. In the second stage, we incorporate
reflection to fully leverage more challenging and valuable questions.
Evaluation of our PersonaMath models on MATH and GSM8K reveals that the
PersonaMath-7B model (based on LLaMA-2-7B) achieves an accuracy of 24.2% on
MATH and 68.7% on GSM8K, surpassing all baseline methods and achieving
state-of-the-art performance. Notably, our dataset contains only 70.3K data
points-merely 17.8% of MetaMathQA and 27% of MathInstruct-yet our model
outperforms these baselines, demonstrating the high quality and diversity of
our dataset, which enables more efficient model training. We open-source the
PersonaMathQA dataset, PersonaMath models, and our code for public usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,
  Lightweight Plugin for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, Ruizhe Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have achieved robust
performance across diverse tasks, but fine-tuning these models for specific
domains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)
methods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a
small subset of parameters. However, existing methods for fusing multiple LoRAs
lack dynamic fusion based on contextual inputs and often increase inference
time due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight
Plugin that employs a mini-MLP module with only 5M parameters to dynamically
fuse multiple LoRAs at the sentence level using top-p sampling strategies. This
approach reduces inference time to less than twice that of single LoRA
inference by leveraging parallel computation. Evaluations across 26
tasks-including multiple-choice questions and question answering-demonstrate
that DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice
datasets and significant improvements in BLEU and ROUGE scores on QA datasets,
outperforming different LLMs backbones under composite task settings. DLP-LoRA
effectively balances performance and efficiency, making it a practical solution
for dynamic multi-task adaptation in LLMs. Our code is available at
https://github.com/MeCuping/DLP-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review, 18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending Context Window of <span class="highlight-title">Large Language Model</span>s from a Distributional
  Perspective <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingsheng Wu. Yuxuan Gu, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling the rotary position embedding (RoPE) has become a common method for
extending the context window of RoPE-based large language models (LLMs).
However, existing scaling methods often rely on empirical approaches and lack a
profound understanding of the internal distribution within RoPE, resulting in
suboptimal performance in extending the context window length. In this paper,
we propose to optimize the context window extending task from the view of
rotary angle distribution. Specifically, we first estimate the distribution of
the rotary angles within the model and analyze the extent to which length
extension perturbs this distribution. Then, we present a novel extension
strategy that minimizes the disturbance between rotary angle distributions to
maintain consistency with the pre-training phase, enhancing the model's
capability to generalize to longer sequences. Experimental results compared to
the strong baseline methods demonstrate that our approach reduces by up to 72%
of the distributional disturbance when extending LLaMA2's context window to 8k,
and reduces by up to 32% when extending to 16k. On the LongBench-E benchmark,
our method achieves an average improvement of up to 4.33% over existing
state-of-the-art methods. Furthermore, Our method maintains the model's
performance on the Hugging Face Open LLM benchmark after context window
extension, with only an average performance fluctuation ranging from -0.12 to
+0.22.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, Accepted to EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Language Models Like Small Vocabularies: Probing the Linguistic
  Abilities of Grapheme- and Phoneme-Based Baby Llamas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Bunzeck, Daniel Duran, Leonie Schade, Sina Zarrieß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current language models use subword-based tokenization algorithms like Byte
Pair Encoding, which put their validity as models of linguistic representations
into question. In this paper, we explore the potential of tokenization-free,
phoneme- and grapheme-based language models. We demonstrate that small models
based on the Llama architecture can achieve strong linguistic performance on
standard syntactic and novel lexical/phonetic benchmarks when trained with
character-level vocabularies. We further show that phoneme-based models without
any graphemic biases almost match grapheme-based models in standard tasks and
novel evaluations. Our findings suggest a promising direction for creating more
linguistically plausible language models that are better suited for
computational studies of language acquisition and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Little Goes a Long Way: Efficient Long Context Training and Inference
  with Partial Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, Hao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training and serving long-context large language models (LLMs) incurs
substantial overhead. To address this, two critical steps are often required: a
pretrained LLM typically undergoes a separate stage for context length
extension by training on long-context data, followed by architectural
modifications to reduce the overhead of KV cache during serving. This paper
argues that integrating length extension with a GPU-friendly KV cache reduction
architecture not only reduces training overhead during length extension, but
also achieves better long-context performance. This leads to our proposed
LongGen, which finetunes a pretrained LLM into an efficient architecture during
length extension. LongGen builds on three key insights: (1) Sparse attention
patterns, such as window attention (attending to recent tokens), attention sink
(initial ones), and blockwise sparse attention (strided token blocks) are
well-suited for building efficient long-context models, primarily due to their
GPU-friendly memory access patterns, enabling efficiency gains not just
theoretically but in practice as well. (2) It is essential for the model to
have direct access to all tokens. A hybrid architecture with 1/3 full attention
layers and 2/3 efficient ones achieves a balanced trade-off between efficiency
and long-context performance. (3) Lightweight training on 5B long-context data
is sufficient to extend the hybrid model's context length from 4K to 128K.
  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its
effectiveness across different scales. During training with 128K-long contexts,
LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,
compared to a full-attention baseline. During inference, LongGen reduces KV
cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding
speedup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Agent</span>-Driven <span class="highlight-title">Large Language Model</span>s for Mandarin Lyric <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Hsiang Liu, Yi-Wen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Language Models have shown impressive in-context learning
abilities, performing well across various tasks with just a prompt. Previous
melody-to-lyric research has been limited by scarce high-quality aligned data
and unclear standard for creativeness. Most efforts focused on general themes
or emotions, which are less valuable given current language model capabilities.
In tonal contour languages like Mandarin, pitch contours are influenced by both
melody and tone, leading to variations in lyric-melody fit. Our study,
validated by the Mpop600 dataset, confirms that lyricists and melody writers
consider this fit during their composition process. In this research, we
developed a multi-agent system that decomposes the melody-to-lyric task into
sub-tasks, with each agent controlling rhyme, syllable count, lyric-melody
alignment, and consistency. Listening tests were conducted via a
diffusion-based singing voice synthesizer to evaluate the quality of lyrics
generated by different agent groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, figures, Accepted at O-COCOSDA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic
  Music: A Focus on Musical Phrase Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinh-Viet-Toan Le, Louis Bigo, Mikaela Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language
Processing to build a vocabulary of subwords, which has been recently applied
to symbolic music. Given that symbolic music can differ significantly from
text, particularly with polyphony, we investigate how BPE behaves with
different types of musical content. This study provides a qualitative analysis
of BPE's behavior across various instrumentations and evaluates its impact on a
musical phrase segmentation task for both monophonic and polyphonic music. Our
findings show that the BPE training process is highly dependent on the
instrumentation and that BPE "supertokens" succeed in capturing abstract
musical content. In a musical phrase segmentation task, BPE notably improves
performance in a polyphonic setting, but enhances performance in monophonic
tunes only within a specific range of BPE merges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 3rd Workshop on NLP for Music and Audio (NLP4MusA,
  co-located with ISMIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric Signatures of Compositionality Across a Language Model's
  Lifetime <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Yoshua Bengio, Emily Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositionality, the notion that the meaning of an expression is constructed
from the meaning of its parts and syntactic rules, permits the infinite
productivity of human language. For the first time, artificial language models
(LMs) are able to match human performance in a number of compositional
generalization tasks. However, much remains to be understood about the
representational mechanisms underlying these abilities. We take a high-level
geometric approach to this problem by relating the degree of compositionality
in a dataset to the intrinsic dimensionality of its representations under an
LM, a measure of feature complexity. We find not only that the degree of
dataset compositionality is reflected in representations' intrinsic
dimensionality, but that the relationship between compositionality and
geometric complexity arises due to learned linguistic features over training.
Finally, our analyses reveal a striking contrast between linear and nonlinear
dimensionality, showing that they respectively encode formal and semantic
aspects of linguistic composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Circuit Compositions: Exploring Modular Structures in Transformer-Based
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Mondorf, Sondre Wold, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in interpretability research is to what extent neural
networks, particularly language models, implement reusable functions via
subnetworks that can be composed to perform more complex tasks. Recent
developments in mechanistic interpretability have made progress in identifying
subnetworks, often referred to as circuits, which represent the minimal
computational subgraph responsible for a model's behavior on specific tasks.
However, most studies focus on identifying circuits for individual tasks
without investigating how functionally similar circuits relate to each other.
To address this gap, we examine the modularity of neural networks by analyzing
circuits for highly compositional subtasks within a transformer-based language
model. Specifically, given a probabilistic context-free grammar, we identify
and compare circuits responsible for ten modular string-edit operations. Our
results indicate that functionally similar circuits exhibit both notable node
overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits
identified can be reused and combined through subnetwork set operations to
represent more complex functional capabilities of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Further Elicit <span class="highlight-title">Reasoning</span> in <span class="highlight-title">LLM</span>s? Critic-Guided Planning with
  Retrieval-Augmentation for Solving Challenging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art large language models (LLMs) exhibit impressive
problem-solving capabilities but may struggle with complex reasoning and
factual correctness. Existing methods harness the strengths of chain-of-thought
and retrieval-augmented generation (RAG) to decompose a complex problem into
simpler steps and apply retrieval to improve factual correctness. These methods
work well on straightforward reasoning tasks but often falter on challenging
tasks such as competitive programming and mathematics, due to frequent
reasoning errors and irrelevant knowledge retrieval. To address this, we
introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a
novel framework that leverages fine-tuned critic models to guide both reasoning
and retrieval processes through planning. CR-Planner solves a problem by
iteratively selecting and executing sub-goals. Initially, it identifies the
most promising sub-goal from reasoning, query generation, and retrieval, guided
by rewards given by a critic model named sub-goal critic. It then executes this
sub-goal through sampling and selecting the optimal output based on evaluations
from another critic model named execution critic. This iterative process,
informed by retrieved information and critic models, enables CR-Planner to
effectively navigate the solution space towards the final answer. We employ
Monte Carlo Tree Search to collect the data for training the critic models,
allowing for a systematic exploration of action sequences and their long-term
impacts. We validate CR-Planner on challenging domain-knowledge-intensive and
reasoning-heavy tasks, including competitive programming, theorem-driven math
reasoning, and complex domain retrieval problems. Our experiments demonstrate
that CR-Planner significantly outperforms baselines, highlighting its
effectiveness in addressing challenging problems by improving both reasoning
and retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CASE: Efficient Curricular Data Pre-training for Building Assistive
  Psychology Expert Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00314v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00314v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Harne, Monjoy Narayan Choudhury, Madhav Rao, TK Srikanth, Seema Mehrotra, Apoorva Vashisht, Aarushi Basu, Manjit Sodhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limited availability of psychologists necessitates efficient
identification of individuals requiring urgent mental healthcare. This study
explores the use of Natural Language Processing (NLP) pipelines to analyze text
data from online mental health forums used for consultations. By analyzing
forum posts, these pipelines can flag users who may require immediate
professional attention. A crucial challenge in this domain is data privacy and
scarcity. To address this, we propose utilizing readily available curricular
texts used in institutes specializing in mental health for pre-training the NLP
pipelines. This helps us mimic the training process of a psychologist. Our work
presents CASE-BERT that flags potential mental health disorders based on forum
text. CASE-BERT demonstrates superior performance compared to existing methods,
achieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the
most commonly reported mental health disorders. Our code and data are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is lost in Normalization? Exploring Pitfalls in Multilingual ASR
  Model Evaluations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavya Manohar, Leena G Pillai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the pitfalls in evaluating multilingual automatic speech
recognition (ASR) models, with a particular focus on Indic language scripts. We
investigate the text normalization routine employed by leading ASR models,
including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,
and their unintended consequences on performance metrics. Our research reveals
that current text normalization practices, while aiming to standardize ASR
outputs for fair comparison, by removing inconsistencies such as variations in
spelling, punctuation, and special characters, are fundamentally flawed when
applied to Indic scripts. Through empirical analysis using text similarity
scores and in-depth linguistic examination, we demonstrate that these flaws
lead to artificially improved performance metrics for Indic languages. We
conclude by proposing a shift towards developing text normalization routines
that leverage native linguistic expertise, ensuring more robust and accurate
evaluations of multilingual ASR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Conjuring: Multi-User Runtime Collaboration with AI in Building
  Virtual 3D Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amina Kobenova, Cyan DeVeaux, Samyak Parajuli, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence has shown promise in prompting virtual
worlds into existence, yet little attention has been given to understanding how
this process unfolds as social interaction. We present Social Conjurer, a
framework for AI-augmented dynamic 3D scene co-creation, where multiple users
collaboratively build and modify virtual worlds in real-time. Through an
expanded set of interactions, including social and tool-based engagements as
well as spatial reasoning, our framework facilitates the creation of rich,
diverse virtual environments. Findings from a preliminary user study (N=12)
provide insight into the user experience of this approach, how social contexts
shape the prompting of spatial environments, and perspective on social
applications of prompt-based 3D co-creation. In addition to highlighting the
potential of AI-supported multi-user world creation and offering new pathways
for AI-augmented creative processes in VR, this article presents a set of
implications for designing human-centered interfaces that incorporate AI models
into 3D content generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding
  issues in arXiv compilation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliminating Position Bias of Language Models: A Mechanistic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Position bias has proven to be a prevalent issue of modern language models
(LMs), where the models prioritize content based on its position within the
given context. This bias often leads to unexpected model failures and hurts
performance, robustness, and reliability across various applications. Our
mechanistic analysis attributes the position bias to two components employed in
nearly all state-of-the-art LMs: causal attention and relative positional
encodings. Based on the analyses, we propose to eliminate position bias (e.g.,
different retrieved documents' orders in QA affect performance) with a
training-free zero-shot approach. Our method changes the causal attention to
bidirectional attention between documents and utilizes model attention values
to decide the relative orders of documents instead of using the order provided
in input prompts, therefore enabling Position-INvariant inferencE (PINE) at the
document level. By eliminating position bias, models achieve better performance
and reliability in downstream tasks, including LM-as-a-judge,
retrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE
is especially useful when adapting LMs for evaluating reasoning pairs: it
consistently provides 8 to 10 percentage points performance gains, making
Llama-3-70B-Instruct perform even better than GPT-4-0125-preview and
GPT-4o-2024-08-06 on the RewardBench reasoning set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Optimal LR Across Token Horizons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset
size and cluster size. It is economically infeasible to extensively tune
hyperparameter for the largest runs. Instead, approximately optimal
hyperparameters must be inferred or \textit{transferred} from smaller
experiments. Hyperparameter transfer across model sizes has been studied in
Yang et al. However, hyperparameter transfer across dataset size -- or token
horizon -- has not been studied yet. To remedy this we conduct a large scale
empirical study on how optimal learning rate (LR) depends on token horizon in
LLM training. We first demonstrate that the optimal LR changes significantly
with token horizon -- longer training necessitates smaller LR. Secondly we
demonstrate the the optimal LR follows a scaling law, and that the optimal LR
for longer horizons can be accurately estimated from shorter horizons via such
scaling laws. We also provide a rule-of-thumb for transferring LR across token
horizons with zero overhead over current practices. Lastly we provide evidence
that LLama-1 used too high LR, and estimate the performance hit from this. We
thus argue that hyperparameter transfer across data size is an important and
overlooked component of LLM training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Controllable Preference Optimization: Toward Controllable
  Multi-Objective Alignment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Zexu Sun, Bowen Sun, Huimin Chen, Ruobing Xie, Jie Zhou, Yankai Lin, <span class="highlight-author">Zhiyuan Liu</span>, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment in artificial intelligence pursues the consistency between model
responses and human preferences as well as values. In practice, the
multifaceted nature of human preferences inadvertently introduces what is known
as the "alignment tax" -a compromise where enhancements in alignment within one
objective (e.g.,harmlessness) can diminish performance in others
(e.g.,helpfulness). However, existing alignment techniques are mostly
unidirectional, leading to suboptimal trade-offs and poor flexibility over
various objectives. To navigate this challenge, we argue the prominence of
grounding LLMs with evident preferences. We introduce controllable preference
optimization (CPO), which explicitly specifies preference scores for different
objectives, thereby guiding the model to generate responses that meet the
requirements. Our experimental analysis reveals that the aligned models can
provide responses that match various preferences among the "3H" (helpfulness,
honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and
alignment goals, we surpass baseline methods in aligning with single
objectives, hence mitigating the impact of the alignment tax and achieving
Pareto improvements in multi-objective alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ README: Bridging Medical Jargon and Lay Understanding for Patient
  Education through Data-Centric NLP <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15561v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15561v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghai Yao, Nandyala Siddharth Kantu, Guanghao Wei, Hieu Tran, Zhangqi Duan, Sunjae Kwon, Zhichao Yang, README annotation team, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement in healthcare has shifted focus toward patient-centric
approaches, particularly in self-care and patient education, facilitated by
access to Electronic Health Records (EHR). However, medical jargon in EHRs
poses significant challenges in patient comprehension. To address this, we
introduce a new task of automatically generating lay definitions, aiming to
simplify complex medical terms into patient-friendly lay language. We first
created the README dataset, an extensive collection of over 50,000 unique
(medical term, lay definition) pairs and 300,000 mentions, each offering
context-aware lay definitions manually annotated by domain experts. We have
also engineered a data-centric Human-AI pipeline that synergizes data
filtering, augmentation, and selection to improve data quality. We then used
README as the training data for models and leveraged a Retrieval-Augmented
Generation method to reduce hallucinations and improve the quality of model
outputs. Our extensive automatic and human evaluations demonstrate that
open-source mobile-friendly models, when fine-tuned with high-quality data, are
capable of matching or even surpassing the performance of state-of-the-art
closed-source large language models like ChatGPT. This research represents a
significant stride in closing the knowledge gap in patient education and
advancing patient-centric healthcare solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Findings of the Association for Computational
  Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Dynamics of <span class="highlight-title">LLM</span> Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ren, Danica J. Sutherland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dynamics, which describes how the learning of specific training
examples influences the model's predictions on other examples, gives us a
powerful tool for understanding the behavior of deep learning systems. We study
the learning dynamics of large language models during different types of
finetuning, by analyzing the step-wise decomposition of how influence
accumulates among different potential responses. Our framework allows a uniform
interpretation of many interesting observations about the training of popular
algorithms for both instruction tuning and preference tuning. In particular, we
propose a hypothetical explanation of why specific types of hallucination are
strengthened after finetuning, e.g., the model might use phrases or facts in
the response for question B to answer question A, or the model might keep
repeating similar simple phrases when generating responses. We also extend our
framework and highlight a unique "squeezing effect" to explain a previously
observed phenomenon in off-policy direct preference optimization (DPO), where
running DPO for too long makes even the desired outputs less likely. This
framework also provides insights into where the benefits of on-policy DPO and
other variants come from. The analysis not only provides a novel perspective of
understanding LLM's finetuning but also inspires a simple, effective method to
improve alignment performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCAR: Efficient Instruction-Tuning for <span class="highlight-title">Large Language Model</span>s via Style
  Consistency-Aware Response Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10882v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10882v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Li, Yuncheng Hua, Thuy-Trang Vu, Haolan Zhan, Lizhen Qu, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that maintaining a consistent response style by
human experts and enhancing data quality in training sets can significantly
improve the performance of fine-tuned Large Language Models (LLMs) while
reducing the number of training examples needed. However, the precise
definition of style and the relationship between style, data quality, and LLM
performance remains unclear. This research identifies two key stylistic
elements in responses: linguistic form and semantic surprisal. We find that,
among training data of comparable quality, higher consistency in these response
elements leads to better LLM performance. Inspired by this, we introduce Style
Consistency-Aware Response Ranking (SCAR), which automatically prioritizes
instruction-response pairs in the training set based on their response
stylistic consistency. By selecting the most style-consistent examples,
sometimes as few as 0.7% of the full dataset, the fine-tuned LLMs can match or
even surpass the performance of models trained on the entire dataset in coding
and open-ended question-answering benchmarks. Code and data are available at
https://github.com/zhuang-li/SCAR .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LogicAsker: Evaluating and Improving the Logical <span class="highlight-title">Reasoning</span> Ability of
  <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LogicAsker, a novel approach for evaluating and enhancing the
logical reasoning capabilities of large language models (LLMs) such as ChatGPT
and GPT-4. Despite LLMs' prowess in tasks like writing assistance, code
generation, and machine translation, assessing their ability to reason has been
challenging. Traditional evaluations often prioritize accuracy on downstream
tasks over direct assessments of reasoning processes. LogicAsker addresses this
gap by employing a set of atomic reasoning skills grounded in propositional and
predicate logic to systematically examine and improve the reasoning prowess of
LLMs. Our methodology reveals significant gaps in LLMs' learning of logical
rules, with identified reasoning failures ranging from 29\% to 90\% across
different models. Moreover, we leverage these findings to construct targeted
demonstration examples and fine-tune data, notably enhancing logical reasoning
in models like GPT-4o by up to 5\%. To our knowledge, this is the first effort
to utilize test case outcomes to effectively refine LLMs' formal reasoning
capabilities. We make our code, data, and results publicly available
(https://github.com/yxwan123/LogicAsker) to facilitate further research and
replication of our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SysCaps: Language Interfaces for Simulation Surrogates of Complex
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Emami, Zhaonan Li, Saumya Sinha, Truc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate models are used to predict the behavior of complex energy systems
that are too expensive to simulate with traditional numerical methods. Our work
introduces the use of language descriptions, which we call "system captions" or
SysCaps, to interface with such surrogates. We argue that interacting with
surrogates through text, particularly natural language, makes these models more
accessible for both experts and non-experts. We introduce a lightweight
multimodal text and timeseries regression model and a training pipeline that
uses large language models (LLMs) to synthesize high-quality captions from
simulation metadata. Our experiments on two real-world simulators of buildings
and wind farms show that our SysCaps-augmented surrogates have better accuracy
on held-out systems than traditional methods while enjoying new generalization
abilities, such as handling semantically related descriptions of the same test
system. Additional experiments also highlight the potential of SysCaps to
unlock language-driven design space exploration and to regularize training
through prompt augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Multi-Token Joint Decoding with Auxiliary Model for <span class="highlight-title">LLM</span>
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success across diverse
tasks, yet their inference processes are hindered by substantial time and
energy demands due to single-token generation at each decoding step. While
previous methods such as speculative decoding mitigate these inefficiencies by
producing multiple tokens per step, each token is still generated by its
single-token distribution, thereby enhancing speed without improving
effectiveness. In contrast, our work simultaneously enhances inference speed
and improves the output effectiveness. We consider multi-token joint decoding
(MTJD), which generates multiple tokens from their joint distribution at each
iteration, theoretically reducing perplexity and enhancing task performance.
However, MTJD suffers from the high cost of sampling from the joint
distribution of multiple tokens. Inspired by speculative decoding, we introduce
multi-token assisted decoding (MTAD), a novel framework designed to accelerate
MTJD. MTAD leverages a smaller auxiliary model to approximate the joint
distribution of a larger model, incorporating a verification mechanism that not
only ensures the accuracy of this approximation, but also improves the decoding
efficiency over conventional speculative decoding. Theoretically, we
demonstrate that MTAD closely approximates exact MTJD with bounded error.
Empirical evaluations using Llama-2 and OPT models ranging from 13B to 70B
parameters across various tasks reveal that MTAD reduces perplexity by 21.2%
and improves downstream performance compared to standard single-token sampling.
Furthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than
conventional speculative decoding methods. These results highlight MTAD's
ability to make multi-token joint decoding both effective and efficient,
promoting more sustainable and high-performance deployment of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tool-Planner: Task Planning with Clusters across Multiple Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated exceptional reasoning
capabilities, enabling them to solve various complex problems. Recently, this
ability has been applied to the paradigm of tool learning. Tool learning
involves providing examples of tool usage and their corresponding functions,
allowing LLMs to formulate plans and demonstrate the process of invoking and
executing each tool. LLMs can address tasks that they cannot complete
independently, thereby enhancing their potential across different tasks.
However, this approach faces two key challenges. First, redundant error
correction leads to unstable planning and long execution time. Additionally,
designing a correct plan among multiple tools is also a challenge in tool
learning. To address these issues, we propose Tool-Planner, a task-processing
framework based on toolkits. Tool-Planner groups tools based on the API
functions with the same function into a toolkit and allows LLMs to implement
planning across the various toolkits. When a tool error occurs, the language
model can reselect and adjust tools based on the toolkit. Experiments show that
our approach demonstrates a high pass and win rate across different datasets
and optimizes the planning scheme for tool learning in models such as GPT-4 and
Claude 3, showcasing the potential of our method. Our code is public at
\url{https://github.com/OceannTwT/Tool-Planner}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48pages second version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TuBA: Cross-Lingual Transferability of Backdoor Attacks in <span class="highlight-title">LLM</span>s with
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The implications of backdoor attacks on English-centric large language models
(LLMs) have been widely examined - such attacks can be achieved by embedding
malicious behaviors during training and activated under specific conditions
that trigger malicious outputs. Despite the increasing support for multilingual
capabilities in open-source and proprietary LLMs, the impact of backdoor
attacks on these systems remains largely under-explored. Our research focuses
on cross-lingual backdoor attacks against multilingual LLMs, particularly
investigating how poisoning the instruction-tuning data for one or two
languages can affect the outputs for languages whose instruction-tuning data
were not poisoned. Despite its simplicity, our empirical analysis reveals that
our method exhibits remarkable efficacy in models like mT5 and GPT-4o, with
high attack success rates, surpassing 90% in more than 7 out of 12 languages
across various scenarios. Our findings also indicate that more powerful models
show increased susceptibility to transferable cross-lingual backdoor attacks,
which also applies to LLMs predominantly pre-trained on English data, such as
Llama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High
Transferability: the backdoor mechanism operates successfully in cross-lingual
response scenarios across 26 languages, achieving an average attack success
rate of 99%, and 2) Robustness: the proposed attack remains effective even
after defenses are applied. These findings expose critical security
vulnerabilities in multilingual LLMs and highlight the urgent need for more
robust, targeted defense strategies to address the unique challenges posed by
cross-lingual backdoor transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Addition is All You Need for Energy-efficient Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Luo, Wei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks spend most computation on floating point tensor
multiplications. In this work, we find that a floating point multiplier can be
approximated by one integer adder with high precision. We propose the
linear-complexity multiplication L-Mul algorithm that approximates floating
point number multiplication with integer addition operations. The new algorithm
costs significantly less computation resource than 8-bit floating point
multiplication but achieves higher precision. Compared to 8-bit floating point
multiplications, the proposed method achieves higher precision but consumes
significantly less bit-level computation. Since multiplying floating point
numbers requires substantially higher energy compared to integer addition
operations, applying the L-Mul operation in tensor processing hardware can
potentially reduce 95% energy cost by element-wise floating point tensor
multiplications and 80% energy cost of dot products. We calculated the
theoretical error expectation of L-Mul, and evaluated the algorithm on a wide
range of textual, visual, and symbolic tasks, including natural language
understanding, structural reasoning, mathematics, and commonsense question
answering. Our numerical analysis experiments agree with the theoretical error
estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable
precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa
outperforms float8_e5m2. Evaluation results on popular benchmarks show that
directly applying L-Mul to the attention mechanism is almost lossless. We
further show that replacing all floating point multiplications with 3-bit
mantissa L-Mul in a transformer model achieves equivalent precision as using
float8_e4m3 as accumulation precision in both fine-tuning and inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is "Typological Diversity" in NLP? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04222v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04222v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esther Ploeger, Wessel Poelman, Miryam de Lhoneux, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The NLP research community has devoted increased attention to languages
beyond English, resulting in considerable improvements for multilingual NLP.
However, these improvements only apply to a small subset of the world's
languages. Aiming to extend this, an increasing number of papers aspires to
enhance generalizable multilingual performance across languages. To this end,
linguistic typology is commonly used to motivate language selection, on the
basis that a broad typological sample ought to imply generalization across a
broad range of languages. These selections are often described as being
'typologically diverse'. In this work, we systematically investigate NLP
research that includes claims regarding 'typological diversity'. We find there
are no set definitions or criteria for such claims. We introduce metrics to
approximate the diversity of language selection along several axes and find
that the results vary considerably across papers. Crucially, we show that
skewed language selection can lead to overestimated multilingual performance.
We recommend future work to include an operationalization of 'typological
diversity' that empirically justifies the diversity of language samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024: Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gemma 2: Improving Open Language Models at a Practical Size 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, Alek Andreev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Gemma 2, a new addition to the Gemma family of
lightweight, state-of-the-art open models, ranging in scale from 2 billion to
27 billion parameters. In this new version, we apply several known technical
modifications to the Transformer architecture, such as interleaving
local-global attentions (Beltagy et al., 2020a) and group-query attention
(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge
distillation (Hinton et al., 2015) instead of next token prediction. The
resulting models deliver the best performance for their size, and even offer
competitive alternatives to models that are 2-3 times bigger. We release all
our models to the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable and diverse evaluation of <span class="highlight-title">LLM</span> medical knowledge mastery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Xien Liu, Chen Ning, Xiao Zhang, Ji Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mastering medical knowledge is crucial for medical-specific LLMs. However,
despite the existence of medical benchmarks like MedQA, a unified framework
that fully leverages existing knowledge bases to evaluate LLMs' mastery of
medical knowledge is still lacking. In the study, we propose a novel framework
PretexEval that dynamically generates reliable and diverse test samples to
evaluate LLMs for any given medical knowledge base. We notice that test samples
produced directly from knowledge bases by templates or LLMs may introduce
factual errors and also lack diversity. To address these issues, we introduce a
novel schema into our proposed evaluation framework that employs predicate
equivalence transformations to produce a series of variants for any given
medical knowledge point. Finally, these produced predicate variants are
converted into textual language, resulting in a series of reliable and diverse
test samples to evaluate whether LLMs fully master the given medical factual
knowledge point. Here, we use our proposed framework to systematically
investigate the mastery of medical factual knowledge of 12 well-known LLMs,
based on two knowledge bases that are crucial for clinical diagnosis and
treatment. The evaluation results illustrate that current LLMs still exhibit
significant deficiencies in fully mastering medical knowledge, despite
achieving considerable success on some famous public benchmarks. These new
findings provide valuable insights for developing medical-specific LLMs,
highlighting that current LLMs urgently need to strengthen their comprehensive
and in-depth mastery of medical knowledge before being applied to real-world
medical scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Isabelle Mohr, Daniel James Williams, Bo Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many use cases require retrieving smaller portions of text, and dense
vector-based retrieval systems often perform better with shorter text segments,
as the semantics are less likely to be over-compressed in the embeddings.
Consequently, practitioners often split text documents into smaller chunks and
encode them separately. However, chunk embeddings created in this way can lose
contextual information from surrounding chunks, resulting in sub-optimal
representations. In this paper, we introduce a novel method called late
chunking, which leverages long context embedding models to first embed all
tokens of the long text, with chunking applied after the transformer model and
just before mean pooling - hence the term late in its naming. The resulting
chunk embeddings capture the full contextual information, leading to superior
results across various retrieval tasks. The method is generic enough to be
applied to a wide range of long-context embedding models and works without
additional training. To further increase the effectiveness of late chunking, we
propose a dedicated fine-tuning approach for embedding models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3rd draft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOPFORMER: Topology-Aware Authorship Attribution of Deepfake Texts with
  Diverse Writing Styles <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12934v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12934v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adaku Uchendu, Thai Le, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have enabled the generation
of open-ended high-quality texts, that are non-trivial to distinguish from
human-written texts. We refer to such LLM-generated texts as deepfake texts.
There are currently over 72K text generation models in the huggingface model
repo. As such, users with malicious intent can easily use these open-sourced
LLMs to generate harmful texts and dis/misinformation at scale. To mitigate
this problem, a computational method to determine if a given text is a deepfake
text or not is desired--i.e., Turing Test (TT). In particular, in this work, we
investigate the more general version of the problem, known as Authorship
Attribution (AA), in a multi-class setting--i.e., not only determining if a
given text is a deepfake text or not but also being able to pinpoint which LLM
is the author. We propose TopFormer to improve existing AA solutions by
capturing more linguistic patterns in deepfake texts by including a Topological
Data Analysis (TDA) layer in the Transformer-based model. We show the benefits
of having a TDA layer when dealing with imbalanced, and multi-style datasets,
by extracting TDA features from the reshaped $pooled\_output$ of our backbone
as input. This Transformer-based model captures contextual representations
(i.e., semantic and syntactic linguistic features), while TDA captures the
shape and structure of data (i.e., linguistic structures). Finally, TopFormer,
outperforms all baselines in all 3 datasets, achieving up to 7\% increase in
Macro F1 score. Our code and datasets are available at:
https://github.com/AdaUchendu/topformer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The 27th European Conference on Artificial Intelligence
  (ECAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Heads are Better than One: Mixture of Modality Knowledge
  Experts for Entity Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Binbin Hu, Ziqi Liu, Wen Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning high-quality multi-modal entity representations is an important goal
of multi-modal knowledge graph (MMKG) representation learning, which can
enhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The
main challenge is to collaboratively model the structural information concealed
in massive triples and the multi-modal features of the entities. Existing
methods focus on crafting elegant entity-wise multi-modal fusion strategies,
yet they overlook the utilization of multi-perspective features concealed
within the modalities under diverse relational contexts. To address this issue,
we introduce a novel framework with Mixture of Modality Knowledge experts
(MoMoK for short) to learn adaptive multi-modal entity representations for
better MMKGC. We design relation-guided modality knowledge experts to acquire
relation-aware modality embeddings and integrate the predictions from
multi-modalities to achieve joint decisions. Additionally, we disentangle the
experts by minimizing their mutual information. Experiments on four public MMKG
benchmarks demonstrate the outstanding performance of MoMoK under complex
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Code and data will be released at
  https://github.com/zjukg/MoMoK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CUTE: Measuring <span class="highlight-title">LLM</span>s' Understanding of Their Tokens <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Edman, Helmut Schmid, Alexander Fraser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show remarkable performance on a wide variety of
tasks. Most LLMs split text into multi-character tokens and process them as
atomic units without direct access to individual characters. This raises the
question: To what extent can LLMs learn orthographic information? To answer
this, we propose a new benchmark, CUTE, which features a collection of tasks
designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs
on CUTE, finding that most of them seem to know the spelling of their tokens,
yet fail to use this information effectively to manipulate text, calling into
question how much of this knowledge is generalizable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Compression in Retrieval-Augmented <span class="highlight-title">Generation</span> for Large
  Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase remarkable abilities, yet they struggle
with limitations such as hallucinations, outdated knowledge, opacity, and
inexplicable reasoning. To address these challenges, Retrieval-Augmented
Generation (RAG) has proven to be a viable solution, leveraging external
databases to improve the consistency and coherence of generated content,
especially valuable for complex, knowledge-rich tasks, and facilitates
continuous improvement by leveraging domain-specific insights. By combining the
intrinsic knowledge of LLMs with the vast, dynamic repositories of external
databases, RAG achieves a synergistic effect. However, RAG is not without its
limitations, including a limited context window, irrelevant information, and
the high processing overhead for extensive contextual data. In this
comprehensive work, we explore the evolution of Contextual Compression
paradigms, providing an in-depth examination of the field. Finally, we outline
the current challenges and suggest potential research and development
directions, paving the way for future advancements in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity or Relation Embeddings? An Analysis of Encoding Strategies for
  Relation Extraction <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Mtumbuka, Steven Schockaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essentially a text classification problem, which can
be tackled by fine-tuning a pre-trained language model (LM). However, a key
challenge arises from the fact that relation extraction cannot
straightforwardly be reduced to sequence or token classification. Existing
approaches therefore solve the problem in an indirect way: they fine-tune an LM
to learn embeddings of the head and tail entities, and then predict the
relationship from these entity embeddings. Our hypothesis in this paper is that
relation extraction models can be improved by capturing relationships in a more
direct way. In particular, we experiment with appending a prompt with a [MASK]
token, whose contextualised representation is treated as a relation embedding.
While, on its own, this strategy significantly underperforms the aforementioned
approach, we find that the resulting relation embeddings are highly
complementary to what is captured by embeddings of the head and tail entity. By
jointly considering both types of representations, we end up with a simple
model that outperforms the state-of-the-art across several relation extraction
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciEx: <span class="highlight-title">Benchmark</span>ing <span class="highlight-title">Large Language Model</span>s on Scientific Exams with Human
  Expert Grading and Automatic Grading <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tu Anh Dinh, Carlos Mullov, Leonard Bärmann, Zhaolin Li, Danni Liu, Simon Reiß, Jueun Lee, Nathan Lerzer, Fabian Ternava, Jianfeng Gao, Tobias Röddiger, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens Böhm, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Large Language Models (LLMs), it is crucial to
have benchmarks which can evaluate the ability of LLMs on different domains.
One common use of LLMs is performing tasks on scientific topics, such as
writing algorithms, querying databases or giving mathematical proofs. Inspired
by the way university students are evaluated on such tasks, in this paper, we
propose SciEx - a benchmark consisting of university computer science exam
questions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)
multilingual, containing both English and German exams, and (2) multi-modal,
containing questions that involve images, and (3) contains various types of
freeform questions with different difficulty levels, due to the nature of
university exams. We evaluate the performance of various state-of-the-art LLMs
on our new benchmark. Since SciEx questions are freeform, it is not
straightforward to evaluate LLM performance. Therefore, we provide human expert
grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx
remain challenging for the current LLMs, where the best LLM only achieves
59.4\% exam grade on average. We also provide detailed comparisons between LLM
performance and student performance on SciEx. To enable future evaluation of
new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.
Our experiments show that, although they do not perform perfectly on solving
the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with
expert grading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KnowTuning: Knowledge-aware Fine-tuning for <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yougang Lyu, Lingyong Yan, Shuaiqiang Wang, Haibo Shi, Dawei Yin, Pengjie Ren, Zhumin Chen, Maarten de Rijke, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their success at many natural language processing (NLP) tasks, large
language models still struggle to effectively leverage knowledge for
knowledge-intensive tasks, manifesting limitations such as generating
incomplete, non-factual, or illogical answers. These limitations stem from
inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address
these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to
improve fine-grained and coarse-grained knowledge awareness of LLMs. We devise
a fine-grained knowledge augmentation stage to train LLMs to identify difficult
fine-grained knowledge in answers. We also propose a coarse-grained knowledge
comparison stage to train LLMs to distinguish between reliable and unreliable
knowledge, in three aspects: completeness, factuality, and logicality.
Extensive experiments on both generic and medical question answering (QA)
datasets confirm the effectiveness of KnowTuning, through automatic and human
evaluations, across various sizes of LLMs. We further verify that KnowTuning
generates more facts with less factual error rate under fine-grained facts
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPal: Autonomous Adaptation to Users for Personal AI Companisonship 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Wenge Liu, Kaishuai Xu, Wenjun Hou, Yi Ouyang, Chak Tou Leong, Xian Wu, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has demonstrated the potential of AI agents to act as
companions that can provide constant emotional support for humans. In this
paper, we emphasize the necessity of autonomous adaptation in personal AI
companionship, an underexplored yet promising direction. Such adaptability is
crucial as it can facilitate more tailored interactions with users and allow
the agent to evolve in response to users' changing needs. However, imbuing
agents with autonomous adaptability presents unique challenges, including
identifying optimal adaptations to meet users' expectations and ensuring a
smooth transition during the adaptation process. To address them, we devise a
hierarchical framework, AutoPal, that enables controllable and authentic
adjustments to the agent's persona based on user interactions. A
personamatching dataset is constructed to facilitate the learning of optimal
persona adaptations. Extensive experiments demonstrate the effectiveness of
AutoPal and highlight the importance of autonomous adaptability in AI
companionship.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientQAT: Efficient Quantization-Aware Training for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are crucial in modern natural language
processing and artificial intelligence. However, they face challenges in
managing their significant memory requirements. Although quantization-aware
training (QAT) offers a solution by reducing memory consumption through low-bit
representations with minimal accuracy loss, it is impractical due to
substantial training resources. To address this, we propose Efficient
Quantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.
EfficientQAT involves two consecutive phases: Block-wise training of all
parameters (Block-AP) and end-to-end training of quantization parameters
(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable
direct training of all parameters in a block-wise manner, reducing accuracy
loss in low-bit scenarios by enhancing the solution space during optimization.
E2E-QP then trains only the quantization parameters (step sizes) end-to-end,
further improving the performance of quantized models by considering
interactions among all sub-modules. Extensive experiments demonstrate that
EfficientQAT outperforms previous quantization methods across a range of
models, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with
scales from 7B to 70B parameters at various quantization bits. For instance,
EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41
hours, with less than 3 points accuracy degradation compared to the full
precision (69.48 vs. 72.41). Code is available at
https://github.com/OpenGVLab/EfficientQAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An efficient and effective quantization technical to improve the
  performance of low-bits LMMs and LVLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Invisible: Captioning Videos with Metaphors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Sumit Shekhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphors are a common communication tool used in our day-to-day life. The
detection and generation of metaphors in textual form have been studied
extensively but metaphors in other forms have been under-explored. Recent
studies have shown that Vision-Language (VL) models cannot understand visual
metaphors in memes and adverts. As of now, no probing studies have been done
that involve complex language phenomena like metaphors with videos. Hence, we
introduce a new VL task of describing the metaphors present in the videos in
our work. To facilitate this novel task, we construct and release a manually
created dataset with 705 videos and 2115 human-written captions, along with a
new metric called Average Concept Distance (ACD), to automatically evaluate the
creativity of the metaphors generated. We also propose a novel low-resource
video metaphor captioning system: GIT-LLaVA, which obtains comparable
performance to SoTA video language models on the proposed task. We perform a
comprehensive analysis of existing video language models on this task and
publish our dataset, models, and benchmark results to enable further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Judging the Judges: A Systematic Investigation of Position Bias in
  Pairwise Comparative Assessments by <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07791v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07791v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, Soroush Vosoughi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge presents a promising alternative to human evaluators across
various tasks, but inherent biases, especially position bias - a tendency to
favor solutions based on their position in the prompt - have compromised its
effectiveness. Our study introduces a systematic framework to examine position
bias in pairwise comparisons, focusing on repetition stability, position
consistency, and preference fairness. This research significantly contributes
to the field by introducing new concepts for understanding position bias and
providing a multi-dimensional framework for evaluations. We conducted
experiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks
and approximately 40 solution-generating models - candidates, resulting in over
100,000 evaluation instances. Our findings confirm that position bias in
capable LLM judges is not due to random chances, along with notable variations
observed across judges and tasks. Moreover, position bias is weakly influenced
by the length of prompt components but significantly impacted by the quality
gap between solutions. These insights can help optimize judge model selections,
improve benchmark design, and inform future research on debiasing strategies,
ultimately enhancing the reliability of LLM judges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span> Confidence Estimation via Black-Box Access 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejaswini Pedapati, Amit Dhurandhar, Soumya Ghosh, Soham Dan, Prasanna Sattigeri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating uncertainty or confidence in the responses of a model can be
significant in evaluating trust not only in the responses, but also in the
model as a whole. In this paper, we explore the problem of estimating
confidence for responses of large language models (LLMs) with simply black-box
or query access to them. We propose a simple and extensible framework where, we
engineer novel features and train a (interpretable) model (viz. logistic
regression) on these features to estimate the confidence. We empirically
demonstrate that our simple framework is effective in estimating confidence of
Flan-ul2, Llama-13b and Mistral-7b on four benchmark Q\&A tasks as well as of
Pegasus-large and BART-large on two benchmark summarization tasks with it
surpassing baselines by even over $10\%$ (on AUROC) in some cases.
Additionally, our interpretable approach provides insight into features that
are predictive of confidence, leading to the interesting and useful discovery
that our confidence models built for one LLM generalize zero-shot across others
on a given dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2-Attention: Hardware-Aware Context Sharding Among Attention Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17678v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17678v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse attention, which selectively attends to a subset of tokens in the
context was supposed to be efficient. However, its theoretical reduction in
FLOPs has rarely translated into wall-clock speed-up over its dense attention
counterparts due to the lack of hardware-aware optimizations like
FlashAttention. Meanwhile, it remains unclear whether sparse attention can
maintain the model's quality at a scale of today's large language models (LLMs)
and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library
that provides kernel optimization for sparse attention customizable at both
per-head and per-context-range levels. S2-Attention enables the exploration of
novel and high-performance sparse attention techniques, which we demonstrate
through extensive ablations across a wide range of sparse attention designs at
various model scales. From these insights, we present several basic guidelines
to design sparse attention that can achieve not only practical efficiency
improvements, but also strong downstream performance. To achieve high
parallelization and optimized memory IO, sparse attention should shard the
context heterogeneously across attention heads, where each head attends to a
different subset of tokens while collectively covering the full context.
Meanwhile, we find hybrid architectures combining sparse and dense attention
particularly beneficial in practice. S2-Attention achieves wall-clock speedup
of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with
strong downstream performance on-par with full attention and perfect retrieval
performance at a 128k context length. At inference, for 7B models, our model,
with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to
dense counterparts. S2-Attention is released with easy-to-customize APIs for
direct usage in Megatron and vLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span> vs RETRO: Exploring the Intersection of Retrieval and
  Parameter-Efficient Fine-Tuning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04528v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04528v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation
(RAG) have become popular methods for adapting large language models while
minimizing compute requirements. In this paper, we apply PEFT methods
(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer
(RETRO) and a baseline GPT model across several sizes, ranging from 823 million
to 48 billion parameters. We show that RETRO models outperform GPT models in
zero-shot settings due to their unique pre-training process but GPT models have
higher performance potential with PEFT. Additionally, our study indicates that
8B parameter models strike an optimal balance between cost and performance and
P-tuning lags behind other PEFT techniques. We further provide a comparative
analysis between applying PEFT to an Instruction-tuned RETRO model and base
RETRO model. This work presents the first comprehensive comparison of various
PEFT methods integrated with RAG, applied to both GPT and RETRO models,
highlighting their relative performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Multilingual Concepts of Human Value in <span class="highlight-title">Large Language Model</span>s:
  Is Value Alignment Consistent, Transferable and Controllable across
  Languages? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18120v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18120v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has revealed that certain abstract concepts are linearly
represented as directions in the representation space of LLMs, predominantly
centered around English. In this paper, we extend this investigation to a
multilingual context, with a specific focus on human values-related concepts
(i.e., value concepts) due to their significance for AI safety. Through our
comprehensive exploration covering 7 types of human values, 16 languages and 3
LLM series with distinct multilinguality (e.g., monolingual, bilingual and
multilingual), we first empirically confirm the presence of value concepts
within LLMs in a multilingual format. Further analysis on the cross-lingual
characteristics of these concepts reveals 3 traits arising from language
resource disparities: cross-lingual inconsistency, distorted linguistic
relationships, and unidirectional cross-lingual transfer between high- and
low-resource languages, all in terms of value concepts. Moreover, we validate
the feasibility of cross-lingual control over value alignment capabilities of
LLMs, leveraging the dominant language as a source language. Ultimately,
recognizing the significant impact of LLMs' multilinguality on our results, we
consolidate our findings and provide prudent suggestions on the composition of
multilingual data for LLMs pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 findings, code&dataset:
  https://github.com/shaoyangxu/Multilingual-Human-Value-Concepts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">Large Language Model</span>s Using Contrast Sets: An Experimental
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Sanwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of Natural Language Inference (NLI), especially in tasks
involving the classification of multiple input texts, the Cross-Entropy Loss
metric is widely employed as a standard for error measurement. However, this
metric falls short in effectively evaluating a model's capacity to understand
language entailments. In this study, we introduce an innovative technique for
generating a contrast set for the Stanford Natural Language Inference (SNLI)
dataset. Our strategy involves the automated substitution of verbs, adverbs,
and adjectives with their synonyms to preserve the original meaning of
sentences. This method aims to assess whether a model's performance is based on
genuine language comprehension or simply on pattern recognition. We conducted
our analysis using the ELECTRA-small model. The model achieved an accuracy of
89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%
on our contrast set, indicating a substantial 17% decline. This outcome led us
to conduct a detailed examination of the model's learning behaviors. Following
this, we improved the model's resilience by fine-tuning it with a
contrast-enhanced training dataset specifically designed for SNLI, which
increased its accuracy to 85.5% on the contrast sets. Our findings highlight
the importance of incorporating diverse linguistic expressions into datasets
for NLI tasks. We hope that our research will encourage the creation of more
inclusive datasets, thereby contributing to the development of NLI models that
are both more sophisticated and effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What's Mine becomes Yours: Defining, Annotating and Detecting
  Context-Dependent Paraphrases in News Interview Dialogs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Wegmann, Tijs van den Broek, Dong Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best practices for high conflict conversations like counseling or customer
support almost always include recommendations to paraphrase the previous
speaker. Although paraphrase classification has received widespread attention
in NLP, paraphrases are usually considered independent from context, and common
models and datasets are not applicable to dialog settings. In this work, we
investigate paraphrases in dialog (e.g., Speaker 1: "That book is mine."
becomes Speaker 2: "That book is yours."). We provide an operationalization of
context-dependent paraphrases, and develop a training for crowd-workers to
classify paraphrases in dialog. We introduce a dataset with utterance pairs
from NPR and CNN news interviews annotated for context-dependent paraphrases.
To enable analyses on label variation, the dataset contains 5,581 annotations
on 600 utterance pairs. We present promising results with in-context learning
and with token classification models for automatic paraphrase detection in
dialog.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as main conference paper to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical
  <span class="highlight-title">Reasoning</span> in <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03887v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03887v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonwoo Kim, Gyoungjin Gim, Yungi Kim, Jihoo Kim, Byungju Kim, Wonseok Lee, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel learning approach designed to enhance both
mathematical reasoning and problem-solving abilities of Large Language Models
(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the
Program-of-Thought (PoT) learning, hypothesizing that prioritizing the learning
of mathematical reasoning ability is helpful for the amplification of
problem-solving ability. Thus, the initial learning with CoT is essential for
solving challenging mathematical problems. To this end, we propose a sequential
learning approach, named SAAS (Solving Ability Amplification Strategy), which
strategically transitions from CoT learning to PoT learning. Our empirical
study, involving an extensive performance comparison using several benchmarks,
demonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The
results underscore the effectiveness of our sequential learning approach,
marking a significant advancement in the field of mathematical reasoning in
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Dual-Phase Accelerated Prompt Optimization <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muchen Yang, Moxin Li, Yongle Li, Zijun Chen, Chongming Gao, Jun<span class="highlight-author">qi Zhang</span>, Yangyang Li, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient-free prompt optimization methods have made significant strides in
enhancing the performance of closed-source Large Language Models (LLMs) across
a wide range of tasks. However, existing approaches make light of the
importance of high-quality prompt initialization and the identification of
effective optimization directions, thus resulting in substantial optimization
steps to obtain satisfactory performance. In this light, we aim to accelerate
prompt optimization process to tackle the challenge of low convergence rate. We
propose a dual-phase approach which starts with generating high-quality initial
prompts by adopting a well-designed meta-instruction to delve into
task-specific information, and iteratively optimize the prompts at the sentence
level, leveraging previous tuning experience to expand prompt candidates and
accept effective ones. Extensive experiments on eight datasets demonstrate the
effectiveness of our proposed method, achieving a consistent accuracy gain over
baselines with less than five optimization steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Urdu Dependency Parsing and Treebank Development: A Syntactic and
  Morphological Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nudrat Habib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parsing is the process of analyzing a sentence's syntactic structure by
breaking it down into its grammatical components. and is critical for various
linguistic applications. Urdu is a low-resource, free word-order language and
exhibits complex morphology. Literature suggests that dependency parsing is
well-suited for such languages. Our approach begins with a basic feature model
encompassing word location, head word identification, and dependency relations,
followed by a more advanced model integrating part-of-speech (POS) tags and
morphological attributes (e.g., suffixes, gender). We manually annotated a
corpus of news articles of varying complexity. Using Maltparser and the
NivreEager algorithm, we achieved a best-labeled accuracy (LA) of 70% and an
unlabeled attachment score (UAS) of 84%, demonstrating the feasibility of
dependency parsing for Urdu.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Sexism in German Online Newspaper Comments with Open-Source
  Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks
  1 and 2, Closed Track) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Bremm, Patrick Gustav Blaneck, Tobias Bornheim, Niklas Grieger, Stephan Bialonski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sexism in online media comments is a pervasive challenge that often manifests
subtly, complicating moderation efforts as interpretations of what constitutes
sexism can vary among individuals. We study monolingual and multilingual
open-source text embeddings to reliably detect sexism and misogyny in
German-language online comments from an Austrian newspaper. We observed
classifiers trained on text embeddings to mimic closely the individual
judgements of human annotators. Our method showed robust performance in the
GermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1
score of 0.597 (4th place, as reported on Codabench). It also accurately
predicted the distribution of human annotations in GerMS-Detect Subtask 2, with
an average Jensen-Shannon distance of 0.301 (2nd place). The computational
efficiency of our approach suggests potential for scalable applications across
various languages and linguistic contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11085v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11085v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilmazcan Ozyurt, Stefan Feuerriegel, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level relation extraction aims at inferring structured human
knowledge from textual documents. State-of-the-art methods for this task use
pre-trained language models (LMs) via fine-tuning, yet fine-tuning is
computationally expensive and cannot adapt to new relation types or new LMs. As
a remedy, we leverage the generalization capabilities of pre-trained LMs and
present a novel framework for document-level in-context few-shot relation
extraction. Our framework has three strengths: it eliminates the need (1) for
named entity recognition and (2) for human annotations of documents, and (3) it
can be updated to new LMs without re-training. We evaluate our framework using
DocRED, the largest publicly available dataset for document-level relation
extraction, and demonstrate that our framework achieves state-of-the-art
performance. We further show that our framework actually performs much better
than the original labels from the development set of DocRED. Finally, we
conduct an extensive benchmark demonstrating the effectiveness of our
framework, achieving state-of-the-art results across six relation extraction
datasets and outperforming more than 30 baseline methods. Unlike our framework,
the baseline methods have large computational overhead (e.g., from
fine-tuning). To the best of our knowledge, we are the first to reformulate the
document-level relation extraction task as a tailored in-context few-shot
learning paradigm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongGenBench: <span class="highlight-title">Benchmark</span>ing Long-Form <span class="highlight-title">Generation</span> in Long Context <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02076v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02076v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wu, Ming Shan Hee, Zhiqing Hu, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In evaluating the long-context capabilities of large language models (LLMs),
benchmarks such as "Needle-in-a-Haystack" (NIAH), Ruler, and Needlebench are
commonly used. While these benchmarks measure how well models understand
long-context input sequences, they do not effectively gauge the quality of
long-form text generation--a critical aspect for applications such as design
proposals and creative writing. To address this gap, we have introduced a new
long-form text evaluation benchmark, LongGenBench, which tests models' ability
to identify specific events within generated long text sequences. In this
benchmark, we prompt long-context LMs to create long-form text that must
include particular events or constraints and evaluate their ability to
incorporate these elements. We evaluated ten long-context LMs across four
distinct scenarios, three types of prompt instructions, and two different
generation-length settings (16K and 32K). Although these models perform well on
NIAH benchmarks, none demonstrated satisfactory performance on the
LongGenBench, raising concerns about their ability to generate coherent
long-form text that follows instructions. Additionally, as the length of the
generated text increases, all models exhibit a significant drop in performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress; Github: https://github.com/mozhu621/LongGenBench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-based Preference Optimization in Abstractive Summarization without
  Human Feedback <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18618v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18618v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaepill Choi, Kyubyung Chae, Jiwoo Song, Yohan Jo, Taesup Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In abstractive summarization, the challenge of producing concise and accurate
summaries arises from the vast amount of information contained in the source
document. Consequently, although Large Language Models (LLMs) can generate
fluent text, they often introduce inaccuracies by hallucinating content not
found in the original source. While supervised fine-tuning methods that
maximize likelihood contribute to this issue, they do not consistently enhance
the faithfulness of the summaries. Preference-based optimization methods, such
as Direct Preference Optimization (DPO), can further refine the model to align
with human preferences. However, these methods still heavily depend on costly
human feedback. In this work, we introduce a novel and straightforward approach
called Model-based Preference Optimization (MPO) to fine-tune LLMs for improved
summarization abilities without any human feedback. By leveraging the model's
inherent summarization capabilities, we create a preference dataset that is
fully generated by the model using different decoding strategies. Our
experiments on standard summarization datasets and various metrics demonstrate
that our proposed MPO significantly enhances the quality of generated summaries
without relying on human feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-as-a-Judge & Reward Model: What They Can and Cannot Do 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge and reward models are widely used alternatives of
multiple-choice questions or human annotators for large language model (LLM)
evaluation. Their efficacy shines in evaluating long-form responses, serving a
critical role as evaluators of leaderboards and as proxies to align LLMs via
reinforcement learning. However, despite their popularity, their effectiveness
in diverse contexts, such as non-English prompts, factual verification, or
challenging questions, remains unexplored. In this paper, we conduct a
comprehensive analysis of automated evaluators, reporting several key findings
on their behavior. First, we discover that English evaluation capabilities
significantly influence language-specific evaluation capabilities, often more
than the language proficiency itself, enabling evaluators trained in English to
easily transfer their skills to other languages. Second, we identify critical
shortcomings, where LLMs fail to detect and penalize errors, such as factual
inaccuracies, cultural misrepresentations, and the presence of unwanted
language. Finally, we find that state-of-the-art evaluators struggle with
challenging prompts, in either English or Korean, underscoring their
limitations in assessing or generating complex reasoning questions. We release
the dataset and codes used.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Samba: Synchronized Set-of-Sequences Modeling for Multiple Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Segu, Luigi Piccinelli, Siyuan Li, Yung-Hsu Yang, Bernt Schiele, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple object tracking in complex scenarios - such as coordinated dance
performances, team sports, or dynamic animal groups - presents unique
challenges. In these settings, objects frequently move in coordinated patterns,
occlude each other, and exhibit long-term dependencies in their trajectories.
However, it remains a key open research question on how to model long-range
dependencies within tracklets, interdependencies among tracklets, and the
associated temporal occlusions. To this end, we introduce Samba, a novel
linear-time set-of-sequences model designed to jointly process multiple
tracklets by synchronizing the multiple selective state-spaces used to model
each tracklet. Samba autoregressively predicts the future track query for each
sequence while maintaining synchronized long-term memory representations across
tracklets. By integrating Samba into a tracking-by-propagation framework, we
propose SambaMOTR, the first tracker effectively addressing the aforementioned
issues, including long-range dependencies, tracklet interdependencies, and
temporal occlusions. Additionally, we introduce an effective technique for
dealing with uncertain observations (MaskObs) and an efficient training recipe
to scale SambaMOTR to longer sequences. By modeling long-range dependencies and
interactions among tracked objects, SambaMOTR implicitly learns to track
objects accurately through occlusions without any hand-crafted heuristics. Our
approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT,
and SportsMOT datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jon Barron, Yinda Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for
real-time differentiable emission-only volume rendering. Unlike recent
rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive
based representation allows for exact volume rendering, rather than alpha
compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does
not suffer from popping artifacts and view dependent density, but still
achieves frame rates of $\sim\!30$ FPS at 720p on an NVIDIA RTX4090. Since our
approach is built upon ray tracing it enables effects such as defocus blur and
camera distortion (e.g. such as from fisheye cameras), which are difficult to
achieve by rasterization. We show that our method is more accurate with fewer
blending issues than 3DGS and follow-up work on view-consistent rendering,
especially on the challenging large-scale scenes from the Zip-NeRF dataset
where it achieves sharpest results among real-time techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://half-potato.gitlab.io/posts/ever</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fabric<span class="highlight-title">Diffusion</span>: High-Fidelity Texture Transfer for 3D Garments
  <span class="highlight-title">Generation</span> from In-The-Wild Clothing <span class="highlight-title">Image</span>s <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Zhang, Yuanhao Wang, Francisco Vicente Carrasco, Chenglei Wu, Jinlong Yang, Thabo Beeler, Fernando De la Torre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FabricDiffusion, a method for transferring fabric textures from
a single clothing image to 3D garments of arbitrary shapes. Existing approaches
typically synthesize textures on the garment surface through 2D-to-3D texture
mapping or depth-aware inpainting via generative models. Unfortunately, these
methods often struggle to capture and preserve texture details, particularly
due to challenging occlusions, distortions, or poses in the input image.
Inspired by the observation that in the fashion industry, most garments are
constructed by stitching sewing patterns with flat, repeatable textures, we
cast the task of clothing texture transfer as extracting distortion-free,
tileable texture materials that are subsequently mapped onto the UV space of
the garment. Building upon this insight, we train a denoising diffusion model
with a large-scale synthetic dataset to rectify distortions in the input
texture image. This process yields a flat texture map that enables a tight
coupling with existing Physically-Based Rendering (PBR) material generation
pipelines, allowing for realistic relighting of the garment under various
lighting conditions. We show that FabricDiffusion can transfer various features
from a single clothing image including texture patterns, material properties,
and detailed prints and logos. Extensive experiments demonstrate that our model
significantly outperforms state-to-the-art methods on both synthetic data and
real-world, in-the-wild clothing images while generalizing to unseen textures
and garment shapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH Asia 2024. Project page:
  https://humansensinglab.github.io/fabric-diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for
  Remote Sensing <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Li, Ruixun Liu, Xiangyong Cao, Deyu Meng, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing image plays an irreplaceable role in fields such as
agriculture, water resources, military, and disaster relief. Pixel-level
interpretation is a critical aspect of remote sensing image applications;
however, a prevalent limitation remains the need for extensive manual
annotation. For this, we try to introduce open-vocabulary semantic segmentation
(OVSS) into the remote sensing context. However, due to the sensitivity of
remote sensing images to low-resolution features, distorted target shapes and
ill-fitting boundaries are exhibited in the prediction mask. To tackle this
issue, we propose a simple and general upsampler, SimFeatUp, to restore lost
spatial information in deep features in a training-free style. Further, based
on the observation of the abnormal response of local patch tokens to [CLS]
token in CLIP, we propose to execute a straightforward subtraction operation to
alleviate the global bias in patch tokens. Extensive experiments are conducted
on 17 remote sensing datasets spanning semantic segmentation, building
extraction, road detection, and flood detection tasks. Our method achieves an
average of 5.8%, 8.2%, 4%, and 15.3% improvement over state-of-the-art methods
on 4 tasks. All codes are released.
\url{https://earth-insights.github.io/SegEarth-OV}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegHeD: Segmentation of Heterogeneous Data for Multiple Sclerosis
  Lesions with Anatomical Constraints <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berke Doga Basaran, Xinru Zhang, Paul M. Matthews, Wenjia Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessment of lesions and their longitudinal progression from brain magnetic
resonance (MR) images plays a crucial role in diagnosing and monitoring
multiple sclerosis (MS). Machine learning models have demonstrated a great
potential for automated MS lesion segmentation. Training such models typically
requires large-scale high-quality datasets that are consistently annotated.
However, MS imaging datasets are often small, segregated across multiple sites,
with different formats (cross-sectional or longitudinal), and diverse
annotation styles. This poses a significant challenge to train a unified MS
lesion segmentation model. To tackle this challenge, we present SegHeD, a novel
multi-dataset multi-task segmentation model that can incorporate heterogeneous
data as input and perform all-lesion, new-lesion, as well as vanishing-lesion
segmentation. Furthermore, we account for domain knowledge about MS lesions,
incorporating longitudinal, spatial, and volumetric constraints into the
segmentation model. SegHeD is assessed on five MS datasets and achieves a high
performance in all, new, and vanishing-lesion segmentation, outperforming
several state-of-the-art methods in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, MICCAI, LDTM Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Image</span>Folder: Autoregressive <span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> with Folded Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, Zhe Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image tokenizers are crucial for visual generative models, e.g., diffusion
models (DMs) and autoregressive (AR) models, as they construct the latent
representation for modeling. Increasing token length is a common approach to
improve the image reconstruction quality. However, tokenizers with longer token
lengths are not guaranteed to achieve better generation quality. There exists a
trade-off between reconstruction and generation quality regarding token length.
In this paper, we investigate the impact of token length on both image
reconstruction and generation and provide a flexible solution to the tradeoff.
We propose ImageFolder, a semantic tokenizer that provides spatially aligned
image tokens that can be folded during autoregressive modeling to improve both
generation efficiency and quality. To enhance the representative capability
without increasing token length, we leverage dual-branch product quantization
to capture different contexts of images. Specifically, semantic regularization
is introduced in one branch to encourage compacted semantic information while
another branch is designed to capture the remaining pixel-level details.
Extensive experiments demonstrate the superior quality of image generation and
shorter token length with ImageFolder tokenizer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/lxa9867/ImageFolder</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEOPARD : A Vision Language Model For Text-Rich Multi-<span class="highlight-title">Image</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Meng Jiang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich images, where text serves as the central visual element guiding the
overall understanding, are prevalent in real-world applications, such as
presentation slides, scanned documents, and webpage snapshots. Tasks involving
multiple text-rich images are especially challenging, as they require not only
understanding the content of individual images but reasoning about
inter-relationships and logical flows across multiple visual inputs. Despite
the importance of these scenarios, current multimodal large language models
(MLLMs) struggle to handle such tasks due to two key challenges: (1) the
scarcity of high-quality instruction tuning datasets for text-rich multi-image
scenarios, and (2) the difficulty in balancing image resolution with visual
feature sequence length. To address these challenges, we propose \OurMethod, a
MLLM designed specifically for handling vision-language tasks involving
multiple text-rich images. First, we curated about one million high-quality
multimodal instruction-tuning data, tailored to text-rich, multi-image
scenarios. Second, we developed an adaptive high-resolution multi-image
encoding module to dynamically optimize the allocation of visual sequence
length based on the original aspect ratios and resolutions of the input images.
Experiments across a wide range of benchmarks demonstrate our model's superior
capabilities in text-rich, multi-image evaluations and competitive performance
in general domain evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/Jill0001/Leopard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailai Feng, Yabo Zhang, Haodong Yu, Zhilong Ji, Jinfeng Bai, Hongzhi Zhang, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artistic typography is a technique to visualize the meaning of input
character in an imaginable and readable manner. With powerful text-to-image
diffusion models, existing methods directly design the overall geometry and
texture of input character, making it challenging to ensure both creativity and
legibility. In this paper, we introduce a dual-branch and training-free method,
namely VitaGlyph, enabling flexible artistic typography along with controllable
geometry change to maintain the readability. The key insight of VitaGlyph is to
treat input character as a scene composed of Subject and Surrounding, followed
by rendering them under varying degrees of geometry transformation. The subject
flexibly expresses the essential concept of input character, while the
surrounding enriches relevant background without altering the shape.
Specifically, we implement VitaGlyph through a three-phase framework: (i)
Knowledge Acquisition leverages large language models to design text
descriptions of subject and surrounding. (ii) Regional decomposition detects
the part that most matches the subject description and divides input glyph
image into subject and surrounding regions. (iii) Typography Stylization
firstly refines the structure of subject region via Semantic Typography, and
then separately renders the textures of Subject and Surrounding regions through
Controllable Compositional Generation. Experimental results demonstrate that
VitaGlyph not only achieves better artistry and readability, but also manages
to depict multiple customize concepts, facilitating more creative and pleasing
artistic typography generation. Our code will be made publicly at
https://github.com/Carlofkl/VitaGlyph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Carlofkl/VitaGlyph</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RADAR: Robust Two-stage Modality-incomplete Industrial Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Miao, Wenqiao Zhang, Juncheng Li, Siliang Tang, Zhaocheng Li, Haochen Shi, Jun Xiao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Industrial Anomaly Detection (MIAD), utilizing 3D point clouds and
2D RGB images to identify the abnormal region of products, plays a crucial role
in industrial quality inspection. However, the conventional MIAD setting
presupposes that all 2D and 3D modalities are paired, overlooking the fact that
multimodal data collected from the real world is often imperfect due to missing
modalities. Consequently, MIAD models that demonstrate robustness against
modal-incomplete data are highly desirable in practice. To address this
practical challenge, we introduce a first-of-its-kind study that
comprehensively investigates Modality-Incomplete Industrial Anomaly Detection
(MIIAD), to consider the imperfect learning environment in which the multimodal
information may be incomplete. Not surprisingly, we discovered that most
existing MIAD approaches are inadequate for addressing MIIAD challenges,
leading to significant performance degradation on the MIIAD benchmark we
developed. In this paper, we propose a novel two-stage Robust
modAlity-imcomplete fusing and Detecting frAmewoRk, abbreviated as RADAR. Our
bootstrapping philosophy is to enhance two stages in MIIAD, improving the
robustness of the Multimodal Transformer: i) In feature fusion, we first
explore learning modality-incomplete instruction, guiding the pre-trained
Multimodal Transformer to robustly adapt to various modality-incomplete
scenarios, and implement adaptive parameter learning based on a HyperNetwork;
ii) In anomaly detection, we construct a real-pseudo hybrid module to highlight
the distinctiveness of modality combinations, further enhancing the robustness
of the MIIAD model. Our experimental results demonstrate that the proposed
RADAR significantly surpasses conventional MIAD methods in terms of
effectiveness and robustness on our newly created MIIAD dataset, underscoring
its practical application value.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComfyGen: Prompt-Adaptive Workflows for Text-to-<span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rinon Gal, Adi Haviv, Yuval Alaluf, Amit H. Bermano, Daniel Cohen-Or, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The practical use of text-to-image generation has evolved from simple,
monolithic models to complex workflows that combine multiple specialized
components. While workflow-based approaches can lead to improved image quality,
crafting effective workflows requires significant expertise, owing to the large
number of available components, their complex inter-dependence, and their
dependence on the generation prompt. Here, we introduce the novel task of
prompt-adaptive workflow generation, where the goal is to automatically tailor
a workflow to each user prompt. We propose two LLM-based approaches to tackle
this task: a tuning-based method that learns from user-preference data, and a
training-free method that uses the LLM to select existing flows. Both
approaches lead to improved image quality when compared to monolithic models or
generic, prompt-independent workflows. Our work shows that prompt-dependent
flow prediction offers a new pathway to improving text-to-image generation
quality, complementing existing research directions in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://comfygen-paper.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HarmoniCa: Harmonizing Training and Inference for Better Feature Cache
  in <span class="highlight-title">Diffusion</span> Transformer Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiTs) have gained prominence for outstanding
scalability and extraordinary performance in generative tasks. However, their
considerable inference costs impede practical deployment. The feature cache
mechanism, which involves storing and retrieving redundant computations across
timesteps, holds promise for reducing per-step inference time in diffusion
models. Most existing caching methods for DiT are manually designed. Although
the learning-based approach attempts to optimize strategies adaptively, it
suffers from discrepancies between training and inference, which hampers both
the performance and acceleration ratio. Upon detailed analysis, we pinpoint
that these discrepancies primarily stem from two aspects: (1) Prior Timestep
Disregard, where training ignores the effect of cache usage at earlier
timesteps, and (2) Objective Mismatch, where the training target (align
predicted noise in each timestep) deviates from the goal of inference (generate
the high-quality image). To alleviate these discrepancies, we propose
HarmoniCa, a novel method that Harmonizes training and inference with a novel
learning-based Caching framework built upon Step-Wise Denoising Training (SDT)
and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional
training paradigm, the newly proposed SDT maintains the continuity of the
denoising process, enabling the model to leverage information from prior
timesteps during training, similar to the way it operates during inference.
Furthermore, we design IEPO, which integrates an efficient proxy mechanism to
approximate the final image error caused by reusing the cached feature.
Therefore, IEPO helps balance final image quality and cache utilization,
resolving the issue of training that only considers the impact of cache usage
on the predicted output at each timestep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniSR: Shadow Removal under Direct and Indirect Lighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiamin Xu, Zelong Li, Yuxin Zheng, Chenyu Huang, Renshu Gu, Weiwei Xu, Gang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shadows can originate from occlusions in both direct and indirect
illumination. Although most current shadow removal research focuses on shadows
caused by direct illumination, shadows from indirect illumination are often
just as pervasive, particularly in indoor scenes. A significant challenge in
removing shadows from indirect illumination is obtaining shadow-free images to
train the shadow removal network. To overcome this challenge, we propose a
novel rendering pipeline for generating shadowed and shadow-free images under
direct and indirect illumination, and create a comprehensive synthetic dataset
that contains over 30,000 image pairs, covering various object types and
lighting conditions. We also propose an innovative shadow removal network that
explicitly integrates semantic and geometric priors through concatenation and
attention mechanisms. The experiments show that our method outperforms
state-of-the-art shadow removal techniques and can effectively generalize to
indoor and outdoor scenes under various lighting conditions, enhancing the
overall effectiveness and applicability of shadow removal methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMUNI: Decomposing Common and Unique Video Signals for <span class="highlight-title">Diffusion</span>-based
  Video <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingzhen Sun, Weining Wang, Xinxin Zhu, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since videos record objects moving coherently, adjacent video frames have
commonness (similar object appearances) and uniqueness (slightly changed
postures). To prevent redundant modeling of common video signals, we propose a
novel diffusion-based framework, named COMUNI, which decomposes the COMmon and
UNIque video signals to enable efficient video generation. Our approach
separates the decomposition of video signals from the task of video generation,
thus reducing the computation complexity of generative models. In particular,
we introduce CU-VAE to decompose video signals and encode them into latent
features. To train CU-VAE in a self-supervised manner, we employ a cascading
merge module to reconstitute video signals and a time-agnostic video decoder to
reconstruct video frames. Then we propose CU-LDM to model latent features for
video generation, which adopts two specific diffusion streams to simultaneously
model the common and unique latent features. We further utilize additional
joint modules for cross modeling of the common and unique latent features, and
a novel position embedding method to ensure the content consistency and motion
coherence of generated videos. The position embedding method incorporates
spatial and temporal absolute position information into the joint modules.
Extensive experiments demonstrate the necessity of decomposing common and
unique video signals for video generation and the effectiveness and efficiency
of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Auto-regressive Text-to-<span class="highlight-title">Image</span> <span class="highlight-title">Generation</span> with Training-free
  Speculative Jacobi Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current large auto-regressive models can generate high-quality,
high-resolution images, but these models require hundreds or even thousands of
steps of next-token prediction during inference, resulting in substantial time
consumption. In existing studies, Jacobi decoding, an iterative parallel
decoding algorithm, has been used to accelerate the auto-regressive generation
and can be executed without training. However, the Jacobi decoding relies on a
deterministic criterion to determine the convergence of iterations. Thus, it
works for greedy decoding but is incompatible with sampling-based decoding
which is crucial for visual quality and diversity in the current
auto-regressive text-to-image generation. In this paper, we propose a
training-free probabilistic parallel decoding algorithm, Speculative Jacobi
Decoding (SJD), to accelerate auto-regressive text-to-image generation. By
introducing a probabilistic convergence criterion, our SJD accelerates the
inference of auto-regressive text-to-image generation while maintaining the
randomness in sampling-based token decoding and allowing the model to generate
diverse images. Specifically, SJD facilitates the model to predict multiple
tokens at each step and accepts tokens based on the probabilistic criterion,
enabling the model to generate images with fewer steps than the conventional
next-token-prediction paradigm. We also investigate the token initialization
strategies that leverage the spatial locality of visual data to further improve
the acceleration ratio under specific scenarios. We conduct experiments for our
proposed SJD on multiple auto-regressive text-to-image generation models,
showing the effectiveness of model acceleration without sacrificing the visual
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COSMIC: Compress Satellite <span class="highlight-title">Image</span>s Efficiently via <span class="highlight-title">Diffusion</span> Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Zhang, Han Qiu, Maosen Zhang, Jun Liu, Bin Chen, Tianwei Zhang, Hewu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapidly increasing number of satellites in space and their enhanced
capabilities, the amount of earth observation images collected by satellites is
exceeding the transmission limits of satellite-to-ground links. Although
existing learned image compression solutions achieve remarkable performance by
using a sophisticated encoder to extract fruitful features as compression and
using a decoder to reconstruct, it is still hard to directly deploy those
complex encoders on current satellites' embedded GPUs with limited computing
capability and power supply to compress images in orbit. In this paper, we
propose COSMIC, a simple yet effective learned compression solution to transmit
satellite images. We first design a lightweight encoder (i.e. reducing FLOPs by
$2.6\sim 5\times $) on satellite to achieve a high image compression ratio to
save satellite-to-ground links. Then, for reconstructions on the ground, to
deal with the feature extraction ability degradation due to simplifying
encoders, we propose a diffusion-based model to compensate image details when
decoding. Our insight is that satellite's earth observation photos are not just
images but indeed multi-modal data with a nature of Text-to-Image pairing since
they are collected with rich sensor data (e.g. coordinates, timestamp, etc.)
that can be used as the condition for diffusion generation. Extensive
experiments show that COSMIC outperforms state-of-the-art baselines on both
perceptual and distortion metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOREL: Enhancing Adversarial Robustness through Multi-Objective
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedjro Salomon Hotegni, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive research has shown that deep neural networks (DNNs) are vulnerable
to slight adversarial perturbations$-$small changes to the input data that
appear insignificant but cause the model to produce drastically different
outputs. In addition to augmenting training data with adversarial examples
generated from a specific attack method, most of the current defense strategies
necessitate modifying the original model architecture components to improve
robustness or performing test-time data purification to handle adversarial
attacks. In this work, we demonstrate that strong feature representation
learning during training can significantly enhance the original model's
robustness. We propose MOREL, a multi-objective feature representation learning
approach, encouraging classification models to produce similar features for
inputs within the same class, despite perturbations. Our training method
involves an embedding space where cosine similarity loss and multi-positive
contrastive loss are used to align natural and adversarial features from the
model encoder and ensure tight clustering. Concurrently, the classifier is
motivated to achieve accurate predictions. Through extensive experiments, we
demonstrate that our approach significantly enhances the robustness of DNNs
against white-box and black-box adversarial attacks, outperforming other
methods that similarly require no architectural changes or test-time data
purification. Our code is available at https://github.com/salomonhotegni/MOREL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mike Ranzinger, Jon Barker, Greg Heinrich, Pavlo Molchanov, Bryan Catanzaro, Andrew Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various visual foundation models have distinct strengths and weaknesses, both
of which can be improved through heterogeneous multi-teacher knowledge
distillation without labels, termed "agglomerative models." We build upon this
body of work by studying the effect of the teachers' activation statistics,
particularly the impact of the loss function on the resulting student model
quality. We explore a standard toolkit of statistical normalization techniques
to better align the different distributions and assess their effects. Further,
we examine the impact on downstream teacher-matching metrics, which motivates
the use of Hadamard matrices. With these matrices, we demonstrate useful
properties, showing how they can be used for isotropic standardization, where
each dimension of a multivariate distribution is standardized using the same
scale. We call this technique "PHI Standardization" (PHI-S) and empirically
demonstrate that it produces the best student model across the suite of methods
studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open3DTrack: Towards Open-Vocabulary 3D Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayesha Ishaq, Mohamed El Amine Boudjoghra, Jean Lahoud, Fahad Shahbaz Khan, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D multi-object tracking plays a critical role in autonomous driving by
enabling the real-time monitoring and prediction of multiple objects'
movements. Traditional 3D tracking systems are typically constrained by
predefined object categories, limiting their adaptability to novel, unseen
objects in dynamic environments. To address this limitation, we introduce
open-vocabulary 3D tracking, which extends the scope of 3D tracking to include
objects beyond predefined categories. We formulate the problem of
open-vocabulary 3D tracking and introduce dataset splits designed to represent
various open-vocabulary scenarios. We propose a novel approach that integrates
open-vocabulary capabilities into a 3D tracking framework, allowing for
generalization to unseen object classes. Our method effectively reduces the
performance gap between tracking known and novel objects through strategic
adaptation. Experimental results demonstrate the robustness and adaptability of
our method in diverse outdoor driving scenarios. To the best of our knowledge,
this work is the first to address open-vocabulary 3D tracking, presenting a
significant advancement for autonomous systems in real-world settings. Code,
trained models, and dataset splits are available publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a vision foundation model for comprehensive assessment of
  Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athira J Jacob, Indraneel Borgohain, Teodora Chitiboi, Puneet Sharma, Dorin Comaniciu, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac magnetic resonance imaging (CMR), considered the gold standard for
noninvasive cardiac assessment, is a diverse and complex modality requiring a
wide variety of image processing tasks for comprehensive assessment of cardiac
morphology and function. Advances in deep learning have enabled the development
of state-of-the-art (SoTA) models for these tasks. However, model training is
challenging due to data and label scarcity, especially in the less common
imaging sequences. Moreover, each model is often trained for a specific task,
with no connection between related tasks. In this work, we introduce a vision
foundation model trained for CMR assessment, that is trained in a
self-supervised fashion on 36 million CMR images. We then finetune the model in
supervised way for 9 clinical tasks typical to a CMR workflow, across
classification, segmentation, landmark localization, and pathology detection.
We demonstrate improved accuracy and robustness across all tasks, over a range
of available labeled dataset sizes. We also demonstrate improved few-shot
learning with fewer labeled samples, a common challenge in medical image
analyses. We achieve an out-of-box performance comparable to SoTA for most
clinical tasks. The proposed method thus presents a resource-efficient, unified
framework for CMR assessment, with the potential to accelerate the development
of deep learning-based solutions for image analysis tasks, even with few
annotated data available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Parameter Potential of Neural Representation for Efficient
  Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gai Zhang, Xinfeng Zhang, Lv Tang, Yue Li, Kai Zhang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, video compression technology has been a prominent research area.
Traditional hybrid video compression framework and end-to-end frameworks
continue to explore various intra- and inter-frame reference and prediction
strategies based on discrete transforms and deep learning techniques. However,
the emerging implicit neural representation (INR) technique models entire
videos as basic units, automatically capturing intra-frame and inter-frame
correlations and obtaining promising performance. INR uses a compact neural
network to store video information in network parameters, effectively
eliminating spatial and temporal redundancy in the original video. However, in
this paper, our exploration and verification reveal that current INR video
compression methods do not fully exploit their potential to preserve
information. We investigate the potential of enhancing network parameter
storage through parameter reuse. By deepening the network, we designed a
feasible INR parameter reuse scheme to further improve compression performance.
Extensive experimental results show that our method significantly enhances the
rate-distortion performance of INR video compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and
  Box-Focused Sampling for 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cao, Yuanliang Jv, Dan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and
have been adapted for 3D Object Detection (3DOD), offering a promising approach
to 3DOD through view-synthesis representation. However, NeRF faces inherent
limitations: (i) limited representational capacity for 3DOD due to its implicit
nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS)
has emerged as an explicit 3D representation that addresses these limitations.
Inspired by these advantages, this paper introduces 3DGS into 3DOD for the
first time, identifying two main challenges: (i) Ambiguous spatial distribution
of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision,
resulting in unclear 3D spatial distribution of Gaussian blobs and poor
differentiation between objects and background, which hinders 3DOD; (ii)
Excessive background blobs: 2D images often include numerous background pixels,
leading to densely reconstructed 3DGS with many noisy Gaussian blobs
representing the background, negatively affecting detection. To tackle the
challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D
images, and propose an elegant and efficient solution by incorporating 2D
Boundary Guidance to significantly enhance the spatial distribution of Gaussian
blobs, resulting in clearer differentiation between objects and their
background. To address the challenge (ii), we propose a Box-Focused Sampling
strategy using 2D boxes to generate object probability distribution in 3D
spaces, allowing effective probabilistic sampling in 3D to retain more object
blobs and reduce noisy background blobs. Benefiting from our designs, our
3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det,
achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet
dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Page: https://github.com/yangcaoai/3DGS-DET</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Extrapolation for Text-to-<span class="highlight-title">image</span> <span class="highlight-title">Generation</span> on Small <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senmao Ye, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image generation requires large amount of training data to
synthesizing high-quality images. For augmenting training data, previous
methods rely on data interpolations like cropping, flipping, and mixing up,
which fail to introduce new information and yield only marginal improvements.
In this paper, we propose a new data augmentation method for text-to-image
generation using linear extrapolation. Specifically, we apply linear
extrapolation only on text feature, and new image data are retrieved from the
internet by search engines. For the reliability of new text-image pairs, we
design two outlier detectors to purify retrieved images. Based on
extrapolation, we construct training samples dozens of times larger than the
original dataset, resulting in a significant improvement in text-to-image
performance. Moreover, we propose a NULL-guidance to refine score estimation,
and apply recurrent affine transformation to fuse text information. Our model
achieves FID scores of 7.91, 9.52 and 5.00 on the CUB, Oxford and COCO
datasets. The code and data will be available on GitHub
(https://github.com/senmaoy/RAT-Diffusion).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMOD: A Large <span class="highlight-title">Multimodal</span> Ophthalmology <span class="highlight-title">Dataset</span> and <span class="highlight-title">Benchmark</span> for Large
  <span class="highlight-title">Vision-Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyue Qin, Yu Yin, Dylan Campbell, Xuansheng Wu, Ke Zou, Yih-Chung Tham, Ninghao Liu, Xiuzhen Zhang, Qingyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ophthalmology relies heavily on detailed image analysis for diagnosis and
treatment planning. While large vision-language models (LVLMs) have shown
promise in understanding complex visual information, their performance on
ophthalmology images remains underexplored. We introduce LMOD, a dataset and
benchmark for evaluating LVLMs on ophthalmology images, covering anatomical
understanding, diagnostic analysis, and demographic extraction. LMODincludes
21,993 images spanning optical coherence tomography, scanning laser
ophthalmoscopy, eye photos, surgical scenes, and color fundus photographs. We
benchmark 13 state-of-the-art LVLMs and find that they are far from perfect for
comprehending ophthalmology images. Models struggle with diagnostic analysis
and demographic extraction, reveal weaknesses in spatial reasoning, diagnostic
analysis, handling out-of-domain queries, and safeguards for handling
biomarkers of ophthalmology images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGBA: Semantic Gaussian Mixture Model-Based LiDAR Bundle Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Ji, Shenghai Yuan, Jianping Li, Pengyu Yin, Haozhi Cao, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR bundle adjustment (BA) is an effective approach to reduce the drifts in
pose estimation from the front-end. Existing works on LiDAR BA usually rely on
predefined geometric features for landmark representation. This reliance
restricts generalizability, as the system will inevitably deteriorate in
environments where these specific features are absent. To address this issue,
we propose SGBA, a LiDAR BA scheme that models the environment as a semantic
Gaussian mixture model (GMM) without predefined feature types. This approach
encodes both geometric and semantic information, offering a comprehensive and
general representation adaptable to various environments. Additionally, to
limit computational complexity while ensuring generalizability, we propose an
adaptive semantic selection framework that selects the most informative
semantic clusters for optimization by evaluating the condition number of the
cost function. Lastly, we introduce a probabilistic feature association scheme
that considers the entire probability density of assignments, which can manage
uncertainties in measurement and initial pose estimation. We have conducted
various experiments and the results demonstrate that SGBA can achieve accurate
and robust pose refinement even in challenging scenarios with low-quality
initial pose estimation and limited geometric features. We plan to open-source
the work for the benefit of the community https://github.com/Ji1Xinyu/SGBA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Saliency-Guided DETR for Moment Retrieval and Highlight Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Gordeev, Vladimir Dokholyan, Irina Tolstykh, Maksim Kuprashevich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches for video moment retrieval and highlight detection are
not able to align text and video features efficiently, resulting in
unsatisfying performance and limited production usage. To address this, we
propose a novel architecture that utilizes recent foundational video models
designed for such alignment. Combined with the introduced Saliency-Guided Cross
Attention mechanism and a hybrid DETR architecture, our approach significantly
enhances performance in both moment retrieval and highlight detection tasks.
For even better improvement, we developed InterVid-MR, a large-scale and
high-quality dataset for pretraining. Using it, our architecture achieves
state-of-the-art results on the QVHighlights, Charades-STA and TACoS
benchmarks. The proposed approach provides an efficient and scalable solution
for both zero-shot and fine-tuning scenarios in video-language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Splatting in Mirrors: Reflection-Aware Rendering via Virtual
  Camera Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Shuzhe Wang, Matias Turkulainen, Junyuan Fang, Juho Kannala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D Gaussian Splatting (3D-GS) have revolutionized
novel view synthesis, facilitating real-time, high-quality image rendering.
However, in scenarios involving reflective surfaces, particularly mirrors,
3D-GS often misinterprets reflections as virtual spaces, resulting in blurred
and inconsistent multi-view rendering within mirrors. Our paper presents a
novel method aimed at obtaining high-quality multi-view consistent reflection
rendering by modelling reflections as physically-based virtual cameras. We
estimate mirror planes with depth and normal estimates from 3D-GS and define
virtual cameras that are placed symmetrically about the mirror plane. These
virtual cameras are then used to explain mirror reflections in the scene. To
address imperfections in mirror plane estimates, we propose a straightforward
yet effective virtual camera optimization method to enhance reflection quality.
We collect a new mirror dataset including three real-world scenarios for more
diverse evaluation. Experimental validation on both Mirror-Nerf and our
real-world dataset demonstrate the efficacy of our approach. We achieve
comparable or superior results while significantly reducing training time
compared to previous state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published on 2024 British Machine Vision Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRUPI: <span class="highlight-title">Dataset</span> Reduction Using Privileged Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Wang, Yantai Yang, Shuaiyu Zhang, Chenghao Sun, Weiya Li, Xuming Hu, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset reduction (DR) seeks to select or distill samples from large datasets
into smaller subsets while preserving performance on target tasks. Existing
methods primarily focus on pruning or synthesizing data in the same format as
the original dataset, typically the input data and corresponding labels.
However, in DR settings, we find it is possible to synthesize more information
beyond the data-label pair as an additional learning target to facilitate model
training. In this paper, we introduce Dataset Reduction Using Privileged
Information (DRUPI), which enriches DR by synthesizing privileged information
alongside the reduced dataset. This privileged information can take the form of
feature labels or attention labels, providing auxiliary supervision to improve
model learning. Our findings reveal that effective feature labels must balance
between being overly discriminative and excessively diverse, with a moderate
level proving optimal for improving the reduced dataset's efficacy. Extensive
experiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI
integrates seamlessly with existing dataset reduction methods, offering
significant performance gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAViD: Domain Adaptive Visually-Rich Document Understanding with
  Synthetic Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Ding, Soyeon Caren Han, Zechuan Li, Hyunsuk Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visually-Rich Documents (VRDs), encompassing elements like charts, tables,
and references, convey complex information across various fields. However,
extracting information from these rich documents is labor-intensive, especially
given their inconsistent formats and domain-specific requirements. While
pretrained models for VRD Understanding have progressed, their reliance on
large, annotated datasets limits scalability. This paper introduces the Domain
Adaptive Visually-rich Document Understanding (DAViD) framework, which utilises
machine-generated synthetic data for domain adaptation. DAViD integrates
fine-grained and coarse-grained document representation learning and employs
synthetic annotations to reduce the need for costly manual labelling. By
leveraging pretrained models and synthetic data, DAViD achieves competitive
performance with minimal annotated datasets. Extensive experiments validate
DAViD's effectiveness, demonstrating its ability to efficiently adapt to
domain-specific VRDU tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KnobGen: Controlling the Sophistication of Artwork in Sketch-Based
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pouyan Navard, Amin Karimi Monsefi, Mengxi Zhou, Wei-Lun Chao, Alper Yilmaz, Rajiv Ramnath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have significantly improved text-to-image
(T2I) generation, but they often struggle to balance fine-grained precision
with high-level control. Methods like ControlNet and T2I-Adapter excel at
following sketches by seasoned artists but tend to be overly rigid, replicating
unintentional flaws in sketches from novice users. Meanwhile, coarse-grained
methods, such as sketch-based abstraction frameworks, offer more accessible
input handling but lack the precise control needed for detailed, professional
use. To address these limitations, we propose KnobGen, a dual-pathway framework
that democratizes sketch-based image generation by seamlessly adapting to
varying levels of sketch complexity and user skill. KnobGen uses a
Coarse-Grained Controller (CGC) module for high-level semantics and a
Fine-Grained Controller (FGC) module for detailed refinement. The relative
strength of these two modules can be adjusted through our knob inference
mechanism to align with the user's specific needs. These mechanisms ensure that
KnobGen can flexibly generate images from both novice sketches and those drawn
by seasoned artists. This maintains control over the final output while
preserving the natural appearance of the image, as evidenced on the
MultiGen-20M dataset and a newly collected sketch dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-LDM: Multi-Modal Latent <span class="highlight-title">Diffusion</span> Model for Sounding Video <span class="highlight-title">Generation</span> <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingzhen Sun, Weining Wang, Yanyuan Qiao, Jiahui Sun, Zihan Qin, Longteng Guo, Xinxin Zhu, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sounding Video Generation (SVG) is an audio-video joint generation task
challenged by high-dimensional signal spaces, distinct data formats, and
different patterns of content information. To address these issues, we
introduce a novel multi-modal latent diffusion model (MM-LDM) for the SVG task.
We first unify the representation of audio and video data by converting them
into a single or a couple of images. Then, we introduce a hierarchical
multi-modal autoencoder that constructs a low-level perceptual latent space for
each modality and a shared high-level semantic feature space. The former space
is perceptually equivalent to the raw signal space of each modality but
drastically reduces signal dimensions. The latter space serves to bridge the
information gap between modalities and provides more insightful cross-modal
guidance. Our proposed method achieves new state-of-the-art results with
significant quality and efficiency gains. Specifically, our method achieves a
comprehensive improvement on all evaluation metrics and a faster training and
sampling speed on Landscape and AIST++ datasets. Moreover, we explore its
performance on open-domain sounding video generation, long sounding video
generation, audio continuation, video continuation, and conditional
single-modal generation tasks for a comprehensive evaluation, where our MM-LDM
demonstrates exciting adaptability and generalization ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imaging foundation model for universal enhancement of non-ideal
  measurement CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Liu, Rongjun Ge, Yuting He, Zhan Wu, Chenyu You, Shuo Li, Yang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-ideal measurement computed tomography (NICT), which sacrifices optimal
imaging standards for new advantages in CT imaging, is expanding the clinical
application scope of CT images. However, with the reduction of imaging
standards, the image quality has also been reduced, extremely limiting the
clinical acceptability. Although numerous studies have demonstrated the
feasibility of deep learning for the NICT enhancement in specific scenarios,
their high data cost and limited generalizability have become large obstacles.
The recent research on the foundation model has brought new opportunities for
building a universal NICT enhancement model - bridging the image quality
degradation with minimal data cost. However, owing to the challenges in the
collection of large pre-training datasets and the compatibility of data
variation, no success has been reported. In this paper, we propose a
multi-scale integrated Transformer AMPlifier (TAMP), the first imaging
foundation model for universal NICT enhancement. It has been pre-trained on a
large-scale physical-driven simulation dataset with 3.6 million NICT-ICT image
pairs, and is able to directly generalize to the NICT enhancement tasks with
various non-ideal settings and body regions. Via the adaptation with few data,
it can further achieve professional performance in real-world specific
scenarios. Our extensive experiments have demonstrated that the proposed TAMP
has significant potential for promoting the exploration and application of NICT
and serving a wider range of medical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coordinate-Based Neural Representation Enabling Zero-Shot Learning for
  3D Multiparametric Quantitative MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyan Lao, Ruimin Feng, Haikun Qi, Zhenfeng Lv, Qiangqiang Liu, Chunlei Liu, Yuyao Zhang, Hongjiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative magnetic resonance imaging (qMRI) offers tissue-specific
physical parameters with significant potential for neuroscience research and
clinical practice. However, lengthy scan times for 3D multiparametric qMRI
acquisition limit its clinical utility. Here, we propose SUMMIT, an innovative
imaging methodology that includes data acquisition and an unsupervised
reconstruction for simultaneous multiparametric qMRI. SUMMIT first encodes
multiple important quantitative properties into highly undersampled k-space. It
further leverages implicit neural representation incorporated with a dedicated
physics model to reconstruct the desired multiparametric maps without needing
external training datasets. SUMMIT delivers co-registered T1, T2, T2*, and
quantitative susceptibility mapping. Extensive simulations and phantom imaging
demonstrate SUMMIT's high accuracy. Additionally, the proposed unsupervised
approach for qMRI reconstruction also introduces a novel zero-shot learning
paradigm for multiparametric imaging applicable to various medical imaging
modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake It Until You Break It: On the Adversarial Robustness of
  AI-generated <span class="highlight-title">Image</span> Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Mavali, Jonas Ricker, David Pape, Yash Sharma, Asja Fischer, Lea Schoenherr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative AI (GenAI) offers countless possibilities for creative and
productive tasks, artificially generated media can be misused for fraud,
manipulation, scams, misinformation campaigns, and more. To mitigate the risks
associated with maliciously generated media, forensic classifiers are employed
to identify AI-generated content. However, current forensic classifiers are
often not evaluated in practically relevant scenarios, such as the presence of
an attacker or when real-world artifacts like social media degradations affect
images. In this paper, we evaluate state-of-the-art AI-generated image (AIGI)
detectors under different attack scenarios. We demonstrate that forensic
classifiers can be effectively attacked in realistic settings, even when the
attacker does not have access to the target model and post-processing occurs
after the adversarial examples are created, which is standard on social media
platforms. These attacks can significantly reduce detection accuracy to the
extent that the risks of relying on detectors outweigh their benefits. Finally,
we propose a simple defense mechanism to make CLIP-based detectors, which are
currently the best-performing detectors, robust against these attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PASS:Test-Time Prompting to Adapt Styles and Semantic Shapes in Medical
  <span class="highlight-title">Image</span> Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyan Zhang, Hao Zheng, Xin You, Yefeng Zheng, Yun Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) has emerged as a promising paradigm to handle the
domain shifts at test time for medical images from different institutions
without using extra training data. However, existing TTA solutions for
segmentation tasks suffer from (1) dependency on modifying the source training
stage and access to source priors or (2) lack of emphasis on shape-related
semantic knowledge that is crucial for segmentation tasks.Recent research on
visual prompt learning achieves source-relaxed adaptation by extended parameter
space but still neglects the full utilization of semantic features, thus
motivating our work on knowledge-enriched deep prompt learning. Beyond the
general concern of image style shifts, we reveal that shape variability is
another crucial factor causing the performance drop. To address this issue, we
propose a TTA framework called PASS (Prompting to Adapt Styles and Semantic
shapes), which jointly learns two types of prompts: the input-space prompt to
reformulate the style of the test image to fit into the pretrained model and
the semantic-aware prompts to bridge high-level shape discrepancy across
domains. Instead of naively imposing a fixed prompt, we introduce an input
decorator to generate the self-regulating visual prompt conditioned on the
input data. To retrieve the knowledge representations and customize
target-specific shape prompts for each test sample, we propose a
cross-attention prompt modulator, which performs interaction between target
representations and an enriched shape prompt bank. Extensive experiments
demonstrate the superior performance of PASS over state-of-the-art methods on
multiple medical image segmentation datasets. The code is available at
https://github.com/EndoluminalSurgicalVision-IMR/PASS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Weakly-Supervised Referring <span class="highlight-title">Image</span> Segmentation via Progressive
  Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaiquan Yang, Yuhao Liu, Jiaying Lin, Gerhard Hancke, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the weakly-supervised referring image segmentation (WRIS)
problem, and focuses on a challenging setup where target localization is
learned directly from image-text pairs. We note that the input text description
typically already contains detailed information on how to localize the target
object, and we also observe that humans often follow a step-by-step
comprehension process (\ie, progressively utilizing target-related attributes
and relations as cues) to identify the target object. Hence, we propose a novel
Progressive Comprehension Network (PCNet) to leverage target-related textual
cues from the input description for progressively localizing the target object.
Specifically, we first use a Large Language Model (LLM) to decompose the input
text description into short phrases. These short phrases are taken as
target-related cues and fed into a Conditional Referring Module (CRM) in
multiple stages, to allow updating the referring text embedding and enhance the
response map for target localization in a multi-stage manner. Based on the CRM,
we then propose a Region-aware Shrinking (RaS) loss to constrain the visual
localization to be conducted progressively in a coarse-to-fine manner across
different stages. Finally, we introduce an Instance-aware Disambiguation (IaD)
loss to suppress instance localization ambiguity by differentiating overlapping
response maps generated by different referring texts on the same image.
Extensive experiments show that our method outperforms SOTA methods on three
common benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge-preserving noise for <span class="highlight-title">diffusion</span> models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jente Vandersanden, Sascha Holl, Xingchang Huang, Gurprit Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical generative diffusion models learn an isotropic Gaussian denoising
process, treating all spatial regions uniformly, thus neglecting potentially
valuable structural information in the data. Inspired by the long-established
work on anisotropic diffusion in image processing, we present a novel
edge-preserving diffusion model that is a generalization of denoising diffusion
probablistic models (DDPM). In particular, we introduce an edge-aware noise
scheduler that varies between edge-preserving and isotropic Gaussian noise. We
show that our model's generative process converges faster to results that more
closely match the target distribution. We demonstrate its capability to better
learn the low-to-mid frequencies within the dataset, which plays a crucial role
in representing shapes and structural information. Our edge-preserving
diffusion process consistently outperforms state-of-the-art baselines in
unconditional image generation. It is also more robust for generative tasks
guided by a shape-based prior, such as stroke-to-image generation. We present
qualitative and quantitative results showing consistent improvements (FID
score) of up to 30% for both tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Fusion for Object Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing images or videos as object-level feature vectors, rather than
pixel-level feature maps, facilitates advanced visual tasks. Object-Centric
Learning (OCL) primarily achieves this by reconstructing the input under the
guidance of Variational Autoencoder (VAE) intermediate representation to drive
so-called \textit{slots} to aggregate as much object information as possible.
However, existing VAE guidance does not explicitly address that objects can
vary in pixel sizes while models typically excel at specific pattern scales. We
propose \textit{Multi-Scale Fusion} (MSF) to enhance VAE guidance for OCL
training. To ensure objects of all sizes fall within VAE's comfort zone, we
adopt the \textit{image pyramid}, which produces intermediate representations
at multiple scales; To foster scale-invariance/variance in object super-pixels,
we devise \textit{inter}/\textit{intra-scale fusion}, which augments
low-quality object super-pixels of one scale with corresponding high-quality
super-pixels from another scale. On standard OCL benchmarks, our technique
improves mainstream methods, including state-of-the-art diffusion-based ones.
The source code is available in the supplemental material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EUFCC-CIR: a Composed <span class="highlight-title">Image</span> Retrieval <span class="highlight-title">Dataset</span> for GLAM Collections <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesc Net, Lluis Gomez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intersection of Artificial Intelligence and Digital Humanities enables
researchers to explore cultural heritage collections with greater depth and
scale. In this paper, we present EUFCC-CIR, a dataset designed for Composed
Image Retrieval (CIR) within Galleries, Libraries, Archives, and Museums (GLAM)
collections. Our dataset is built on top of the EUFCC-340K image labeling
dataset and contains over 180K annotated CIR triplets. Each triplet is composed
of a multi-modal query (an input image plus a short text describing the desired
attribute manipulations) and a set of relevant target images. The EUFCC-CIR
dataset fills an existing gap in CIR-specific resources for Digital Humanities.
We demonstrate the value of the EUFCC-CIR dataset by highlighting its unique
qualities in comparison to other existing CIR datasets and evaluating the
performance of several zero-shot CIR baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV Workshop (AI4DH2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene
  by Primitives and Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Jiang, Qihao Zhao, Hossein Rahmani, De Wen Soh, Jun Liu, Na Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, with the development of Neural Radiance Fields and Gaussian
Splatting, 3D reconstruction techniques have achieved remarkably high fidelity.
However, the latent representations learnt by these methods are highly
entangled and lack interpretability. In this paper, we propose a novel
part-aware compositional reconstruction method, called GaussianBlock, that
enables semantically coherent and disentangled representations, allowing for
precise and physical editing akin to building blocks, while simultaneously
maintaining high fidelity. Our GaussianBlock introduces a hybrid representation
that leverages the advantages of both primitives, known for their flexible
actionability and editability, and 3D Gaussians, which excel in reconstruction
quality. Specifically, we achieve semantically coherent primitives through a
novel attention-guided centering loss derived from 2D semantic priors,
complemented by a dynamic splitting and fusion strategy. Furthermore, we
utilize 3D Gaussians that hybridize with primitives to refine structural
details and enhance fidelity. Additionally, a binding inheritance strategy is
employed to strengthen and maintain the connection between the two. Our
reconstructed scenes are evidenced to be disentangled, compositional, and
compact across diverse benchmarks, enabling seamless, direct and precise
editing while maintaining high quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward a Holistic Evaluation of Robustness in CLIP Models <span class="chip">NeurIPS'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Tu, Weijian Deng, Tom Gedeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) models have shown significant
potential, particularly in zero-shot classification across diverse distribution
shifts. Building on existing evaluations of overall classification robustness,
this work aims to provide a more comprehensive assessment of CLIP by
introducing several new perspectives. First, we investigate their robustness to
variations in specific visual factors. Second, we assess two critical safety
objectives--confidence uncertainty and out-of-distribution detection--beyond
mere classification accuracy. Third, we evaluate the finesse with which CLIP
models bridge the image and text modalities. Fourth, we extend our examination
to 3D awareness in CLIP models, moving beyond traditional 2D image
understanding. Finally, we explore the interaction between vision and language
encoders within modern large multimodal models (LMMs) that utilize CLIP as the
visual backbone, focusing on how this interaction impacts classification
robustness. In each aspect, we consider the impact of six factors on CLIP
models: model architecture, training distribution, training set size,
fine-tuning, contrastive loss, and test-time prompts. Our study uncovers
several previously unknown insights into CLIP. For instance, the architecture
of the visual encoder in CLIP plays a significant role in their robustness
against 3D corruption. CLIP models tend to exhibit a bias towards shape when
making predictions. Moreover, this bias tends to diminish after fine-tuning on
ImageNet. Vision-language models like LLaVA, leveraging the CLIP vision
encoder, could exhibit benefits in classification performance for challenging
categories over CLIP alone. Our findings are poised to offer valuable guidance
for enhancing the robustness and reliability of CLIP models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, extension of NeurIPS'23 work: A Closer Look at
  the Robustness of Contrastive Language-Image Pre-Training (CLIP). arXiv admin
  note: text overlap with arXiv:2402.07410</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robo-MUTUAL: Robotic <span class="highlight-title">Multimodal</span> Task Specification via Unimodal Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxiong Li, Zhihao Wang, Jinliang Zheng, Xiaoai Zhou, Guanming Wang, Guanglu Song, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Junzhi Yu, Xianyuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal task specification is essential for enhanced robotic performance,
where \textit{Cross-modality Alignment} enables the robot to holistically
understand complex task instructions. Directly annotating multimodal
instructions for model training proves impractical, due to the sparsity of
paired multimodal data. In this study, we demonstrate that by leveraging
unimodal instructions abundant in real data, we can effectively teach robots to
learn multimodal task specifications. First, we endow the robot with strong
\textit{Cross-modality Alignment} capabilities, by pretraining a robotic
multimodal encoder using extensive out-of-domain data. Then, we employ two
Collapse and Corrupt operations to further bridge the remaining modality gap in
the learned multimodal representation. This approach projects different
modalities of identical task goal as interchangeable representations, thus
enabling accurate robotic operations within a well-aligned multimodal latent
space. Evaluation across more than 130 tasks and 4000 evaluations on both
simulated LIBERO benchmark and real robot platforms showcases the superior
capabilities of our proposed framework, demonstrating significant advantage in
overcoming data constraints in robotic learning. Website:
zh1hao.wang/Robo_MUTUAL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiraGe: Editable 2D <span class="highlight-title">Image</span>s using Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Waczyńska, Tomasz Szczepanik, Piotr Borycki, Sławomir Tadeja, Thomas Bohné, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) approximate discrete data through
continuous functions and are commonly used for encoding 2D images. Traditional
image-based INRs employ neural networks to map pixel coordinates to RGB values,
capturing shapes, colors, and textures within the network's weights. Recently,
GaussianImage has been proposed as an alternative, using Gaussian functions
instead of neural networks to achieve comparable quality and compression. Such
a solution obtains a quality and compression ratio similar to classical INR
models but does not allow image modification. In contrast, our work introduces
a novel method, MiraGe, which uses mirror reflections to perceive 2D images in
3D space and employs flat-controlled Gaussians for precise 2D image editing.
Our approach improves the rendering quality and allows realistic image
modifications, including human-inspired perception of photos in the 3D world.
Thanks to modeling images in 3D space, we obtain the illusion of 3D-based
modification in 2D images. We also show that our Gaussian representation can be
easily combined with a physics engine to produce physics-based modification of
2D images. Consequently, MiraGe allows for better quality than the standard
approach and natural modification of 2D images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater
  Scene Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wang, Nantheera Anantrasirichai, Fan Zhang, David Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting (3DGS) offers the capability to achieve real-time high
quality 3D scene rendering. However, 3DGS assumes that the scene is in a clear
medium environment and struggles to generate satisfactory representations in
underwater scenes, where light absorption and scattering are prevalent and
moving objects are involved. To overcome these, we introduce a novel Gaussian
Splatting-based method, UW-GS, designed specifically for underwater
applications. It introduces a color appearance that models distance-dependent
color variation, employs a new physics-based density control strategy to
enhance clarity for distant objects, and uses a binary motion mask to handle
dynamic content. Optimized with a well-designed loss function supporting for
scattering media and strengthened by pseudo-depth maps, UW-GS outperforms
existing methods with PSNR gains up to 1.26dB. To fully verify the
effectiveness of the model, we also developed a new underwater dataset, S-UW,
with dynamic object masks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexuan Ding, Lei Wang, Liyun Zhu, Tom Gedeon, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision tasks, features often come from diverse representations,
domains, and modalities, such as text, images, and videos. Effectively fusing
these features is essential for robust performance, especially with the
availability of powerful pre-trained models like vision-language models.
However, common fusion methods, such as concatenation, element-wise operations,
and non-linear techniques, often fail to capture structural relationships, deep
feature interactions, and suffer from inefficiency or misalignment of features
across domains. In this paper, we shift from high-dimensional feature space to
a lower-dimensional, interpretable graph space by constructing similarity
graphs that encode feature relationships at different levels, e.g., clip,
frame, patch, token, etc. To capture deeper interactions, we use graph power
expansions and introduce a learnable graph fusion operator to combine these
graph powers for more effective fusion. Our approach is relationship-centric,
operates in a homogeneous space, and is mathematically principled, resembling
element-wise similarity score aggregation via multilinear polynomials. We
demonstrate the effectiveness of our graph-based fusion method on video anomaly
detection, showing strong performance across multi-representational,
multi-modal, and multi-domain feature fusion tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Research paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quo Vadis RankList-based System in Face Recognition? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zhang, Manuel Günther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition in the wild has gained a lot of focus in the last few years,
and many face recognition models are designed to verify faces in medium-quality
images. Especially due to the availability of large training datasets with
similar conditions, deep face recognition models perform exceptionally well in
such tasks. However, in other tasks where substantially less training data is
available, such methods struggle, especially when required to compare
high-quality enrollment images with low-quality probes. On the other hand,
traditional RankList-based methods have been developed that compare faces
indirectly by comparing to cohort faces with similar conditions. In this paper,
we revisit these RankList methods and extend them to use the logits of the
state-of-the-art DaliFace network, instead of an external cohort. We show that
through a reasonable Logit-Cohort Selection (LoCoS) the performance of
RankList-based functions can be improved drastically. Experiments on two
challenging face recognition datasets not only demonstrate the enhanced
performance of our proposed method but also set the stage for future
advancements in handling diverse image qualities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at IJCB 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osher Rafaeli, Tal Svoray, Ariel Nahlieli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soil sinkholes significantly influence soil degradation, but their irregular
shapes, along with interference from shadow and vegetation, make it challenging
to accurately quantify their properties using remotely sensed data. We present
a novel framework for sinkhole segmentation that combines traditional
topographic computations of closed depressions with the newly developed
prompt-based Segment Anything Model (SAM). Within this framework, termed
SinkSAM, we highlight four key improvements: (1) The integration of topographic
computations with SAM enables pixel-level refinement of sinkhole boundaries
segmentation; (2) A coherent mathematical prompting strategy, based on closed
depressions, addresses the limitations of purely learning-based models (CNNs)
in detecting and segmenting undefined sinkhole features, while improving
generalization to new, unseen regions; (3) Using Depth Anything V2 monocular
depth for automatic prompts eliminates photogrammetric biases, enabling
sinkhole mapping without the dependence on LiDAR data; and (4) An established
sinkhole database facilitates fine-tuning of SAM, improving its zero-shot
performance in sinkhole segmentation. These advancements allow the deployment
of SinkSAM, in an unseen test area, in the highly variable semiarid region,
achieving an intersection-over-union (IoU) of 40.27\% and surpassing previous
results. This paper also presents the first SAM implementation for sinkhole
segmentation and demonstrates the robustness of SinkSAM in extracting sinkhole
maps using a single RGB image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurgPointTransformer: Vertebrae Shape Completion with RGB-D Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidana Massalimova, Florentin Liebmann, Sascha Jecklin, Fabio Carrillo, Farshad Mazda, Philipp Fürnstahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art computer- and robot-assisted surgery systems heavily depend
on intraoperative imaging technologies such as CT and fluoroscopy to generate
detailed 3D visualization of the patient's anatomy. While imaging techniques
are highly accurate, they are based on ionizing radiation and expose patients
and clinicians. This study introduces an alternative, radiation-free approach
for reconstructing the 3D spine anatomy using RGB-D data. Drawing inspiration
from the 3D "mental map" that surgeons form during surgeries, we introduce
SurgPointTransformer, a shape completion approach for surgical applications
that can accurately reconstruct the unexposed spine regions from sparse
observations of the exposed surface.
  Our method involves two main steps: segmentation and shape completion. The
segmentation step includes spinal column localization and segmentation,
followed by vertebra-wise segmentation. The segmented vertebra point clouds are
then subjected to SurgPointTransformer, which leverages an attention mechanism
to learn patterns between visible surface features and the underlying anatomy.
For evaluation, we utilize an ex-vivo dataset of nine specimens. Their CT data
is used to establish ground truth data that were used to compare to the outputs
of our methods. Our method significantly outperforms the state-of-the-art
baselines, achieving an average Chamfer Distance of 5.39, an F-Score of 0.85,
an Earth Mover's Distance of 0.011, and a Signal-to-Noise Ratio of 22.90 dB.
  This study demonstrates the potential of our reconstruction method for 3D
vertebral shape completion. It enables 3D reconstruction of the entire lumbar
spine and surgical guidance without ionizing radiation or invasive imaging. Our
work contributes to computer-aided and robot-assisted surgery, advancing the
perception and intelligence of these systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decorrelation-based Self-Supervised Visual Representation Learning for
  Writer Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadip Maitra, Shree Mitra, Siladittya Manna, Saumik Bhattacharya, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has developed rapidly over the last decade and has
been applied in many areas of computer vision. Decorrelation-based
self-supervised pretraining has shown great promise among non-contrastive
algorithms, yielding performance at par with supervised and contrastive
self-supervised baselines. In this work, we explore the decorrelation-based
paradigm of self-supervised learning and apply the same to learning
disentangled stroke features for writer identification. Here we propose a
modified formulation of the decorrelation-based framework named SWIS which was
proposed for signature verification by standardizing the features along each
dimension on top of the existing framework. We show that the proposed framework
outperforms the contemporary self-supervised learning framework on the writer
identification benchmark and also outperforms several supervised methods as
well. To the best of our knowledge, this work is the first of its kind to apply
self-supervised learning for learning representations for writer verification
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis
  under Diverse Camera Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingdong Hu, Zhening Liu, Jiawei Shao, Zehong Lin, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The feed-forward based 3D Gaussian Splatting method has demonstrated
exceptional capability in real-time human novel view synthesis. However,
existing approaches are restricted to dense viewpoint settings, which limits
their flexibility in free-viewpoint rendering across a wide range of camera
view angle discrepancies. To address this limitation, we propose a real-time
pipeline named EVA-Gaussian for 3D human novel view synthesis across diverse
camera settings. Specifically, we first introduce an Efficient cross-View
Attention (EVA) module to accurately estimate the position of each 3D Gaussian
from the source images. Then, we integrate the source images with the estimated
Gaussian position map to predict the attributes and feature embeddings of the
3D Gaussians. Moreover, we employ a recurrent feature refiner to correct
artifacts caused by geometric errors in position estimation and enhance visual
fidelity.To further improve synthesis quality, we incorporate a powerful anchor
loss function for both 3D Gaussian attributes and human face landmarks.
Experimental results on the THuman2.0 and THumansit datasets showcase the
superiority of our EVA-Gaussian approach in rendering quality across diverse
camera settings. Project page:
https://zhenliuzju.github.io/huyingdong/EVA-Gaussian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Labyrinth of Links: Navigating the Associative Maze of Multi-modal
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have exhibited impressive
capability. However, recently many deficiencies of MLLMs have been found
compared to human intelligence, $\textit{e.g.}$, hallucination. To drive the
MLLMs study, the community dedicated efforts to building larger benchmarks with
complex tasks. In this paper, we propose benchmarking an essential but usually
overlooked intelligence: $\textbf{association}$, a human's basic capability to
link observation and prior practice memory. To comprehensively investigate
MLLM's performance on the association, we formulate the association task and
devise a standard benchmark based on adjective and verb semantic concepts.
Instead of costly data annotation and curation, we propose a convenient
$\textbf{annotation-free}$ construction method transforming the general dataset
for our association tasks. Simultaneously, we devise a rigorous data refinement
process to eliminate confusion in the raw dataset. Building on this database,
we establish three levels of association tasks: single-step, synchronous, and
asynchronous associations. Moreover, we conduct a comprehensive investigation
into the MLLMs' zero-shot association capabilities, addressing multiple
dimensions, including three distinct memory strategies, both open-source and
closed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the
involvement of human experts. Our systematic investigation shows that current
open-source MLLMs consistently exhibit poor capability in our association
tasks, even the currently state-of-the-art GPT-4V(vision) also has a
significant gap compared to humans. We believe our benchmark would pave the way
for future MLLM studies. $\textit{Our data and code are available at:}$
https://mvig-rhos.com/llm_inception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSIM: A Copula-based similarity index sensitive to local changes for
  <span class="highlight-title">Image</span> quality assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safouane El Ghazouali, Umberto Michelucci, Yassin El Hillali, Hichem Nouira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image similarity metrics play an important role in computer vision
applications, as they are used in image processing, computer vision and machine
learning. Furthermore, those metrics enable tasks such as image retrieval,
object recognition and quality assessment, essential in fields like healthcare,
astronomy and surveillance. Existing metrics, such as PSNR, MSE, SSIM, ISSM and
FSIM, often face limitations in terms of either speed, complexity or
sensitivity to small changes in images. To address these challenges, a novel
image similarity metric, namely CSIM, that combines real-time while being
sensitive to subtle image variations is investigated in this paper. The novel
metric uses Gaussian Copula from probability theory to transform an image into
vectors of pixel distribution associated to local image patches. These vectors
contain, in addition to intensities and pixel positions, information on the
dependencies between pixel values, capturing the structural relationships
within the image. By leveraging the properties of Copulas, CSIM effectively
models the joint distribution of pixel intensities, enabling a more nuanced
comparison of image patches making it more sensitive to local changes compared
to other metrics. Experimental results demonstrate that CSIM outperforms
existing similarity metrics in various image distortion scenarios, including
noise, compression artifacts and blur. The metric's ability to detect subtle
differences makes it suitable for applications requiring high precision, such
as medical imaging, where the detection of minor anomalies can be of a high
importance. The results obtained in this work can be reproduced from this
Github repository: https://github.com/safouaneelg/copulasimilarity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHAP-CAT: A interpretable multi-modal framework enhancing WSI
  classification via virtual staining and shapley-value-based <span class="highlight-title">multimodal</span> fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Yu Mao, Nan Guan, Chun Jason Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multimodal model has demonstrated promise in histopathology. However,
most multimodal models are based on H\&E and genomics, adopting increasingly
complex yet black-box designs. In our paper, we propose a novel interpretable
multimodal framework named SHAP-CAT, which uses a Shapley-value-based dimension
reduction technique for effective multimodal fusion. Starting with two paired
modalities -- H\&E and IHC images, we employ virtual staining techniques to
enhance limited input data by generating a new clinical-related modality.
Lightweight bag-level representations are extracted from image modalities and a
Shapley-value-based mechanism is used for dimension reduction. For each
dimension of the bag-level representation, attribution values are calculated to
indicate how changes in the specific dimensions of the input affect the model
output. In this way, we select a few top important dimensions of bag-level
representation for each image modality to late fusion. Our experimental results
demonstrate that the proposed SHAP-CAT framework incorporating synthetic
modalities significantly enhances model performance, yielding a 5\% increase in
accuracy for the BCI, an 8\% increase for IHC4BC-ER, and an 11\% increase for
the IHC4BC-PR dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgriCLIP: Adapting CLIP for Agriculture and Livestock via
  Domain-Specialized Cross-Model Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umair Nawaz, Muhammad Awais, Hanan Gani, Muzammal Naseer, Fahad Khan, Salman Khan, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capitalizing on vast amount of image-text data, large-scale vision-language
pre-training has demonstrated remarkable zero-shot capabilities and has been
utilized in several applications. However, models trained on general everyday
web-crawled data often exhibit sub-optimal performance for specialized domains,
likely due to domain shift. Recent works have tackled this problem for some
domains (e.g., healthcare) by constructing domain-specialized image-text data.
However, constructing a dedicated large-scale image-text dataset for
sustainable area of agriculture and livestock is still open to research.
Further, this domain desires fine-grained feature learning due to the subtle
nature of the downstream tasks (e.g, nutrient deficiency detection, livestock
breed classification). To address this we present AgriCLIP, a vision-language
foundational model dedicated to the domain of agriculture and livestock. First,
we propose a large-scale dataset, named ALive, that leverages customized prompt
generation strategy to overcome the scarcity of expert annotations. Our ALive
dataset covers crops, livestock, and fishery, with around 600,000 image-text
pairs. Second, we propose a training pipeline that integrates both contrastive
and self-supervised learning to learn both global semantic and local
fine-grained domain-specialized features. Experiments on diverse set of 20
downstream tasks demonstrate the effectiveness of AgriCLIP framework, achieving
an absolute gain of 7.8\% in terms of average zero-shot classification
accuracy, over the standard CLIP adaptation via domain-specialized ALive
dataset. Our ALive dataset and code can be accessible at
\href{https://github.com/umair1221/AgriCLIP/tree/main}{Github}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongru Yan, Yu Zheng, Yueqi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skins wrapping around our bodies, leathers covering over the sofa, sheet
metal coating the car - it suggests that objects are enclosed by a series of
continuous surfaces, which provides us with informative geometry prior for
objectness deduction. In this paper, we propose Gaussian-Det which leverages
Gaussian Splatting as surface representation for multi-view based 3D object
detection. Unlike existing monocular or NeRF-based methods which depict the
objects via discrete positional data, Gaussian-Det models the objects in a
continuous manner by formulating the input Gaussians as feature descriptors on
a mass of partial surfaces. Furthermore, to address the numerous outliers
inherently introduced by Gaussian splatting, we accordingly devise a Closure
Inferring Module (CIM) for the comprehensive surface-based objectness
deduction. CIM firstly estimates the probabilistic feature residuals for
partial surfaces given the underdetermined nature of Gaussian Splatting, which
are then coalesced into a holistic representation on the overall surface
closure of the object proposal. In this way, the surface information
Gaussian-Det exploits serves as the prior on the quality and reliability of
objectness and the information basis of proposal refinement. Experiments on
both synthetic and real-world datasets demonstrate that Gaussian-Det
outperforms various existing approaches, in terms of both average precision and
recall.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Zero-Shot Learning for Visual Dehazing of Urological Surgical
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renkai Wu, Xianjin Wang, Pengchen Liang, Zhenyu Zhang, Qing Chang, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot-assisted surgery has profoundly influenced current forms of minimally
invasive surgery. However, in transurethral suburethral urological surgical
robots, they need to work in a liquid environment. This causes vaporization of
the liquid when shearing and heating is performed, resulting in bubble
atomization that affects the visual perception of the robot. This can lead to
the need for uninterrupted pauses in the surgical procedure, which makes the
surgery take longer. To address the atomization characteristics of liquids
under urological surgical robotic vision, we propose an unsupervised zero-shot
dehaze method (RSF-Dehaze) for urological surgical robotic vision.
Specifically, the proposed Region Similarity Filling Module (RSFM) of
RSF-Dehaze significantly improves the recovery of blurred region tissues. In
addition, we organize and propose a dehaze dataset for robotic vision in
urological surgery (USRobot-Dehaze dataset). In particular, this dataset
contains the three most common urological surgical robot operation scenarios.
To the best of our knowledge, we are the first to organize and propose a
publicly available dehaze dataset for urological surgical robot vision. The
proposed RSF-Dehaze proves the effectiveness of our method in three urological
surgical robot operation scenarios with extensive comparative experiments with
20 most classical and advanced dehazing and image recovery algorithms. The
proposed source code and dataset are available at
https://github.com/wurenkai/RSF-Dehaze .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Signal Adversarial Examples <span class="highlight-title">Generation</span> for Signal Detection Network via
  White-Box Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Li, Linyuan Wang, Guangwei Xiong, Bin Yan, Dekui Ma, Jinxian Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development and application of deep learning in signal detection
tasks, the vulnerability of neural networks to adversarial attacks has also
become a security threat to signal detection networks. This paper defines a
signal adversarial examples generation model for signal detection network from
the perspective of adding perturbations to the signal. The model uses the
inequality relationship of L2-norm between time domain and time-frequency
domain to constrain the energy of signal perturbations. Building upon this
model, we propose a method for generating signal adversarial examples utilizing
gradient-based attacks and Short-Time Fourier Transform. The experimental
results show that under the constraint of signal perturbation energy ratio less
than 3%, our adversarial attack resulted in a 28.1% reduction in the mean
Average Precision (mAP), a 24.7% reduction in recall, and a 30.4% reduction in
precision of the signal detection network. Compared to random noise
perturbation of equivalent intensity, our adversarial attack demonstrates a
significant attack effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, submitted to Mobile Networks and Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Cancer Likeness: A Statistical Approach for Pathological
  <span class="highlight-title">Image</span> Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toshiki Kindo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new statistical approach to automatically
identify cancer regions in pathological images. The proposed method is built
from statistical theory in line with evidence-based medicine. The two core
technologies are the classification information of image features, which was
introduced based on information theory and which cancer features take positive
values, normal features take negative values, and the calculation technique for
determining their spatial distribution. This method then estimates areas where
the classification information content shows a positive value as cancer areas
in the pathological image. The method achieves AUCs of 0.95 or higher in cancer
classification tasks. In addition, the proposed method has the practical
advantage of not requiring a precise demarcation line between cancer and
normal. This frees pathologists from the monotonous and tedious work of
building consensus with other pathologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Physics From Video: Unsupervised Physical Parameter Estimation
  for Continuous Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Castañeda Garcia, Jan van Gemert, Daan Brinks, Nergis Tömen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting physical dynamical system parameters from videos is of great
interest to applications in natural science and technology. The
state-of-the-art in automatic parameter estimation from video is addressed by
training supervised deep networks on large datasets. Such datasets require
labels, which are difficult to acquire. While some unsupervised techniques --
which depend on frame prediction -- exist, they suffer from long training
times, instability under different initializations, and are limited to
hand-picked motion problems. In this work, we propose a method to estimate the
physical parameters of any known, continuous governing equation from single
videos; our solution is suitable for different dynamical systems beyond motion
and is robust to initialization compared to previous approaches. Moreover, we
remove the need for frame prediction by implementing a KL-divergence-based loss
function in the latent space, which avoids convergence to trivial solutions and
reduces model size and compute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing the Latent <span class="highlight-title">Diffusion</span> Model for Training-Free <span class="highlight-title">Image</span> Style
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Masui, Mayu Otani, Masahiro Nomura, Hideki Nakayama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently shown the ability to generate high-quality
images. However, controlling its generation process still poses challenges. The
image style transfer task is one of those challenges that transfers the visual
attributes of a style image to another content image. Typical obstacle of this
task is the requirement of additional training of a pre-trained model. We
propose a training-free style transfer algorithm, Style Tracking Reverse
Diffusion Process (STRDP) for a pretrained Latent Diffusion Model (LDM). Our
algorithm employs Adaptive Instance Normalization (AdaIN) function in a
distinct manner during the reverse diffusion process of an LDM while tracking
the encoding history of the style image. This algorithm enables style transfer
in the latent space of LDM for reduced computational cost, and provides
compatibility for various LDM models. Through a series of experiments and a
user study, we show that our method can quickly transfer the style of an image
without additional training. The speed, compatibility, and training-free aspect
of our algorithm facilitates agile experiments with combinations of styles and
LDMs for extensive application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anti-biofouling Lensless Camera System with Deep Learning based <span class="highlight-title">Image</span>
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Ide, Tomohiro Kawahara, Hiroshi Ueno, Daiki Yanagidaira, Susumu Takatsuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been an increasing demand for underwater cameras
that monitor the condition of offshore structures and check the number of
individuals in aqua culture environments with long-period observation. One of
the significant issues with this observation is that biofouling sticks to the
aperture and lens densely and prevents cameras from capturing clear images.
This study examines an underwater camera that applies material technologies
with high inherent resistance to biofouling and computer vision technologies
based on image reconstruction by deep learning to lens-less cameras. For this
purpose, our prototype camera uses a coded aperture with 1k rectangular shape
pinholes in a thin metal plate, such as copper, which hinder the growth of
biofouling and keep the surface clean. Although images taken by lens-less
cameras are usually not well formed due to lack of the traditional glass-based
lens, a deep learning approach using ViT (Vision Transformer) has recently
demonstrated reconstructing original photo images well and our study shows that
using gated MLP (Multilayer Perceptron) also yields good results. On the other
hand, a certain degree of thickness for bio-repellence materials is required to
exhibit their effect the thickness of aperture is necessary to use apertures
sufficiently thinner than the size of the pinholes to avoid unintentional
reflection and absorption on the sidewalls. Therefore, we prepared a
sufficiently thin plate for image reconstruction and now currently we conduct
tests of the lens-less camera of the bio-repellence aperture with actual
seawater environments to determine whether it can sufficiently demonstrate the
biofouling effect compared with usual camera with only waterproof.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, Ocean Optics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-quality Animatable Eyelid Shapes from Lightweight Captures <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Lyu, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality eyelid reconstruction and animation are challenging for the
subtle details and complicated deformations. Previous works usually suffer from
the trade-off between the capture costs and the quality of details. In this
paper, we propose a novel method that can achieve detailed eyelid
reconstruction and animation by only using an RGB video captured by a mobile
phone. Our method utilizes both static and dynamic information of eyeballs
(e.g., positions and rotations) to assist the eyelid reconstruction,
cooperating with an automatic eyeball calibration method to get the required
eyeball parameters. Furthermore, we develop a neural eyelid control module to
achieve the semantic animation control of eyelids. To the best of our
knowledge, we present the first method for high-quality eyelid reconstruction
and animation from lightweight captures. Extensive experiments on both
synthetic and real data show that our method can provide more detailed and
realistic results compared with previous methods based on the same-level
capture setups. The code is available at https://github.com/StoryMY/AniEyelid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGGRAPH Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Generalizable Vision-Language Robotic Manipulation: A <span class="highlight-title">Benchmark</span>
  and <span class="highlight-title">LLM</span>-guided 3D Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Garcia, Shizhe Chen, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizing language-conditioned robotic policies to new tasks remains a
significant challenge, hampered by the lack of suitable simulation benchmarks.
In this paper, we address this gap by introducing GemBench, a novel benchmark
to assess generalization capabilities of vision-language robotic manipulation
policies. GemBench incorporates seven general action primitives and four levels
of generalization, spanning novel placements, rigid and articulated objects,
and complex long-horizon tasks. We evaluate state-of-the-art approaches on
GemBench and also introduce a new method. Our approach 3D-LOTUS leverages rich
3D information for action prediction conditioned on language. While 3D-LOTUS
excels in both efficiency and performance on seen tasks, it struggles with
novel tasks. To address this, we present 3D-LOTUS++, a framework that
integrates 3D-LOTUS's motion planning capabilities with the task planning
capabilities of LLMs and the object grounding accuracy of VLMs. 3D-LOTUS++
achieves state-of-the-art performance on novel tasks of GemBench, setting a new
standard for generalization in robotic manipulation. The benchmark, codes and
trained models are available at
\url{https://www.di.ens.fr/willow/research/gembench/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training
  with Limited Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01445v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01445v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyuan Wei, Fanjiang Ye, Ori Yonay, Xingyu Chen, Baixi Sun, Dingwen Tao, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing studies of training state-of-the-art Contrastive Language-Image
Pretraining (CLIP) models on large-scale data involve hundreds of or even
thousands of GPUs due to the requirement of a large batch size. However, such a
large amount of resources is not accessible to most people. While advanced
compositional optimization techniques for optimizing global contrastive losses
have been demonstrated effective for removing the requirement of large batch
size, their performance on large-scale data remains underexplored and not
optimized. To bridge the gap, this paper explores several aspects of CLIP
training with limited resources (e.g., up to tens of GPUs). First, we introduce
FastCLIP, a general CLIP training framework built on advanced compositional
optimization techniques while designed and optimized for the distributed
setting. Our framework is equipped with an efficient gradient reduction
strategy to reduce communication overhead. Second, to further boost training
efficiency, we investigate three components of the framework from an
optimization perspective: the schedule of the inner learning rate, the update
rules of the temperature parameter and the model parameters, respectively.
Experiments on different strategies for each component shed light on how to
conduct CLIP training more efficiently. Finally, we benchmark the performance
of FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different
compute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7
million, 9.1 million to 315 million image-text pairs to demonstrate the
significant improvement of FastCLIP in the resource-limited setting. We release
the code of FastCLIP at https://github.com/Optimization-AI/fast_clip .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Test-Time Adaptation with State-Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mona Schirmer, Dan Zhang, Eric Nalisnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution shifts between training and test data are inevitable over the
lifecycle of a deployed model, leading to performance decay. Adapting a model
on test samples can help mitigate this drop in performance. However, most
test-time adaptation methods have focused on synthetic corruption shifts,
leaving a variety of distribution shifts underexplored. In this paper, we focus
on distribution shifts that evolve gradually over time, which are common in the
wild but challenging for existing methods, as we show. To address this, we
propose STAD, a probabilistic state-space model that adapts a deployed model to
temporal distribution shifts by learning the time-varying dynamics in the last
set of hidden features. Without requiring labels, our model infers
time-evolving class prototypes that act as a dynamic classification head.
Through experiments on real-world temporal distribution shifts, we show that
our method excels in handling small batch sizes and label shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time
  Series Forecasters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Joy Wang, Jianling Sun, Chenghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have emerged as a promising approach in time series
forecasting (TSF). Existing approaches either repurpose large language models
(LLMs) or build large-scale time series datasets to develop TSF foundation
models for universal forecasting. However, these methods face challenges due to
the severe cross-domain gap or in-domain heterogeneity. This paper explores a
new road to building a TSF foundation model from rich, high-quality natural
images. Our key insight is that a visual masked autoencoder, pre-trained on the
ImageNet dataset, can naturally be a numeric series forecaster. By
reformulating TSF as an image reconstruction task, we bridge the gap between
image pre-training and TSF downstream tasks. Surprisingly, without further
adaptation in the time-series domain, the proposed VisionTS could achieve
superior zero-shot forecasting performance compared to existing TSF foundation
models. With fine-tuning for one epoch, VisionTS could further improve the
forecasting and achieve state-of-the-art performance in most cases. Extensive
experiments reveal intrinsic similarities between images and real-world time
series, suggesting visual models may offer a ``free lunch'' for TSF and
highlight the potential for future cross-modality research. Our code is
publicly available at https://github.com/Keytoyze/VisionTS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: add more experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-skill Transferability-based Data Selection for Large
  <span class="highlight-title">Vision-Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Lee, Boyang Li, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning, or supervised finetuning on extensive task-specific data,
is necessary for Large Vision-Language Models (LVLMs) to generalize well across
a broad range of vision-language (VL) tasks. However, training on large VL
datasets can become prohibitively expensive. In this work, we introduce
COINCIDE, an effective and scalable data selection technique that uses a small
model as a reference model to select visual instruction tuning data for
efficient finetuning of a target LVLM, focusing on diversity and
transferability. Specifically, we cluster the training data using internal
activations from a small model, which identifies VL concept-skill compositions
needed by a target LVLM. We then sample data from these diverse clusters by
considering their density and transferability, or the ability to transfer well
to other concept-skill compositions. This approach ensures the diversity of
these compositions, which is vital for LVLM generalization. Extensive
experiments demonstrate that COINCIDE achieves superior performance and data
selection efficiency against 8 strong baselines on two distinct datasets:
LLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE
achieves performance comparable to the LVLM finetuned on the whole dataset,
with 70% reduction of the wall-clock running time. On the Vision-Flan dataset,
our method achieves superior results with only 16.7% of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian
  Splatting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in 3D Gaussian splatting (3D-GS) have not only
facilitated real-time rendering through modern GPU rasterization pipelines but
have also attained state-of-the-art rendering quality. Nevertheless, despite
its exceptional rendering quality and performance on standard datasets, 3D-GS
frequently encounters difficulties in accurately modeling specular and
anisotropic components. This issue stems from the limited ability of spherical
harmonics (SH) to represent high-frequency information. To overcome this
challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic
spherical Gaussian (ASG) appearance field instead of SH for modeling the
view-dependent appearance of each 3D Gaussian. Additionally, we have developed
a coarse-to-fine training strategy to improve learning efficiency and eliminate
floaters caused by overfitting in real-world scenes. Our experimental results
demonstrate that our method surpasses existing approaches in terms of rendering
quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to
model scenes with specular and anisotropic components without increasing the
number of 3D Gaussians. This improvement extends the applicability of 3D GS to
handle intricate scenarios with specular and anisotropic surfaces. Project page
is https://ingra14m.github.io/Spec-Gaussian-website/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding the Robustness of <span class="highlight-title">Diffusion</span>-Based Purification: A
  Stochastic Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Liu, Kezhao Liu, Yao Xiao, Ziyi Dong, Xiaogang Xu, Pengxu Wei, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-Based Purification (DBP) has emerged as an effective defense
mechanism against adversarial attacks. The efficacy of DBP has been attributed
to the forward diffusion process, which narrows the distribution gap between
clean and adversarial images through the addition of Gaussian noise. Although
this explanation has some theoretical support, the significance of its
contribution to robustness remains unclear. In this paper, we argue that the
inherent stochasticity in the DBP process is the primary driver of its
robustness. To explore this, we introduce a novel Deterministic White-Box
(DW-box) evaluation protocol to assess robustness in the absence of
stochasticity and to analyze the attack trajectories and loss landscapes. Our
findings suggest that DBP models primarily leverage stochasticity to evade
effective attack directions, and their ability to purify adversarial
perturbations can be weak. To further enhance the robustness of DBP models, we
introduce Adversarial Denoising Diffusion Training (ADDT), which incorporates
classifier-guided adversarial perturbations into diffusion training, thereby
strengthening the DBP models' ability to purify adversarial perturbations.
Additionally, we propose Rank-Based Gaussian Mapping (RBGM) to make
perturbations more compatible with diffusion models. Experimental results
validate the effectiveness of ADDT. In conclusion, our study suggests that
future research on DBP can benefit from the perspective of decoupling the
stochasticity-based and purification-based robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3DSAM-adapter: Holistic adaptation of SAM from 2D to 3D for promptable
  tumor segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizhan Gong, Yuan Zhong, Wenao Ma, Jinpeng Li, Zhao Wang, Jingyang Zhang, Pheng-Ann Heng, Qi Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite that the segment anything model (SAM) achieved impressive results on
general-purpose semantic segmentation with strong generalization ability on
daily images, its demonstrated performance on medical image segmentation is
less precise and not stable, especially when dealing with tumor segmentation
tasks that involve objects of small sizes, irregular shapes, and low contrast.
Notably, the original SAM architecture is designed for 2D natural images,
therefore would not be able to extract the 3D spatial information from
volumetric medical data effectively. In this paper, we propose a novel
adaptation method for transferring SAM from 2D to 3D for promptable medical
image segmentation. Through a holistically designed scheme for architecture
modification, we transfer the SAM to support volumetric inputs while retaining
the majority of its pre-trained parameters for reuse. The fine-tuning process
is conducted in a parameter-efficient manner, wherein most of the pre-trained
parameters remain frozen, and only a few lightweight spatial adapters are
introduced and tuned. Regardless of the domain gap between natural and medical
data and the disparity in the spatial arrangement between 2D and 3D, the
transformer trained on natural images can effectively capture the spatial
patterns present in volumetric medical images with only lightweight
adaptations. We conduct experiments on four open-source tumor segmentation
datasets, and with a single click prompt, our model can outperform domain
state-of-the-art medical image segmentation models on 3 out of 4 tasks,
specifically by 8.25%, 29.87%, and 10.11% for kidney tumor, pancreas tumor,
colon cancer segmentation, and achieve similar performance for liver tumor
segmentation. We also compare our adaptation method with existing popular
adapters, and observed significant performance improvement on most datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dimensionality Reduction and Nearest Neighbors for Improving
  Out-of-Distribution Detection in Medical <span class="highlight-title">Image</span> Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        McKell Woodland, Nihil Patel, Austin Castelo, Mais Al Taie, Mohamed Eltaher, Joshua P. Yung, Tucker J. Netherton, Tiffany L. Calderone, Jessica I. Sanchez, Darrel W. Cleere, Ahmed Elsaiey, Nakul Gupta, David Victor, Laura Beretta, Ankit B. Patel, Kristy K. Brock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinically deployed deep learning-based segmentation models are known to fail
on data outside of their training distributions. While clinicians review the
segmentations, these models tend to perform well in most instances, which could
exacerbate automation bias. Therefore, detecting out-of-distribution images at
inference is critical to warn the clinicians that the model likely failed. This
work applied the Mahalanobis distance (MD) post hoc to the bottleneck features
of four Swin UNETR and nnU-net models that segmented the liver on T1-weighted
magnetic resonance imaging and computed tomography. By reducing the dimensions
of the bottleneck features with either principal component analysis or uniform
manifold approximation and projection, images the models failed on were
detected with high performance and minimal computational load. In addition,
this work explored a non-parametric alternative to the MD, a k-th nearest
neighbors distance (KNN). KNN drastically improved scalability and performance
over MD when both were applied to raw and average-pooled bottleneck features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:020. Expansion of
  "Dimensionality Reduction for Improving Out-of-Distribution Detection in
  Medical Image Segmentation" arXiv:2308.03723. Code available at
  https://github.com/mckellwoodland/dimen_reduce_mahal
  (https://zenodo.org/records/13881989)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Ming Cheng, Zirui Wang, Victor Adrian Prisacariu, Tristan Braud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We leverage 3D Gaussian Splatting (3DGS) as a scene representation and
propose a novel test-time camera pose refinement framework, GSLoc. This
framework enhances the localization accuracy of state-of-the-art absolute pose
regression and scene coordinate regression methods. The 3DGS model renders
high-quality synthetic images and depth maps to facilitate the establishment of
2D-3D correspondences. GSLoc obviates the need for training feature extractors
or descriptors by operating directly on RGB images, utilizing the 3D foundation
model, MASt3R, for precise 2D matching. To improve the robustness of our model
in challenging outdoor environments, we incorporate an exposure-adaptive module
within the 3DGS framework. Consequently, GSLoc enables efficient one-shot pose
refinement given a single RGB query and a coarse initial pose estimation. Our
proposed approach surpasses leading NeRF-based optimization methods in both
accuracy and runtime across indoor and outdoor visual localization benchmarks,
achieving new state-of-the-art accuracy on two indoor datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed a small bug in the first version and achieved new
  state-of-the-art accuracy. The project page is available at
  https://gsloc.active.vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Image</span> Copy Detection for <span class="highlight-title">Diffusion</span> Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wang, Yifan Sun, Zhentao Tan, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images produced by diffusion models are increasingly popular in digital
artwork and visual marketing. However, such generated images might replicate
content from existing ones and pose the challenge of content originality.
Existing Image Copy Detection (ICD) models, though accurate in detecting
hand-crafted replicas, overlook the challenge from diffusion models. This
motivates us to introduce ICDiff, the first ICD specialized for diffusion
models. To this end, we construct a Diffusion-Replication (D-Rep) dataset and
correspondingly propose a novel deep embedding method. D-Rep uses a
state-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000
image-replica pairs, which are manually annotated into 6 replication levels
ranging from 0 (no replication) to 5 (total replication). Our method,
PDF-Embedding, transforms the replication level of each image-replica pair into
a probability density function (PDF) as the supervision signal. The intuition
is that the probability of neighboring replication levels should be continuous
and smooth. Experimental results show that PDF-Embedding surpasses
protocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by
utilizing PDF-Embedding, we find that the replication ratios of well-known
diffusion models against an open-source gallery range from 10% to 20%. The
project is publicly available at https://icdiff.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Diet: Can Trimming PET/CT <span class="highlight-title">Dataset</span>s Enhance Lesion Segmentation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Jaus, Simon Reiß, Jens Klesiek, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we describe our approach to compete in the autoPET3 datacentric
track. While conventional wisdom suggests that larger datasets lead to better
model performance, recent studies indicate that excluding certain training
samples can enhance model accuracy. We find that in the autoPETIII dataset, a
model that is trained on the entire dataset exhibits undesirable
characteristics by producing a large number of false positives particularly for
PSMA-PETs. We counteract this by removing the easiest samples from the training
dataset as measured by the model loss before retraining from scratch. Using the
proposed approach we manage to drive down the false negative volume and improve
upon the baseline model in both false negative volume and dice score on the
preliminary test set. Code and pre-trained models are available at
github.com/alexanderjaus/autopet3_datadiet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometry-Aware Attenuation Learning for Sparse-View CBCT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhentao Liu, Yu Fang, Changjian Li, Han Wu, Yuan Liu, Dinggang Shen, Zhiming Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cone Beam Computed Tomography (CBCT) plays a vital role in clinical imaging.
Traditional methods typically require hundreds of 2D X-ray projections to
reconstruct a high-quality 3D CBCT image, leading to considerable radiation
exposure. This has led to a growing interest in sparse-view CBCT reconstruction
to reduce radiation doses. While recent advances, including deep learning and
neural rendering algorithms, have made strides in this area, these methods
either produce unsatisfactory results or suffer from time inefficiency of
individual optimization. In this paper, we introduce a novel geometry-aware
encoder-decoder framework to solve this problem. Our framework starts by
encoding multi-view 2D features from various 2D X-ray projections with a 2D CNN
encoder. Leveraging the geometry of CBCT scanning, it then back-projects the
multi-view 2D features into the 3D space to formulate a comprehensive
volumetric feature map, followed by a 3D CNN decoder to recover 3D CBCT image.
Importantly, our approach respects the geometric relationship between 3D CBCT
image and its 2D X-ray projections during feature back projection stage, and
enjoys the prior knowledge learned from the data population. This ensures its
adaptability in dealing with extremly sparse view inputs without individual
training, such as scenarios with only 5 or 10 X-ray projections. Extensive
evaluations on two simulated datasets and one real-world dataset demonstrate
exceptional reconstruction quality and time efficiency of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking the Potential: Multi-task Deep Learning for Spaceborne
  Quantitative Monitoring of Fugitive Methane Plumes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12870v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12870v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxin Si, Shiliang Fu, Wei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As global warming intensifies, increased attention is being paid to
monitoring fugitive methane emissions and detecting gas plumes from landfills.
We have divided methane emission monitoring into three subtasks: methane
concentration inversion, plume segmentation, and emission rate estimation.
Traditional algorithms face certain limitations: methane concentration
inversion typically employs the matched filter, which is sensitive to the
global spectrum distribution and prone to significant noise. There is scant
research on plume segmentation, with many studies depending on manual
segmentation, which can be subjective. The estimation of methane emission rate
frequently uses the IME algorithm, which necessitates meteorological
measurement data. Utilizing the WENT landfill site in Hong Kong along with
PRISMA hyperspectral satellite imagery, we introduce a novel deep
learning-based framework for quantitative methane emission monitoring from
remote sensing images that is grounded in physical simulation. We create
simulated methane plumes using large eddy simulation (LES) and various
concentration maps of fugitive emissions using the radiative transfer equation
(RTE), while applying augmentation techniques to construct a simulated PRISMA
dataset. We train a U-Net network for methane concentration inversion, a Mask
R-CNN network for methane plume segmentation, and a ResNet-50 network for
methane emission rate estimation. All three deep networks yield higher
validation accuracy compared to traditional algorithms. Furthermore, we combine
the first two subtasks and the last two subtasks to design multi-task learning
models, MTL-01 and MTL-02, both of which outperform single-task models in terms
of accuracy. Our research exemplifies the application of multi-task deep
learning to quantitative methane monitoring and can be generalized to a wide
array of methane monitoring tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dapeng Feng, Zhiqiang Chen, Yizhen Yin, Shipeng Zhong, Yuhua Qi, Hongbo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with
photorealistic scene reconstruction emerging as a key challenge. To address
this, we introduce Computational Alignment for Real-Time Gaussian Splatting
SLAM (CaRtGS), a novel method enhancing the efficiency and quality of
photorealistic scene reconstruction in real-time environments. Leveraging 3D
Gaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and
processing speed, which is crucial for scene photorealistic reconstruction. Our
approach tackles computational misalignment in Gaussian Splatting SLAM
(GS-SLAM) through an adaptive strategy that optimizes training, addresses
long-tail optimization, and refines densification. Experiments on Replica and
TUM-RGBD datasets demonstrate CaRtGS's effectiveness in achieving high-fidelity
rendering with fewer Gaussian primitives. This work propels SLAM towards
real-time, photorealistic dense rendering, significantly advancing
photorealistic scene representation. For the benefit of the research community,
we release the code on our project website:
https://dapengfeng.github.io/cartgs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Upon a thorough internal review, we have identified that our
  manuscript lacks proper citation for a critical expression within the
  methodology section. In this revised version, we add Taming-3DGS as a
  citation in the splat-wise backpropagation statement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span>$^2$: Dynamic 3D Content <span class="highlight-title">Generation</span> via Score Composition of
  Video and Multi-view <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02148v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02148v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Yang, Zijie Pan, Chun Gu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D generation are predominantly propelled by
improvements in 3D-aware image diffusion models. These models are pretrained on
Internet-scale image data and fine-tuned on massive 3D data, offering the
capability of producing highly consistent multi-view images. However, due to
the scarcity of synchronized multi-view video data, it remains challenging to
adapt this paradigm to 4D generation directly. Despite that, the available
video and 3D data are adequate for training video and multi-view diffusion
models separately that can provide satisfactory dynamic and geometric priors
respectively. To take advantage of both, this paper presents Diffusion$^2$, a
novel framework for dynamic 3D content creation that reconciles the knowledge
about geometric consistency and temporal smoothness from these models to
directly sample dense multi-view multi-frame images which can be employed to
optimize continuous 4D representation. Specifically, we design a simple yet
effective denoising strategy via score composition of pretrained video and
multi-view diffusion models based on the probability structure of the target
image array. To alleviate the potential conflicts between two heterogeneous
scores, we further introduce variance-reducing sampling via interpolated steps,
facilitating smooth and stable generation. Owing to the high parallelism of the
proposed image generation process and the efficiency of the modern 4D
reconstruction pipeline, our framework can generate 4D content within few
minutes. Notably, our method circumvents the reliance on expensive and
hard-to-scale 4D data, thereby having the potential to benefit from the scaling
of the foundation video and multi-view diffusion models. Extensive experiments
demonstrate the efficacy of our proposed framework in generating highly
seamless and consistent 4D assets under various types of conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evidence-based Match-status-Aware Gait Recognition for Out-of-Gallery
  Gait Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08007v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08007v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heming Du, Chen Liu, Ming Wang, Lincheng Li, Shunli Zhang, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing gait recognition methods typically identify individuals based on the
similarity between probe and gallery samples. However, these methods often
neglect the fact that the gallery may not contain identities corresponding to
the probes, leading to incorrect recognition.To identify Out-of-Gallery (OOG)
gait queries, we propose an Evidence-based Match-status-Aware Gait Recognition
(EMA-GR) framework. Inspired by Evidential Deep Learning (EDL), EMA-GR is
designed to quantify the uncertainty associated with the match status of
recognition. Thus, EMA-GR identifies whether the probe has a counterpart in the
gallery. Specifically, we adopt an evidence collector to gather match status
evidence from a recognition result pair and parameterize a Dirichlet
distribution over the gathered evidence, following the Dempster-Shafer Theory
of Evidence (DST). We measure the uncertainty and predict the match status of
the recognition results, and thus determine whether the probe is an OOG
query.To the best of our knowledge, our method is the first attempt to tackle
OOG queries in gait recognition. Moreover, EMA-GR is agnostic against gait
recognition methods and improves the robustness against OOG queries. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
on datasets with OOG queries, and can also generalize well to other
identity-retrieval tasks. Importantly, our method surpasses existing
state-of-the-art methods by a substantial margin, achieving a 51.26%
improvement when the OOG query rate is around 50% on OUMVLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Scanner Domain Shift on Deep Learning Performance in
  Medical Imaging: an Experimental Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Guo, Darui Lu, Gregory Szumel, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Medical images acquired using different scanners and protocols can
differ substantially in their appearance. This phenomenon, scanner domain
shift, can result in a drop in the performance of deep neural networks which
are trained on data acquired by one scanner and tested on another. This
significant practical issue is well-acknowledged, however, no systematic study
of the issue is available across different modalities and diagnostic tasks.
Materials and Methods: In this paper, we present a broad experimental study
evaluating the impact of scanner domain shift on convolutional neural network
performance for different automated diagnostic tasks. We evaluate this
phenomenon in common radiological modalities, including X-ray, CT, and MRI.
Results: We find that network performance on data from a different scanner is
almost always worse than on same-scanner data, and we quantify the degree of
performance drop across different datasets. Notably, we find that this drop is
most severe for MRI, moderate for X-ray, and quite small for CT, on average,
which we attribute to the standardized nature of CT acquisition systems which
is not present in MRI or X-ray. We also study how injecting varying amounts of
target domain data into the training set, as well as adding noise to the
training data, helps with generalization. Conclusion: Our results provide
extensive experimental evidence and quantification of the extent of performance
drop caused by scanner domain shift in deep learning across different
modalities, with the goal of guiding the future development of robust deep
learning models for medical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flex3D: Feed-Forward 3D <span class="highlight-title">Generation</span> With Flexible Reconstruction Model
  And Input View Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Han, Jianyuan Wang, Andrea Vedaldi, Philip Torr, Filippos Kokkinos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating high-quality 3D content from text, single images, or sparse view
images remains a challenging task with broad applications. Existing methods
typically employ multi-view diffusion models to synthesize multi-view images,
followed by a feed-forward process for 3D reconstruction. However, these
approaches are often constrained by a small and fixed number of input views,
limiting their ability to capture diverse viewpoints and, even worse, leading
to suboptimal generation results if the synthesized views are of poor quality.
To address these limitations, we propose Flex3D, a novel two-stage framework
capable of leveraging an arbitrary number of high-quality input views. The
first stage consists of a candidate view generation and curation pipeline. We
employ a fine-tuned multi-view image diffusion model and a video diffusion
model to generate a pool of candidate views, enabling a rich representation of
the target 3D object. Subsequently, a view selection pipeline filters these
views based on quality and consistency, ensuring that only the high-quality and
reliable views are used for reconstruction. In the second stage, the curated
views are fed into a Flexible Reconstruction Model (FlexRM), built upon a
transformer architecture that can effectively process an arbitrary number of
inputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane
representation, enabling efficient and detailed 3D generation. Through
extensive exploration of design and training strategies, we optimize FlexRM to
achieve superior performance in both reconstruction and generation tasks. Our
results demonstrate that Flex3D achieves state-of-the-art performance, with a
user study winning rate of over 92% in 3D generation tasks when compared to
several of the latest feed-forward 3D generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://junlinhan.github.io/projects/flex3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Invisible: Captioning Videos with Metaphors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Sumit Shekhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphors are a common communication tool used in our day-to-day life. The
detection and generation of metaphors in textual form have been studied
extensively but metaphors in other forms have been under-explored. Recent
studies have shown that Vision-Language (VL) models cannot understand visual
metaphors in memes and adverts. As of now, no probing studies have been done
that involve complex language phenomena like metaphors with videos. Hence, we
introduce a new VL task of describing the metaphors present in the videos in
our work. To facilitate this novel task, we construct and release a manually
created dataset with 705 videos and 2115 human-written captions, along with a
new metric called Average Concept Distance (ACD), to automatically evaluate the
creativity of the metaphors generated. We also propose a novel low-resource
video metaphor captioning system: GIT-LLaVA, which obtains comparable
performance to SoTA video language models on the proposed task. We perform a
comprehensive analysis of existing video language models on this task and
publish our dataset, models, and benchmark results to enable further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion meets Attention: Video Motion Prompts <span class="chip">ACML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03179v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03179v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qixiang Chen, Lei Wang, Piotr Koniusz, Tom Gedeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos contain rich spatio-temporal information. Traditional methods for
extracting motion, used in tasks such as action recognition, often rely on
visual contents rather than precise motion features. This phenomenon is
referred to as 'blind motion extraction' behavior, which proves inefficient in
capturing motions of interest due to a lack of motion-guided cues. Recently,
attention mechanisms have enhanced many computer vision tasks by effectively
highlighting salient visual areas. Inspired by this, we propose a modified
Sigmoid function with learnable slope and shift parameters as an attention
mechanism to modulate motion signals from frame differencing maps. This
approach generates a sequence of attention maps that enhance the processing of
motion-related video content. To ensure temporal continuity and smoothness of
the attention maps, we apply pair-wise temporal attention variation
regularization to remove unwanted motions (e.g., noise) while preserving
important ones. We then perform Hadamard product between each pair of attention
maps and the original video frames to highlight the evolving motions of
interest over time. These highlighted motions, termed video motion prompts, are
subsequently used as inputs to the model instead of the original video frames.
We formalize this process as a motion prompt layer and incorporate the
regularization term into the loss function to learn better motion prompts. This
layer serves as an adapter between the model and the video data, bridging the
gap between traditional 'blind motion extraction' and the extraction of
relevant motions of interest. We show that our lightweight, plug-and-play
motion prompt layer seamlessly integrates into models like SlowFast, X3D, and
TimeSformer, enhancing performance on benchmarks such as FineGym and MPII
Cooking 2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 16th Asian Conference on Machine Learning (ACML 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attack-Augmentation Mixing-Contrastive Skeletal Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binqian Xu, Xiangbo Shu, Jiachao Zhang, Rui Yan, Guo-Sen Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning, relying on effective positive and negative sample
pairs, is beneficial to learn informative skeleton representations in
unsupervised skeleton-based action recognition. To achieve these positive and
negative pairs, existing weak/strong data augmentation methods have to randomly
change the appearance of skeletons for indirectly pursuing semantic
perturbations. However, such approaches have two limitations: i) solely
perturbing appearance cannot well capture the intrinsic semantic information of
skeletons, and ii) randomly perturbation may change the original
positive/negative pairs to soft positive/negative ones. To address the above
dilemma, we start the first attempt to explore an attack-based augmentation
scheme that additionally brings in direct semantic perturbation, for
constructing hard positive pairs and further assisting in constructing hard
negative pairs. In particular, we propose a novel Attack-Augmentation
Mixing-Contrastive skeletal representation learning (A$^2$MC) to contrast hard
positive features and hard negative features for learning more robust skeleton
representations. In A$^2$MC, Attack-Augmentation (Att-Aug) is designed to
collaboratively perform targeted and untargeted perturbations of skeletons via
attack and augmentation respectively, for generating high-quality hard positive
features. Meanwhile, Positive-Negative Mixer (PNM) is presented to mix hard
positive features and negative features for generating hard negative features,
which are adopted for updating the mixed memory banks. Extensive experiments on
three public datasets demonstrate that A$^2$MC is competitive with the
state-of-the-art methods. The code will be accessible on A$^2$MC
(https://github.com/1xbq1/A2MC).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffSSD: A <span class="highlight-title">Diffusion</span>-Based <span class="highlight-title">Dataset</span> For Speech Forensics <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kratika Bhagtani, Amit Kumar Singh Yadav, Paolo Bestagini, Edward J. Delp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based speech generators are ubiquitous. These methods can generate
very high quality synthetic speech and several recent incidents report their
malicious use. To counter such misuse, synthetic speech detectors have been
developed. Many of these detectors are trained on datasets which do not include
diffusion-based synthesizers. In this paper, we demonstrate that existing
detectors trained on one such dataset, ASVspoof2019, do not perform well in
detecting synthetic speech from recent diffusion-based synthesizers. We propose
the Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of
about 200 hours of labeled speech, including synthetic speech generated by 8
diffusion-based open-source and 2 commercial generators. We also examine the
performance of existing synthetic speech detectors on DiffSSD in both
closed-set and open-set scenarios. The results highlight the importance of this
dataset in detecting synthetic speech generated from recent open-source and
commercial speech generators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArtVLM: Attribute Recognition Through Vision-Based Prefix Language
  Modeling <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04102v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04102v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Yicheng Zhu, Keren Ye, Junjie Ke, Jiahui Yu, Leonidas Guibas, Peyman Milanfar, Feng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing and disentangling visual attributes from objects is a foundation
to many computer vision applications. While large vision language
representations like CLIP had largely resolved the task of zero-shot object
recognition, zero-shot visual attribute recognition remains a challenge because
CLIP's contrastively-learned vision-language representation cannot effectively
capture object-attribute dependencies. In this paper, we target this weakness
and propose a sentence generation-based retrieval formulation for attribute
recognition that is novel in 1) explicitly modeling a to-be-measured and
retrieved object-attribute relation as a conditional probability graph, which
converts the recognition problem into a dependency-sensitive language-modeling
problem, and 2) applying a large pretrained Vision-Language Model (VLM) on this
reformulation and naturally distilling its knowledge of image-object-attribute
relations to use towards attribute recognition. Specifically, for each
attribute to be recognized on an image, we measure the visual-conditioned
probability of generating a short sentence encoding the attribute's relation to
objects on the image. Unlike contrastive retrieval, which measures likelihood
by globally aligning elements of the sentence to the image, generative
retrieval is sensitive to the order and dependency of objects and attributes in
the sentence. We demonstrate through experiments that generative retrieval
consistently outperforms contrastive retrieval on two visual reasoning
datasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual
Genome Attribute Ranking (VGARank).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2024. Contact: zhuwilliam[at]google[dot]com. GitHub:
  https://github.com/google-research/google-research/tree/master/attribute_with_prefixlm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $σ$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial
  Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Emanuele Cinà, Francesco Villani, Maura Pintor, Lea Schönherr, Battista Biggio, Marcello Pelillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the adversarial robustness of deep networks to gradient-based
attacks is challenging. While most attacks consider $\ell_2$- and
$\ell_\infty$-norm constraints to craft input perturbations, only a few
investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular,
$\ell_0$-norm attacks remain the least studied due to the inherent complexity
of optimizing over a non-convex and non-differentiable constraint. However,
evaluating adversarial robustness under these attacks could reveal weaknesses
otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm
attacks. In this work, we propose a novel $\ell_0$-norm attack, called
$\sigma$-zero, which leverages a differentiable approximation of the $\ell_0$
norm to facilitate gradient-based optimization, and an adaptive projection
operator to dynamically adjust the trade-off between loss minimization and
perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet
datasets, involving robust and non-robust models, show that $\sigma$-zero finds
minimum $\ell_0$-norm adversarial examples without requiring any time-consuming
hyperparameter tuning, and that it outperforms all competing sparse attacks in
terms of success rate, perturbation size, and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/Cinofix/sigma-zero-adversarial-attack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Organized Grouped Discrete Representation for Object-Centric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03553v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03553v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-Centric Learning (OCL) represents dense image or video pixels as
sparse object features. Representative methods utilize discrete representation
composed of Variational Autoencoder (VAE) template features to suppress
pixel-level information redundancy and guide object-level feature aggregation.
The most recent advancement, Grouped Discrete Representation (GDR), further
decomposes these template features into attributes. However, its naive channel
grouping as decomposition may erroneously group channels belonging to different
attributes together and discretize them as sub-optimal template attributes,
which losses information and harms expressivity. We propose Organized GDR
(OGDR) to organize channels belonging to the same attributes together for
correct decomposition from features into attributes. In unsupervised
segmentation experiments, OGDR is fully superior to GDR in augmentating
classical transformer-based OCL methods; it even improves state-of-the-art
diffusion-based ones. Codebook PCA and representation similarity analyses show
that compared with GDR, our OGDR eliminates redundancy and preserves
information better for guiding object representation learning. The source code
is available in the supplementary material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A method for supervoxel-wise association studies of age and other
  non-imaging variables from coronary computed tomography angiograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Öfverstedt, Elin Lundström, Göran Bergström, Joel Kullberg, Håkan Ahlström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of associations between an individual's age and imaging and
non-imaging data is an active research area that attempts to aid understanding
of the effects and patterns of aging. In this work we have conducted a
supervoxel-wise association study between both volumetric and tissue density
features in coronary computed tomography angiograms and the chronological age
of a subject, to understand the localized changes in morphology and tissue
density with age. To enable a supervoxel-wise study of volume and tissue
density, we developed a novel method based on image segmentation, inter-subject
image registration, and robust supervoxel-based correlation analysis, to
achieve a statistical association study between the images and age. We evaluate
the registration methodology in terms of the Dice coefficient for the heart
chambers and myocardium, and the inverse consistency of the transformations,
showing that the method works well in most cases with high overlap and inverse
consistency. In a sex-stratified study conducted on a subset of $n=1388$ images
from the SCAPIS study, the supervoxel-wise analysis was able to find localized
associations with age outside of the commonly segmented and analyzed
sub-regions, and several substantial differences between the sexes in the
association of age and volume.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WEEP: A method for spatial interpretation of weakly supervised CNN
  models in computational pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15238v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15238v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Sharma, Bojing Liu, Mattias Rantalainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning enables the modelling of high-resolution histopathology
whole-slide images (WSI). Weakly supervised learning of tile-level data is
typically applied for tasks where labels only exist on the patient or WSI level
(e.g. patient outcomes or histological grading). In this context, there is a
need for improved spatial interpretability of predictions from such models. We
propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model
interpretation. It provides a principled yet straightforward way to establish
the spatial area of WSI required for assigning a particular prediction label.
We demonstrate WEEP on a binary classification task in the area of breast
cancer computational pathology. WEEP is easy to implement, is directly
connected to the model-based decision process, and offers information relevant
to both research and diagnostic applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grouped Discrete Representation Guides Object-Centric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Similar to humans perceiving visual scenes as objects, Object-Centric
Learning (OCL) can abstract dense images or videos into sparse object-level
features. Transformer-based OCL handles complex textures well due to the
decoding guidance of discrete representation, obtained by discretizing noisy
features in image or video feature maps using template features from a
codebook. However, treating features as minimal units overlooks their composing
attributes, thus impeding model generalization; indexing features with natural
numbers loses attribute-level commonalities and characteristics, thus
diminishing heuristics for model convergence. We propose \textit{Grouped
Discrete Representation} (GDR) to address these issues by grouping features
into attributes and indexing them with tuple numbers. In extensive experiments
across different query initializations, dataset modalities, and model
architectures, GDR consistently improves convergence and generalizability.
Visualizations show that our method effectively captures attribute-level
information in features. The source code will be available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PerSense: Personalized Instance Segmentation in Dense <span class="highlight-title">Image</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ibraheem Siddiqui, Muhammad Umer Sheikh, Hassan Abid, Muhammad Haris Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large-scale pre-training, vision foundational models showcase
notable performance benefits. Recent segmentation algorithms for natural scenes
have advanced significantly. However, existing models still struggle to
automatically segment personalized instances in dense and crowded scenarios,
where severe occlusions, scale variations, and background clutter pose a
challenge to accurately delineate densely packed instances of the target
object. To address this, we propose PerSense, an end-to-end, training-free, and
model-agnostic one-shot framework for Personalized instance Segmentation in
dense images. We develop a new baseline capable of automatically generating
instance-level point prompts via proposing a novel Instance Detection Module
(IDM) that leverages density maps, encapsulating spatial distribution of
objects in an image. To mitigate false positives within generated point
prompts, we design Point Prompt Selection Module (PPSM). Both IDM and PPSM
transform density maps into personalized precise point prompts for
instance-level segmentation and offer a seamless integration in our
model-agnostic framework. We also introduce a feedback mechanism which enables
PerSense to improve the accuracy of density maps by automating the exemplar
selection process for density map generation. To promote algorithmic advances
and effective tools for this relatively underexplored task, we introduce
PerSense-D, a diverse dataset exclusive to personalized instance segmentation
in dense images. Our extensive experiments establish PerSense superiority in
dense scenarios by achieving an mIoU of 71.61% on PerSense-D, outperforming
recent SOTA models by significant margins of +47.16%, +42.27%, +8.83%, and
+5.69%. Additionally, our qualitative findings demonstrate the adaptability of
our framework to images captured in-the-wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report of PerSense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DITTO: Demonstration Imitation by Trajectory Transformation <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Heppert, Max Argus, Tim Welschehold, Thomas Brox, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching robots new skills quickly and conveniently is crucial for the
broader adoption of robotic systems. In this work, we address the problem of
one-shot imitation from a single human demonstration, given by an RGB-D video
recording. We propose a two-stage process. In the first stage we extract the
demonstration trajectory offline. This entails segmenting manipulated objects
and determining their relative motion in relation to secondary objects such as
containers. In the online trajectory generation stage, we first re-detect all
objects, then warp the demonstration trajectory to the current scene and
execute it on the robot. To complete these steps, our method leverages several
ancillary models, including those for segmentation, relative object pose
estimation, and grasp prediction. We systematically evaluate different
combinations of correspondence and re-detection methods to validate our design
decision across a diverse range of tasks. Specifically, we collect and
quantitatively test on demonstrations of ten different tasks including
pick-and-place tasks as well as articulated object manipulation. Finally, we
perform extensive evaluations on a real robot system to demonstrate the
effectiveness and utility of our approach in real-world scenarios. We make the
code publicly available at http://ditto.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 3 tables, accepted at IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with
  Parallel Spike-driven Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Liu, Jiankai Tang, Yongli Chen, Haoxiang Li, Jiahao Qi, Siwei Li, Kegang Wang, Jie Gan, Yuntao Wang, Hong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks (ANNs) can help camera-based remote
photoplethysmography (rPPG) in measuring cardiac activity and physiological
signals from facial videos, such as pulse wave, heart rate and respiration rate
with better accuracy. However, most existing ANN-based methods require
substantial computing resources, which poses challenges for effective
deployment on mobile devices. Spiking neural networks (SNNs), on the other
hand, hold immense potential for energy-efficient deep learning owing to their
binary and event-driven architecture. To the best of our knowledge, we are the
first to introduce SNNs into the realm of rPPG, proposing a hybrid neural
network (HNN) model, the Spiking-PhysFormer, aimed at reducing power
consumption. Specifically, the proposed Spiking-PhyFormer consists of an
ANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based
predictor head. First, to simplify the transformer block while preserving its
capacity to aggregate local and global spatio-temporal features, we design a
parallel spike transformer block to replace sequential sub-blocks.
Additionally, we propose a simplified spiking self-attention mechanism that
omits the value parameter without compromising the model's performance.
Experiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD
demonstrate that the proposed model achieves a 12.4\% reduction in power
consumption compared to PhysFormer. Additionally, the power consumption of the
transformer block is reduced by a factor of 12.2, while maintaining decent
performance as PhysFormer and other ANN-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Mingxuan Liu, Jiankai Tang and Yongli Chen are co-first authors of
  the article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphology-based non-rigid registration of coronary computed tomography
  and intravascular <span class="highlight-title">image</span>s through virtual catheter path optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Kadry, Abhishek Karmakar, Andreas Schuh, Kersten Peterson, Michiel Schaap, David Marlevi, Charles Taylor, Elazer Edelman, Farhad Nezami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary computed tomography angiography (CCTA) provides 3D information on
obstructive coronary artery disease, but cannot fully visualize high-resolution
features within the vessel wall. Intravascular imaging, in contrast, can
spatially resolve atherosclerotic in cross sectional slices, but is limited in
capturing 3D relationships between each slice. Co-registering CCTA and
intravascular images enables a variety of clinical research applications but is
time consuming and user-dependent. This is due to intravascular images
suffering from non-rigid distortions arising from irregularities in the imaging
catheter path. To address these issues, we present a morphology-based framework
for the rigid and non-rigid matching of intravascular images to CCTA images. To
do this, we find the optimal virtual catheter path that samples the coronary
artery in CCTA image space to recapitulate the coronary artery morphology
observed in the intravascular image. We validate our framework on a
multi-center cohort of 40 patients using bifurcation landmarks as ground truth
for longitudinal and rotational registration. Our registration approach
significantly outperforms other approaches for bifurcation alignment. By
providing a differentiable framework for multi-modal vascular co-registration,
our framework reduces the manual effort required to conduct large-scale
multi-modal clinical studies and enables the development of machine
learning-based co-registration approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions in Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HC-Mamba: Vision MAMBA with Hybrid Convolutional Techniques for Medical
  <span class="highlight-title">Image</span> Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05007v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05007v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic medical image segmentation technology has the potential to expedite
pathological diagnoses, thereby enhancing the efficiency of patient care.
However, medical images often have complex textures and structures, and the
models often face the problem of reduced image resolution and information loss
due to downsampling. To address this issue, we propose HC-Mamba, a new medical
image segmentation model based on the modern state space model Mamba.
Specifically, we introduce the technique of dilated convolution in the HC-Mamba
model to capture a more extensive range of contextual information without
increasing the computational cost by extending the perceptual field of the
convolution kernel. In addition, the HC-Mamba model employs depthwise separable
convolutions, significantly reducing the number of parameters and the
computational power of the model. By combining dilated convolution and
depthwise separable convolutions, HC-Mamba is able to process large-scale
medical image data at a much lower computational cost while maintaining a high
level of performance. We conduct comprehensive experiments on segmentation
tasks including organ segmentation and skin lesion, and conduct extensive
experiments on Synapse, ISIC17 and ISIC18 to demonstrate the potential of the
HC-Mamba model in medical image segmentation. The experimental results show
that HC-Mamba exhibits competitive performance on all these datasets, thereby
proving its effectiveness and usefulness in medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3figures, 3tabels, fixed data leak</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Scalability of Self-Training for Open-Vocabulary Temporal
  Action Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vocabulary size in temporal action localization (TAL) is limited by the
scarcity of large-scale annotated datasets. To overcome this, recent works
integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL
(OV-TAL). However, despite the success of VLMs trained on extensive datasets,
existing OV-TAL methods still rely on human-labeled TAL datasets of limited
size to train action localizers, limiting their generalizability. In this
paper, we explore the scalability of self-training with unlabeled YouTube
videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic
action localizer is trained on a human-labeled TAL dataset to generate
pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled
dataset is then used to train the localizer. Extensive experiments demonstrate
that leveraging web-scale videos in self-training significantly enhances the
generalizability of an action localizer. Additionally, we identify limitations
in existing OV-TAL evaluation schemes and propose a new benchmark for thorough
assessment. Finally, we showcase the TAL performance of the large multimodal
model Gemini-1.5 on our new benchmark. Code is released at
https://github.com/HYUNJS/STOV-TAL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional <span class="highlight-title">Diffusion</span> on Web-Scale <span class="highlight-title">Image</span> Pairs leads to Diverse <span class="highlight-title">Image</span>
  Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14857v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14857v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manoj Kumar, Neil Houlsby, Emiel Hoogeboom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating image variations, where a model produces variations of an input
image while preserving the semantic context has gained increasing attention.
Current image variation techniques involve adapting a text-to-image model to
reconstruct an input image conditioned on the same image. We first demonstrate
that a diffusion model trained to reconstruct an input image from frozen
embeddings, can reconstruct the image with minor variations. Second, inspired
by how text-to-image models learn from web-scale text-image pairs, we explore a
new pretraining strategy to generate image variations using a large collection
of image pairs. Our diffusion model \textit{Semantica} receives a random
(encoded) image from a webpage as conditional input and denoises another noisy
random image from the same webpage. We carefully examine various design choices
for the image encoder, given its crucial role in extracting relevant context
from the input image. Once trained, \textit{Semantica} can adaptively generate
new images from a dataset by simply using images from that dataset as input.
Finally, we identify limitations in standard image consistency metrics for
evaluating image variations and propose alternative metrics based on few-shot
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamCatalyst: Fast and High-Quality 3D <span class="highlight-title">Editing</span> via Controlling
  Editability and Identity Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwook Kim, Seonho Lee, Jaeyo Shin, Jiho Choi, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score distillation sampling (SDS) has emerged as an effective framework in
text-driven 3D editing tasks, leveraging diffusion models for 3D consistent
editing. However, existing SDS-based 3D editing methods suffer from long
training times and produce low-quality results. We identify that the root cause
of this performance degradation is their conflict with the sampling dynamics of
diffusion models. Addressing this conflict allows us to treat SDS as a
diffusion reverse process for 3D editing via sampling from data space. In
contrast, existing methods naively distill the score function using diffusion
models. From these insights, we propose DreamCatalyst, a novel framework that
considers these sampling dynamics in the SDS framework. Specifically, we devise
the optimization process of our DreamCatalyst to approximate the diffusion
reverse process in editing tasks, thereby aligning with diffusion sampling
dynamics. As a result, DreamCatalyst successfully reduces training time and
improves editing quality. Our method offers two modes: (1) a fast mode that
edits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than
current state-of-the-art NeRF editing methods, and (2) a high-quality mode that
produces superior results about 8 times faster than these methods. Notably, our
high-quality mode outperforms current state-of-the-art NeRF editing methods in
terms of both speed and quality. DreamCatalyst also surpasses the
state-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing
itself as an effective and model-agnostic 3D editing solution. See more
extensive results on our project page: https://dream-catalyst.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ProjectPage: https://dream-catalyst.github.io Code:
  https://github.com/kaist-cvml/DreamCatalyst (Appendix included)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More precise edge detections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Edge detection (ED) is a base task in computer vision. While the
performance of the ED algorithm has been improved greatly by introducing
CNN-based models, current models still suffer from unsatisfactory precision
rates especially when only a low error toleration distance is allowed.
Therefore, model architecture for more precise predictions still needs an
investigation. On the other hand, the unavoidable noise training data provided
by humans would lead to unsatisfactory model predictions even when inputs are
edge maps themselves, which also needs a solution. In this paper, more precise
ED models are presented with cascaded skipping density blocks (CSDB). Our
models obtain state-of-the-art(SOTA) predictions in several datasets,
especially in average precision rate (AP), over a high-standard benchmark,
which is confirmed by extensive experiments. Also, a novel modification on data
augmentation for training is employed, which allows noiseless data to be
employed in model training for the first time, and thus further improves the
model performance. The relative Python codes can be found on
https://github.com/Hao-B-Shu/SDPED.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Context Window Extension: A New Perspective for Long Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongchen Wei, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have demonstrated impressive performance in
short video understanding tasks but face great challenges when applied to long
video understanding. In contrast, Large Language Models (LLMs) exhibit
outstanding capabilities in modeling long texts. Existing work attempts to
address this issue by introducing long video-text pairs during training.
However, these approaches require substantial computational and data resources.
In this paper, we tackle the challenge of long video understanding from the
perspective of context windows, aiming to apply LMMs to long video tasks
without retraining on long video datasets. We first conduct an in-depth
analysis of why pretrained LMMs struggle to understand lengthy video content,
identifying that discrepancies between visual and language modalities lead to
different context windows for visual and language tokens, making it difficult
to directly extend the visual tokens to match the language context window.
Based on this, we propose to adapt LMMs for long video understanding tasks by
extending the visual context window, eliminating the need for retraining on
large scalelong video datasets. To further mitigate the significant memory
consumption caused by long sequences, we introduce a progressive pooling
inference strategy that selectively adjusts the spatial resolution of frame
embeddings, reducing the number of visual tokens while retaining important
spatial information. Across multiple long video understanding benchmarks, our
method consistently improves the performance as the number of video frames
increases. On the MLVU benchmark, our method outperforms GPT-4o, even though
our model size is only 7B. Additionally, in the 256-frame setting, our method
reduces memory usage by approximately 45% compared to the baseline, without
introducing any performance loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PROXI: Challenging the GNNs for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astrit Tola, Jack Myrick, Baris Coskunuzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, Graph Neural Networks (GNNs) have transformed graph
representation learning. In the widely adopted message-passing GNN framework,
nodes refine their representations by aggregating information from neighboring
nodes iteratively. While GNNs excel in various domains, recent theoretical
studies have raised concerns about their capabilities. GNNs aim to address
various graph-related tasks by utilizing such node representations, however,
this one-size-fits-all approach proves suboptimal for diverse tasks.
  Motivated by these observations, we conduct empirical tests to compare the
performance of current GNN models with more conventional and direct methods in
link prediction tasks. Introducing our model, PROXI, which leverages proximity
information of node pairs in both graph and attribute spaces, we find that
standard machine learning (ML) models perform competitively, even outperforming
cutting-edge GNN models when applied to these proximity metrics derived from
node neighborhoods and attributes. This holds true across both homophilic and
heterophilic networks, as well as small and large benchmark datasets, including
those from the Open Graph Benchmark (OGB). Moreover, we show that augmenting
traditional GNNs with PROXI significantly boosts their link prediction
performance. Our empirical findings corroborate the previously mentioned
theoretical observations and imply that there exists ample room for enhancement
in current GNN models to reach their potential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the expressiveness and spectral bias of KANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Jonathan W. Siegel, Ziming Liu, Thomas Y. Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Networks (KAN) \cite{liu2024kan} were very recently
proposed as a potential alternative to the prevalent architectural backbone of
many deep learning models, the multi-layer perceptron (MLP). KANs have seen
success in various tasks of AI for science, with their empirical efficiency and
accuracy demostrated in function regression, PDE solving, and many more
scientific problems.
  In this article, we revisit the comparison of KANs and MLPs, with emphasis on
a theoretical perspective. On the one hand, we compare the representation and
approximation capabilities of KANs and MLPs. We establish that MLPs can be
represented using KANs of a comparable size. This shows that the approximation
and representation capabilities of KANs are at least as good as MLPs.
Conversely, we show that KANs can be represented using MLPs, but that in this
representation the number of parameters increases by a factor of the KAN grid
size. This suggests that KANs with a large grid size may be more efficient than
MLPs at approximating certain functions. On the other hand, from the
perspective of learning and optimization, we study the spectral bias of KANs
compared with MLPs. We demonstrate that KANs are less biased toward low
frequencies than MLPs. We highlight that the multi-level learning feature
specific to KANs, i.e. grid extension of splines, improves the learning process
for high-frequency components. Detailed comparisons with different choices of
depth, width, and grid sizes of KANs are made, shedding some light on how to
choose the hyperparameters in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient $1$-bit tensor approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex W. Neal Riasanovsky, Sarah El Kazdadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a spatially efficient decomposition of matrices and
arbitrary-order tensors as linear combinations of tensor products of $\{-1,
1\}$-valued vectors. For any matrix $A \in \mathbb{R}^{m \times n}$, $$A - R_w
= S_w C_w T_w^\top = \sum_{j=1}^w c_j \cdot \mathbf{s}_j \mathbf{t}_j^\top$$ is
a {\it $w$-width signed cut decomposition of $A$}. Here $C_w =
"diag"(\mathbf{c}_w)$ for some $\mathbf{c}_w \in \mathbb{R}^w,$ and $S_w, T_w$,
and the vectors $\mathbf{s}_j, \mathbf{t}_j$ are $\{-1, 1\}$-valued. To store
$(S_w, T_w, C_w)$, we may pack $w \cdot (m + n)$ bits, and require only $w$
floating point numbers. As a function of $w$, $\|R_w\|_F$ exhibits exponential
decay when applied to #f32 matrices with i.i.d. $\mathcal N (0, 1)$ entries.
Choosing $w$ so that $(S_w, T_w, C_w)$ has the same memory footprint as a
\textit{f16} or \textit{bf16} matrix, the relative error is comparable. Our
algorithm yields efficient signed cut decompositions in $20$ lines of
pseudocode. It reflects a simple modification from a celebrated 1999 paper [1]
of Frieze and Kannan. As a first application, we approximate the weight
matrices in the open \textit{Mistral-7B-v0.1} Large Language Model to a $50\%$
spatial compression. Remarkably, all $226$ remainder matrices have a relative
error $<6\%$ and the expanded model closely matches \textit{Mistral-7B-v0.1} on
the {\it huggingface} leaderboard [2]. Benchmark performance degrades slowly as
we reduce the spatial compression from $50\%$ to $25\%$. We optimize our open
source \textit{rust} implementation [3] with \textit{simd} instructions on
\textit{avx2} and \textit{avx512} architectures. We also extend our algorithm
from matrices to tensors of arbitrary order and use it to compress a picture of
the first author's cat Angus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, one cat picture reused a lot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Be<span class="highlight-title">llm</span>an <span class="highlight-title">Diffusion</span>: Generative Modeling as Learning a Linear Operator in
  the Distribution Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangming Li, Chieh-Hsin Lai, Carola-Bibiane Schönlieb, Yuki Mitsufuji, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Generative Models (DGMs), including Energy-Based Models (EBMs) and
Score-based Generative Models (SGMs), have advanced high-fidelity data
generation and complex continuous distribution approximation. However, their
application in Markov Decision Processes (MDPs), particularly in distributional
Reinforcement Learning (RL), remains underexplored, with conventional
histogram-based methods dominating the field. This paper rigorously highlights
that this application gap is caused by the nonlinearity of modern DGMs, which
conflicts with the linearity required by the Bellman equation in MDPs. For
instance, EBMs involve nonlinear operations such as exponentiating energy
functions and normalizing constants. To address this, we introduce Bellman
Diffusion, a novel DGM framework that maintains linearity in MDPs through
gradient and scalar field modeling. With divergence-based training techniques
to optimize neural network proxies and a new type of stochastic differential
equation (SDE) for sampling, Bellman Diffusion is guaranteed to converge to the
target distribution. Our empirical results show that Bellman Diffusion achieves
accurate field estimations and is a capable image generator, converging 1.5x
faster than the traditional histogram-based baseline in distributional RL
tasks. This work enables the effective integration of DGMs into MDP
applications, unlocking new avenues for advanced decision-making frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Driven Feature Selection and Engineering for Genotype Data
  with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Lee, Shu Yang, Jae Young Baik, Xiaoxi Liu, Zhen Tan, Dawei Li, Zixuan Wen, Bojian Hou, Duy Duong-Tran, Tianlong Chen, Li Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting phenotypes with complex genetic bases based on a small,
interpretable set of variant features remains a challenging task.
Conventionally, data-driven approaches are utilized for this task, yet the high
dimensional nature of genotype data makes the analysis and prediction
difficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and
their success in processing complex biomedical concepts, we set to examine the
ability of LLMs in feature selection and engineering for tabular genotype data,
with a novel knowledge-driven framework. We develop FREEFORM, Free-flow
Reasoning and Ensembling for Enhanced Feature Output and Robust Modeling,
designed with chain-of-thought and ensembling principles, to select and
engineer features with the intrinsic knowledge of LLMs. Evaluated on two
distinct genotype-phenotype datasets, genetic ancestry and hereditary hearing
loss, we find this framework outperforms several data-driven methods,
particularly on low-shot regimes. FREEFORM is available as open-source
framework at GitHub: https://github.com/PennShenLab/FREEFORM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thermodynamic Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxwell Aifer, Samuel Duffield, Kaelan Donatella, Denis Melanson, Phoebe Klett, Zach Belateche, Gavin Crooks, Antonio J. Martinez, Patrick J. Coles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fully Bayesian treatment of complicated predictive models (such as deep
neural networks) would enable rigorous uncertainty quantification and the
automation of higher-level tasks including model selection. However, the
intractability of sampling Bayesian posteriors over many parameters inhibits
the use of Bayesian methods where they are most needed. Thermodynamic computing
has emerged as a paradigm for accelerating operations used in machine learning,
such as matrix inversion, and is based on the mapping of Langevin equations to
the dynamics of noisy physical systems. Hence, it is natural to consider the
implementation of Langevin sampling algorithms on thermodynamic devices. In
this work we propose electronic analog devices that sample from Bayesian
posteriors by realizing Langevin dynamics physically. Circuit designs are given
for sampling the posterior of a Gaussian-Gaussian model and for Bayesian
logistic regression, and are validated by simulations. It is shown, under
reasonable assumptions, that the Bayesian posteriors for these models can be
sampled in time scaling with $\ln(d)$, where $d$ is dimension. For the
Gaussian-Gaussian model, the energy cost is shown to scale with $ d \ln(d)$.
These results highlight the potential for fast, energy-efficient Bayesian
inference using thermodynamic computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating on RLHF methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Kutalev, Sergei Markoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we investigate the alignment of Large Language Models
according to human preferences. We discuss the features of training a
Preference Model, which simulates human preferences, and the methods and
details we found essential for achieving the best results. We also discuss
using Reinforcement Learning to fine-tune Large Language Models and describe
the challenges we faced and the ways to overcome them. Additionally, we present
our experience with the Direct Preference Optimization method, which enables us
to align a Large Language Model with human preferences without creating a
separate Preference Model. As our contribution, we introduce the approach for
collecting a preference dataset through perplexity filtering, which makes the
process of creating such a dataset for a specific Language Model much easier
and more cost-effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning To Solve Differential Equation Constrained Optimization
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincenzo Di Vito, Mostafa Mohammadian, Kyri Baker, Ferdinando Fioretto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential equations (DE) constrained optimization plays a critical role in
numerous scientific and engineering fields, including energy systems, aerospace
engineering, ecology, and finance, where optimal configurations or control
strategies must be determined for systems governed by ordinary or stochastic
differential equations. Despite its significance, the computational challenges
associated with these problems have limited their practical use. To address
these limitations, this paper introduces a learning-based approach to
DE-constrained optimization that combines techniques from proxy optimization
and neural differential equations. The proposed approach uses a dual-network
architecture, with one approximating the control strategies, focusing on
steady-state constraints, and another solving the associated DEs. This
combination enables the approximation of optimal strategies while accounting
for dynamic constraints in near real-time. Experiments across problems in
energy optimization and finance modeling show that this method provides full
compliance with dynamic constraints and it produces results up to 25 times more
precise than other methods which do not explicitly model the system's dynamic
equations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-RAG: Enhanced Retrieval-Augmented <span class="highlight-title">Reasoning</span> with Open-Source Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, Md Rizwan Parvez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual
accuracy of Large Language Models (LLMs), but existing methods often suffer
from limited reasoning capabilities in effectively using the retrieved
evidence, particularly when using open-source LLMs. To mitigate this gap, we
introduce a novel framework, Open-RAG, designed to enhance reasoning
capabilities in RAG with open-source LLMs. Our framework transforms an
arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)
model capable of handling complex reasoning tasks, including both single- and
multi-hop queries. Open-RAG uniquely trains the model to navigate challenging
distractors that appear relevant but are misleading. As a result, Open-RAG
leverages latent learning, dynamically selecting relevant experts and
integrating external knowledge effectively for more accurate and contextually
relevant responses. In addition, we propose a hybrid adaptive retrieval method
to determine retrieval necessity and balance the trade-off between performance
gain and inference speed. Experimental results show that the Llama2-7B-based
Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,
Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source
our code and models at https://openragmoe.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings. Website:
  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composing Global Optimizers to <span class="highlight-title">Reasoning</span> Tasks via Algebraic Objects in
  Neural Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove rich algebraic structures of the solution space for 2-layer neural
networks with quadratic activation and $L_2$ loss, trained on reasoning tasks
in Abelian group (e.g., modular addition). Such a rich structure enables
analytical construction of global optimal solutions from partial solutions that
only satisfy part of the loss, despite its high nonlinearity. We coin the
framework as CoGO (Composing Global Optimizers). Specifically, we show that the
weight space over different numbers of hidden nodes of the 2-layer network is
equipped with a semi-ring algebraic structure, and the loss function to be
optimized consists of monomial potentials, which are ring homomorphism,
allowing partial solutions to be composed into global ones by ring addition and
multiplication. Our experiments show that around $95\%$ of the solutions
obtained by gradient descent match exactly our theoretical constructions.
Although the global optimizers constructed only required a small number of
hidden nodes, our analysis on gradient dynamics shows that
over-parameterization asymptotically decouples training dynamics and is
beneficial. We further show that training dynamics favors simpler solutions
under weight decay, and thus high-order global optimizers such as perfect
memorization are unfavorable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopER: Topological Embeddings in Graph Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astrit Tola, Funmilola Mary Taiwom, Cuneyt Gurcan Akcora, Baris Coskunuzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph embeddings play a critical role in graph representation learning,
allowing machine learning models to explore and interpret graph-structured
data. However, existing methods often rely on opaque, high-dimensional
embeddings, limiting interpretability and practical visualization.
  In this work, we introduce Topological Evolution Rate (TopER), a novel,
low-dimensional embedding approach grounded in topological data analysis. TopER
simplifies a key topological approach, Persistent Homology, by calculating the
evolution rate of graph substructures, resulting in intuitive and interpretable
visualizations of graph data. This approach not only enhances the exploration
of graph datasets but also delivers competitive performance in graph clustering
and classification tasks. Our TopER-based models achieve or surpass
state-of-the-art results across molecular, biological, and social network
datasets in tasks such as classification, clustering, and visualization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamical-generative downscaling of climate model ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ignacio Lopez-Gomez, Zhong Yi Wan, Leonardo Zepeda-Núñez, Tapio Schneider, John Anderson, Fei Sha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regional high-resolution climate projections are crucial for many
applications, such as agriculture, hydrology, and natural hazard risk
assessment. Dynamical downscaling, the state-of-the-art method to produce
localized future climate information, involves running a regional climate model
(RCM) driven by an Earth System Model (ESM), but it is too computationally
expensive to apply to large climate projection ensembles. We propose a novel
approach combining dynamical downscaling with generative artificial
intelligence to reduce the cost and improve the uncertainty estimates of
downscaled climate projections. In our framework, an RCM dynamically downscales
ESM output to an intermediate resolution, followed by a generative diffusion
model that further refines the resolution to the target scale. This approach
leverages the generalizability of physics-based models and the sampling
efficiency of diffusion models, enabling the downscaling of large multi-model
ensembles. We evaluate our method against dynamically-downscaled climate
projections from the CMIP6 ensemble. Our results demonstrate its ability to
provide more accurate uncertainty bounds on future regional climate than
alternatives such as dynamical downscaling of smaller ensembles, or traditional
empirical statistical downscaling methods. We also show that
dynamical-generative downscaling results in significantly lower errors than
bias correction and spatial disaggregation (BCSD), and captures more accurately
the spectra and multivariate correlations of meteorological fields. These
characteristics make the dynamical-generative framework a flexible, accurate,
and efficient way to downscale large ensembles of climate projections,
currently out of reach for pure dynamical downscaling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trained Transformer Classifiers Generalize and Exhibit Benign
  Overfitting In-Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer Frei, Gal Vardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have the capacity to act as supervised learning algorithms: by
properly encoding a set of labeled training ("in-context") examples and an
unlabeled test example into an input sequence of vectors of the same dimension,
the forward pass of the transformer can produce predictions for that unlabeled
test example. A line of recent work has shown that when linear transformers are
pre-trained on random instances for linear regression tasks, these trained
transformers make predictions using an algorithm similar to that of ordinary
least squares. In this work, we investigate the behavior of linear transformers
trained on random linear classification tasks. Via an analysis of the implicit
regularization of gradient descent, we characterize how many pre-training tasks
and in-context examples are needed for the trained transformer to generalize
well at test-time. We further show that in some settings, these trained
transformers can exhibit "benign overfitting in-context": when in-context
examples are corrupted by label flipping noise, the transformer memorizes all
of its in-context examples (including those with noisy labels) yet still
generalizes near-optimally for clean test examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Binary Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikash Singh, Matthew Khanzadeh, Vincent Davis, Harrison Rush, Emanuele Rossi, Jesse Shrader, Pietro Lio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Bayesian Binary Search (BBS), a novel probabilistic variant of the
classical binary search/bisection algorithm. BBS leverages machine
learning/statistical techniques to estimate the probability density of the
search space and modifies the bisection step to split based on probability
density rather than the traditional midpoint, allowing for the learned
distribution of the search space to guide the search algorithm. Search space
density estimation can flexibly be performed using supervised probabilistic
machine learning techniques (e.g., Gaussian process regression, Bayesian neural
networks, quantile regression) or unsupervised learning algorithms (e.g.,
Gaussian mixture models, kernel density estimation (KDE), maximum likelihood
estimation (MLE)). We demonstrate significant efficiency gains of using BBS on
both simulated data across a variety of distributions and in a real-world
binary search use case of probing channel balances in the Bitcoin Lightning
Network, for which we have deployed the BBS algorithm in a production setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Earth Surface Forecasting under Extreme Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar J. Pellicer-Valero, Miguel-Ángel Fernández-Torres, Chaonan Ji, Miguel D. Mahecha, Gustau Camps-Valls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With climate change-related extreme events on the rise, high dimensional
Earth observation data presents a unique opportunity for forecasting and
understanding impacts on ecosystems. This is, however, impeded by the
complexity of processing, visualizing, modeling, and explaining this data. To
showcase how this challenge can be met, here we train a convolutional long
short-term memory-based architecture on the novel DeepExtremeCubes dataset.
DeepExtremeCubes includes around 40,000 long-term Sentinel-2 minicubes (January
2016-October 2022) worldwide, along with labeled extreme events, meteorological
data, vegetation land cover, and topography map, sampled from locations
affected by extreme climate events and surrounding areas. When predicting
future reflectances and vegetation impacts through kernel normalized difference
vegetation index, the model achieved an R$^2$ score of 0.9055 in the test set.
Explainable artificial intelligence was used to analyze the model's predictions
during the October 2020 Central South America compound heatwave and drought
event. We chose the same area exactly one year before the event as
counterfactual, finding that the average temperature and surface pressure are
generally the best predictors under normal conditions. In contrast, minimum
anomalies of evaporation and surface latent heat flux take the lead during the
event. A change of regime is also observed in the attributions before the
event, which might help assess how long the event was brewing before happening.
The code to replicate all experiments and figures in this paper is publicly
available at https://github.com/DeepExtremes/txyXAI
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision-Focused Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Cortes-Gomez, Carlos Patiño, Yewon Byun, Steven Wu, Eric Horvitz, Bryan Wilder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing interest in ''decision-focused'' machine learning methods
which train models to account for how their predictions are used in downstream
optimization problems. Doing so can often improve performance on subsequent
decision problems. However, current methods for uncertainty quantification do
not incorporate any information at all about downstream decisions. We develop a
framework based on conformal prediction to produce prediction sets that account
for a downstream decision loss function, making them more appropriate to inform
high-stakes decision-making. Our approach harnesses the strengths of conformal
methods--modularity, model-agnosticism, and statistical coverage
guarantees--while incorporating downstream decisions and user-specified utility
functions. We prove that our methods retain standard coverage guarantees.
Empirical evaluation across a range of datasets and utility metrics
demonstrates that our methods achieve significantly lower decision loss
compared to standard conformal methods. Additionally, we present a real-world
use case in healthcare diagnosis, where our method effectively incorporates the
hierarchical structure of dermatological diseases. It successfully generates
sets with coherent diagnostic meaning, aiding the triage process during
dermatology diagnosis and illustrating how our method can ground high-stakes
decision-making on external domain knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegHeD: Segmentation of Heterogeneous Data for Multiple Sclerosis
  Lesions with Anatomical Constraints <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berke Doga Basaran, Xinru Zhang, Paul M. Matthews, Wenjia Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessment of lesions and their longitudinal progression from brain magnetic
resonance (MR) images plays a crucial role in diagnosing and monitoring
multiple sclerosis (MS). Machine learning models have demonstrated a great
potential for automated MS lesion segmentation. Training such models typically
requires large-scale high-quality datasets that are consistently annotated.
However, MS imaging datasets are often small, segregated across multiple sites,
with different formats (cross-sectional or longitudinal), and diverse
annotation styles. This poses a significant challenge to train a unified MS
lesion segmentation model. To tackle this challenge, we present SegHeD, a novel
multi-dataset multi-task segmentation model that can incorporate heterogeneous
data as input and perform all-lesion, new-lesion, as well as vanishing-lesion
segmentation. Furthermore, we account for domain knowledge about MS lesions,
incorporating longitudinal, spatial, and volumetric constraints into the
segmentation model. SegHeD is assessed on five MS datasets and achieves a high
performance in all, new, and vanishing-lesion segmentation, outperforming
several state-of-the-art methods in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, MICCAI, LDTM Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Protein Sequence and Expression Level to Analysis Molecular
  Characterization of Breast Cancer Subtypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Sholehrasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer's complexity and variability pose significant challenges in
understanding its progression and guiding effective treatment. This study aims
to integrate protein sequence data with expression levels to improve the
molecular characterization of breast cancer subtypes and predict clinical
outcomes. Using ProtGPT2, a language model designed for protein sequences, we
generated embeddings that capture the functional and structural properties of
proteins sequence. These embeddings were integrated with protein expression
level to form enriched biological representations, which were analyzed using
machine learning methods like ensemble K-means for clustering and XGBoost for
classification. Our approach enabled successful clustering of patients into
biologically distinct groups and accurately predicted clinical outcomes such as
survival and biomarkers status, achieving high performance metrics, notably an
F1 score of 0.88 for survival and 0.87 for biomarkers status prediction.
Analysis of feature importance highlighted key proteins like KMT2C, GCN1, and
CLASP2, linked to hormone receptor and Human Epidermal Growth Factor Receptor 2
(HER2) expression, which play a role in tumor progression and patient outcomes,
respectively. Furthermore, protein-protein interaction networks and correlation
analyses revealed the interdependence of proteins that may influence breast
cancer subtype behaviors. These findings suggest that integrating protein
sequence and expression data provides valuable insights into tumor biology and
has significant potential to enhance personalized treatment strategies in
breast cancer care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TorchSISSO: A PyTorch-Based Implementation of the Sure Independence
  Screening and Sparsifying Operator for Efficient and Interpretable Model
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhav Muthyala, Farshud Sorourifar, Joel A. Paulson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic regression (SR) is a powerful machine learning approach that
searches for both the structure and parameters of algebraic models, offering
interpretable and compact representations of complex data. Unlike traditional
regression methods, SR explores progressively complex feature spaces, which can
uncover simple models that generalize well, even from small datasets. Among SR
algorithms, the Sure Independence Screening and Sparsifying Operator (SISSO)
has proven particularly effective in the natural sciences, helping to
rediscover fundamental physical laws as well as discover new interpretable
equations for materials property modeling. However, its widespread adoption has
been limited by performance inefficiencies and the challenges posed by its
FORTRAN-based implementation, especially in modern computing environments. In
this work, we introduce TorchSISSO, a native Python implementation built in the
PyTorch framework. TorchSISSO leverages GPU acceleration, easy integration, and
extensibility, offering a significant speed-up and improved accuracy over the
original. We demonstrate that TorchSISSO matches or exceeds the performance of
the original SISSO across a range of tasks, while dramatically reducing
computational time and improving accessibility for broader scientific
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All <span class="highlight-title">LLM</span> Reasoners Are Created Equal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arian Hosseini, Alessandro Sordoni, Daniel Toyama, Aaron Courville, Rishabh Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the depth of grade-school math (GSM) problem-solving capabilities of
LLMs. To this end, we evaluate their performance on pairs of existing math word
problems together so that the answer to the second problem depends on correctly
answering the first problem. Our findings reveal a significant reasoning gap in
most LLMs, that is performance difference between solving the compositional
pairs and solving each question independently. This gap is more pronounced in
smaller, more cost-efficient, and math-specialized models. Moreover,
instruction-tuning recipes and code generation have varying effects across LLM
sizes, while finetuning on GSM can lead to task overfitting. Our analysis
indicates that large reasoning gaps are not because of test-set leakage, but
due to distraction from additional context and poor second-hop reasoning.
Overall, LLMs exhibit systematic differences in their reasoning abilities,
despite what their performance on standard benchmarks indicates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leray-Schauder Mappings for Operator Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Zappala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an algorithm for learning operators between Banach spaces, based
on the use of Leray-Schauder mappings to learn a finite-dimensional
approximation of compact subspaces. We show that the resulting method is a
universal approximator of (possibly nonlinear) operators. We demonstrate the
efficiency of the approach on two benchmark datasets showing it achieves
results comparable to state of the art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table. Comments are welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PreND: Enhancing Intrinsic Motivation in Reinforcement Learning through
  Pre-trained Network Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadamin Davoodabadi, Negin Hashemi Dijujin, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intrinsic motivation, inspired by the psychology of developmental learning in
infants, stimulates exploration in agents without relying solely on sparse
external rewards. Existing methods in reinforcement learning like Random
Network Distillation (RND) face significant limitations, including (1) relying
on raw visual inputs, leading to a lack of meaningful representations, (2) the
inability to build a robust latent space, (3) poor target network
initialization and (4) rapid degradation of intrinsic rewards. In this paper,
we introduce Pre-trained Network Distillation (PreND), a novel approach to
enhance intrinsic motivation in reinforcement learning (RL) by improving upon
the widely used prediction-based method, RND. PreND addresses these challenges
by incorporating pre-trained representation models into both the target and
predictor networks, resulting in more meaningful and stable intrinsic rewards,
while enhancing the representation learned by the model. We also tried simple
but effective variants of the predictor network optimization by controlling the
learning rate. Through experiments on the Atari domain, we demonstrate that
PreND significantly outperforms RND, offering a more robust intrinsic
motivation signal that leads to better exploration, improving overall
performance and sample efficiency. This research highlights the importance of
target and predictor networks representation in prediction-based intrinsic
motivation, setting a new direction for improving RL agents' learning
efficiency in sparse reward environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mimicking Human Intuition: Cognitive Belief-Driven Q-Learning <span class="chip">ICLR 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingrui Gu, Guanren Qiao, Chuyi Jiang, Tianqing Xia, Hangyu Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning encounters challenges in various environments related
to robustness and explainability. Traditional Q-learning algorithms cannot
effectively make decisions and utilize the historical learning experience. To
overcome these limitations, we propose Cognitive Belief-Driven Q-Learning
(CBDQ), which integrates subjective belief modeling into the Q-learning
framework, enhancing decision-making accuracy by endowing agents with
human-like learning and reasoning capabilities. Drawing inspiration from
cognitive science, our method maintains a subjective belief distribution over
the expectation of actions, leveraging a cluster-based subjective belief model
that enables agents to reason about the potential probability associated with
each decision. CBDQ effectively mitigates overestimated phenomena and optimizes
decision-making policies by integrating historical experiences with current
contextual information, mimicking the dynamics of human decision-making. We
evaluate the proposed method on discrete control benchmark tasks in various
complicate environments. The results demonstrate that CBDQ exhibits stronger
adaptability, robustness, and human-like characteristics in handling these
environments, outperforming other baselines. We hope this work will give
researchers a fresh perspective on understanding and explaining Q-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review by ICLR 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recursive Abstractive Processing for Retrieval in Dynamic <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charbel Chucri, Rami Azouz, Joachim Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent retrieval-augmented models enhance basic methods by building a
hierarchical structure over retrieved text chunks through recursive embedding,
clustering, and summarization. The most relevant information is then retrieved
from both the original text and generated summaries. However, such approaches
face limitations with dynamic datasets, where adding or removing documents over
time complicates the updating of hierarchical representations formed through
clustering. We propose a new algorithm to efficiently maintain the
recursive-abstractive tree structure in dynamic datasets, without compromising
performance. Additionally, we introduce a novel post-retrieval method that
applies query-focused recursive abstractive processing to substantially improve
context quality. Our method overcomes the limitations of other approaches by
functioning as a black-box post-retrieval layer compatible with any retrieval
algorithm. Both algorithms are validated through extensive experiments on
real-world datasets, demonstrating their effectiveness in handling dynamic data
and improving retrieval performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASeR: Learning to Adaptively Select Reward Models with Multi-Armed
  Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward Models (RMs) play a crucial role in aligning LLMs with human
preferences, enhancing their performance by ranking outputs during inference or
iterative training. However, the degree to which an RM generalizes to new tasks
is often not known a priori (e.g. some RMs may excel at scoring creative
writing vs. math reasoning). Therefore, using only one fixed RM while training
LLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs
simultaneously can be prohibitively computationally-intensive and challenging
due to conflicting signals from different RMs, potentially degrading
performance. To address these challenges, we introduce LASeR (Learning to
Adaptively Select Rewards), which iteratively trains LLMs using multiple RMs,
selecting and utilizing the most well-suited RM for each instance to rank
outputs and generate preference data, framed as a multi-armed bandit problem.
Our results on commonsense and math reasoning tasks demonstrate that LASeR can
boost iterative LLM optimization by optimizing for multiple RMs, improving the
absolute average accuracy of Llama-3-8B over three datasets by 2.67% over
training with ensemble RM scores while also showing superior training
efficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of
instruction-following prompts, we find that using Llama-3-8B LASeR leads to a
71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending
to long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an
average improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA
over random RM selection when used with best-of-n sampling. LASeR is robust to
noisy rewards and generalizes to multiple settings. Finally, LASeR's RM
selection changes depending on the underlying task or instance and we verify
the presence of conflicting preferences from multiple RMs that can be mitigated
using LASeR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages; First two authors contributed equally. Code:
  https://github.com/duykhuongnguyen/LASeR-MAB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Robustness of Reward Models for Mathematical <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghwan Kim, Dongjin Kang, Taeyoon Kwon, Hyungjoo Chae, Jungsoo Won, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are key in reinforcement learning from human feedback (RLHF)
systems, aligning the model behavior with human preferences. Particularly in
the math domain, there have been plenty of studies using reward models to align
policies for improving reasoning capabilities. Recently, as the importance of
reward models has been emphasized, RewardBench is proposed to understand their
behavior. However, we figure out that the math subset of RewardBench has
different representations between chosen and rejected completions, and relies
on a single comparison, which may lead to unreliable results as it only see an
isolated case. Therefore, it fails to accurately present the robustness of
reward models, leading to a misunderstanding of its performance and potentially
resulting in reward hacking. In this work, we introduce a new design for
reliable evaluation of reward models, and to validate this, we construct
RewardMATH, a benchmark that effectively represents the robustness of reward
models in mathematical reasoning tasks. We demonstrate that the scores on
RewardMATH strongly correlate with the results of optimized policy and
effectively estimate reward overoptimization, whereas the existing benchmark
shows almost no correlation. The results underscore the potential of our design
to enhance the reliability of evaluation, and represent the robustness of
reward model. We make our code and data publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Knowledge Concept Annotation and Question Representation
  Learning for Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilmazcan Ozyurt, Stefan Feuerriegel, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge tracing (KT) is a popular approach for modeling students' learning
progress over time, which can enable more personalized and adaptive learning.
However, existing KT approaches face two major limitations: (1) they rely
heavily on expert-defined knowledge concepts (KCs) in questions, which is
time-consuming and prone to errors; and (2) KT methods tend to overlook the
semantics of both questions and the given KCs. In this work, we address these
challenges and present KCQRL, a framework for automated knowledge concept
annotation and question representation learning that can improve the
effectiveness of any existing KT model. First, we propose an automated KC
annotation process using large language models (LLMs), which generates question
solutions and then annotates KCs in each solution step of the questions.
Second, we introduce a contrastive learning approach to generate semantically
rich embeddings for questions and solution steps, aligning them with their
associated KCs via a tailored false negative elimination approach. These
embeddings can be readily integrated into existing KT models, replacing their
randomly initialized embeddings. We demonstrate the effectiveness of KCQRL
across 15 KT algorithms on two large real-world Math learning datasets, where
we achieve consistent performance improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Theoretical Understanding of Synthetic Data in <span class="highlight-title">LLM</span>
  Post-Training: A Reverse-Bottleneck Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Gan, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data has become a pivotal resource in post-training tasks for large
language models (LLMs) due to the scarcity of high-quality, specific data.
While various methods have been developed to generate synthetic data, there
remains a discernible gap between the practical effects of synthetic data and
our theoretical comprehension. To address this challenge, we commence by
presenting a detailed modeling of the prevalent synthetic data generation
process. Building upon this modeling, we demonstrate that the generalization
capability of the post-trained model is critically determined by the
information gain derived from the generative model, as analyzed from a novel
reverse-bottleneck perspective. Moreover, we introduce the concept of
Generalization Gain via Mutual Information (GGMI) and elucidate the
relationship between generalization gain and information gain. This analysis
serves as a theoretical foundation for synthetic data generation and further
highlights its connection with the generalization capability of post-trained
models, offering an understanding about the design of synthetic data generation
techniques and the optimization of the post-training process. We open source
our code through an anonymous GitHub repository at
https://anonymous.4open.science/r/Understanding-Synthetic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-TTT: A Meta-learning Minimax Framework For Test-Time Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tao, Li Shen, Soumik Mondal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time domain adaptation is a challenging task that aims to adapt a
pre-trained model to limited, unlabeled target data during inference. Current
methods that rely on self-supervision and entropy minimization underperform
when the self-supervised learning (SSL) task does not align well with the
primary objective. Additionally, minimizing entropy can lead to suboptimal
solutions when there is limited diversity within minibatches. This paper
introduces a meta-learning minimax framework for test-time training on batch
normalization (BN) layers, ensuring that the SSL task aligns with the primary
task while addressing minibatch overfitting. We adopt a mixed-BN approach that
interpolates current test batch statistics with the statistics from source
domains and propose a stochastic domain synthesizing method to improve model
generalization and robustness to domain shifts. Extensive experiments
demonstrate that our method surpasses state-of-the-art techniques across
various domain adaptation and generalization benchmarks, significantly
enhancing the pre-trained model's robustness on unseen domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performant, Memory Efficient and Scalable Multi-<span class="highlight-title">Agent</span> Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omayma Mahjoub, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon du Toit, Jemma Daniel, Louay Ben Nessir, Louise Beyers, Claude Formanek, Liam Clark, Arnu Pretorius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the field of multi-agent reinforcement learning (MARL) progresses towards
larger and more complex environments, achieving strong performance while
maintaining memory efficiency and scalability to many agents becomes
increasingly important. Although recent research has led to several advanced
algorithms, to date, none fully address all of these key properties
simultaneously. In this work, we introduce Sable, a novel and theoretically
sound algorithm that adapts the retention mechanism from Retentive Networks to
MARL. Sable's retention-based sequence modelling architecture allows for
computationally efficient scaling to a large number of agents, as well as
maintaining a long temporal context, making it well-suited for large-scale
partially observable environments. Through extensive evaluations across six
diverse environments, we demonstrate how Sable is able to significantly
outperform existing state-of-the-art methods in the majority of tasks (34 out
of 45, roughly 75\%). Furthermore, Sable demonstrates stable performance as we
scale the number of agents, handling environments with more than a thousand
agents while exhibiting a linear increase in memory usage. Finally, we conduct
ablation studies to isolate the source of Sable's performance gains and confirm
its efficient computational memory usage. Our results highlight Sable's
performance and efficiency, positioning it as a leading approach to MARL at
scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOREL: Enhancing Adversarial Robustness through Multi-Objective
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedjro Salomon Hotegni, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive research has shown that deep neural networks (DNNs) are vulnerable
to slight adversarial perturbations$-$small changes to the input data that
appear insignificant but cause the model to produce drastically different
outputs. In addition to augmenting training data with adversarial examples
generated from a specific attack method, most of the current defense strategies
necessitate modifying the original model architecture components to improve
robustness or performing test-time data purification to handle adversarial
attacks. In this work, we demonstrate that strong feature representation
learning during training can significantly enhance the original model's
robustness. We propose MOREL, a multi-objective feature representation learning
approach, encouraging classification models to produce similar features for
inputs within the same class, despite perturbations. Our training method
involves an embedding space where cosine similarity loss and multi-positive
contrastive loss are used to align natural and adversarial features from the
model encoder and ensure tight clustering. Concurrently, the classifier is
motivated to achieve accurate predictions. Through extensive experiments, we
demonstrate that our approach significantly enhances the robustness of DNNs
against white-box and black-box adversarial attacks, outperforming other
methods that similarly require no architectural changes or test-time data
purification. Our code is available at https://github.com/salomonhotegni/MOREL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification with Bayesian Higher Order ReLU KANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Giroux, Cristiano Fanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the first method of uncertainty quantification in the domain of
Kolmogorov-Arnold Networks, specifically focusing on (Higher Order) ReLUKANs to
enhance computational efficiency given the computational demands of Bayesian
methods. The method we propose is general in nature, providing access to both
epistemic and aleatoric uncertainties. It is also capable of generalization to
other various basis functions. We validate our method through a series of
closure tests, including simple one-dimensional functions and application to
the domain of (Stochastic) Partial Differential Equations. Referring to the
latter, we demonstrate the method's ability to correctly identify functional
dependencies introduced through the inclusion of a stochastic term. The code
supporting this work can be found at
https://github.com/wmdataphys/Bayesian-HR-KAN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Positional Attention: Out-of-Distribution Generalization and
  Expressivity for Neural Algorithmic <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veličković, Kimon Fountoulakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a growing interest in the ability of neural networks to solve
algorithmic tasks, such as arithmetic, summary statistics, and sorting. While
state-of-the-art models like Transformers have demonstrated good generalization
performance on in-distribution tasks, their out-of-distribution (OOD)
performance is poor when trained end-to-end. In this paper, we focus on value
generalization, a common instance of OOD generalization where the test
distribution has the same input sequence length as the training distribution,
but the value ranges in the training and test distributions do not necessarily
overlap. To address this issue, we propose that using fixed positional
encodings to determine attention weights-referred to as positional
attention-enhances empirical OOD performance while maintaining expressivity. We
support our claim about expressivity by proving that Transformers with
positional attention can effectively simulate parallel algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mike Ranzinger, Jon Barker, Greg Heinrich, Pavlo Molchanov, Bryan Catanzaro, Andrew Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various visual foundation models have distinct strengths and weaknesses, both
of which can be improved through heterogeneous multi-teacher knowledge
distillation without labels, termed "agglomerative models." We build upon this
body of work by studying the effect of the teachers' activation statistics,
particularly the impact of the loss function on the resulting student model
quality. We explore a standard toolkit of statistical normalization techniques
to better align the different distributions and assess their effects. Further,
we examine the impact on downstream teacher-matching metrics, which motivates
the use of Hadamard matrices. With these matrices, we demonstrate useful
properties, showing how they can be used for isotropic standardization, where
each dimension of a multivariate distribution is standardized using the same
scale. We call this technique "PHI Standardization" (PHI-S) and empirically
demonstrate that it produces the best student model across the suite of methods
studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VinePPO: Unlocking RL Potential For <span class="highlight-title">LLM</span> <span class="highlight-title">Reasoning</span> Through Refined Credit
  Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly applied to complex reasoning
tasks that require executing several complex steps before receiving any reward.
Properly assigning credit to these steps is essential for enhancing model
performance. Proximal Policy Optimization (PPO), a state-of-the-art
reinforcement learning (RL) algorithm used for LLM finetuning, employs value
networks to tackle credit assignment. However, value networks face challenges
in predicting the expected cumulative rewards accurately in complex reasoning
tasks, often leading to high-variance updates and suboptimal performance. In
this work, we systematically evaluate the efficacy of value networks and reveal
their significant shortcomings in reasoning-heavy LLM tasks, showing that they
barely outperform a random baseline when comparing alternative steps. To
address this, we propose VinePPO, a straightforward approach that leverages the
flexibility of language environments to compute unbiased Monte Carlo-based
estimates, bypassing the need for large value networks. Our method consistently
outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with
fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These
results emphasize the importance of accurate credit assignment in RL finetuning
of LLM and demonstrate VinePPO's potential as a superior alternative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Covariance Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Cavallo, Zhan Gao, Elvin Isufi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Covariance Neural Networks (VNNs) perform graph convolutions on the
covariance matrix of tabular data and achieve success in a variety of
applications. However, the empirical covariance matrix on which the VNNs
operate may contain many spurious correlations, making VNNs' performance
inconsistent due to these noisy estimates and decreasing their computational
efficiency. To tackle this issue, we put forth Sparse coVariance Neural
Networks (S-VNNs), a framework that applies sparsification techniques on the
sample covariance matrix before convolution. When the true covariance matrix is
sparse, we propose hard and soft thresholding to improve covariance estimation
and reduce computational cost. Instead, when the true covariance is dense, we
propose stochastic sparsification where data correlations are dropped in
probability according to principled strategies. We show that S-VNNs are more
stable than nominal VNNs as well as sparse principal component analysis. By
analyzing the impact of sparsification on their behavior, we provide novel
connections between S-VNN stability and data distribution. We support our
theoretical findings with experimental results on various application
scenarios, ranging from brain data to human action recognition, and show an
improved task performance, stability, and computational efficiency of S-VNNs
compared with nominal VNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Generative Modeling with Improved Sample Efficiency through
  Sequential Greedy Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klaus-Rudolf Kladny, Bernhard Schölkopf, Michael Muehlebach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models lack rigorous statistical guarantees for their outputs and
are therefore unreliable in safety-critical applications. In this work, we
propose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a
sequential conformal prediction method producing prediction sets that satisfy a
rigorous statistical guarantee called conformal admissibility control. This
guarantee states that with high probability, the prediction sets contain at
least one admissible (or valid) example. To this end, our method first samples
an initial set of i.i.d. examples from a black box generative model. Then, this
set is iteratively pruned via so-called greedy filters. As a consequence of the
iterative generation procedure, admissibility of the final prediction set
factorizes as a Markov chain. This factorization is crucial, because it allows
to control each factor separately, using conformal prediction. In comparison to
prior work, our method demonstrates a large reduction in the number of
admissibility evaluations during calibration. This reduction is important in
safety-critical applications, where these evaluations must be conducted
manually by domain experts and are therefore costly and time consuming. We
highlight the advantages of our method in terms of admissibility evaluations
and cardinality of the prediction sets through experiments in natural language
generation and molecular graph extension tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smaller Confidence Intervals From IPW Estimators via Data-Dependent
  Coarsening <span class="chip">COLT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alkis Kalavasis, Anay Mehrotra, Manolis Zampetakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse propensity-score weighted (IPW) estimators are prevalent in causal
inference for estimating average treatment effects in observational studies.
Under unconfoundedness, given accurate propensity scores and $n$ samples, the
size of confidence intervals of IPW estimators scales down with $n$, and,
several of their variants improve the rate of scaling. However, neither IPW
estimators nor their variants are robust to inaccuracies: even if a single
covariate has an $\varepsilon>0$ additive error in the propensity score, the
size of confidence intervals of these estimators can increase arbitrarily.
Moreover, even without errors, the rate with which the confidence intervals of
these estimators go to zero with $n$ can be arbitrarily slow in the presence of
extreme propensity scores (those close to 0 or 1).
  We introduce a family of Coarse IPW (CIPW) estimators that captures existing
IPW estimators and their variants. Each CIPW estimator is an IPW estimator on a
coarsened covariate space, where certain covariates are merged. Under mild
assumptions, e.g., Lipschitzness in expected outcomes and sparsity of extreme
propensity scores, we give an efficient algorithm to find a robust estimator:
given $\varepsilon$-inaccurate propensity scores and $n$ samples, its
confidence interval size scales with $\varepsilon+1/\sqrt{n}$. In contrast,
under the same assumptions, existing estimators' confidence interval sizes are
$\Omega(1)$ irrespective of $\varepsilon$ and $n$. Crucially, our estimator is
data-dependent and we show that no data-independent CIPW estimator can be
robust to inaccuracies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the 37th Conference on Learning Theory
  (COLT) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and Consistent Graph Neural Networks for Distributed Mesh-based
  Data-driven Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Barwey, Riccardo Balin, Bethany Lusch, Saumil Patel, Ramesh Balakrishnan, Pinaki Pal, Romit Maulik, Venkatram Vishwanath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work develops a distributed graph neural network (GNN) methodology for
mesh-based modeling applications using a consistent neural message passing
layer. As the name implies, the focus is on enabling scalable operations that
satisfy physical consistency via halo nodes at sub-graph boundaries. Here,
consistency refers to the fact that a GNN trained and evaluated on one rank
(one large graph) is arithmetically equivalent to evaluations on multiple ranks
(a partitioned graph). This concept is demonstrated by interfacing GNNs with
NekRS, a GPU-capable exascale CFD solver developed at Argonne National
Laboratory. It is shown how the NekRS mesh partitioning can be linked to the
distributed GNN training and inference routines, resulting in a scalable
mesh-based data-driven modeling workflow. We study the impact of consistency on
the scalability of mesh-based GNNs, demonstrating efficient scaling in
consistent GNNs for up to O(1B) graph nodes on the Frontier exascale
supercomputer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Statistics With Unknown Truncation, Polynomial Time
  Algorithms, Beyond Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane H. Lee, Anay Mehrotra, Manolis Zampetakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the estimation of distributional parameters when samples are shown
only if they fall in some unknown set $S \subseteq \mathbb{R}^d$. Kontonis,
Tzamos, and Zampetakis (FOCS'19) gave a $d^{\mathrm{poly}(1/\varepsilon)}$ time
algorithm for finding $\varepsilon$-accurate parameters for the special case of
Gaussian distributions with diagonal covariance matrix. Recently, Diakonikolas,
Kane, Pittas, and Zarifis (COLT'24) showed that this exponential dependence on
$1/\varepsilon$ is necessary even when $S$ belongs to some well-behaved
classes. These works leave the following open problems which we address in this
work: Can we estimate the parameters of any Gaussian or even extend beyond
Gaussians? Can we design $\mathrm{poly}(d/\varepsilon)$ time algorithms when
$S$ is a simple set such as a halfspace?
  We make progress on both of these questions by providing the following
results:
  1. Toward the first question, we give a $d^{\mathrm{poly}(\ell/\varepsilon)}$
time algorithm for any exponential family that satisfies some structural
assumptions and any unknown set $S$ that is $\varepsilon$-approximable by
degree-$\ell$ polynomials. This result has two important applications:
  1a) The first algorithm for estimating arbitrary Gaussian distributions from
samples truncated to an unknown $S$; and
  1b) The first algorithm for linear regression with unknown truncation and
Gaussian features.
  2. To address the second question, we provide an algorithm with runtime
$\mathrm{poly}(d/\varepsilon)$ that works for a set of exponential families
(containing all Gaussians) when $S$ is a halfspace or an axis-aligned
rectangle.
  Along the way, we develop tools that may be of independent interest,
including, a reduction from PAC learning with positive and unlabeled samples to
PAC learning with positive and negative samples that is robust to certain
covariate shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the 65th IEEE Symposium on Foundations
  of Computer Science (FOCS), 2024; abstract shortened for arXiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending Contextual Self-Modulation: Meta-Learning Across Modalities,
  Task Dimensionalities, and Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roussel Desmond Nzoyem, David A. W. Barton, Tom Deakin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual Self-Modulation (CSM) is a potent regularization mechanism for the
Neural Context Flow (NCF) framework which demonstrates powerful meta-learning
of physical systems. However, CSM has limitations in its applicability across
different modalities and in high-data regimes. In this work, we introduce two
extensions: $i$CSM, which expands CSM to infinite-dimensional tasks, and
StochasticNCF, which improves scalability. These extensions are demonstrated
through comprehensive experimentation on a range of tasks, including dynamical
systems with parameter variations, computer vision challenges, and curve
fitting problems. $i$CSM embeds the contexts into an infinite-dimensional
function space, as opposed to CSM which uses finite-dimensional context
vectors. StochasticNCF enables the application of both CSM and $i$CSM to
high-data scenarios by providing an unbiased approximation of meta-gradient
updates through a sampled set of nearest environments. Additionally, we
incorporate higher-order Taylor expansions via Taylor-Mode automatic
differentiation, revealing that higher-order approximations do not necessarily
enhance generalization. Finally, we demonstrate how CSM can be integrated into
other meta-learning frameworks with FlashCAVIA, a computationally efficient
extension of the CAVIA meta-learning framework (Zintgraf et al. 2019).
FlashCAVIA outperforms its predecessor across various benchmarks and reinforces
the utility of bi-level optimization techniques. Together, these contributions
establish a robust framework for tackling an expanded spectrum of meta-learning
tasks, offering practical insights for out-of-distribution generalization. Our
open-sourced library, designed for flexible integration of self-modulation into
contextual meta-learning workflows, is available at
\url{github.com/ddrous/self-mod}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ shapiq: Shapley Interactions for Machine Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Muschalik, Hubert Baniecki, Fabian Fumagalli, Patrick Kolpaczki, Barbara Hammer, Eyke Hüllermeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Originally rooted in game theory, the Shapley Value (SV) has recently become
an important tool in machine learning research. Perhaps most notably, it is
used for feature attribution and data valuation in explainable artificial
intelligence. Shapley Interactions (SIs) naturally extend the SV and address
its limitations by assigning joint contributions to groups of entities, which
enhance understanding of black box machine learning models. Due to the
exponential complexity of computing SVs and SIs, various methods have been
proposed that exploit structural assumptions or yield probabilistic estimates
given limited resources. In this work, we introduce shapiq, an open-source
Python package that unifies state-of-the-art algorithms to efficiently compute
SVs and any-order SIs in an application-agnostic framework. Moreover, it
includes a benchmarking suite containing 11 machine learning applications of
SIs with pre-computed games and ground-truth values to systematically assess
computational performance across domains. For practitioners, shapiq is able to
explain and visualize any-order feature interactions in predictions of models,
including vision transformers, language models, as well as XGBoost and LightGBM
with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and
consolidate the application of SVs and SIs in machine learning that facilitates
future research. The source code and documentation are available at
https://github.com/mmschlk/shapiq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Framework of Horizontal-Vertical Hybrid Federated Learning for
  EdgeIoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Li, Yilei Liang, Xin Yuan, Wei Ni, Jon Crowcroft, Chau Yuen, Ozgur B. Akan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter puts forth a new hybrid horizontal-vertical federated learning
(HoVeFL) for mobile edge computing-enabled Internet of Things (EdgeIoT). In
this framework, certain EdgeIoT devices train local models using the same data
samples but analyze disparate data features, while the others focus on the same
features using non-independent and identically distributed (non-IID) data
samples. Thus, even though the data features are consistent, the data samples
vary across devices. The proposed HoVeFL formulates the training of local and
global models to minimize the global loss function. Performance evaluations on
CIFAR-10 and SVHN datasets reveal that the testing loss of HoVeFL with 12
horizontal FL devices and six vertical FL devices is 5.5% and 25.2% higher,
respectively, compared to a setup with six horizontal FL devices and 12
vertical FL devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Offline Value Function Learning with Bisimulation-based
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brahma S. Pavse, Yudong Chen, Qiaomin Xie, Josiah P. Hanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, offline value function learning is the procedure
of using an offline dataset to estimate the expected discounted return from
each state when taking actions according to a fixed target policy. The
stability of this procedure, i.e., whether it converges to its fixed-point,
critically depends on the representations of the state-action pairs. Poorly
learned representations can make value function learning unstable, or even
divergent. Therefore, it is critical to stabilize value function learning by
explicitly shaping the state-action representations. Recently, the class of
bisimulation-based algorithms have shown promise in shaping representations for
control. However, it is still unclear if this class of methods can stabilize
value function learning. In this work, we investigate this question and answer
it affirmatively. We introduce a bisimulation-based algorithm called kernel
representations for offline policy evaluation (KROPE). KROPE uses a kernel to
shape state-action representations such that state-action pairs that have
similar immediate rewards and lead to similar next state-action pairs under the
target policy also have similar representations. We show that KROPE: 1) learns
stable representations and 2) leads to lower value error than baselines. Our
analysis provides new theoretical insight into the stability properties of
bisimulation-based methods and suggests that practitioners can use these
methods for stable and accurate evaluation of offline reinforcement learning
agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moral Alignment for <span class="highlight-title">LLM</span> <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making agents based on pre-trained Large Language Models (LLMs) are
increasingly being deployed across various domains of human activity. While
their applications are currently rather specialized, several research efforts
are under way to develop more generalist agents. As LLM-based systems become
more agentic, their influence on human activity will grow and the transparency
of this will decrease. Consequently, developing effective methods for aligning
them to human values is vital.
  The prevailing practice in alignment often relies on human preference data
(e.g., in RLHF or DPO), in which values are implicit and are essentially
deduced from relative preferences over different model outputs. In this work,
instead of relying on human feedback, we introduce the design of reward
functions that explicitly encode core human values for Reinforcement
Learning-based fine-tuning of foundation agent models. Specifically, we use
intrinsic rewards for the moral alignment of LLM agents.
  We evaluate our approach using the traditional philosophical frameworks of
Deontological Ethics and Utilitarianism, quantifying moral rewards for agents
in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)
environment. We also show how moral fine-tuning can be deployed to enable an
agent to unlearn a previously developed selfish strategy. Finally, we find that
certain moral strategies learned on the IPD game generalize to several other
matrix game environments. In summary, we demonstrate that fine-tuning with
intrinsic rewards is a promising general solution for aligning LLM agents to
human values, and it might represent a more transparent and cost-effective
alternative to currently predominant alignment techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Graph Prompt Work? A Data Operation Perspective with Theoretical
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qunzhong Wang, Xiangguo Sun, Hong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, graph prompting has emerged as a promising research
direction, enabling the learning of additional tokens or subgraphs appended to
the original graphs without requiring retraining of pre-trained graph models
across various applications. This novel paradigm, shifting from the traditional
pretraining and finetuning to pretraining and prompting has shown significant
empirical success in simulating graph data operations, with applications
ranging from recommendation systems to biological networks and graph
transferring. However, despite its potential, the theoretical underpinnings of
graph prompting remain underexplored, raising critical questions about its
fundamental effectiveness. The lack of rigorous theoretical proof of why and
how much it works is more like a dark cloud over the graph prompt area to go
further. To fill this gap, this paper introduces a theoretical framework that
rigorously analyzes graph prompting from a data operation perspective. Our
contributions are threefold: First, we provide a formal guarantee theorem,
demonstrating graph prompts capacity to approximate graph transformation
operators, effectively linking upstream and downstream tasks. Second, we derive
upper bounds on the error of these data operations by graph prompts for a
single graph and extend this discussion to batches of graphs, which are common
in graph model training. Third, we analyze the distribution of data operation
errors, extending our theoretical findings from linear graph models (e.g., GCN)
to non-linear graph models (e.g., GAT). Extensive experiments support our
theoretical results and confirm the practical implications of these guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fira: Can We Achieve Full-rank Training of <span class="highlight-title">LLM</span>s Under Low-rank
  Constraint? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Kaituo Feng, Changsheng Li, Xunhao Lai, Xiangyu Yue, Ye Yuan, Guoren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank training has emerged as a promising approach for reducing memory
usage in training Large Language Models (LLMs). Previous methods either rely on
decomposing weight matrices (e.g., LoRA), or seek to decompose gradient
matrices (e.g., GaLore) to ensure reduced memory consumption. However, both of
them constrain the training in a low-rank subspace, thus inevitably leading to
sub-optimal performance. This raises a question: whether it is possible to
consistently preserve the low-rank constraint for memory efficiency, while
achieving full-rank training (i.e., training with full-rank gradients of
full-rank weights) to avoid inferior outcomes? In this paper, we propose a new
plug-and-play training framework for LLMs called Fira, as the first attempt to
achieve this goal. First, we observe an interesting phenomenon during LLM
training: the scaling impact of adaptive optimizers (e.g., Adam) on the
gradient norm remains similar from low-rank to full-rank training. Based on
this observation, we propose a norm-based scaling method, which utilizes the
scaling impact of low-rank optimizers as substitutes for that of original
full-rank optimizers to enable full-rank training. In this way, we can preserve
the low-rank constraint in the optimizer while achieving full-rank training for
better performance. Moreover, we find that there are sudden gradient rises
during the optimization process, potentially causing loss spikes. To address
this, we further put forward a norm-growth limiter to smooth the gradient via
regulating the relative increase of gradient norms. Extensive experiments on
the pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA
and GaLore, achieving performance that is comparable to or even better than
full-rank training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/xichen-fy/Fira</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Using Certified Training towards Empirical Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro De Palma, Serge Durand, Zakaria Chihani, François Terrier, Caterina Urban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is arguably the most popular way to provide empirical
robustness against specific adversarial examples. While variants based on
multi-step attacks incur significant computational overhead, single-step
variants are vulnerable to a failure mode known as catastrophic overfitting,
which hinders their practical utility for large perturbations. A parallel line
of work, certified training, has focused on producing networks amenable to
formal guarantees of robustness against any possible attack. However, the wide
gap between the best-performing empirical and certified defenses has severely
limited the applicability of the latter. Inspired by recent developments in
certified training, which rely on a combination of adversarial attacks with
network over-approximations, and by the connections between local linearity and
catastrophic overfitting, we present experimental evidence on the practical
utility and limitations of using certified training towards empirical
robustness. We show that, when tuned for the purpose, a recent certified
training algorithm can prevent catastrophic overfitting on single-step attacks,
and that it can bridge the gap to multi-step baselines under appropriate
experimental settings. Finally, we present a novel regularizer for network
over-approximations that can achieve similar effects while markedly reducing
runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRUPI: <span class="highlight-title">Dataset</span> Reduction Using Privileged Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Wang, Yantai Yang, Shuaiyu Zhang, Chenghao Sun, Weiya Li, Xuming Hu, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset reduction (DR) seeks to select or distill samples from large datasets
into smaller subsets while preserving performance on target tasks. Existing
methods primarily focus on pruning or synthesizing data in the same format as
the original dataset, typically the input data and corresponding labels.
However, in DR settings, we find it is possible to synthesize more information
beyond the data-label pair as an additional learning target to facilitate model
training. In this paper, we introduce Dataset Reduction Using Privileged
Information (DRUPI), which enriches DR by synthesizing privileged information
alongside the reduced dataset. This privileged information can take the form of
feature labels or attention labels, providing auxiliary supervision to improve
model learning. Our findings reveal that effective feature labels must balance
between being overly discriminative and excessively diverse, with a moderate
level proving optimal for improving the reduced dataset's efficacy. Extensive
experiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI
integrates seamlessly with existing dataset reduction methods, offering
significant performance gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Red Teaming with GOAT: the Generative Offensive <span class="highlight-title">Agent</span> Tester 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Pavlova, Erik Brinkman, Krithika Iyer, Vitor Albiero, Joanna Bitton, Hailey Nguyen, Joe Li, Cristian Canton Ferrer, Ivan Evtimov, Aaron Grattafiori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Red teaming assesses how large language models (LLMs) can produce content
that violates norms, policies, and rules set during their safety training.
However, most existing automated methods in the literature are not
representative of the way humans tend to interact with AI models. Common users
of AI models may not have advanced knowledge of adversarial machine learning
methods or access to model internals, and they do not spend a lot of time
crafting a single highly effective adversarial prompt. Instead, they are likely
to make use of techniques commonly shared online and exploit the multiturn
conversational nature of LLMs. While manual testing addresses this gap, it is
an inefficient and often expensive process. To address these limitations, we
introduce the Generative Offensive Agent Tester (GOAT), an automated agentic
red teaming system that simulates plain language adversarial conversations
while leveraging multiple adversarial prompting techniques to identify
vulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by
prompting a general-purpose model in a way that encourages reasoning through
the choices of methods available, the current target model's response, and the
next steps. Our approach is designed to be extensible and efficient, allowing
human testers to focus on exploring new areas of risk while automation covers
the scaled adversarial stress-testing of known risk territory. We present the
design and evaluation of GOAT, demonstrating its effectiveness in identifying
vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama
3.1 and 88% against GPT-4 on the JailbreakBench dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ENTP: Encoder-only Next Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Ewer, Daewon Chae, Thomas Zeng, Jinkyu Kim, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next-token prediction models have predominantly relied on decoder-only
Transformers with causal attention, driven by the common belief that causal
attention is essential to prevent "cheating" by masking future tokens. We
challenge this widely accepted notion and argue that this design choice is
about efficiency rather than necessity. While decoder-only Transformers are
still a good choice for practical reasons, they are not the only viable option.
In this work, we introduce Encoder-only Next Token Prediction (ENTP). We
explore the differences between ENTP and decoder-only Transformers in
expressive power and complexity, highlighting potential advantages of ENTP. We
introduce the Triplet-Counting task and show, both theoretically and
experimentally, that while ENTP can perform this task easily, a decoder-only
Transformer cannot. Finally, we empirically demonstrate ENTP's superior
performance across various realistic tasks, such as length generalization and
in-context learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Model Discovery Using Domain Decomposition and PINNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tirtho S. Saha, Alexander Heinlein, Cordula Reisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We enhance machine learning algorithms for learning model parameters in
complex systems represented by ordinary differential equations (ODEs) with
domain decomposition methods. The study evaluates the performance of two
approaches, namely (vanilla) Physics-Informed Neural Networks (PINNs) and
Finite Basis Physics-Informed Neural Networks (FBPINNs), in learning the
dynamics of test models with a quasi-stationary longtime behavior. We test the
approaches for data sets in different dynamical regions and with varying noise
level. As results, we find a better performance for the FBPINN approach
compared to the vanilla PINN approach, even in cases with data from only a
quasi-stationary time domain with few dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFE: Semantic Adaptive Feature Extraction with Rate Control for 6G
  Wireless Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuna Yan, Lixin Li, Xin Zhang, Wensheng Lin, Wenchi Cheng, Zhu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most current Deep Learning-based Semantic Communication (DeepSC) systems are
designed and trained exclusively for particular single-channel conditions,
which restricts their adaptability and overall bandwidth utilization. To
address this, we propose an innovative Semantic Adaptive Feature Extraction
(SAFE) framework, which significantly improves bandwidth efficiency by allowing
users to select different sub-semantic combinations based on their channel
conditions. This paper also introduces three advanced learning algorithms to
optimize the performance of SAFE framework as a whole. Through a series of
simulation experiments, we demonstrate that the SAFE framework can effectively
and adaptively extract and transmit semantics under different channel bandwidth
conditions, of which effectiveness is verified through objective and subjective
quality evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynFrs: An Efficient Framework for Machine Unlearning in Random Forest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shurong Wang, Zhuoyang Shen, Xinbao Qiao, Tongning Zhang, Meng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random Forests are widely recognized for establishing efficacy in
classification and regression tasks, standing out in various domains such as
medical diagnosis, finance, and personalized recommendations. These domains,
however, are inherently sensitive to privacy concerns, as personal and
confidential data are involved. With increasing demand for the right to be
forgotten, particularly under regulations such as GDPR and CCPA, the ability to
perform machine unlearning has become crucial for Random Forests. However,
insufficient attention was paid to this topic, and existing approaches face
difficulties in being applied to real-world scenarios. Addressing this gap, we
propose the DynFrs framework designed to enable efficient machine unlearning in
Random Forests while preserving predictive accuracy. Dynfrs leverages
subsampling method Occ(q) and a lazy tag strategy Lzy, and is still adaptable
to any Random Forest variant. In essence, Occ(q) ensures that each sample in
the training set occurs only in a proportion of trees so that the impact of
deleting samples is limited, and Lzy delays the reconstruction of a tree node
until necessary, thereby avoiding unnecessary modifications on tree structures.
In experiments, applying Dynfrs on Extremely Randomized Trees yields
substantial improvements, achieving orders of magnitude faster unlearning
performance and better predictive accuracy than existing machine unlearning
methods for Random Forests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Augmented Robust Algorithmic Recourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Kayastha, Vasilis Gkatzelis, Shahin Jabbari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of machine learning models in high-stakes domains can have
a major negative impact, especially on individuals who receive undesirable
outcomes. Algorithmic recourse provides such individuals with suggestions of
minimum-cost improvements they can make to achieve a desirable outcome in the
future. However, machine learning models often get updated over time and this
can cause a recourse to become invalid (i.e., not lead to the desirable
outcome). The robust recourse literature aims to choose recourses that are less
sensitive, even against adversarial model changes, but this comes at a higher
cost. To overcome this obstacle, we initiate the study of algorithmic recourse
through the learning-augmented framework and evaluate the extent to which a
designer equipped with a prediction regarding future model changes can reduce
the cost of recourse when the prediction is accurate (consistency) while also
limiting the cost even when the prediction is inaccurate (robustness). We
propose a novel algorithm for this problem, study the robustness-consistency
trade-off, and analyze how prediction accuracy affects performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coordinate-Based Neural Representation Enabling Zero-Shot Learning for
  3D Multiparametric Quantitative MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyan Lao, Ruimin Feng, Haikun Qi, Zhenfeng Lv, Qiangqiang Liu, Chunlei Liu, Yuyao Zhang, Hongjiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative magnetic resonance imaging (qMRI) offers tissue-specific
physical parameters with significant potential for neuroscience research and
clinical practice. However, lengthy scan times for 3D multiparametric qMRI
acquisition limit its clinical utility. Here, we propose SUMMIT, an innovative
imaging methodology that includes data acquisition and an unsupervised
reconstruction for simultaneous multiparametric qMRI. SUMMIT first encodes
multiple important quantitative properties into highly undersampled k-space. It
further leverages implicit neural representation incorporated with a dedicated
physics model to reconstruct the desired multiparametric maps without needing
external training datasets. SUMMIT delivers co-registered T1, T2, T2*, and
quantitative susceptibility mapping. Extensive simulations and phantom imaging
demonstrate SUMMIT's high accuracy. Additionally, the proposed unsupervised
approach for qMRI reconstruction also introduces a novel zero-shot learning
paradigm for multiparametric imaging applicable to various medical imaging
modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake It Until You Break It: On the Adversarial Robustness of
  AI-generated <span class="highlight-title">Image</span> Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Mavali, Jonas Ricker, David Pape, Yash Sharma, Asja Fischer, Lea Schoenherr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative AI (GenAI) offers countless possibilities for creative and
productive tasks, artificially generated media can be misused for fraud,
manipulation, scams, misinformation campaigns, and more. To mitigate the risks
associated with maliciously generated media, forensic classifiers are employed
to identify AI-generated content. However, current forensic classifiers are
often not evaluated in practically relevant scenarios, such as the presence of
an attacker or when real-world artifacts like social media degradations affect
images. In this paper, we evaluate state-of-the-art AI-generated image (AIGI)
detectors under different attack scenarios. We demonstrate that forensic
classifiers can be effectively attacked in realistic settings, even when the
attacker does not have access to the target model and post-processing occurs
after the adversarial examples are created, which is standard on social media
platforms. These attacks can significantly reduce detection accuracy to the
extent that the risks of relying on detectors outweigh their benefits. Finally,
we propose a simple defense mechanism to make CLIP-based detectors, which are
currently the best-performing detectors, robust against these attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Truncated Kernel Stochastic Gradient Descent on Spheres 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JinHui Bai, Lei Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the structure of spherical harmonics, we propose the truncated
kernel stochastic gradient descent (T-kernel SGD) algorithm with a least-square
loss function for spherical data fitting. T-kernel SGD employs a "truncation"
operation, enabling the application of a series-based kernel function in
stochastic gradient descent, thereby avoiding the difficulties of finding
suitable closed-form kernel functions in high-dimensional spaces. In contrast
to traditional kernel SGD, T-kernel SGD is more effective in balancing bias and
variance by dynamically adjusting the hypothesis space during iterations. The
most significant advantage of the proposed algorithm is that it can achieve
theoretically optimal convergence rates using a constant step size (independent
of the sample size) while overcoming the inherent saturation problem of kernel
SGD. Additionally, we leverage the structure of spherical polynomials to derive
an equivalent T-kernel SGD, significantly reducing storage and computational
costs compared to kernel SGD. Typically, T-kernel SGD requires only
$\mathcal{O}(n^{1+\frac{d}{d-1}\epsilon})$ computational complexity and
$\mathcal{O}(n^{\frac{d}{d-1}\epsilon})$ storage to achieve optimal rates for
the d-dimensional sphere, where $0<\epsilon<\frac{1}{2}$ can be arbitrarily
small if the optimal fitting or the underlying space possesses sufficient
regularity. This regularity is determined by the smoothness parameter of the
objective function and the decaying rate of the eigenvalues of the integral
operator associated with the kernel function, both of which reflect the
difficulty of the estimation problem. Our main results quantitatively
characterize how this prior information influences the convergence of T-kernel
SGD. The numerical experiments further validate the theoretical findings
presented in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayes' Power for Explaining In-Context Learning Generalizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Müller, Noah Hollmann, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, neural network training has been primarily viewed as an
approximation of maximum likelihood estimation (MLE). This interpretation
originated in a time when training for multiple epochs on small datasets was
common and performance was data bound; but it falls short in the era of
large-scale single-epoch trainings ushered in by large self-supervised setups,
like language models. In this new setup, performance is compute-bound, but data
is readily available. As models became more powerful, in-context learning
(ICL), i.e., learning in a single forward-pass based on the context, emerged as
one of the dominant paradigms. In this paper, we argue that a more useful
interpretation of neural network behavior in this era is as an approximation of
the true posterior, as defined by the data-generating process. We demonstrate
this interpretations' power for ICL and its usefulness to predict
generalizations to previously unseen tasks. We show how models become robust
in-context learners by effectively composing knowledge from their training
data. We illustrate this with experiments that reveal surprising
generalizations, all explicable through the exact posterior. Finally, we show
the inherent constraints of the generalization capabilities of posteriors and
the limitations of neural networks in approximating these posteriors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRTF Estimation using a Score-based Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etienne Thuillier, Jean-Marie Lemercier, Eloi Moliner, Timo Gerkmann, Vesa Välimäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a head-related transfer function (HRTF) estimation method which
relies on a data-driven prior given by a score-based diffusion model. The HRTF
is estimated in reverberant environments using natural excitation signals, e.g.
human speech. The impulse response of the room is estimated along with the HRTF
by optimizing a parametric model of reverberation based on the statistical
behaviour of room acoustics. The posterior distribution of HRTF given the
reverberant measurement and excitation signal is modelled using the score-based
HRTF prior and a log-likelihood approximation. We show that the resulting
method outperforms several baselines, including an oracle recommender system
that assigns the optimal HRTF in our training set based on the smallest
distance to the true HRTF at the given direction of arrival. In particular, we
show that the diffusion prior can account for the large variability of
high-frequency content in HRTFs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source
  Instruction Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, Igor Gitman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning continues to be a critical challenge in large language
model (LLM) development with significant interest. However, most of the
cutting-edge progress in mathematical reasoning with LLMs has become
\emph{closed-source} due to lack of access to training data. This lack of data
access limits researchers from understanding the impact of different choices
for synthesizing and utilizing the data. With the goal of creating a
high-quality finetuning (SFT) dataset for math reasoning, we conduct careful
ablation experiments on data synthesis using the recently released
\texttt{Llama3.1} family of models. Our experiments show that: (a) solution
format matters, with excessively verbose solutions proving detrimental to SFT
performance, (b) data generated by a strong teacher outperforms
\emph{on-policy} data generated by a weak student model, (c) SFT is robust to
low-quality solutions, allowing for imprecise data filtering, and (d) question
diversity is crucial for achieving data scaling gains. Based on these insights,
we create the OpenMathInstruct-2 dataset, which consists of 14M
question-solution pairs ($\approx$ 600K unique questions), making it nearly
eight times larger than the previous largest open-source math reasoning
dataset. Finetuning the \texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2
outperforms \texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\% (51.9\%
$\rightarrow$ 67.8\%). Finally, to accelerate the open-source efforts, we
release the code, the finetuned models, and the OpenMathInstruct-2 dataset
under a commercially permissive license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrative Decoding: Improve Factuality via Implicit Self-consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Xiao Liang, Yeyun Gong, Wen Xiao, Song Wang, Yuji Zhang, Wenjun Hou, Kaishuai Xu, Wenge Liu, Wenjie Li, Jian Jiao, Qi Chen, Peng Cheng, Wayne Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-consistency-based approaches, which involve repeatedly sampling multiple
outputs and selecting the most consistent one as the final response, prove to
be remarkably effective in improving the factual accuracy of large language
models. Nonetheless, existing methods usually have strict constraints on the
task format, largely limiting their applicability. In this paper, we present
Integrative Decoding (ID), to unlock the potential of self-consistency in
open-ended generation tasks. ID operates by constructing a set of inputs, each
prepended with a previously sampled response, and then processes them
concurrently, with the next token being selected by aggregating of all their
corresponding predictions at each decoding step. In essence, this simple
approach implicitly incorporates self-consistency in the decoding objective.
Extensive evaluation shows that ID consistently enhances factuality over a wide
range of language models, with substantial improvements on the TruthfulQA
(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance
gains amplify progressively as the number of sampled responses increases,
indicating the potential of ID to scale up with repeated sampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CASE: Efficient Curricular Data Pre-training for Building Assistive
  Psychology Expert Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00314v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00314v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Harne, Monjoy Narayan Choudhury, Madhav Rao, TK Srikanth, Seema Mehrotra, Apoorva Vashisht, Aarushi Basu, Manjit Sodhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limited availability of psychologists necessitates efficient
identification of individuals requiring urgent mental healthcare. This study
explores the use of Natural Language Processing (NLP) pipelines to analyze text
data from online mental health forums used for consultations. By analyzing
forum posts, these pipelines can flag users who may require immediate
professional attention. A crucial challenge in this domain is data privacy and
scarcity. To address this, we propose utilizing readily available curricular
texts used in institutes specializing in mental health for pre-training the NLP
pipelines. This helps us mimic the training process of a psychologist. Our work
presents CASE-BERT that flags potential mental health disorders based on forum
text. CASE-BERT demonstrates superior performance compared to existing methods,
achieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the
most commonly reported mental health disorders. Our code and data are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training
  with Limited Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01445v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01445v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyuan Wei, Fanjiang Ye, Ori Yonay, Xingyu Chen, Baixi Sun, Dingwen Tao, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing studies of training state-of-the-art Contrastive Language-Image
Pretraining (CLIP) models on large-scale data involve hundreds of or even
thousands of GPUs due to the requirement of a large batch size. However, such a
large amount of resources is not accessible to most people. While advanced
compositional optimization techniques for optimizing global contrastive losses
have been demonstrated effective for removing the requirement of large batch
size, their performance on large-scale data remains underexplored and not
optimized. To bridge the gap, this paper explores several aspects of CLIP
training with limited resources (e.g., up to tens of GPUs). First, we introduce
FastCLIP, a general CLIP training framework built on advanced compositional
optimization techniques while designed and optimized for the distributed
setting. Our framework is equipped with an efficient gradient reduction
strategy to reduce communication overhead. Second, to further boost training
efficiency, we investigate three components of the framework from an
optimization perspective: the schedule of the inner learning rate, the update
rules of the temperature parameter and the model parameters, respectively.
Experiments on different strategies for each component shed light on how to
conduct CLIP training more efficiently. Finally, we benchmark the performance
of FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different
compute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7
million, 9.1 million to 315 million image-text pairs to demonstrate the
significant improvement of FastCLIP in the resource-limited setting. We release
the code of FastCLIP at https://github.com/Optimization-AI/fast_clip .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Test-Time Adaptation with State-Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mona Schirmer, Dan Zhang, Eric Nalisnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution shifts between training and test data are inevitable over the
lifecycle of a deployed model, leading to performance decay. Adapting a model
on test samples can help mitigate this drop in performance. However, most
test-time adaptation methods have focused on synthetic corruption shifts,
leaving a variety of distribution shifts underexplored. In this paper, we focus
on distribution shifts that evolve gradually over time, which are common in the
wild but challenging for existing methods, as we show. To address this, we
propose STAD, a probabilistic state-space model that adapts a deployed model to
temporal distribution shifts by learning the time-varying dynamics in the last
set of hidden features. Without requiring labels, our model infers
time-evolving class prototypes that act as a dynamic classification head.
Through experiments on real-world temporal distribution shifts, we show that
our method excels in handling small batch sizes and label shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time
  Series Forecasters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Joy Wang, Jianling Sun, Chenghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have emerged as a promising approach in time series
forecasting (TSF). Existing approaches either repurpose large language models
(LLMs) or build large-scale time series datasets to develop TSF foundation
models for universal forecasting. However, these methods face challenges due to
the severe cross-domain gap or in-domain heterogeneity. This paper explores a
new road to building a TSF foundation model from rich, high-quality natural
images. Our key insight is that a visual masked autoencoder, pre-trained on the
ImageNet dataset, can naturally be a numeric series forecaster. By
reformulating TSF as an image reconstruction task, we bridge the gap between
image pre-training and TSF downstream tasks. Surprisingly, without further
adaptation in the time-series domain, the proposed VisionTS could achieve
superior zero-shot forecasting performance compared to existing TSF foundation
models. With fine-tuning for one epoch, VisionTS could further improve the
forecasting and achieve state-of-the-art performance in most cases. Extensive
experiments reveal intrinsic similarities between images and real-world time
series, suggesting visual models may offer a ``free lunch'' for TSF and
highlight the potential for future cross-modality research. Our code is
publicly available at https://github.com/Keytoyze/VisionTS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: add more experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-skill Transferability-based Data Selection for Large
  <span class="highlight-title">Vision-Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Lee, Boyang Li, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning, or supervised finetuning on extensive task-specific data,
is necessary for Large Vision-Language Models (LVLMs) to generalize well across
a broad range of vision-language (VL) tasks. However, training on large VL
datasets can become prohibitively expensive. In this work, we introduce
COINCIDE, an effective and scalable data selection technique that uses a small
model as a reference model to select visual instruction tuning data for
efficient finetuning of a target LVLM, focusing on diversity and
transferability. Specifically, we cluster the training data using internal
activations from a small model, which identifies VL concept-skill compositions
needed by a target LVLM. We then sample data from these diverse clusters by
considering their density and transferability, or the ability to transfer well
to other concept-skill compositions. This approach ensures the diversity of
these compositions, which is vital for LVLM generalization. Extensive
experiments demonstrate that COINCIDE achieves superior performance and data
selection efficiency against 8 strong baselines on two distinct datasets:
LLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE
achieves performance comparable to the LVLM finetuned on the whole dataset,
with 70% reduction of the wall-clock running time. On the Vision-Flan dataset,
our method achieves superior results with only 16.7% of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliminating Position Bias of Language Models: A Mechanistic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Position bias has proven to be a prevalent issue of modern language models
(LMs), where the models prioritize content based on its position within the
given context. This bias often leads to unexpected model failures and hurts
performance, robustness, and reliability across various applications. Our
mechanistic analysis attributes the position bias to two components employed in
nearly all state-of-the-art LMs: causal attention and relative positional
encodings. Based on the analyses, we propose to eliminate position bias (e.g.,
different retrieved documents' orders in QA affect performance) with a
training-free zero-shot approach. Our method changes the causal attention to
bidirectional attention between documents and utilizes model attention values
to decide the relative orders of documents instead of using the order provided
in input prompts, therefore enabling Position-INvariant inferencE (PINE) at the
document level. By eliminating position bias, models achieve better performance
and reliability in downstream tasks, including LM-as-a-judge,
retrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE
is especially useful when adapting LMs for evaluating reasoning pairs: it
consistently provides 8 to 10 percentage points performance gains, making
Llama-3-70B-Instruct perform even better than GPT-4-0125-preview and
GPT-4o-2024-08-06 on the RewardBench reasoning set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Optimal LR Across Token Horizons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset
size and cluster size. It is economically infeasible to extensively tune
hyperparameter for the largest runs. Instead, approximately optimal
hyperparameters must be inferred or \textit{transferred} from smaller
experiments. Hyperparameter transfer across model sizes has been studied in
Yang et al. However, hyperparameter transfer across dataset size -- or token
horizon -- has not been studied yet. To remedy this we conduct a large scale
empirical study on how optimal learning rate (LR) depends on token horizon in
LLM training. We first demonstrate that the optimal LR changes significantly
with token horizon -- longer training necessitates smaller LR. Secondly we
demonstrate the the optimal LR follows a scaling law, and that the optimal LR
for longer horizons can be accurately estimated from shorter horizons via such
scaling laws. We also provide a rule-of-thumb for transferring LR across token
horizons with zero overhead over current practices. Lastly we provide evidence
that LLama-1 used too high LR, and estimate the performance hit from this. We
thus argue that hyperparameter transfer across data size is an important and
overlooked component of LLM training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Privacy-Preserving Relational Data Synthesis via Probabilistic
  Relational Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Luttermann, Ralf Möller, Mattis Hartwig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic relational models provide a well-established formalism to
combine first-order logic and probabilistic models, thereby allowing to
represent relationships between objects in a relational domain. At the same
time, the field of artificial intelligence requires increasingly large amounts
of relational training data for various machine learning tasks. Collecting
real-world data, however, is often challenging due to privacy concerns, data
protection regulations, high costs, and so on. To mitigate these challenges,
the generation of synthetic data is a promising approach. In this paper, we
solve the problem of generating synthetic relational data via probabilistic
relational models. In particular, we propose a fully-fledged pipeline to go
from relational database to probabilistic relational model, which can then be
used to sample new synthetic relational data points from its underlying
probability distribution. As part of our proposed pipeline, we introduce a
learning algorithm to construct a probabilistic relational model from a given
relational database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Proceedings of the 47th German Conference on
  Artificial Intelligence (KI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformers are Minimax Optimal Nonparametric In-Context Learners <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juno Kim, Tai Nakamaki, Taiji Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) of large language models has proven to be a
surprisingly effective method of learning a new task from only a few
demonstrative examples. In this paper, we study the efficacy of ICL from the
viewpoint of statistical learning theory. We develop approximation and
generalization error bounds for a transformer composed of a deep neural network
and one linear attention layer, pretrained on nonparametric regression tasks
sampled from general function spaces including the Besov space and piecewise
$\gamma$-smooth class. We show that sufficiently trained transformers can
achieve -- and even improve upon -- the minimax optimal estimation risk in
context by encoding the most relevant basis representations during pretraining.
Our analysis extends to high-dimensional or sequential data and distinguishes
the \emph{pretraining} and \emph{in-context} generalization gaps. Furthermore,
we establish information-theoretic lower bounds for meta-learners w.r.t. both
the number of tasks and in-context examples. These findings shed light on the
roles of task diversity and representation learning for ICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; 40 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOPE for a Robust Parameterization of Long-memory State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annan Yu, Michael W. Mahoney, N. Benjamin Erichson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-space models (SSMs) that utilize linear, time-invariant (LTI) systems
are known for their effectiveness in learning long sequences. To achieve
state-of-the-art performance, an SSM often needs a specifically designed
initialization, and the training of state matrices is on a logarithmic scale
with a very small learning rate. To understand these choices from a unified
perspective, we view SSMs through the lens of Hankel operator theory. Building
upon it, we develop a new parameterization scheme, called HOPE, for LTI systems
that utilizes Markov parameters within Hankel operators. Our approach helps
improve the initialization and training stability, leading to a more robust
parameterization. We efficiently implement these innovations by nonuniformly
sampling the transfer functions of LTI systems, and they require fewer
parameters compared to canonical SSMs. When benchmarked against
HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel
operators demonstrates improved performance on Long-Range Arena (LRA) tasks.
Moreover, our new parameterization endows the SSM with non-decaying memory
within a fixed time window, which is empirically corroborated by a sequential
CIFAR-10 task with padded noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Dynamics of <span class="highlight-title">LLM</span> Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ren, Danica J. Sutherland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dynamics, which describes how the learning of specific training
examples influences the model's predictions on other examples, gives us a
powerful tool for understanding the behavior of deep learning systems. We study
the learning dynamics of large language models during different types of
finetuning, by analyzing the step-wise decomposition of how influence
accumulates among different potential responses. Our framework allows a uniform
interpretation of many interesting observations about the training of popular
algorithms for both instruction tuning and preference tuning. In particular, we
propose a hypothetical explanation of why specific types of hallucination are
strengthened after finetuning, e.g., the model might use phrases or facts in
the response for question B to answer question A, or the model might keep
repeating similar simple phrases when generating responses. We also extend our
framework and highlight a unique "squeezing effect" to explain a previously
observed phenomenon in off-policy direct preference optimization (DPO), where
running DPO for too long makes even the desired outputs less likely. This
framework also provides insights into where the benefits of on-policy DPO and
other variants come from. The analysis not only provides a novel perspective of
understanding LLM's finetuning but also inspires a simple, effective method to
improve alignment performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent <span class="highlight-title">Diffusion</span> Models for Controllable RNA Sequence <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixuan Huang, Yukang Yang, Kaidi Fu, Yanyi Chu, Le Cong, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents RNAdiffusion, a latent diffusion model for generating and
optimizing discrete RNA sequences of variable lengths. RNA is a key
intermediary between DNA and protein, exhibiting high sequence diversity and
complex three-dimensional structures to support a wide range of functions. We
utilize pretrained BERT-type models to encode raw RNA sequences into
token-level, biologically meaningful representations. A Query Transformer is
employed to compress such representations into a set of fixed-length latent
vectors, with an autoregressive decoder trained to reconstruct RNA sequences
from these latent variables. We then develop a continuous diffusion model
within this latent space. To enable optimization, we integrate the gradients of
reward models--surrogates for RNA functional properties--into the backward
diffusion process, thereby generating RNAs with high reward scores. Empirical
results confirm that RNAdiffusion generates non-coding RNAs that align with
natural distributions across various biological metrics. Further, we fine-tune
the diffusion model on mRNA 5' untranslated regions (5'-UTRs) and optimize
sequences for high translation efficiencies. Our guided diffusion model
effectively generates diverse 5'-UTRs with high Mean Ribosome Loading (MRL) and
Translation Efficiency (TE), outperforming baselines in balancing rewards and
structural stability trade-off. Our findings hold potential for advancing RNA
sequence-function research and therapeutic RNA design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Wang, Min Xiao, Yirong Zhou, Chengyan Wang, Naiming Wu, Yi Li, Yiwen Gong, Shufu Chang, Yinyin Chen, Liuhong Zhu, Jianjun Zhou, Congbo Cai, He Wang, Di Guo, Guang Yang, Xiaobo Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic magnetic resonance imaging (MRI) plays an indispensable role in
cardiac diagnosis. To enable fast imaging, the k-space data can be undersampled
but the image reconstruction poses a great challenge of high-dimensional
processing. This challenge necessitates extensive training data in deep
learning reconstruction methods. In this work, we propose a novel and efficient
approach, leveraging a dimension-reduced separable learning scheme that can
perform exceptionally well even with highly limited training data. We design
this new approach by incorporating spatiotemporal priors into the development
of a Deep Separable Spatiotemporal Learning network (DeepSSL), which unrolls an
iteration process of a 2D spatiotemporal reconstruction model with both
temporal low-rankness and spatial sparsity. Intermediate outputs can also be
visualized to provide insights into the network behavior and enhance
interpretability. Extensive results on cardiac cine datasets demonstrate that
the proposed DeepSSL surpasses state-of-the-art methods both visually and
quantitatively, while reducing the demand for training cases by up to 75%.
Additionally, its preliminary adaptability to unseen cardiac patients has been
verified through a blind reader study conducted by experienced radiologists and
cardiologists. Furthermore, DeepSSL enhances the accuracy of the downstream
task of cardiac segmentation and exhibits robustness in prospectively
undersampled real-time cardiac MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test Time Learning for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panayiotis Christou, Shichu Chen, Xupeng Chen, Parijat Dube
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-series forecasting has seen significant advancements with the
introduction of token prediction mechanisms such as multi-head attention.
However, these methods often struggle to achieve the same performance as in
language modeling, primarily due to the quadratic computational cost and the
complexity of capturing long-range dependencies in time-series data.
State-space models (SSMs), such as Mamba, have shown promise in addressing
these challenges by offering efficient solutions with linear RNNs capable of
modeling long sequences with larger context windows. However, there remains
room for improvement in accuracy and scalability.
  We propose the use of Test-Time Training (TTT) modules in a parallel
architecture to enhance performance in long-term time series forecasting.
Through extensive experiments on standard benchmark datasets, we demonstrate
that TTT modules consistently outperform state-of-the-art models, including the
Mamba-based TimeMachine, particularly in scenarios involving extended sequence
and prediction lengths. Our results show significant improvements in Mean
Squared Error (MSE) and Mean Absolute Error (MAE), especially on larger
datasets such as Electricity, Traffic, and Weather, underscoring the
effectiveness of TTT in capturing long-range dependencies. Additionally, we
explore various convolutional architectures within the TTT framework, showing
that even simple configurations like 1D convolution with small filters can
achieve competitive results. This work sets a new benchmark for time-series
forecasting and lays the groundwork for future research in scalable,
high-performance forecasting models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strategies for Pretraining Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Zhou, Cooper Lorsung, AmirPouya Hemmasian, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining for partial differential equation (PDE) modeling has recently
shown promise in scaling neural operators across datasets to improve
generalizability and performance. Despite these advances, our understanding of
how pretraining affects neural operators is still limited; studies generally
propose tailored architectures and datasets that make it challenging to compare
or examine different pretraining frameworks. To address this, we compare
various pretraining methods without optimizing architecture choices to
characterize pretraining dynamics on different models and datasets as well as
to understand its scaling and generalization behavior. We find that pretraining
is highly dependent on model and dataset choices, but in general transfer
learning or physics-based pretraining strategies work best. In addition,
pretraining performance can be further improved by using data augmentations.
Lastly, pretraining can be additionally beneficial when fine-tuning in scarce
data regimes or when generalizing to downstream data similar to the pretraining
distribution. Through providing insights into pretraining neural operators for
physics prediction, we hope to motivate future work in developing and
evaluating pretraining methods for PDEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SysCaps: Language Interfaces for Simulation Surrogates of Complex
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Emami, Zhaonan Li, Saumya Sinha, Truc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate models are used to predict the behavior of complex energy systems
that are too expensive to simulate with traditional numerical methods. Our work
introduces the use of language descriptions, which we call "system captions" or
SysCaps, to interface with such surrogates. We argue that interacting with
surrogates through text, particularly natural language, makes these models more
accessible for both experts and non-experts. We introduce a lightweight
multimodal text and timeseries regression model and a training pipeline that
uses large language models (LLMs) to synthesize high-quality captions from
simulation metadata. Our experiments on two real-world simulators of buildings
and wind farms show that our SysCaps-augmented surrogates have better accuracy
on held-out systems than traditional methods while enjoying new generalization
abilities, such as handling semantically related descriptions of the same test
system. Additional experiments also highlight the potential of SysCaps to
unlock language-driven design space exploration and to regularize training
through prompt augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Multi-Token Joint Decoding with Auxiliary Model for <span class="highlight-title">LLM</span>
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success across diverse
tasks, yet their inference processes are hindered by substantial time and
energy demands due to single-token generation at each decoding step. While
previous methods such as speculative decoding mitigate these inefficiencies by
producing multiple tokens per step, each token is still generated by its
single-token distribution, thereby enhancing speed without improving
effectiveness. In contrast, our work simultaneously enhances inference speed
and improves the output effectiveness. We consider multi-token joint decoding
(MTJD), which generates multiple tokens from their joint distribution at each
iteration, theoretically reducing perplexity and enhancing task performance.
However, MTJD suffers from the high cost of sampling from the joint
distribution of multiple tokens. Inspired by speculative decoding, we introduce
multi-token assisted decoding (MTAD), a novel framework designed to accelerate
MTJD. MTAD leverages a smaller auxiliary model to approximate the joint
distribution of a larger model, incorporating a verification mechanism that not
only ensures the accuracy of this approximation, but also improves the decoding
efficiency over conventional speculative decoding. Theoretically, we
demonstrate that MTAD closely approximates exact MTJD with bounded error.
Empirical evaluations using Llama-2 and OPT models ranging from 13B to 70B
parameters across various tasks reveal that MTAD reduces perplexity by 21.2%
and improves downstream performance compared to standard single-token sampling.
Furthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than
conventional speculative decoding methods. These results highlight MTAD's
ability to make multi-token joint decoding both effective and efficient,
promoting more sustainable and high-performance deployment of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dimensionality Reduction and Nearest Neighbors for Improving
  Out-of-Distribution Detection in Medical <span class="highlight-title">Image</span> Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        McKell Woodland, Nihil Patel, Austin Castelo, Mais Al Taie, Mohamed Eltaher, Joshua P. Yung, Tucker J. Netherton, Tiffany L. Calderone, Jessica I. Sanchez, Darrel W. Cleere, Ahmed Elsaiey, Nakul Gupta, David Victor, Laura Beretta, Ankit B. Patel, Kristy K. Brock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinically deployed deep learning-based segmentation models are known to fail
on data outside of their training distributions. While clinicians review the
segmentations, these models tend to perform well in most instances, which could
exacerbate automation bias. Therefore, detecting out-of-distribution images at
inference is critical to warn the clinicians that the model likely failed. This
work applied the Mahalanobis distance (MD) post hoc to the bottleneck features
of four Swin UNETR and nnU-net models that segmented the liver on T1-weighted
magnetic resonance imaging and computed tomography. By reducing the dimensions
of the bottleneck features with either principal component analysis or uniform
manifold approximation and projection, images the models failed on were
detected with high performance and minimal computational load. In addition,
this work explored a non-parametric alternative to the MD, a k-th nearest
neighbors distance (KNN). KNN drastically improved scalability and performance
over MD when both were applied to raw and average-pooled bottleneck features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:020. Expansion of
  "Dimensionality Reduction for Improving Out-of-Distribution Detection in
  Medical Image Segmentation" arXiv:2308.03723. Code available at
  https://github.com/mckellwoodland/dimen_reduce_mahal
  (https://zenodo.org/records/13881989)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Almost Sure Convergence of Average Reward Temporal Difference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19546v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19546v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Blaser, Shangtong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular average reward Temporal Difference (TD) learning is perhaps the
simplest and the most fundamental policy evaluation algorithm in average reward
reinforcement learning. After at least 25 years since its discovery, we are
finally able to provide a long-awaited almost sure convergence analysis.
Namely, we are the first to prove that, under very mild conditions, tabular
average reward TD converges almost surely to a sample path dependent fixed
point. Key to this success is a new general stochastic approximation result
concerning nonexpansive mappings with Markovian and additive noise, built on
recent advances in stochastic Krasnoselskii-Mann iterations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Bootstrap: New Privacy Analysis and Inference
  Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06140v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06140v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanyu Wang, Guang Cheng, Jordan Awan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private (DP) mechanisms protect individual-level information
by introducing randomness into the statistical analysis procedure. Despite the
availability of numerous DP tools, there remains a lack of general techniques
for conducting statistical inference under DP. We examine a DP bootstrap
procedure that releases multiple private bootstrap estimates to infer the
sampling distribution and construct confidence intervals (CIs). Our privacy
analysis presents new results on the privacy cost of a single DP bootstrap
estimate, applicable to any DP mechanism, and identifies some misapplications
of the bootstrap in the existing literature. For the composition of the DP
bootstrap, we present a numerical method to compute the exact privacy cost of
releasing multiple DP bootstrap estimates, and using the Gaussian-DP (GDP)
framework (Dong et al., 2022), we show that the release of $B$ DP bootstrap
estimates from mechanisms satisfying $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP
asymptotically satisfies $\mu$-GDP as $B$ goes to infinity. Then, we perform
private statistical inference by post-processing the DP bootstrap estimates. We
prove that our point estimates are consistent, our standard CIs are
asymptotically valid, and both enjoy optimal convergence rates. To further
improve the finite performance, we use deconvolution with DP bootstrap
estimates to accurately infer the sampling distribution. We derive CIs for
tasks such as population mean estimation, logistic regression, and quantile
regression, and we compare them to existing methods using simulations and
real-world experiments on 2016 Canada Census data. Our private CIs achieve the
nominal coverage level and offer the first approach to private inference for
quantile regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Context Flows for Meta-Learning of Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02154v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02154v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roussel Desmond Nzoyem, David A. W. Barton, Tom Deakin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Ordinary Differential Equations (NODEs) often struggle to adapt to new
dynamic behaviors caused by parameter changes in the underlying system, even
when these dynamics are similar to previously observed behaviors. This problem
becomes more challenging when the changing parameters are unobserved, meaning
their value or influence cannot be directly measured when collecting data. To
address this issue, we introduce Neural Context Flow (NCF), a robust and
interpretable Meta-Learning framework that includes uncertainty estimation. NCF
uses higher-order Taylor expansion to enable contextual self-modulation,
allowing context vectors to influence dynamics from other domains while also
modulating themselves. After establishing convergence guarantees, we
empirically test NCF and compare it to related adaptation methods. Our results
show that NCF achieves state-of-the-art Out-of-Distribution performance on 5
out of 6 linear and non-linear benchmark problems. Through extensive
experiments, we explore the flexible model architecture of NCF and the encoded
representations within the learned context vectors. Our findings highlight the
potential implications of NCF for foundational models in the physical sciences,
offering a promising approach to improving the adaptability and generalization
of NODEs in various scientific applications. Our code is openly available at
\url{https://github.com/ddrous/ncflow}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 19 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Futuristic Autonomous Experimentation--A Surprise-Reacting
  Sequential Experiment Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00600v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00600v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imtiaz Ahmed, Satish Bukkapatnam, Bhaskar Botcha, Yu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An autonomous experimentation platform in manufacturing is supposedly capable
of conducting a sequential search for finding suitable manufacturing conditions
by itself or even for discovering new materials with minimal human
intervention. The core of the intelligent control of such platforms is a policy
to decide where to conduct the next experiment based on what has been done thus
far. Such policy inevitably trades off between exploitation and exploration.
Currently, the prevailing approach is to use various acquisition functions in
the Bayesian optimization framework. We discuss whether it is beneficial to
trade off exploitation versus exploration by measuring the element and degree
of surprise associated with the immediate past observation. We devise a
surprise-reacting policy using two existing surprise metrics, known as the
Shannon surprise and Bayesian surprise. Our analysis shows that the
surprise-reacting policy appears to be better suited for quickly characterizing
the overall landscape of a response surface under resource constraints. We do
not claim that we have a fully autonomous experimentation system but believe
that the surprise-reacting capability benefits the automation of sequential
decisions in autonomous experimentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fitting an ellipsoid to a quadratic number of random points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afonso S. Bandeira, Antoine Maillard, Shahar Mendelson, Elliot Paquette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem $(\mathrm{P})$ of fitting $n$ standard Gaussian
random vectors in $\mathbb{R}^d$ to the boundary of a centered ellipsoid, as
$n, d \to \infty$. This problem is conjectured to have a sharp feasibility
transition: for any $\varepsilon > 0$, if $n \leq (1 - \varepsilon) d^2 / 4$
then $(\mathrm{P})$ has a solution with high probability, while $(\mathrm{P})$
has no solutions with high probability if $n \geq (1 + \varepsilon) d^2 /4$. So
far, only a trivial bound $n \geq d^2 / 2$ is known on the negative side, while
the best results on the positive side assume $n \leq d^2 /
\mathrm{polylog}(d)$. In this work, we improve over previous approaches using a
key result of Bartl & Mendelson (2022) on the concentration of Gram matrices of
random vectors under mild assumptions on their tail behavior. This allows us to
give a simple proof that $(\mathrm{P})$ is feasible with high probability when
$n \leq d^2 / C$, for a (possibly large) constant $C > 0$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages; Update (v2) to match the published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEG-Language Modeling for Pathology Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Gijsen, Kerstin Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal language modeling constitutes a recent breakthrough which
leverages advances in large language models to pretrain capable multimodal
models. The integration of natural language during pretraining has been shown
to significantly improve learned representations, particularly in computer
vision. However, the efficacy of multimodal language modeling in the realm of
functional brain data, specifically for advancing pathology detection, remains
unexplored. This study pioneers EEG-language models trained on clinical reports
and 15000 EEGs. We extend methods for multimodal alignment to this novel domain
and investigate which textual information in reports is useful for training
EEG-language models. Our results indicate that models learn richer
representations from being exposed to a variety of report segments, including
the patient's clinical history, description of the EEG, and the physician's
interpretation. Compared to models exposed to narrower clinical text
information, we find such models to retrieve EEGs based on clinical reports
(and vice versa) with substantially higher accuracy. Yet, this is only observed
when using a contrastive learning approach. Particularly in regimes with few
annotations, we observe that representations of EEG-language models can
significantly improve pathology detection compared to those of EEG-only models,
as demonstrated by both zero-shot classification and linear probes. In sum,
these results highlight the potential of integrating brain activity data with
clinical text, suggesting that EEG-language models represent significant
progress for clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MallowsPO: Fine-Tune Your <span class="highlight-title">LLM</span> with Preference Dispersions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14953v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14953v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxian Chen, Hanyang Zhao, Henry Lam, David Yao, Wenpin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has recently emerged as a popular
approach to improve reinforcement learning with human feedback (RLHF), leading
to better techniques to fine-tune large language models (LLM). A weakness of
DPO, however, lies in its lack of capability to characterize the diversity of
human preferences. Inspired by Mallows' theory of preference ranking, we
develop in this paper a new approach, the MallowsPO. A distinct feature of this
approach is a dispersion index, which reflects the dispersion of human
preference to prompts. We show that existing DPO models can be reduced to
special cases of this dispersion index, thus unified with MallowsPO. More
importantly, we demonstrate (empirically) how to use this dispersion index to
enhance the performance of DPO in a broad array of benchmark tasks, from
synthetic bandit selection to controllable generations and dialogues, while
maintaining great generalization capabilities. MallowsPO is also compatible
with other SOTA offline preference optimization methods, boosting nearly 2\%
extra LC win rate when used as a plugin for fine-tuning Llama3-Instruct.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Multi-<span class="highlight-title">Agent</span> Reinforcement Learning for Zero-Shot Scalable
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Guo, Daming Shi, Junjie Yu, Wenhui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of multi-agent reinforcement learning (MARL) is significantly
transforming various fields like autonomous vehicle networks. However,
real-world multi-agent systems typically contain multiple roles, and the scale
of these systems dynamically fluctuates. Consequently, in order to achieve
zero-shot scalable collaboration, it is essential that strategies for different
roles can be updated flexibly according to the scales, which is still a
challenge for current MARL frameworks. To address this, we propose a novel MARL
framework named Scalable and Heterogeneous Proximal Policy Optimization
(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL
networks. We first leverage a latent network to learn strategy patterns for
each agent adaptively. Second, we introduce a heterogeneous layer to be
inserted into decision-making networks, whose parameters are specifically
generated by the learned latent variables. Our approach is scalable as all the
parameters are shared except for the heterogeneous layer, and gains both
inter-individual and temporal heterogeneity, allowing SHPPO to adapt
effectively to varying scales. SHPPO exhibits superior performance in classic
MARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google
Research Football (GRF), showcasing enhanced zero-shot scalability, and
offering insights into the learned latent variables' impact on team performance
by visualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequential transport maps using SoS density estimation and
  $α$-divergences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Zanger, Olivier Zahm, Tiangang Cui, Martin Schreiber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transport-based density estimation methods are receiving growing interest
because of their ability to efficiently generate samples from the approximated
density. We further invertigate the sequential transport maps framework
proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of
composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first
estimating an intermediate density of moderate complexity, and then by
computing the exact KR map from a reference density to the precomputed
approximate density. In our work, we explore the use of Sum-of-Squares (SoS)
densities and $\alpha$-divergences for approximating the intermediate
densities. Combining SoS densities with $\alpha$-divergence interestingly
yields convex optimization problems which can be efficiently solved using
semidefinite programming. The main advantage of $\alpha$-divergences is to
enable working with unnormalized densities, which provides benefits both
numerically and theoretically. In particular, we provide a new convergence
analyses of the sequential transport maps based on information geometric
properties of $\alpha$-divergences. The choice of intermediate densities is
also crucial for the efficiency of the method. While tempered (or annealed)
densities are the state-of-the-art, we introduce diffusion-based intermediate
densities which permits to approximate densities known from samples only. Such
intermediate densities are well-established in machine learning for generative
modeling. Finally we propose low-dimensional maps (or lazy maps) for dealing
with high-dimensional problems and numerically demonstrate our methods on
Bayesian inference problems and unsupervised learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NuwaTS: a Foundation Model Mending Every Incomplete Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinguo Cheng, Chunwei Yang, Wanlin Cai, Yuxuan Liang, Qingsong Wen, Yuankai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series imputation is critical for many real-world applications and has
been widely studied. However, existing models often require specialized designs
tailored to specific missing patterns, variables, or domains which limits their
generalizability. In addition, current evaluation frameworks primarily focus on
domain-specific tasks and often rely on time-wise train/validation/test data
splits, which fail to rigorously assess a model's ability to generalize across
unseen variables or domains. In this paper, we present \textbf{NuwaTS}, a novel
framework that repurposes Pre-trained Language Models (PLMs) for general time
series imputation. Once trained, NuwaTS can be applied to impute missing data
across any domain. We introduce specialized embeddings for each sub-series
patch, capturing information about the patch, its missing data patterns, and
its statistical characteristics. By combining contrastive learning with the
imputation task, we train PLMs to create a versatile, one-for-all imputation
model. Additionally, we employ a plug-and-play fine-tuning approach, enabling
efficient adaptation to domain-specific tasks with minimal adjustments. To
evaluate cross-variable and cross-domain generalization, we propose a new
benchmarking protocol that partitions the datasets along the variable
dimension. Experimental results on over seventeen million time series samples
from diverse domains demonstrate that NuwaTS outperforms state-of-the-art
domain-specific models across various datasets under the proposed benchmarking
protocol. Furthermore, we show that NuwaTS generalizes to other time series
tasks, such as forecasting. Our codes are available at
https://github.com/Chengyui/NuwaTS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Longhorn: State Space Models are Amortized Online Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14207v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14207v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models are built on sequence modeling via next-token
prediction. While the Transformer remains the dominant architecture for
sequence modeling, its quadratic decoding complexity in sequence length poses a
major limitation. State-space models (SSMs) present a competitive alternative,
offering linear decoding efficiency while maintaining parallelism during
training. However, most existing SSMs rely on linear recurrence designs that
appear somewhat ad hoc. In this work, we explore SSM design through the lens of
online learning, conceptualizing SSMs as meta-modules for specific online
learning problems. This approach links SSM design to formulating precise online
learning objectives, with state transition rules derived from solving these
objectives. Based on this insight, we introduce a novel deep SSM architecture,
Longhorn, whose update resembles the closed-form solution for solving the
online associative recall problem. Our experimental results show that Longhorn
outperforms state-of-the-art SSMs, including the Mamba model, on standard
sequence modeling benchmarks, language modeling, and vision tasks.
Specifically, Longhorn achieves a 1.8x improvement in sample efficiency
compared to Mamba, and can extrapolate over contexts that are up to 16x longer
during inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAMLET: Graph Transformer Neural Operator for Partial Differential
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Bryutkin, Jiahao Huang, Zhongying Deng, Guang Yang, Carola-Bibiane Schönlieb, Angelica Aviles-Rivero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel graph transformer framework, HAMLET, designed to address
the challenges in solving partial differential equations (PDEs) using neural
networks. The framework uses graph transformers with modular input encoders to
directly incorporate differential equation information into the solution
process. This modularity enhances parameter correspondence control, making
HAMLET adaptable to PDEs of arbitrary geometries and varied input formats.
Notably, HAMLET scales effectively with increasing data complexity and noise,
showcasing its robustness. HAMLET is not just tailored to a single type of
physical simulation, but can be applied across various domains. Moreover, it
boosts model resilience and performance, especially in scenarios with limited
data. We demonstrate, through extensive experiments, that our framework is
capable of outperforming current techniques for PDEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Graph Pooling <span class="highlight-title">Benchmark</span>: Effectiveness, Robustness and
  Generalizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09031v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09031v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyun Wang, Junyu Luo, Yanxin Shen, Ming Zhang, Siyu Heng, Xiao Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph pooling has gained attention for its ability to obtain effective node
and graph representations for various downstream tasks. Despite the recent
surge in graph pooling approaches, there is a lack of standardized experimental
settings and fair benchmarks to evaluate their performance. To address this
issue, we have constructed a comprehensive benchmark that includes 17 graph
pooling methods and 28 different graph datasets. This benchmark systematically
assesses the performance of graph pooling methods in three dimensions, i.e.,
effectiveness, robustness, and generalizability. We first evaluate the
performance of these graph pooling approaches across different tasks including
graph classification, graph regression and node classification. Then, we
investigate their performance under potential noise attacks and
out-of-distribution shifts in real-world scenarios. We also involve detailed
efficiency analysis, backbone analysis, parameter analysis and visualization to
provide more evidence. Extensive experiments validate the strong capability and
applicability of graph pooling approaches in various scenarios, which can
provide valuable insights and guidance for deep geometric learning research.
The source code of our benchmark is available at
https://github.com/goose315/Graph_Pooling_Benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Fairness and Mitigating MADness in Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Mayer, Lorenzo Luzi, Ali Siahkoohi, Don H. Johnson, Richard G. Baraniuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models unfairly penalize data belonging to minority classes,
suffer from model autophagy disorder (MADness), and learn biased estimates of
the underlying distribution parameters. Our theoretical and empirical results
show that training generative models with intentionally designed hypernetworks
leads to models that 1) are more fair when generating datapoints belonging to
minority classes 2) are more stable in a self-consumed (i.e., MAD) setting, and
3) learn parameters that are less statistically biased. To further mitigate
unfairness, MADness, and bias, we introduce a regularization term that
penalizes discrepancies between a generative model's estimated weights when
trained on real data versus its own synthetic data. To facilitate training
existing deep generative models within our framework, we offer a scalable
implementation of hypernetworks that automatically generates a hypernetwork
architecture for any given generative model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Graph Rewiring and Feature Denoising via Spectral Resonance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Linkerhägner, Cheng Shi, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In graph learning the graph and the node features both contain noisy
information about the node labels. In this paper we propose joint denoising and
rewiring (JDR)--an algorithm to jointly rewire the graph and denoise the
features, which improves the performance of downstream node classification
graph neural nets (GNNs). JDR improves the alignment between the leading
eigenspaces of graph and feature matrices. To approximately solve the
associated non-convex optimization problem we propose a heuristic that
efficiently handles real-world graph datasets with multiple classes and
different levels of homophily or heterophily. We theoretically justify JDR in a
stylized setting and verify the effectiveness of our approach through extensive
experiments on synthetic and real-world graph datasets. The results show that
JDR consistently outperforms existing rewiring methods on node classification
using GNNs as downstream models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closed-loop <span class="highlight-title">Diffusion</span> Control of Complex Physical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Wei, Haodong Feng, Yuchen Yang, Ruiqi Feng, Peiyan Hu, Xiang Zheng, Tao Zhang, Dixia Fan, Tailin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The control problems of complex physical systems have broad applications in
science and engineering. Previous studies have shown that generative control
methods based on diffusion models offer significant advantages for solving
these problems. However, existing generative control approaches face challenges
in both performance and efficiency when extended to the closed-loop setting,
which is essential for effective control. In this paper, we propose an
efficient Closed-Loop Diffusion method for Physical systems Control
(CL-DiffPhyCon). By employing an asynchronous denoising framework for different
physical time steps, CL-DiffPhyCon generates control signals conditioned on
real-time feedback from the environment with significantly reduced
computational cost during sampling. Additionally, the control process could be
further accelerated by incorporating fast sampling techniques, such as DDIM. We
evaluate CL-DiffPhyCon on two tasks: 1D Burgers' equation control and 2D
incompressible fluid control. The results demonstrate that CL-DiffPhyCon
achieves superior control performance with significant improvements in sampling
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientQAT: Efficient Quantization-Aware Training for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are crucial in modern natural language
processing and artificial intelligence. However, they face challenges in
managing their significant memory requirements. Although quantization-aware
training (QAT) offers a solution by reducing memory consumption through low-bit
representations with minimal accuracy loss, it is impractical due to
substantial training resources. To address this, we propose Efficient
Quantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.
EfficientQAT involves two consecutive phases: Block-wise training of all
parameters (Block-AP) and end-to-end training of quantization parameters
(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable
direct training of all parameters in a block-wise manner, reducing accuracy
loss in low-bit scenarios by enhancing the solution space during optimization.
E2E-QP then trains only the quantization parameters (step sizes) end-to-end,
further improving the performance of quantized models by considering
interactions among all sub-modules. Extensive experiments demonstrate that
EfficientQAT outperforms previous quantization methods across a range of
models, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with
scales from 7B to 70B parameters at various quantization bits. For instance,
EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41
hours, with less than 3 points accuracy degradation compared to the full
precision (69.48 vs. 72.41). Code is available at
https://github.com/OpenGVLab/EfficientQAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An efficient and effective quantization technical to improve the
  performance of low-bits LMMs and LVLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Synthesis of Green Architectural Tactics for ML-Enabled Systems <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heli Järvenpää, Patricia Lago, Justus Bogner, Grace Lewis, Henry Muccini, Ipek Ozkaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of artificial intelligence (AI) and machine learning (ML)
has generated growing interest in understanding their environmental impact and
the challenges associated with designing environmentally friendly ML-enabled
systems. While Green AI research, i.e., research that tries to minimize the
energy footprint of AI, is receiving increasing attention, very few concrete
guidelines are available on how ML-enabled systems can be designed to be more
environmentally sustainable. In this paper, we provide a catalog of 30 green
architectural tactics for ML-enabled systems to fill this gap. An architectural
tactic is a high-level design technique to improve software quality, in our
case environmental sustainability. We derived the tactics from the analysis of
51 peer-reviewed publications that primarily explore Green AI, and validated
them using a focus group approach with three experts. The 30 tactics we
identified are aimed to serve as an initial reference guide for further
exploration into Green AI from a software engineering perspective, and assist
in designing sustainable ML-enabled systems. To enhance transparency and
facilitate their widespread use and extension, we make the tactics available
online in easily consumable formats. Wide-spread adoption of these tactics has
the potential to substantially reduce the societal impact of ML-enabled systems
regarding their energy and carbon footprint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2024 International Conference on
  Software Engineering - Software Engineering in Society (ICSE-SEIS'2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing and Contrasting Deep Learning Weather Prediction Backbones on
  Navier-Stokes and Atmospheric Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Karlbauer, Danielle C. Maddix, Abdul Fatir Ansari, Boran Han, Gaurav Gupta, Yuyang Wang, Andrew Stuart, Michael W. Mahoney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remarkable progress in the development of Deep Learning Weather Prediction
(DLWP) models positions them to become competitive with traditional numerical
weather prediction (NWP) models. Indeed, a wide number of DLWP architectures --
based on various backbones, including U-Net, Transformer, Graph Neural Network
(GNN), and Fourier Neural Operator (FNO) -- have demonstrated their potential
at forecasting atmospheric states. However, due to differences in training
protocols, forecast horizons, and data choices, it remains unclear which (if
any) of these methods and architectures are most suitable for weather
forecasting and for future model development. Here, we step back and provide a
detailed empirical analysis, under controlled conditions, comparing and
contrasting the most prominent DLWP models, along with their backbones. We
accomplish this by predicting synthetic two-dimensional incompressible
Navier-Stokes and real-world global weather dynamics. In terms of accuracy,
memory consumption, and runtime, our results illustrate various tradeoffs. For
example, on synthetic data, we observe favorable performance of FNO; and on the
real-world WeatherBench dataset, our results demonstrate the suitability of
ConvLSTM and SwinTransformer for short-to-mid-ranged forecasts. For long-ranged
weather rollouts of up to 365 days, we observe superior stability and physical
soundness in architectures that formulate a spherical data representation,
i.e., GraphCast and Spherical FNO. In addition, we observe that all of these
model backbones "saturate," i.e., none of them exhibit so-called neural
scaling, which highlights an important direction for future work on these and
related models. The code is available at
https://github.com/amazon-science/dlwp-benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Graph Representation Learning via Edge Temporal States Modeling
  and Structure-reinforced Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengxiang Hu, Guobing Zou, Song Yang, Shiyi Lin, Yanglan Gan, Bofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graph representation learning has emerged as a crucial research area,
driven by the growing need for analyzing time-evolving graph data in real-world
applications. While recent approaches leveraging recurrent neural networks
(RNNs) and graph neural networks (GNNs) have shown promise, they often fail to
adequately capture the impact of temporal edge states on inter-node
relationships, consequently overlooking the dynamic changes in node features
induced by these evolving relationships. Furthermore, these methods suffer from
GNNs' inherent over-smoothing problem, which hinders the extraction of global
structural features. To address these challenges, we introduce the Recurrent
Structure-reinforced Graph Transformer (RSGT), a novel framework for dynamic
graph representation learning. It first designs a heuristic method to
explicitly model edge temporal states by employing different edge types and
weights based on the differences between consecutive snapshots, thereby
integrating varying edge temporal states into the graph's topological
structure. We then propose a structure-reinforced graph transformer that
captures temporal node representations encoding both graph topology and
evolving dynamics through a recurrent learning paradigm, enabling the
extraction of both local and global structural features. Comprehensive
experiments on four real-world datasets demonstrate RSGT's superior performance
in discrete dynamic graph representation learning, consistently outperforming
existing methods in dynamic link prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the Elsevier for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RADAR: Robust Two-stage Modality-incomplete Industrial Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Miao, Wenqiao Zhang, Juncheng Li, Siliang Tang, Zhaocheng Li, Haochen Shi, Jun Xiao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Industrial Anomaly Detection (MIAD), utilizing 3D point clouds and
2D RGB images to identify the abnormal region of products, plays a crucial role
in industrial quality inspection. However, the conventional MIAD setting
presupposes that all 2D and 3D modalities are paired, overlooking the fact that
multimodal data collected from the real world is often imperfect due to missing
modalities. Consequently, MIAD models that demonstrate robustness against
modal-incomplete data are highly desirable in practice. To address this
practical challenge, we introduce a first-of-its-kind study that
comprehensively investigates Modality-Incomplete Industrial Anomaly Detection
(MIIAD), to consider the imperfect learning environment in which the multimodal
information may be incomplete. Not surprisingly, we discovered that most
existing MIAD approaches are inadequate for addressing MIIAD challenges,
leading to significant performance degradation on the MIIAD benchmark we
developed. In this paper, we propose a novel two-stage Robust
modAlity-imcomplete fusing and Detecting frAmewoRk, abbreviated as RADAR. Our
bootstrapping philosophy is to enhance two stages in MIIAD, improving the
robustness of the Multimodal Transformer: i) In feature fusion, we first
explore learning modality-incomplete instruction, guiding the pre-trained
Multimodal Transformer to robustly adapt to various modality-incomplete
scenarios, and implement adaptive parameter learning based on a HyperNetwork;
ii) In anomaly detection, we construct a real-pseudo hybrid module to highlight
the distinctiveness of modality combinations, further enhancing the robustness
of the MIIAD model. Our experimental results demonstrate that the proposed
RADAR significantly surpasses conventional MIAD methods in terms of
effectiveness and robustness on our newly created MIIAD dataset, underscoring
its practical application value.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Parameter Potential of Neural Representation for Efficient
  Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gai Zhang, Xinfeng Zhang, Lv Tang, Yue Li, Kai Zhang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, video compression technology has been a prominent research area.
Traditional hybrid video compression framework and end-to-end frameworks
continue to explore various intra- and inter-frame reference and prediction
strategies based on discrete transforms and deep learning techniques. However,
the emerging implicit neural representation (INR) technique models entire
videos as basic units, automatically capturing intra-frame and inter-frame
correlations and obtaining promising performance. INR uses a compact neural
network to store video information in network parameters, effectively
eliminating spatial and temporal redundancy in the original video. However, in
this paper, our exploration and verification reveal that current INR video
compression methods do not fully exploit their potential to preserve
information. We investigate the potential of enhancing network parameter
storage through parameter reuse. By deepening the network, we designed a
feasible INR parameter reuse scheme to further improve compression performance.
Extensive experimental results show that our method significantly enhances the
rate-distortion performance of INR video compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing the Latent <span class="highlight-title">Diffusion</span> Model for Training-Free <span class="highlight-title">Image</span> Style
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Masui, Mayu Otani, Masahiro Nomura, Hideki Nakayama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently shown the ability to generate high-quality
images. However, controlling its generation process still poses challenges. The
image style transfer task is one of those challenges that transfers the visual
attributes of a style image to another content image. Typical obstacle of this
task is the requirement of additional training of a pre-trained model. We
propose a training-free style transfer algorithm, Style Tracking Reverse
Diffusion Process (STRDP) for a pretrained Latent Diffusion Model (LDM). Our
algorithm employs Adaptive Instance Normalization (AdaIN) function in a
distinct manner during the reverse diffusion process of an LDM while tracking
the encoding history of the style image. This algorithm enables style transfer
in the latent space of LDM for reduced computational cost, and provides
compatibility for various LDM models. Through a series of experiments and a
user study, we show that our method can quickly transfer the style of an image
without additional training. The speed, compatibility, and training-free aspect
of our algorithm facilitates agile experiments with combinations of styles and
LDMs for extensive application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social Media Authentication and Combating Deepfakes using Semi-fragile
  Invisible <span class="highlight-title">Image</span> Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Varma Nadimpalli, Ajita Rattani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the significant advances in deep generative models for image and video
synthesis, Deepfakes and manipulated media have raised severe societal
concerns. Conventional machine learning classifiers for deepfake detection
often fail to cope with evolving deepfake generation technology and are
susceptible to adversarial attacks. Alternatively, invisible image watermarking
is being researched as a proactive defense technique that allows media
authentication by verifying an invisible secret message embedded in the image
pixels. A handful of invisible image watermarking techniques introduced for
media authentication have proven vulnerable to basic image processing
operations and watermark removal attacks. In response, we have proposed a
semi-fragile image watermarking technique that embeds an invisible secret
message into real images for media authentication. Our proposed watermarking
framework is designed to be fragile to facial manipulations or tampering while
being robust to benign image-processing operations and watermark removal
attacks. This is facilitated through a unique architecture of our proposed
technique consisting of critic and adversarial networks that enforce high image
quality and resiliency to watermark removal efforts, respectively, along with
the backbone encoder-decoder and the discriminator networks. Thorough
experimental investigations on SOTA facial Deepfake datasets demonstrate that
our proposed model can embed a $64$-bit secret as an imperceptible image
watermark that can be recovered with a high-bit recovery accuracy when benign
image processing operations are applied while being non-recoverable when unseen
Deepfake manipulations are applied. In addition, our proposed watermarking
technique demonstrates high resilience to several white-box and black-box
watermark removal attacks. Thus, obtaining state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Transactions (Digital Threats: Research and Practice)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffSSD: A <span class="highlight-title">Diffusion</span>-Based <span class="highlight-title">Dataset</span> For Speech Forensics <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kratika Bhagtani, Amit Kumar Singh Yadav, Paolo Bestagini, Edward J. Delp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based speech generators are ubiquitous. These methods can generate
very high quality synthetic speech and several recent incidents report their
malicious use. To counter such misuse, synthetic speech detectors have been
developed. Many of these detectors are trained on datasets which do not include
diffusion-based synthesizers. In this paper, we demonstrate that existing
detectors trained on one such dataset, ASVspoof2019, do not perform well in
detecting synthetic speech from recent diffusion-based synthesizers. We propose
the Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of
about 200 hours of labeled speech, including synthetic speech generated by 8
diffusion-based open-source and 2 commercial generators. We also examine the
performance of existing synthetic speech detectors on DiffSSD in both
closed-set and open-set scenarios. The results highlight the importance of this
dataset in detecting synthetic speech generated from recent open-source and
commercial speech generators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large
  Language Models <span class="chip">EMNLP24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Xianghu Yue, Xiaoxue Gao, Chen Zhang, Luis Fernando D'Haro, Robby T. Tan, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various audio-LLMs (ALLMs) have been explored recently for tackling different
audio tasks simultaneously using a single, unified model. While existing
evaluations of ALLMs primarily focus on single-audio tasks, real-world
applications often involve processing multiple audio streams simultaneously. To
bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark
that consists of 20 datasets from 11 multi-audio tasks encompassing both speech
and sound scenarios. Comprehensive experiments on MAE demonstrate that the
existing ALLMs, while being powerful in comprehending primary audio elements in
individual audio inputs, struggling to handle multi-audio scenarios. To this
end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among
multiple similar audios using discriminative learning on our proposed synthetic
data. The results demonstrate that the proposed MALLM outperforms all baselines
and achieves high data efficiency using synthetic data without requiring human
annotations. The proposed MALLM opens the door for ALLMs towards multi-audio
processing era and brings us closer to replicating human auditory capabilities
in machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP24 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Frame Patching for FoV-based Point Cloud Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongyu Zong, Yixiang Mao, Chen Li, Yong Liu, Yao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many XR applications require the delivery of volumetric video to users with
six degrees of freedom (6-DoF) movements. Point Cloud has become a popular
volumetric video format. A dense point cloud consumes much higher bandwidth
than a 2D/360 degree video frame. User Field of View (FoV) is more dynamic with
6-DoF movement than 3-DoF movement. To save bandwidth, FoV-adaptive streaming
predicts a user's FoV and only downloads point cloud data falling in the
predicted FoV. However, it is vulnerable to FoV prediction errors, which can be
significant when a long buffer is utilized for smoothed streaming. In this
work, we propose a multi-round progressive refinement framework for point cloud
video streaming. Instead of sequentially downloading point cloud frames, our
solution simultaneously downloads/patches multiple frames falling into a
sliding time-window, leveraging the inherent scalability of octree-based
point-cloud coding. The optimal rate allocation among all tiles of active
frames are solved analytically using the heterogeneous tile rate-quality
functions calibrated by the predicted user FoV. Multi-frame
downloading/patching simultaneously takes advantage of the streaming smoothness
resulting from long buffer and the FoV prediction accuracy at short buffer
length. We evaluate our streaming solution using simulations driven by real
point cloud videos, real bandwidth traces, and 6-DoF FoV traces of real users.
Our solution is robust against the bandwidth/FoV prediction errors, and can
deliver high and smooth view quality in the face of bandwidth variations and
dynamic user and point cloud movements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Transactions on Multimedia (under review)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-10-10T05:31:14.366586834Z">
            2024-10-10 05:31:14 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
